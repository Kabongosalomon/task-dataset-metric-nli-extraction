<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Bottlenecks for Multimodal Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
							<email>anagrani@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
							<email>shanyang@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
							<email>aarnab@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
							<email>arenjansen@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
							<email>cordelias@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
							<email>chensun@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Attention Bottlenecks for Multimodal Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Humans perceive the world by concurrently processing and fusing highdimensional inputs from multiple modalities such as vision and audio. Machine perception models, in stark contrast, are typically modality-specific and optimised for unimodal benchmarks, and hence late-stage fusion of final representations or predictions from each modality ('late-fusion') is still a dominant paradigm for multimodal video classification. Instead, we introduce a novel transformer based architecture that uses 'fusion bottlenecks' for modality fusion at multiple layers. Compared to traditional pairwise self-attention, our model forces information between different modalities to pass through a small number of bottleneck latents, requiring the model to collate and condense relevant information in each modality and share what is necessary. We find that such a strategy improves fusion performance, at the same time reducing computational cost. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple audio-visual classification benchmarks including Audioset, Epic-Kitchens and VGGSound. All code and models will be released.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Simultaneous multimodal sensations are a crucial enabler of human perceptual learning <ref type="bibr" target="#b56">[57]</ref>. For artificial learning systems, however, designing a unified model for modality fusion is challenging due to a number of factors: (i) variations in learning dynamics between modalities <ref type="bibr" target="#b62">[63]</ref>, (ii) different noise topologies, with some modality streams containing more information for the task at hand than others, as well as (iii) specialised input representations. The difference in input representations between audio and vision is particularly stark -many state of the art audio classification methods rely on short term Fourier analysis to produce log-mel spectrograms, often using them as inputs to CNN architectures designed for images <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b54">55]</ref>. These time-frequency representations have different distributions to images -multiple acoustic objects can have energy at the same frequency, and the translation invariances of CNNs may no longer be a desired property (while an acoustic object can be shifted in time, a shift in frequency could alter the meaning entirely). In contrast, the visual stream in a video is three-dimensional (two spatial and one temporal), and while different spatial regions of the image correspond to different objects, there is the unique challenge of high redundancy across multiple frames. Hence input representations, and consequently neural network architectures and benchmarks tend to vary wildly for different modalities. For simplicity, the dominant paradigm for multimodal fusion therefore often consists of an ad-hoc scheme that involves integrating separate audio and visual networks via their output representations or scores i.e. 'late-fusion' <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>In this work, we present a new transformer based model for audiovisual fusion in video. Despite originally being proposed for NLP tasks, there has been recent interest in transformers <ref type="bibr" target="#b60">[61]</ref> as universal perceptual models <ref type="bibr" target="#b31">[32]</ref>, due to their ability to model dense correlations between tokens, at the same time making few assumptions about their inputs (and because continuous perceptual inputs can be tokenised). By dividing dense continuous signals into patches and rasterising them 35th Conference on Neural Information Processing Systems (NeurIPS 2021). <ref type="figure">Figure 1</ref>: Cross-modal Fusion. Unlike late fusion (left), where no cross-modal information is exchanged in the model until after the classifier, we investigate two pathways for the exchange of cross-modal information. The first is via standard pairwise self attention across all hidden units in a layer, but applied only to later layers in the model -mid fusion <ref type="bibr">(middle, left)</ref>. We also propose the use of 'fusion bottlenecks' (middle, right) that restrict attention flow within a layer through tight latent units. Both forms of restriction can be applied in conjunction (Bottleneck Mid Fusion) for optimal performance (right). We show B = 2 bottleneck units and 3 hidden units per modality. Grey boxes indicate tokens that receive attention flow from both audio and video tokens. to 1D tokens, transformers have been shown to perform competitively for image (ViT <ref type="bibr" target="#b17">[18]</ref>) and video classification (ViViT <ref type="bibr" target="#b5">[6]</ref>), and more recently, audio classification (AST <ref type="bibr" target="#b25">[26]</ref>). Because these models are able to elegantly handle variable length sequences, a natural first extension would be to feed in a sequence of both visual and auditory patches to a transformer, with minimal changes to the architecture. This 'early fusion' model allows free attention flow between different spatial and temporal regions in the image, as well as across frequency and time in the audio spectrogram. While theoretically appealing, we hypothesise that full pairwise attention at all layers of the model is not necessary because audio and visual inputs contain dense, fine-grained information, much of which is redundant. This is particularly the case for video, as shown by the performance of 'factorised' versions of <ref type="bibr" target="#b5">[6]</ref>. Such a model would also not scale well to longer videos due to the quadratic complexity of pairwise attention with token sequence length. To mitigate this, we propose two methods to restrict the flow of attention in our model. The first follows from a common paradigm in multimodal learning, which is to restrict cross-modal flow to later layers of the network, allowing early layers to specialise in learning and extracting unimodal patterns. Henceforth this is is referred to as 'mid fusion' <ref type="figure">(Fig. 1</ref>, middle left), where the layer at which cross-modal interactions are introduced is called the 'fusion layer'. The two extreme versions of this are 'early fusion' (all layers are cross-modal) and 'late fusion' (all are unimodal) which we compare to as a baselines. Our second idea (and main contribution), is to restrict cross-modal attention flow between tokens within a layer. We do this by allowing free attention flow within a modality, but force our model to collate and 'condense' information from each modality before sharing it with the other. The core idea is to introduce a small set of latent fusion units that form an 'attention bottleneck', through which cross-modal interactions within a layer must pass. We demonstrate that this 'bottlenecked' version, which we name Multimodal Bottleneck Transformer (MBT), outperforms or matches its unrestricted counterpart, but with lower computational cost.</p><p>Concretely, we make the following contributions: (i) We propose a new architecture (MBT) for audiovisual fusion. Our model restricts the flow of cross-modal information between latent units through tight fusion 'bottlenecks', that force the model to collect and 'condense' the most relevant inputs in each modality (and therefore share only that which is necessary with the other modality). This avoids the quadratic scaling cost of full pairwise attention, and leads to performance gains with less compute; (ii) We apply MBT to image and spectogram patches <ref type="figure">(Fig. 2)</ref>, and explore a number of ablations related to the fusion layer, the sampling of inputs and data size; and finally (iii) We set the new state-of-the-art for video classification across a number of popular audio-visual benchmarks, including AudioSet <ref type="bibr" target="#b23">[24]</ref>, Epic-Kitchens100 <ref type="bibr" target="#b13">[14]</ref> and VGGSound <ref type="bibr" target="#b11">[12]</ref>. On the Audioset dataset, we outperform the current state of the art by 5.9 mAP (12.7% relative improvement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Audiovisual learning: Audiovisual multimodal learning has a rich history, both before and during the deep learning era <ref type="bibr" target="#b52">[53]</ref>. Given the limited available data and computational resources, early work focused on relatively simple early-stage (e.g. stacking hand-designed features) and late-stage  <ref type="figure">Figure 2</ref>: A Multimodal Fusion Transformer applied to audiovisual inputs. The input sequence consists of image and spectrogram patches. These are then projected into tokens and appended to special CLS (classification) and FSN (fusion bottleneck) tokens. Our transformer encoder then uses self attention to model unimodal information, and restricts cross-modal information flow via cross attention with the bottleneck tokens at multiple layers of the network.</p><p>(e.g. score fusion) techniques <ref type="bibr" target="#b12">[13]</ref>. Deep learning has allowed more sophisticated strategies in which modality-specific or joint latents are implicitly learned to mediate the fusion. The result has enabled major advances in a range of downstream supervised audiovisual tasks <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b18">19]</ref>. In the supervised setting, multiple modality-specific convolution networks can be jointly trained, whose intermediate activations are then combined by summation <ref type="bibr" target="#b35">[36]</ref> or via 'lateral connections' <ref type="bibr" target="#b63">[64]</ref>. In the unsupervised setting, audiovisual learning is commonly used to learn good unimodal representations, with a popular pretraining task being to synchronise signals from different modalities via a contrastive loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, however each modality is usually encoded separately under this setup.</p><p>Multimodal transformers: The self attention operation of transformers provides a natural mechanism to connect multimodal signals. Multimodal transformers have been applied to various tasks including audio enhancement <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b59">60]</ref>, speech recognition <ref type="bibr" target="#b26">[27]</ref>, image segmentation <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b59">60]</ref>, crossmodal sequence generation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b55">56]</ref>, image and video retrieval <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8]</ref>, visual navigation <ref type="bibr" target="#b50">[51]</ref> and image/video captioning/classification <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b30">31]</ref>. For many works, the inputs to transformers are the output representations of single modality CNNs <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b22">23]</ref> -unlike these works we use transformer blocks throughout, using only a single convolutional layer to rasterise 2D patches. The tokens from different modalities are usually combined directly as inputs to the transformers <ref type="bibr" target="#b41">[42]</ref>, for example, the recently released Perceiver model <ref type="bibr" target="#b31">[32]</ref> introduces an iterative attention mechanism which takes concatenated raw multimodal signals as inputs, which corresponds to our 'early fusion' baseline. In contrast, we carefully examine the impact of different modality fusion strategies, including limiting cross-modal attention flow to later layers of our model, and 'channeling' cross-modal connections through bottlenecks in our proposed Multimodal Bottleneck Transformer (MBT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multimodal fusion transformers</head><p>In this section we describe our proposed Multimodal Bottleneck Transformer (MBT). We begin by summarising the recently proposed Vision Transformer (ViT) <ref type="bibr" target="#b17">[18]</ref> and Audio Spectrogram Transformer (AST) <ref type="bibr" target="#b25">[26]</ref>, developed for image and audio classification respectively, in Sec. 3.1. We then describe our extension to the audio-visual fusion case. We discuss three different token fusion strategies (Sec. 3.2), and finally discuss the fusion pathway in the entire model (Sec. 3.3), which involves restricting multimodal fusion to certain layers of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The ViT and AST architectures</head><p>Vision Transformer (ViT) <ref type="bibr" target="#b17">[18]</ref> (and a recent extension to audio -Audio Spectrogram Transformer (AST) <ref type="bibr" target="#b25">[26]</ref>) adapts the Transformer architecture <ref type="bibr" target="#b60">[61]</ref>, originally designed for natural language processing, to process 2D inputs with minimal changes. The key insight is to extract N nonoverlapping patches from the RGB image (or the audio spectrogram), x i ? R h?w , and convert them into a series of 1D tokens z i ? R d , as follows:</p><formula xml:id="formula_0">z = g(x; E, z cls ) = [z cls , Ex 1 , Ex 2 , ..., Ex N ] + p.<label>(1)</label></formula><p>Here, E is a linear projection mapping each token to R d , z cls is a special token prepended to this sequence so that its representation at the final layer can be passed to a classifier for classification tasks <ref type="bibr" target="#b16">[17]</ref>, and p ? R (N +1)?d is a learned positional embedding added to the tokens to retain positional information (as all subsequent self-attention operations are permutation invariant).</p><p>The tokens are then passed through an encoder consisting of a sequence of L transformer layers. Each transformer layer consists of Multi-Headed Self-Attention (MSA), Layer Normalisation (LN) and Multilayer Perceptron (MLP) blocks applied using residual connections. We denote a transformer layer, z l+1 = Transformer(z l ) as</p><formula xml:id="formula_1">y l = MSA(LN(z l )) + z l<label>(2)</label></formula><p>z l+1 = MLP(LN(y l )) + y l .</p><p>(3) Here, the MSA operation <ref type="bibr" target="#b60">[61]</ref> computes dot-product attention <ref type="bibr" target="#b60">[61]</ref> where the queries, keys and values are all linear projections of the same tensor, MSA(X) = Attention(W Q X, W K X, W V X). We further define Multi-Headed Cross Attention (MCA) between two tensors, X and Y, where X forms the query and Y forms the keys and values which are used to reweight the query as</p><formula xml:id="formula_2">MCA(X, Y) = Attention(W Q X, W K Y, W V Y)</formula><p>. This will be used in our multimodal case, as described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal transformer</head><p>We now describe our extension to the multimodal case. We begin by discussing three different token fusion strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Fusion via vanilla self-attention</head><p>We begin by describing a 'vanilla' fusion model, which simply consists of the regular transformer applied to multimodal inputs. Our method of tokenising video is straightforward -given a video clip of length t seconds, we uniformly sample F RGB frames and convert the audio waveform into a single spectrogram. We then embed each frame and the spectrogram independently following the encoding proposed in ViT <ref type="bibr" target="#b17">[18]</ref>, and concatenate all tokens together into a single sequence.</p><p>Formally, if we have extracted a total of N v RGB patches from all F sampled frames, x rgb ? R Nv?d , and N a spectrogram patches, x spec ? R Na?d , our sequence of tokens is z = [z rgb ||z spec ] where z rgb = g(x rgb ; E rgb , z cls-rgb ) and z spec = g(x spec ; E spec , z cls-spec ). <ref type="formula">(4)</ref> Here, [z rgb ||z spec ] denotes the concatenation of the tokens for each modality. We use different projections E rgb and E spec for RGB and spectrogram patches respectively, and prepend a separate classification token for each modality.</p><p>Our multimodal encoder then applies a series of transformer layers in the same manner as above.</p><p>Attention is allowed to flow freely through the network, i.e. each RGB token can attend to all other RGB and spectrogram tokens as follows: z l+1 = Transformer(z l ; ?) with model parameters ?.</p><p>Here Transformer refers to a standard transformer layer with vanilla self-attention blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Fusion with modality-specific parameters</head><p>We can generalise this model by allowing each modality to have its own dedicated parameters ? rgb and ? spec , but still exchange information via the attention mechanism. For this purpose, we define a Cross-Transformer layer:</p><formula xml:id="formula_3">z l+1 rgb = Cross-Transformer(z l rgb , z l ; ? rgb )<label>(5)</label></formula><p>z l+1 spec = Cross-Transformer(z l spec , z l ; ? spec ), where the Cross-Transformer employs the generalised cross-attention operation that takes two sets of inputs z 1 and z 2 that are not necessarily overlapping. This layer follows the original transformer layer with the difference being that Eq. 2 becomes</p><formula xml:id="formula_4">y l = MCA(LN(z l 1 ), LN(z l 2 )) + z l 1 .<label>(6)</label></formula><p>Finally, note that we have explicitly defined the parameters, ? rgb and ? spec of the cross-transformer layers in Eq. 5 as they are different for each modality. However, when ? rgb and ? spec are equal, (? rgb = ? spec = ?), the computation defined in Eq. 5 is equivalent to Sec. 3.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Fusion via attention bottlenecks</head><p>In order to tame the quadratic complexity of pairwise attention, we next introduce a small set of B fusion bottleneck tokens z fsn = [z 1 fsn , z 2 fsn , . . . , z B fsn ] to our input sequence (see <ref type="figure">Fig. 2</ref>). The input sequence is now z = [z rgb ||z fsn ||z spec ]. <ref type="bibr" target="#b6">(7)</ref> We then restrict all cross-modal attention flow in our model to be via these bottleneck tokens. More formally for layer l, we compute token representations as follows:</p><formula xml:id="formula_5">[z l+1 i ||? l+1 fsni ] = Transformer([z l i ||z l fsn ]; ? i ) (8) z l+1 fsn = Avg i (? l+1 fsni )<label>(9)</label></formula><p>Here i indexes each modality, in this case RGB and Spec, and z rgb and z spec can only exchange information via the bottleneck z fsn within a transformer layer. We first create modality specific temporary bottleneck fusion tokens? fsni , which are updated separately and simultaneously with audio and visual information (Equation <ref type="formula">8</ref>). The final fusion tokens from each cross-modal update are then averaged in <ref type="bibr">Equation 9</ref>. We also experiment with asymmetric updates for the bottleneck tokens (see appendix) and found performance was robust to this choice. We keep the number of bottleneck tokens in the network to be much smaller than the total number of latent units per modality (B N v and B N a ). Because all cross-modal attention flow must pass through these units, these tight 'fusion' bottlenecks force the model to condense information from each modality and share that which is necessary. As we show in the experiments, this increases or maintains performance for multimodal fusion, at the same time reducing computational complexity. We also note that our formulation is generic to the type and the number of modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Where to fuse: early, mid and late</head><p>The above strategies discuss fusion within a layer, and in most transformer architectures (such as ViT), every layer consists of an identical set of operations. A common paradigm in multimodal learning, however, is to restrict early layers of a network to focus on unimodal processing, and only introduce cross-modal connections at later layers. This is conceptually intuitive if we believe lower layers are involved in processing low level features, while higher layers are focused on learning semantic concepts -low-level visual features such as edges and corners in images might not have a particular sound signature, and therefore might not benefit from early fusion with audio <ref type="bibr" target="#b63">[64]</ref>.</p><p>This can be implemented with our model as follows: We initially perform vanilla self-attention among tokens from a single modality for L f layers. Thereafter, we concatenate all latent tokens together,</p><formula xml:id="formula_6">z L f = [z L f rgb ||z L f spec ]</formula><p>and pass them through the remaining L ? L f layers where the tokens are fused according to Sec. 3.2. Here, L f = 0 corresponds to an 'early-fusion' model, L f = L a 'late-fusion' model, and 0 &lt; L f &lt; L a 'mid-fusion' one. More formally, this can be denoted as</p><formula xml:id="formula_7">z l+1 rgb = Transformer(z l rgb ; ? rgb ), z l+1 spec = Transformer(z l spec ; ? spec ) if l &lt; L f z l = [z l rgb ||z l spec ], z l+1 = Multimodal-Transformer(z l ; ? spec , ? rgb ) otherwise</formula><p>where Multimodal-Transformer(?) can refer to either of the 3 fusion strategies described in Sec 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification</head><p>For all model variants described above, we pass output representations of the CLS tokens z L cls-rgb and z L cls-spec to the same linear classifier and average the pre-softmax logits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We apply MBT to the task of video classification. In this section we first describe the datasets used to train and test multimodal fusion and their respective evaluation protocols (Sec. 4.1), then discuss implementation details (Sec. 4.2). We then ablate the key design choices in our model (Sec. 4.3), before finally comparing our model to the state of the art (Sec. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and evaluation protocol</head><p>We experiment with three video classification datasets -AudioSet <ref type="bibr" target="#b23">[24]</ref>, Epic-Kitchens-100 <ref type="bibr" target="#b13">[14]</ref> and VGGSound <ref type="bibr" target="#b11">[12]</ref>, described in more detail below. Results on two additional datasets Moments in Time <ref type="bibr" target="#b46">[47]</ref> and Kinetics <ref type="bibr" target="#b34">[35]</ref> are provided in the appendix. AudioSet <ref type="bibr" target="#b23">[24]</ref> consists of almost 2 million 10-second video clips from YouTube, annotated with 527 classes. Like other YouTube datasets, this is a dynamic dataset (we only use the clips still available online). This gives us 20,361 clips for the balanced train set (henceforth referred to as mini-AudioSet or miniAS) and 18,589 clips for the test set. This test set is exactly the same as recent works we compare to, including Perceiver <ref type="bibr" target="#b31">[32]</ref>. Instead of using the 2M unbalanced training set, we train on a (slightly more) balanced subset consisting of 500K samples (AS-500K). Details are provided in the appendix. Because each sample has multiple labels, we train with a binary cross-entropy (BCE) loss and report mean average precision (mAP) over all classes, following standard practice.</p><p>Epic-Kitchens 100 <ref type="bibr" target="#b13">[14]</ref> consists of egocentric videos capturing daily kitchen activities. The dataset consists of 90,000 variable length clips spanning 100 hours. We report results for action recognition following standard protocol <ref type="bibr" target="#b13">[14]</ref> -each action label is a combination of a verb and noun, and we predict both using a single network with two 'heads', both trained with a cross-entropy loss. The top scoring verb and action pair predicted by the network are used, and Top-1 action accuracy is the primary metric. Actions are mainly short-term (average length is 2.6s with minimum length 0.25s).</p><p>VGGSound <ref type="bibr" target="#b11">[12]</ref> contains almost 200K video clips of length 10s, annotated with 309 sound classes consisting of human actions, sound-emitting objects and human-object interactions. Unlike AudioSet, the sound source for each clip is 'visually present' in the video. This was ensured during dataset creation through the use of image classifiers. After filtering clips that are no longer available on YouTube, we end up with 172,427 training and 14,448 test clips. We train with a standard crossentropy loss for classification and report Top-1 and Top-5 classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Our backbone architecture follows that of ViT <ref type="bibr" target="#b17">[18]</ref> identically, specifically we use ViT-Base (ViT-B, L = 12, N H = 12, d = 3072) 1 initialised from ImageNet-21K <ref type="bibr" target="#b15">[16]</ref>, however we note that our method is agnostic to transformer backbone. Unless otherwise specialised, we use B = 4 bottleneck tokens for all experiments with bottleneck fusion. Bottleneck tokens are initialized using a Gaussian with mean of 0 and standard deviation of 0.02, similar to the positional embeddings in the public ViT <ref type="bibr" target="#b17">[18]</ref> code. We randomly sample clips of t seconds for training. RGB frames for all datasets are extracted at 25 fps. For AudioSet and VGGSound we sample 8 RGB frames over the sampling window of length t with a uniform stride of length (t ? 25)/8. We extract 16 ? 16 patches from each frame of size 224 ? 224, giving us a total of 8 ? 14 ? 14 = 1568 patches per video. For Epic-Kitchens (because the segments are shorter), we sample 32 frames with stride 1. Audio for all datasets is sampled at 16kHz and converted to mono channel. Similar to <ref type="bibr" target="#b25">[26]</ref>, we extract log mel spectrograms with a frequency dimension of 128 computed using a 25ms Hamming window with hop length 10ms. This gives us an input of size 128 ? 100t for t seconds of audio. Spectrogram patches are extracted with size 16 ? 16, giving us 50 ? 8 = 400 patches for 8 seconds of audio. For images we apply the standard data augmentations used in <ref type="bibr" target="#b5">[6]</ref> (random crop, flip, colour jitter), and for spectrograms we use SpecAugment <ref type="bibr" target="#b49">[50]</ref> with a max time mask length of 192 frames and max frequency mask length of 48 bins following AST <ref type="bibr" target="#b25">[26]</ref>. We set the base learning rate to 0.5 and train for 50 epochs, using Mixup <ref type="bibr" target="#b66">[67]</ref> with ? = 0.3 and stochastic depth regularisation <ref type="bibr" target="#b29">[30]</ref> with probability p = 0.3. All models (across datasets) are trained with a batch size of 64, synchronous SGD with momentum of 0.9, and a cosine learning rate schedule with warmup of 2.5 epochs on TPU accelerators using the Scenic library <ref type="bibr" target="#b14">[15]</ref>. Inference: Following standard practice, we uniformly sample multiple temporal crops from the clip and average per-view logits to obtain the final result. The number of test crops is set to 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation analysis</head><p>In this section we investigate the impact of the different architectural choices in MBT. Unless otherwise specified, we use the mini-AudioSet split for training and report results on the AudioSet eval split. More ablations on backbone size and pretraining initalisation can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Fusion strategies</head><p>We implement all the three fusion strategies described in Sec. 3.2: (i) Vanilla self-attention -Unrestricted pairwise attention between all latent units within a layer;</p><p>(ii) Vanilla cross-attention with separate weights: Same as above, but we now have separate weights for each modality. The latent units are updated via pairwise attention with all other latent units from both modalities; and finally (iii) Bottleneck fusion: Here all cross-modal attention must pass through bottleneck fusion latents. Note that these three fusion strategies only describe attention flow between tokens within a layer. For strategies (ii) and (iii), we also conduct experiments showing the impact of restricting cross-modal attention to layers after a fixed fusion layer L f . We investigate models with different fusion layers, L f = 0, 2, 4, 6, 8, 10, 12, and present the results in <ref type="figure" target="#fig_0">Fig. 3</ref>. <ref type="bibr" target="#b1">2</ref> Sharing weights for both modalities: We first investigate the impact of sharing the encoder weights for both modalities (strategy (i) vs (ii)). The results can be found in <ref type="figure" target="#fig_6">Fig. 7</ref> in the appendix. When modalities are fused at earlier layers, using separate encoders improves performance. For models with later fusion layers, performance is similar for both models. We hence use separate modality weights for further experiments. Fusion layer: We then investigate the impact of varying the fusion layer L f , for the latter two strategies: (ii) Vanilla Cross-Attention and (iii) Bottleneck Fusion. We conduct experiments with L f = 0, 2, 4, 6, 8, 10, 12. We fix the input span t to 4s and the number of bottleneck tokens B to 4. We conduct 3 runs for each experiment and report mean and std deviation. As can be seen from <ref type="figure" target="#fig_0">Fig. 3 (left)</ref>, 'mid fusion' outperforms both early (L f = 0) and late fusion (L f = 12), with optimal performance obtained by using fusion layer L f = 10 for vanilla cross-attention and L f = 8 for bottleneck attention. This suggests that the model benefits from restricting cross-modal connections to later layers, allowing earlier layers to specialise to learning unimodal features, however still benefits from multiple layers of cross-modal information flow. In appendix D , we confirm that mid fusion outperforms late fusion across a number of different datasets. Attention bottlenecks: In <ref type="figure" target="#fig_0">Fig. 3</ref>, we also examine the effect of bottleneck attention vs vanilla cross-attention for multimodal fusion. We find that for all values of L f restricting flow to bottlenecks improves or maintains performance, with improvements more prominent at lower values of L f . At L f = 10, both perform similarly, note that at this stage we only have 3 fusion layers in the model. Our best performing model uses attention bottlenecks with L f = 8, and we fix this for all further experiments. We also compare the amount of computation, measured in GFLOPs, for both fusion strategies ( <ref type="figure" target="#fig_0">Fig. 3, right)</ref>. Using a small number of bottleneck tokens (in our experiments B = 4) adds negligible extra computation over a late fusion model, with computation remaining largely constant with varying fusion layer L f . This is in contrast to vanilla cross-fusion, which has a non-negligible computational cost for every layer it is applied to. We note that for early fusion (L f = 0), bottleneck fusion outperforms vanilla cross-attention by over 2 mAP, with less than half the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of bottleneck tokens B:</head><p>We experiment with B = 4, 36, 64, 256 and 1024, and find that performance is relatively consistent (all within 0.5 mAP). We hence fix the number of tokens to B = 4 for all experiments. It is interesting that with such a small number of cross-modal connections through only 4 hidden units (B = 4) at each cross-modal layer, we get large performance gains over late fusion <ref type="figure" target="#fig_0">(Fig. 3)</ref>, highlighting the importance of allowing cross-modal information to flow at multiple layers of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Input sampling and dataset size</head><p>In this section we investigate the impact of different modality sampling strategies. We also compare to single modality baselines -the visual-only and audio-only baselines consist of a vanilla transformer model applied to only the RGB or spectrogram patches respectively. Sampling window size t: An advantage of our transformer based model is that we can easily input variable length token sequences. We experiment with varying the sampling window t with the following values t = 2, 4, 6 and 8 seconds (note that all videos in AudioSet are 10s), and show results in <ref type="figure" target="#fig_2">Fig. 4</ref>  <ref type="bibr" target="#b2">3</ref> . At inference, we uniformly sample multiple windows covering the entire video. While the number of spectrogram patches N a changes with t, we keep the number of RGB patches N v     fixed by changing the stride of frames (to avoid running out of memory). Our results indicate that the performance of both the audio and audio-visual fusion model increases with input span, however the performance of the visual-only model slightly decreases (we hypothesize that this is due to the increased fixed stride, meaning fewer frames are randomly sampled during training). We fix t = 8s in all further experiments. Synchronous vs asynchronous sampling: Given that auditory and visual events may not always be perfected aligned in videos <ref type="bibr" target="#b35">[36]</ref>, we also investigate asynchronous sampling of different modalities.</p><p>Here input windows are sampled independently from the entire video clip for each modality. Results are provided in <ref type="figure" target="#fig_7">Fig. 8</ref> in the appendix. We find performance to be largely robust to either case, and so for simplicity we use synchronised sampling for all further experiments. Modality MixUp: While applying Mixup regularization <ref type="bibr" target="#b66">[67]</ref> to training, we note that there are two different ways to apply it for multimodal inputs -the standard approach is to sample one set of mixup weights from a Beta distribution using the parameter ?, and use it to generate all virtual modality-label pairs <ref type="bibr" target="#b66">[67]</ref>. We also explore a modified version which we call modality mixup, which samples an independent weight for each modality. Modality mixup imposes stronger augmentation than standard mixup, leading to a slight improvement (42.6 mAP to 43.9 mAP) on AudioSet. Impact of dataset size: We show the impact of varying the number of training samples in <ref type="figure" target="#fig_4">Fig. 5</ref>, and find a monotonic increase with dataset size (more steeply for audio-only than visual-only).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Comparison to single modality performance: We compare MBT to visual-only and audio-only baselines on AudioSet <ref type="table" target="#tab_1">(Table 1)</ref>, Epic-Kitchens <ref type="table" target="#tab_2">(Table 2)</ref> and VGGSound <ref type="table" target="#tab_3">(Table 3</ref>). Note we use the best parameters obtained via the ablations above, i.e. bottleneck fusion with t = 8, B = 4, F l = 8 and modality mixup. For all datasets, multimodal fusion outperforms the higher-performing single modality baseline, demonstrating the value of complementary information. The relative importance of modalities for the classification labels varies (audio-only has higher relative performance for AudioSet and lower for Epic-Kitchens, while both audio and visual baselines are equally strong for VGGSound). This is (unsurprisingly) largely a function of the dataset annotation procedure and positions VGGSound as a uniquely suitable dataset for fusion. We also show that audio-visual fusion    a relatively new dataset, we compare to two existing audio-only works 4 <ref type="table" target="#tab_3">(Table 3)</ref>, and set the first audiovisual benchmark (that we are aware of) on this dataset. Visualisation of attention maps Finally, we compute maps of the attention from the output CLS tokens to the RGB image input space using Attention Rollout <ref type="bibr" target="#b0">[1]</ref>. Results on test images for both a vanilla fusion model and MBT trained on Audioset-mini (fusion layer L f = 8) are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. We show the attention maps summed over all the frames in the video clip. We note that first, the model focuses on semantically salient regions in the video for audio classification, particularly regions where there is motion that creates or modifies sound, i.e. the mouth of humans making sounds, fingertips on a piano, hands and instruments. This is unlike state of the art sound source localisation techniques trained with images <ref type="bibr" target="#b10">[11]</ref>, which tend to highlight the entire object. We further note that the attention maps for MBT are more localised to these regions, showing that the tight bottlenecks do force the model to focus only on the image patches that are actually relevant for the audio classification task and which benefit from early fusion with audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a new transformer architecture (MBT) for audiovisual fusion, and explore a number of different fusion strategies using cross-attention between latent tokens. We propose a novel strategy to restrict cross-modal attention via a small set of fusion 'bottlenecks', and demonstrate that this improves performance over vanilla cross-attention at lower computational cost, achieving state of the art results on a number of benchmarks. Future work will involve extending MBT to other modalities such as text and optical flow. Limitations: The fusion layer is a hyperparameter and may need to be tuned specifically for different tasks and datasets. We also only explore fully supervised fusion, and future work will tackle extensions to a self-supervised learning framework. Broader impact: Multimodal fusion strategies are important for machine learning, as fusing complementary information from different modalities can increase robustness when applied to real world applications. We also note that transformers are in general compute-heavy, which can have adverse environmental effects. We propose a token fusion method via bottlenecks that helps reduce computational complexity when applying transformers for multimodal fusion. Finally, we observe that training datasets contain biases that may render models trained on them unsuitable for certain applications. It is thus possible that people use classification models (intentionally or not) to make decisions that impact different groups in society differently, and it is important to keep this in mind when deploying, analysing and building upon these models.</p><p>Here the bottleneck tokens are updated twice, first with visual information (Equation 10), and then with audio information <ref type="figure" target="#fig_8">(Equation 11</ref>). We also experimented with updating the bottlenecks with audio information first and compare both variations to the symmetric update in <ref type="table" target="#tab_5">Table 4</ref>. We find performance is robust to all variations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB first</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Backbone architecture</head><p>We experiment with three standard ViT <ref type="bibr" target="#b17">[18]</ref> backbones, ViT-Small, ViT-Base and ViT-Large on both Audioset-mini and VGGSound. We report results in <ref type="table">Table 5</ref> for audiovisual fusion with our best MBT model. We find that performance increases from ViT-Small to ViT-Base, but then drops for ViT-Large. This could be due to the fact that these datasets are on the smaller side, and more data might be required to take advantage of larger models.</p><p>Backbone AS-mini VGGSound  <ref type="table">Table 5</ref>: Performance with varying backbones on AS-mini and VGGSound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 The impact of weight sharing</head><p>We investigate the impact of sharing the encoder weights for both modalities (strategy (i) vs (ii)) as described in Sec. 4.3.1 . Results are provided in <ref type="figure" target="#fig_6">Fig. 7</ref> for different fusion layers L f . When modalities are fused at earlier layers, using separate encoders improves performance. For models with later fusion layers, performance is similar for both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Input sampling</head><p>Here we investigate asynchronous sampling of different modalities (where input windows are sampled independently from the entire video clip for each modality) as compared to synchronous sampling. Results are provided in <ref type="figure" target="#fig_7">Fig. 8</ref> for different input span lengths t. Over multiple runs we find that performance is largely robust to either sampling choice. We hypothesise that asynchronous sampling provides the following trade-off: while it introduces a misalignment between the two modality inputs, slight shifts are also a good source of temporal augmentation. As the video clip span length grows, the possible options for misalignment between inputs are less severe, while the impact of additional augmentation is more evident.</p><p>In <ref type="table" target="#tab_8">Table 6</ref>, we provide the results in numerical form used to create <ref type="figure" target="#fig_2">Fig. 4</ref> . We perform 3 runs per experiment and report mean and standard deviation. All segments in AudioSet are 10 seconds long.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Per class performance</head><p>We also examine per-class average precision (AP) results for our best model trained on the mini-Audioset (note that this dataset has 527 classes). We first show the results for the 60 top ranked classes in Audioset (by audio-visual mAP performance) in <ref type="figure">Fig. 9</ref>. We show the per class AP using our best fusion model (MBT), as well as the performance of audio only and visual only baselines. Audio-visual fusion improves performance over audio only or visual only for almost all (57 out of 60) classes, except for 'bagpiping', 'emergency vehicle' and 'didgeridoo' which have strong audio signatures. We then analyse the top 60 classes for which fusion has the largest improvement over single modality performance, over audio-only <ref type="figure">(Figure 10, top)</ref> and visual-only <ref type="figure">(Figure 10, bottom)</ref>. For some classes such as 'bicycle' and 'shuffling cards', fusion improves over the audio-only baseline by over 60% in absolute AP. The class that benefits most from audio-visual fusion over a visual-only baseline is 'Whistling' (almost 80% improvement in absolute AP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Datasets</head><p>In this section we report results on 2 additional datasets, Moments in Time <ref type="bibr" target="#b46">[47]</ref> and Kinetics <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Moments In Time</head><p>Moments In Time <ref type="bibr" target="#b46">[47]</ref> consists of 800,000, 3-second clips from YouTube videos. The videos are diverse and capture dynamic scenes involving animals, objects, people, or natural phenomena. The videos are labelled with 330 verb classes, each associated with over 1,000 videos. We show results for MBT compared to single modality baselines in <ref type="table" target="#tab_11">Table 7</ref>. Our first observation is that audio-only performance is much lower than visual-only. This is largely a function of the annotation procedure for the dataset, however we also note that clips are only 3 seconds long, and as shown in <ref type="figure" target="#fig_2">Fig. 4</ref> , audio-only performance is heavily dependant on the span length t on Audioset, suggesting that it may be difficult to recognise audio events from shorter inputs. Our fusion model provides a further modest 1% boost to performance over the visual-only baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Kinetics</head><p>Kinetics <ref type="bibr" target="#b34">[35]</ref> consists of 10-second videos sampled at 25fps from YouTube. We evaluate on both Kinetics 400 <ref type="bibr" target="#b34">[35]</ref> and a commonly used subset Kinetics-Sound <ref type="bibr" target="#b3">[4]</ref>, containing 400 and 36 classes respectively. As these are dynamic datasets (videos may be removed from YouTube), we train and test on 209,552 and 17,069 videos respectively for Kinetics and report results on 1,165 videos for Kinetics-Sound. Results for MBT compared to single modality baselines are shown in <ref type="table" target="#tab_12">Table 8</ref>. We note that on the entire Kinetics test set, our fusion model outperforms the visual only baseline by about 1% in top 1 accuracy (in line with other works <ref type="bibr" target="#b63">[64]</ref> that demonstrate that audio for the large part does not improve performance for most Kinetics classes). This gap is widened, however, for the    <ref type="figure">Figure 10</ref>: Top 60 classes that have the highest gain with fusion over a audio only (top) and visual only (bottom) baseline. Note how fusion improves the per class AP for certain classes by over 50% over a unimodal model. As expected, the classes that benefit most from visual information are 'bicycle' and 'shuffling cards' and the class that benefits most from audio is 'Whistling'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difference between fusion and visual-only performance</head><p>Kinetics-Sound subset of the dataset (over 4%), as expected because this subset consists of classes in Kinetics selected to have a strong audio signature <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Dataset Variations for MBT vs Late Fusion</head><p>In this section we further analyse the significance of our method across all the popular video classification datasets used in the paper (most ablations results are only shown for mini-Audioset in the main paper). We note that the gap between MBT and late-fusion is highly dataset dependant (see <ref type="table" target="#tab_14">Table 9</ref>), with our method providing an even greater advantage for Epic-Kitchens (almost 6% difference in Top 1 action accuracy).    <ref type="bibr" target="#b34">[35]</ref> and Kinetics Sound <ref type="bibr" target="#b3">[4]</ref>. We report top-1 and top-5 classification accuracy. AV: Refers to audio-visual fusion. ? Note the Kinetics-Sound test set has reduced since this work as videos have been removed from YouTube, hence this is not a direct comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Transfer learning</head><p>We use checkpoints pretrained on VGGSound, Kinetics400 and AS-500K and finetune them on Audioset-mini and VGGSound (note we use a ViT-B backbone for these experiments, and report results for audiovisual fusion with our best MBT model). Results are provided in <ref type="table" target="#tab_1">Table 10</ref>. While Kinetics400 pretraining gives a slight 0.7% mAP boost on AS-mini, VGGSound initialisation gives a substantial 3% mAP boost over Imagenet Initialisation. On VGGSound, AS500K pretraining gives a more modest boost of 1.2% Top 1 Acc, while Kinetics pretraining does not help (expected as VGGSound is a larger dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F AS-500K details</head><p>The original unbalanced AudioSet training set consists of almost 2M samples, and is extremely unbalanced with most samples either labelled as speech or music. To improve training efficiency, we create a slightly more balanced subset called AudioSet-500K. The main issue is that AudioSet is multilabel, and this makes balancing difficult. We create AS-500K by greedily restricting the maximum number of samples per class to be 200K. Given the distribution of labels, this gives us a total size of 508,994 samples. We provide the full histogram of labels in <ref type="figure" target="#fig_8">Fig. 11</ref> (note the number of samples is on a log 10 scale).    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classes in Audioset</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The impact of using attention bottlenecks for fusion on performance (left) and compute (right) at different fusion layers L f on AudioSet, using clip span t = 4 and B = 4 bottleneck tokens. Attention bottlenecks improve performance at lower computational cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The effect of varying input clip span t on the AudioSet test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The effect of training data size on the AudioSet test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Attention Maps. We compute maps of the attention from the output CLS tokens to the RGB image input space for a vanilla self-attention model and MBT on the Audioset test set. For each video clip, we show the original middle frame on the left with the ground truth labels overlayed at the bottom. The attention is particularly focused on sound source regions in the video that contain motion, eg. the fingertips on the piano, the hands on the string instrument, faces of humans. The bottlenecks in MBT further force the attention to be localised to smaller regions of the images (i.e the mouth of the baby on the top left and the mouth of the woman singing on the bottom right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>The effect of sharing weights for vanilla fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Asynchronous vs synchronous sampling of RGB and spectrogram inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Class label histogram in the AudioSet-500K split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison to SOTA on AudioSet<ref type="bibr" target="#b23">[24]</ref>. We report mean average precision (mAP). We outperform works that train on the full Audioset (2M samples), while we train on only 500K samples.</figDesc><table><row><cell>Model</cell><cell cols="4">Training Set A only V only AV Fusion</cell></row><row><cell>GBlend [63]</cell><cell>MiniAS</cell><cell>29.1</cell><cell>22.1</cell><cell>37.8</cell></row><row><cell>GBlend [63]</cell><cell>FullAS-2M</cell><cell>32.4</cell><cell>18.8</cell><cell>41.8</cell></row><row><cell cols="2">Attn Audio-Visual [21] FullAS-2M</cell><cell>38.4</cell><cell>25.7</cell><cell>46.2</cell></row><row><cell>Perceiver [32]</cell><cell>FullAS-2M</cell><cell>38.4</cell><cell>25.8</cell><cell>44.2</cell></row><row><cell>MBT</cell><cell>MiniAS</cell><cell>31.3</cell><cell>27.7</cell><cell>43.9</cell></row><row><cell>MBT</cell><cell>AS-500K</cell><cell>44.3</cell><cell>32.3</cell><cell>52.1</cell></row><row><cell>Model</cell><cell cols="4">Modalities Verb Noun Action</cell></row><row><cell>Damen et al. [14]</cell><cell>A</cell><cell>42.1</cell><cell>21.5</cell><cell>14.8</cell></row><row><cell cols="2">AudioSlowFast [37] ? A</cell><cell cols="2">46.5 22.78</cell><cell>15.4</cell></row><row><cell>TSN [62]</cell><cell>V, F</cell><cell>60.2</cell><cell>46.0</cell><cell>33.2</cell></row><row><cell>TRN [68]</cell><cell>V, F</cell><cell>65.9</cell><cell>45.4</cell><cell>35.3</cell></row><row><cell>TBN [36]</cell><cell>A, V, F</cell><cell>66.0</cell><cell>47.2</cell><cell>36.7</cell></row><row><cell>TSM [45]</cell><cell>V, F</cell><cell>67.9</cell><cell>49.0</cell><cell>38.3</cell></row><row><cell>SlowFast [22]</cell><cell>V</cell><cell>65.6</cell><cell>50.0</cell><cell>38.5</cell></row><row><cell>MBT</cell><cell>A</cell><cell>44.3</cell><cell>22.4</cell><cell>13.0</cell></row><row><cell>MBT</cell><cell>V</cell><cell>62.0</cell><cell>56.4</cell><cell>40.7</cell></row><row><cell>MBT</cell><cell>A, V</cell><cell>64.8</cell><cell>58.0</cell><cell>43.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Optical flow. ?Uses pretraining on VGGSound.provides slight performance gains for traditionally video only datasets such as Kinetics and Moments in Time (details provided in Appendix C ). We also examine per-class performance on the Audioset dataset (Figures 9 and 10 in the Appendix), and find that for the top 60 classes (ranked by overall performance), audio-visual fusion improves performance over audio only or visual only for almost all (57 out of 60) classes, except for 'bagpiping', 'emergency vehicle' and 'didgeridoo' which have strong audio signatures. For classes such as 'bicycle' and 'shuffling cards' where audio signals are weaker, fusion improves over the audio-only baseline by over 60% in absolute AP. Comparison to state of the art: We compare MBT to previous fusion methods on AudioSet inTable 1. We outperform all previous works on fusion (even though we only train on a quarter of the training set -500K samples), including the recently introduced Perceiver [32] which uses early fusion followed by multiple self attention layers, and Attn Audio-Visual<ref type="bibr" target="#b20">[21]</ref> which uses self-attention fusion on top of individual modality CNNs. We compare to previous video classification methods on Epic-Kitchens inTable 2, and note that our model outperforms all previous works that use vision only, as well as TBN<ref type="bibr" target="#b35">[36]</ref> which uses three modalities -RGB, audio and optical flow. Given VGGSound is</figDesc><table><row><cell>Model</cell><cell cols="3">Modalities Top-1 Acc Top-5 Acc</cell></row><row><cell>Chen et al ? [12]</cell><cell>A</cell><cell>48.8</cell><cell>76.5</cell></row><row><cell cols="2">AudioSlowFast ? [37] A</cell><cell>50.1</cell><cell>77.9</cell></row><row><cell>MBT</cell><cell>A</cell><cell>52.3</cell><cell>78.1</cell></row><row><cell>MBT</cell><cell>V</cell><cell>51.2</cell><cell>72.6</cell></row><row><cell>MBT</cell><cell>A,V</cell><cell>64.1</cell><cell>85.6</cell></row></table><note>Comparison to SOTA on EpicKitchens-100 [14]. Modalities are A: Audio, V: Visual, F:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison</figDesc><table><row><cell>Mid Frame</cell><cell>Vanilla Fusion</cell><cell>MBT</cell><cell>Mid Frame</cell><cell>Vanilla Fusion</cell><cell>MBT</cell></row><row><cell>Baby cry</cell><cell></cell><cell></cell><cell>String instrument</cell><cell></cell><cell></cell></row><row><cell>Piano, music</cell><cell></cell><cell></cell><cell>yodeling</cell><cell></cell><cell></cell></row></table><note>to the state of the art on VGGSound [12]. Modalities are A: Audio, V: Visual, F: Optical flow. ? We calculate metrics on our test set for a fair comparison using the scores provided by the authors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Asymmetric vs symmetric bottleneck updates.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>10?0.54 29.91?0.21 30.08?0.21 30.55?0.22 Audio-Visual 37.95?0.51 40.32?0.20 41.51?0.24 42.37?0.44</figDesc><table><row><cell>Span Length t</cell><cell>2s</cell><cell>4s</cell><cell>6s</cell><cell>8s</cell></row><row><cell>Visual only</cell><cell cols="4">26.23?0.16 25.74?0.18 25.68?0.02 25.43?0.02</cell></row><row><cell>Audio only</cell><cell>27.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>The effect of varying input clip span t on performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Per-class average precision for the top 60 classes in Audioset ranked by mAP. Best viewed in colour and zoomed in. Note how audio-visual fusion helps improve performance over audio only for almost all classes. The visual only model performs well for classes that have a stronger visual signature than audio, eg 'bicycle', 'mechanical fan', 'boat' and 'arrow'.</figDesc><table><row><cell></cell><cell>audio-visual</cell><cell>visual-only</cell><cell>audio-only</cell></row><row><cell>1</cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell></row><row><cell>0.4 0.6 AP</cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell></row><row><cell>0</cell><cell cols="3">Wind instrument Snoring Arrow Brass instrument Fireworks Frying Theremin Battle cry Mechanical fan Fowl Boat Harp Harmonica Banjo Tuning fork Sailboat Howl Hair dryer Moo Goose Cattle Bagpipes Gong Crow Fire engine Music Whale vocalization Machine gun Blender Emergency vehicle Chopping Didgeridoo Bicycle Subway Harpsichord Owl Ice cream truck Police car Hoot French horn Vacuum cleaner Sink Turkey Gobble Fire alarm Sanding Whistling Railroad car Rail transport Siren Train Shofar Singing bowl Accordion Artillery fire Shuffling cards Cash register Change ringing Sewing machine Civil defense siren</cell></row><row><cell cols="4">Truck Domestic animals Whip Microwave oven Dishes Horse Ratchet Wind noise Clock Hoot Wood Splinter Eruption Owl Cash register Wild animals Lawn mower Toilet flush Waterfall Wind Rowboat Roaring cats Artillery fire Clickety-clack Patter Writing Subway Sawing Zither Bus Mechanisms Goose Caw Dog Pulleys Crow Coin Toothbrush Ship Run Whale vocalization Vacuum cleaner Pig Helicopter Boat Blender Gears Rodents Sailboat Caterwaul Sink Mechanical fan Fire Sewing machine Shuffle Clip-clop Motorboat Shuffling cards Bicycle Figure 9: Arrow 0 0.2 0.4 0.6 AP Difference between fusion and audio-only performance</cell></row><row><cell>0 0.2 0.4 0.6 0.8</cell><cell cols="3">Cough Dishes Thunderstorm Music of Africa Doorbell Stomach rumble Fireworks Trance music Drum kit Cutlery Harmonica Finger snapping Singing bowl Thunder Howl Bicycle bell Rain on surface Siren Saxophone Hair dryer Fly Throbbing Tuning fork Electronic dance music Crumpling Telephone dialing Chime Opera Chatter Emergency vehicle Techno Pink noise Mallet percussion Burping Bagpipes Wind chime Environmental noise Hiccup Rain Clicking Train horn Applause Telephone bell ringing Pour Dial tone Snoring Sink Dubstep Mosquito String section Air horn Electronic tuner Timpani Car alarm Knock Wood block Sanding Angry music Police car Whistling</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison to state of the art on Moments in Time<ref type="bibr" target="#b46">[47]</ref>. We report top 1 and top 5 classification accuracy. AV: Refers to audio-visual fusion.</figDesc><table><row><cell>Model</cell><cell cols="2">Kinetics</cell><cell cols="2">Kinetics-Sounds</cell></row><row><cell></cell><cell cols="3">Top-1 Top-5 Top-1</cell><cell>Top-5</cell></row><row><cell>blVNet [20]</cell><cell>73.5</cell><cell>91.2</cell><cell>-</cell><cell>-</cell></row><row><cell>STM[34]</cell><cell>73.7</cell><cell>91.6</cell><cell>-</cell><cell>-</cell></row><row><cell>TEA [44]</cell><cell>76.1</cell><cell>92.5</cell><cell>-</cell><cell>-</cell></row><row><cell>TS S3D-G [65]</cell><cell>77.2</cell><cell>93.0</cell><cell>-</cell><cell>-</cell></row><row><cell>3-stream SATT [9]</cell><cell>77.7</cell><cell>93.2</cell><cell>-</cell><cell>-</cell></row><row><cell>AVSlowFast, R101 [64]</cell><cell>78.8</cell><cell>93.6</cell><cell>85.0 ?</cell><cell>-</cell></row><row><cell>LGD-3D R101 [52]</cell><cell>79.4</cell><cell>94.4</cell><cell>-</cell><cell>-</cell></row><row><cell>SlowFast R101-NL [22]</cell><cell>79.8</cell><cell>93.9</cell><cell>-</cell><cell>-</cell></row><row><cell>ViViT-Base [6]</cell><cell>80.0</cell><cell>94.0</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours (Audio-only)</cell><cell>25.0</cell><cell>43.9</cell><cell>52.6</cell><cell>71.5</cell></row><row><cell>Ours (Visual-only)</cell><cell>79.4</cell><cell>94.0</cell><cell>80.7</cell><cell>94.9</cell></row><row><cell>MBT (AV)</cell><cell>80.8</cell><cell>94.6</cell><cell>85.0</cell><cell>96.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Comparison to state of the art on Kinetics</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Dataset mini-Audioset Epic-Kitchens VGGSound Moments in Time Kinetics</figDesc><table><row><cell>Late Fusion</cell><cell>41.80</cell><cell>37.90</cell><cell>63.3</cell><cell>36.48</cell><cell>77.0</cell></row><row><cell>MBT</cell><cell>43.92</cell><cell>43.40</cell><cell>64.1</cell><cell>37.26</cell><cell>80.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>MBT vs late Fusion for different datasets. For each dataset we report the widely used primary metric, i.e. Audioset: mAP, Epic-Kitchens: Top-1 action accuracy, VGGSound, Moments in Time and Kinetics: Top-1 classification accuracy.</figDesc><table><row><cell cols="3">Initialisation Checkpoint AS-mini VGGSound</cell></row><row><cell>ImageNet init.</cell><cell>43.3</cell><cell>64.1</cell></row><row><cell>VGGSound init.</cell><cell>46.6</cell><cell>N/A</cell></row><row><cell>K400 init.</cell><cell>44.0</cell><cell>64.0</cell></row><row><cell>AS-500K init.</cell><cell>N/A</cell><cell>65.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Transfer learning on Audioset-mini and VGGSound.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">L is the number of transformer layers, NH is the number of self-attention heads with hidden dimension d.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that L f = 12 refers to late fusion, where logits are only aggregated after the classifiers, and neither fusion strategy (ii) nor (iii) is applied, but we show results on the same plot for convenience.<ref type="bibr" target="#b2">3</ref> Averaged over 3 runs. Because error bars are small in the plot we also provide them inTable 6in the appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To fairly compare to these works, we obtain the scores on the full VGGSound test set from the authors, and compute accuracy metrics on our slightly smaller test set as described in Sec. 4.1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>We would like to thank Joao Carreira for helpful discussions on the Perceiver <ref type="bibr" target="#b31">[32]</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Ablations on mini-Audioset</head><p>In this section we expand on the ablations provided in Sec. 4.3 of the main paper. Unless otherwise specified, ablations are performed using Audioset-mini as the training set and the Audioset test set for evaluation. For most experiments we conduct 3 runs and report mean and standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Symmetric vs asymmetric bottleneck updates</head><p>We also experiment with an asymmetric bottleneck update. This involves replacing Eq. 8 and 9 with the following:</p><p>[z l+1 spec ||z l+1 fsn ] = Transformer([z l spec ||? l+1 fsn ]; ? spec )</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00928</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linagzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-supervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Soundnet: Learning sound representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Revisiting the effectiveness of off-the-shelf temporal modeling approaches for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlong</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03805</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Localizing visual sounds the hard way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VGGSound: A large-scale audio-visual dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Audio-visual integration in multimodal communication. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="837" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13256</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Rescaling egocentric vision. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.11403</idno>
		<title level="m">Scenic: A JAX library for computer vision research and beyond</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinatan</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">More is less: Learning efficient video representations by big-little network and depthwise temporal aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00869</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Large scale audiovisual learning of sounds with weakly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haytham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Fayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01595</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong Duc</forename><surname>Dao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.03766</idno>
		<title level="m">The activitynet largescale activity recognition challenge 2018 summary</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">AST: audio spectrogram transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01778</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of spoken language with visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Decoupling the role of data, attention, and losses in multimodal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00529</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-modal dense video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Iashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="958" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perceiver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03206</idno>
		<title level="m">General perception with iterative attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coincidence, categorization, and consolidation: Learning to recognize sounds with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif A</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saurous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audiovisual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5492" to="5501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Slow-fast auditory streams for audio recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="855" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning for robust feature generation in audiovisual emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">Mower</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP. IEEE</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Parameter efficient multimodal transformers for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04124</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Entangled transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8928" to="8937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to generate diverse dance motions with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaman</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08171</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learn to dance with aist++: Music conditioned 3d dance generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanazawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.08779</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>ieee</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7082" to="7092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="502" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Episodic transformer for vision-andlanguage navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pashevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep multimodal learning: A survey on recent advances and trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhanesh</forename><surname>Ramachandram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="96" to="108" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Assemblenet: Searching for multi-stream neural connectivity in video architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13209</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks and data augmentation for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="283" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Look before you speak: Visually contextualized utterances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Paul Hongsuck Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The development of embodied cognition: Six lessons from babies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial life</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="13" to="29" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Into the wild with audioscope: Unsupervised audio-visual separation of on-screen sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hershey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.01143</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">What makes training multi-modal classification networks hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12695" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Audiovisual slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08740</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A) as well as analyse the per-class average precision of fusion over single modality baselines (Sec. B). We then provide results on two additional datasets, Moments in Time and Kinetics in Sec. C and perform some preliminary transfer learning experiments</title>
	</analytic>
	<monogr>
		<title level="m">Here we provide additional ablation results on mini-Audioset</title>
		<meeting><address><addrLine>Sec</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>in Sec. E. Finally we provide details on the AS-500K split</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
