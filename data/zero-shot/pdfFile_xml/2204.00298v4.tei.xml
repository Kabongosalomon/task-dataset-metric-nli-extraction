<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unitail: Detecting, Reading, and Matching in Retail Scene</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Chen</surname></persName>
							<email>fangyic@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Dou</surname></persName>
							<email>jiachend@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shentong</forename><surname>Mo</surname></persName>
							<email>shentonm@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uzair</forename><surname>Ahmed</surname></persName>
							<email>uzaira@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
							<email>marioss@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unitail: Detecting, Reading, and Matching in Retail Scene</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Product Detection</term>
					<term>Product Recognition</term>
					<term>Text Detection</term>
					<term>Text Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To make full use of computer vision technology in stores, it is required to consider the actual needs that fit the characteristics of the retail scene. Pursuing this goal, we introduce the United Retail Datasets (Unitail), a large-scale benchmark of basic visual tasks on products that challenges algorithms for detecting, reading, and matching. With 1.8M quadrilateral-shaped instances annotated, the Unitail offers a detection dataset to align product appearance better. Furthermore, it provides a gallery-style OCR dataset containing 1454 product categories, 30k text regions, and 21k transcriptions to enable robust reading on products and motivate enhanced product matching. Besides benchmarking the datasets using various start-of-the-arts, we customize a new detector for product detection and provide a simple OCR-based matching solution that verifies its effectiveness. The Unitail and its evaluation server is publicly available at https://unitedretail.github.io.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the rise of deep learning, numerous computer vision algorithms have been developed and have pushed many real-world applications to a satisfactory level. Currently, various visual sensors (fixed cameras, robots, drones, mobile phones, etc.) are deployed in retail stores, enabling advanced computer vision methods in shopping and restocking. Scene Product Recognition (SPR) is the foundation module in most of these frameworks, such as planogram compliance, out-of-stock managing, and automatic check-out. SPR refers to the automatic detection and recognition of products in complex retail scenes. It comprises steps that first localize products and then recognize them via the localized appearance, analogous to many recognition tasks. However, scene products have their characteristics: they are densely-packed, low-shot, fine-grained, and widely-categorized. These innate characteristics result in obvious challenges and will be a continuing problem. Recent datasets in retail scenes follow the typical setting in the common scene to initiate the momentum of research in SPR. For instance, the SKU110k dataset <ref type="bibr">[18]</ref>, which has recently enabled large-scale product detection, is following the MS-COCO <ref type="bibr" target="#b24">[37]</ref> style annotation and evaluation metric. Despite their significant value, the underpaid attention to the SPR's characteristics leads us to the question: what is the next advance towards SPR?</p><p>Firstly, traditional detection targets poorly comply with the actual needs, causing improper image alignment of the product appearances. Detection targets in common scenes <ref type="bibr" target="#b24">[37,</ref><ref type="bibr" target="#b36">66,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10]</ref> are usually defined as covering the utmost visible entirety of an object with a minimal rectangle box. This format is inherited by most existing retail datasets <ref type="bibr">[18,</ref><ref type="bibr">63,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr">59,</ref><ref type="bibr" target="#b1">2]</ref>. However, because occlusion occurs more frequently between products (the densely-packed characteristic), improper alignments can easily hinder the detection performance. Detectors equipped with Non-Maximum Suppression (NMS) suffer from the overlaps among the axisaligned rectangular bounding boxes (AABB) and rotated rectangular bounding boxes (RBOX). Moreover, poor alignment leads to inconsistent image registration of the same products, which brings extra difficulties to accurate recognition.</p><p>Secondly, even in the well-aligned cases, products from intra-classes require discriminative features due to their fine-grained characteristic. On the one hand, a slight variation in the product packaging can significantly change the product price, especially for the visually similar but textually different regions such as brand/model, flavour/version, ingredient/material, count/net weight. This requires SPR algorithms to pay attention to the particular text patterns. On the other hand, due to the labelling effort on thousands of categories per store (the widely-categorized characteristic), the available samples per category are scarce (the low-shot characteristic), which degrades the SPR robustness. These two constraints are in conjunction with our empirical observation that visual classifiers could frequently make mistakes when products look similar but vary in text information.</p><p>In this paper, we introduce the United Retail Datasets (Unitail) that responds to these issues. The Unitail is a comprehensive benchmark composed of two datasets: Unitail-Det and Unitail-OCR, and currently supports four tasks in real-world retail scene: Product Detection, Text Detection, Text Recognition, and Product Matching. <ref type="figure">(Fig.1</ref>)</p><p>Unitail-Det, as one of the largest quadrilateral object detection datasets in terms of instance number and the only existing product dataset in quadrilateral annotations by far, is designed to support well-aligned product detection. Unitail-Det enjoys two key features: 1. Bounding boxes of products are densely annotated in the quadrilateral style that cover the frontal face of products. Practically, Quadrilaterals (QUADs) adequately reflect the shapes and poses of most products regardless of the viewing angles, and efficiently cover the irregular shapes. The frontal faces of products provide distinguishable visual information and keep the appearances consistent. 2. In order to evaluate the robustness of the detectors across stores, the test set consists of two subsets to support both origin-domain and cross-domain evaluation. While one subset shares the domain  <ref type="figure">Fig. 1</ref>: The Unitail is a large-scale benchmark in retail scene that consists of two sub-datasets and supports four basic tasks.</p><p>with the training set, the other is independently collected from other different stores, with diverse optical sensors, and from various camera perspectives.</p><p>Unitail-OCR (Optical Character Recognition) aims to drive research and applications using visual texts as representations for products. This direction is partially inspired by the customers' behavior: people can glance and recognize ice cream but need to scrutinize the flavor and calories to make a purchase. It is organized into three tasks: text detection, text recognition, and product matching. Product images in Unitail-OCR are selected from the Unitail-Det and benefit from the quadrilateral aligned annotations. Each is equipped with onproduct text location and textual contents together with its category. Due to the product's low-shot and widely-categorized characteristics, product recognition is operated by matching within an open-set gallery. To the best of our knowledge, Unitail-OCR is the first dataset to support OCR models' training and evaluation on the retail products, and it is experimentally verified to fill in the domain blank; when evaluated on a wide variety of product texts, models trained on Unitail-OCR outperform those trained on common scene texts <ref type="bibr">[26]</ref>. It is also the first dataset that enables the exploration of text-based solutions to product matching.</p><p>Based on the proposed Unitail, we design two baselines. To detect products, we analyze the limitation of applying generic object detectors in the retail scene and design RetailDet to detect quadrilateral products. To match products using visual texts on 2D space, we encode text features with spatial positional encoding and use Hungarian Algorithm [31] that calculates optimal assignment plans between varying text sequences.</p><p>Our contributions are summarized in three folds: (1) we introduce the Unitail, a comprehensive benchmark for well-aligned textually enhanced SPR. <ref type="bibr" target="#b1">(2)</ref> We benchmark the tasks of Unitail with various off-the-shelf methods, including <ref type="bibr" target="#b46">[76,</ref><ref type="bibr">62,</ref><ref type="bibr">61,</ref><ref type="bibr" target="#b21">34,</ref><ref type="bibr" target="#b28">41,</ref><ref type="bibr" target="#b35">65,</ref><ref type="bibr">47,</ref><ref type="bibr">21,</ref><ref type="bibr">55,</ref><ref type="bibr">54,</ref><ref type="bibr" target="#b40">70,</ref><ref type="bibr" target="#b20">33,</ref><ref type="bibr" target="#b19">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">23,</ref><ref type="bibr">22,</ref><ref type="bibr">57,</ref><ref type="bibr" target="#b32">44]</ref>. (3) we design two baselines for product detection and text-based product matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The retail scene has drawn attention to the computer vision community for an extended period. The early evolution of product datasets <ref type="bibr" target="#b27">[40,</ref><ref type="bibr">29,</ref><ref type="bibr">52]</ref> facilitates reliable training and evaluation and drives research in this challenging field. Recently, large-scale datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">18]</ref> has enabled deep learning based approaches. Datasets related to SPR can be split into two groups: product detection and product recognition. We address each in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Product Detection Datasets</head><p>Detection is the first step in SPR entailing the presence of products that are typically represented by rectangular bounding boxes. The GroZi-120 <ref type="bibr" target="#b27">[40]</ref> was created using in situ and in vitro data to study the product detection and recognition with 120 grocery products varying in color, size, and shape. The D2S <ref type="bibr" target="#b13">[14]</ref> is an instance segmentation benchmark for sparsely-placed product detection, with 21000 images and 72447 instances. The SKU110k[18] and the SKU110k-r <ref type="bibr" target="#b33">[45]</ref> provide 11762 images with 1.7M on-shelf product instances; they are annotated with axis-aligned bounding boxes and rotated bounding boxes, respectively.</p><p>For the detection of coarsely categorized products, The RPC [63] is designed for checkout scenarios containing 0.4M instances and 200 products. The Grocery Shelves dataset [59] took 354 shelf images and 13k instances, and around 3k instances are noted in 10 brand classes. The Locount <ref type="bibr" target="#b1">[2]</ref> simultaneously considers the detection and counting for groups of products, with an impressive number of 1.9M instances and 140 categories.</p><p>Despite the significant values of these datasets, we are still challenged by the availability of optimal bounding boxes. In the proposed Unitail-Det, we aim to shape products into quadrilaterals whose appearances are well aligned. The dataset also provides evaluation targets in the origin-domain and cross-domain, bringing algorithms closer to practical deployment.  <ref type="bibr" target="#b5">[6]</ref> is crawled from web sources by searching 50K product names, consequently containing 2.5 million noisy images without human annotations. The ABO <ref type="bibr" target="#b7">[8]</ref> dataset covers 576 categories that studies the 3D object understanding. The Grozi-3.2K <ref type="bibr" target="#b14">[15]</ref> contains 80 fine-grained grocery categories and 8350 images for multi-label classification. To the best of our knowledge, there is a lack of a dataset that encourages leveraging both visual and textual information for product recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Product Recognition Datasets</head><p>OCR on Retail Products Product recognition by texts is challenging. The lack of datasets obstructs the relevant research on this topic. The most relevant dataset is Products-6K[17] where the Google Cloud Vision API is employed to extract the textual information to enhance products' descriptions. But the texts were not labelled by human annotators, and text location information is missing, so it is infeasible to support any advance to OCR related tasks.</p><p>There are a couple of attempts that use off-the-shelf OCR models for assisted shopping. <ref type="bibr" target="#b15">[16]</ref> presented a system on which users search products by name and OCR models return the texts on products so that a product list ranked by word histogram is generated for the users. <ref type="bibr" target="#b31">[43]</ref> recognize texts and then apply text embedding and language models to extract features for product verification.</p><p>Other Related Datasets Many OCR datasets [56, <ref type="bibr" target="#b30">42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">26,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b25">38,</ref><ref type="bibr">51,</ref><ref type="bibr" target="#b39">69,</ref><ref type="bibr">24,</ref><ref type="bibr">20]</ref> exist prior to Unitail-OCR. Typically, an OCR dataset supports text detection and text recognition and so enables text spotting. The ICDAR2015[26] and CTW1500 <ref type="bibr" target="#b25">[38]</ref> are two widely applied benchmarks for training and evaluating OCR models in common scene. The ICDAR2015 has 1,000 training and 500 testing images causally shot indoor and outdoor with word-level annotations for each text. The CTW1500 contains 10k text (3.5k of them are curved boxes) in 1500 images collected from the internet. Compared to them, Unitail-OCR is the first that focuses on product texts and supports object-level matching task at the same time. Product texts are usually artistic words with substantial character distortions, which are hard to be localize and recognize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Unitail</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unitail-Det</head><p>Image Collection Practically, the industry utilizes a variety of sensors under different conditions for product detection. The resolution and camera angles cover an extensive range by different sensors. For example, fixed cameras are mounted on the ceiling in most cases, and customers prefer to photograph with mobile devices. The product categories in different stores also span a great range. With these factors in mind, we collect images from two sources to support origindomain and cross-domain detection. In the origin domain, training and testing rotate warp <ref type="figure">Fig. 2</ref>: Quadrilateral (in green) is a nature fit to product in real scene, removing more noisy contexts than AABB (in violet) and RBOX (in red)). images are supposed to share the same domain and are taken from similar perspectives in the same stores by the same sensors. As a result, we select the 11,744 images from the prior largest product dataset, SKU110k [18], to form the origin domain. In the cross domain, we collect 500 images in different stores through multiple sensors, covering unseen categories and camera angles.</p><p>Annotation We annotate each product with a quadrilateral style bounding box, denoted as QUAD. <ref type="figure">Fig.2</ref> is an illustration of its advance. A QUAD refers to 4 points p tl , p tr , p br , p bl with 8 degrees of freedom (x tl , y tl , x tr , y tr , x br , y br , x bl , y bl ). For regular products shaped mainly in cuboid and cylinder, the (x tl ,y tl ) is defined as the top-left corners of their frontal faces, and the other points represent the rest 3 corners in clockwise order. For spherical, cones, and other shapes whose corners are difficult to identify, and for irregularly shaped products where so defined quadrilateral box cannot cover the entire frontal face, we first draw the minimum AABB and then adjust the four corners according to the camera perspective. As belabored, the frontal face of a product has the most representative information and is also critical for appearance consistency, but we still annotate the side face if the front face is invisible.</p><p>Totally, 1,777,108 QUADs are annotated by 13 well-trained annotators in 3 rounds of verification. The origin-domain is split to training (8,216 images, 1,215,013 QUADs), validation (588 images, 92,128 QUADs), and origin-domain testing set (2,940 images, 432,896 QUADs). The cross-domain supports a testing set (500 images, 37,071 QUADs). Their density and scale are shown in <ref type="figure" target="#fig_0">Fig.3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unitail-OCR</head><p>Gallery and Testing Suite A product gallery setup is a common practice in the retail industry for product matching applications. All known categories are first registered in the gallery. In case of a query product, the matching algorithms find the top ranked category in the gallery. The gallery of Unitail-OCR contains The pie chart reflects sections that source images were collected from. The bar chart is a histogram for the count of words on products. The font size of the words reflects the frequency of occurrence.</p><p>1454 fine-grained and one-shot product categories. Among these products, 10709 text regions and 7565 legible text transcriptions (words) are annotated. This enables the gallery to act as the training source and the matching reference.</p><p>The testing suite contains four components: 1. 3012 products labeled with 18972 text regions for text detection. 2. Among the pre-localized text regions, 13416 legible word-level transcriptions for text recognition. 3. 10k product samples from the 1454 categories for general evaluation on product matching. 4. From the 10k products, we select 2.4k fine-grained samples (visually similar for humans) for hard-example evaluation on product matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Collection and Annotation</head><p>Images are gathered from the Unitail-Det cross-domain and cropped and affine transformed according to the quadrilateral bounding boxes to form an upright appearance. We remove the low-quality images with low resolution and high blurriness. Some products kept in the Unitail-OCR might exclude text regions, like those from the produce and clothes departments. We randomly select one sample from each category to form the product gallery, and the remaining is further augmented by randomly adjusting the brightness and cropping for matching purposes.</p><p>We annotate 29681 text regions from 4466 products as quadrilateral text boxes. <ref type="figure" target="#fig_1">Fig.4</ref> shows the statistics. The bounding boxes are first classified as legible or illegible. For the 20981 legible ones, the alphanumeric transcriptions are annotated ignoring letter case and symbols. Numerical values with units are commonly seen on products such as 120mg, and we regard them as entire words. We also provide a vocabulary that covers all words present. The usage of vocabulary is more practical in our case than in other scenes [26], because the presence of products and texts are usually known in advance by the store owner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tasks and Evaluation Metrics</head><p>Product Detection Task The goal is to detect products as quadrilaterals from complex backgrounds. Unitail-Det supports the training and evaluation.</p><p>We use the geometric mean of mean average precision (mAP) calculated on the origin-domain test set and cross-domain test set as the primary metric for the product detection, where the mAP is calculated in MS-COCO style <ref type="bibr" target="#b24">[37]</ref>. Compared to arithmetic mean, the geometric mean is more sensitive when the model overfits to origin-domain but gains low performance on the cross-domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Detection Task</head><p>The goal is to detect text regions from pre-localized product images. Unitail-OCR supports the training and evaluation.</p><p>We adopt the widely used precision, recall and hmean <ref type="bibr">[26,</ref><ref type="bibr" target="#b25">38]</ref> for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Recogniton Task</head><p>The goal is to recognize words over a set of prelocalized text regions. Unitail-OCR supports the training and evaluation. We adopt the normalized edit distance (NED) [27] and word-level accuracy for evaluation. The edit distance between two words is defined by the minimum number of characters edited (insert, delete or substitute) required to change one into the other, then it is normalized by the length of the word and averaged on all ground-truths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Product Matching Task</head><p>The goal is to recognize products by matching a set of query samples to the Unitail-OCR gallery. The task is split into two tracks: Hard Example Track, which is evaluated on 2.5k selected hard examples; this track is designed for scenarios in which products are visually similar (for example pharmacy stores). And General Track, which is conducted on all 10k samples.</p><p>We adopt the top-1 accuracy as the evaluation metric.</p><p>4 Two Baselines Designed for The Unitail</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Customized Detector for Product Detection</head><p>Recent studies <ref type="bibr" target="#b41">[71,</ref><ref type="bibr">58,</ref><ref type="bibr" target="#b44">74,</ref><ref type="bibr">28,</ref><ref type="bibr" target="#b3">4]</ref> on generic object detection apply prior-art DenseBoxstyle head [23] to multiple feature pyramid levels. The feature pyramid is generated via feature pyramid network (FPN) <ref type="bibr" target="#b22">[35]</ref> and contains different levels that are gradually down-sampled but semantically enhanced. An anchor-free detection head is then attached to classify each pixel on the feature pyramid and predict axis-aligned bounding boxes (AABB). During training, assigning ground-truths to each feature pixels on the feature pyramid plays a key role. On each pyramid level, the centerness [58] is widely used. It is an indicator to value how far a pixel locates from the center of a ground-truth: the farther, the more likely it is to predict an inaccurate box, and the lower centerness score it gains. Across pyramid levels, various strategies are proposed to determine which level should be assigned, and they are grouped into scale-based and loss-based strategies. The scale-based <ref type="bibr" target="#b22">[35,</ref><ref type="bibr">28,</ref><ref type="bibr">50</ref>,58] assigns ground-truths to different levels in terms of their scales. The larger scale, the higher level is assigned so that the needs of receptive field and resolution of feature maps are balanced. The loss-based like Soft Selection <ref type="bibr" target="#b44">[74]</ref> assigns groundtruths by calculating their losses on all levels, and trains an auxiliary network that re-weights the losses. Our design, RetailDet, adopts the DenseBox style architecture but predicts the four corners of quadrilateral by 8-channel regression head. During training, we found the prior assignment strategies unsuitable for quadrilateral products, which is specified below. Centerness The previous definition of the centerness [58,74] is shown in Eq.1,</p><formula xml:id="formula_0">C F COS (p) = [ min(d l p , d r p ) max(d l p , d r p ) ? min(d t p , d b p ) max(d t p , d b p ) ] 0.5<label>(1)</label></formula><p>by the Eq.1 and <ref type="figure" target="#fig_2">Fig.5(a)</ref>, a location p keeps the same distance to the left/right boundaries (d l p = d r p ) and to the top/bottom boundaries (d t p = d b p ) will gain the highest centerness 1, and other pixels gain degraded score by Eq.1.</p><p>Limitation When adopting the same centerness to quadrilaterals, as shown in <ref type="figure" target="#fig_2">Fig.5(b)</ref>, the center can be far away from a distant corner, which leads to unbalanced regression difficulty and lack of receptive field from that corner.</p><p>Our Solution We first re-define the center as the center of gravity ( <ref type="figure" target="#fig_2">Fig.5(c)</ref>), because it is the geometric center and represents the mean position of all the points in the shape, which mitigates the unbalanced regression difficulties. We then propose Eq.2 to calculate the quad-centerness for any p,</p><formula xml:id="formula_1">C QU AD (p) = [ min(d l p , d l g ) max(d l , d l g ) ? min(d r p , d r g ) max(d r p , d r g ) ? min(d t p , d t g ) max(d t p , d t g ) ? min(d b p , d b g ) max(d b p , d b g ) ] 0.5 (2)</formula><p>where the d l/r/t/b g denotes the distances between the gravity center g and the left/right/top/bottom boundaries. The d l/r/t/b p denotes the distances between the p and the boundaries. If p locates on the gravity center, its quad-centerness gains the highest value as 1. Otherwise, it is gradually degraded (See <ref type="figure" target="#fig_2">Fig.5 (d)</ref>).</p><p>It is mentionable that when applied to AABB, Eq.2 is mathematically equivalent to Eq.1, which is proved in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Selection</head><p>The loss-based Soft Selection in <ref type="bibr" target="#b44">[74]</ref> outperforms scale-based strategies on generic objects because it assigns ground-truths to multiple levels and re-weights their losses. This is achieved by calculating losses for each object on all levels and using the losses to train an auxiliary network that predicts the re-weighting factors.</p><p>Limitation Instances per image are numerous in densely-packed retail scene, and Soft Selection is highly inefficient (5?slower) due to the auxiliary network.</p><p>Our Solution Can we maintain the merit of Soft Selection while accelerating the assignment? We approach this issue by mimicking the loss re-weighting mechanism of the auxiliary network using scale-based calculation. This is feasible because we find the Soft Selection, in essence, follows scale-based law (detailed in supplementary). Thus, we design Soft Scale (SS) in Eq.3,4,5,6. For an arbitrary shaped object O with area area O , SS assigns it to two adjacent levels l i and l j by Eq.3,4 and calculates the loss-reweighting factors F li , F lj by Eq.5,6.</p><formula xml:id="formula_2">l i = ?l org + log 2 ( ? area O /224)? (3) l j = ?l org + log 2 ( ? area O /224)? (4) F li = log 2 ( ? area O /224) ? ?log 2 ( ? area O /224)? (5) F lj = 1 ? F li (6)</formula><p>where 224 is the ImageNet pre-training size. Objects with exact area 224 2 is assigned to l org , in which case l i = l j = l org . If an object is with area 223 2 , SS assigns it to l org with F lorg = 0.994, and also to (l org ? 1) with F (lorg?1) = 0.006. In this work we fix l org to be level 5 of feature pyramid. SS operates rapidly as scale-based strategies and keeps the loss-reweighting like Soft Selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Simple Baseline for Product Matching</head><p>A number of directions can be explored on this new task, while in this paper, we design the simplest solution to verify the motivation: people glance and recognize the product, and if products looks similar, they further scrutinize the text (if appears) to make decision. To this end, we first apply a well-trained image classifier that extracts visual features f v gi from each gallery image g i and feature f v p from query image p, and calculate the cosine similarity between each pair (f v gi , f v p ) (termed as sim v i ). If the highest ranking value sim v 1 and the second highest</p><formula xml:id="formula_3">sim v 2 are close (sim v 1 -sim v 2 ? t),</formula><p>we then read on products and calculate the textual similarity (termed as sim t ) to make decision by Eq.7,</p><formula xml:id="formula_4">Decision = argmax i?[1,2] w ? sim t (g i , p) + (1 ? w) ? sim v i (7)</formula><p>where threshold t and coefficient w are tuned on validation set.</p><p>Our design focuses on how to calculate sim t . We denote the on-product texts obtained from ground-truth or OCR prediction as S = {s 1 , s 2 , . . . , s N } where  N varies. People may propose to utilize sequence-to-one models (like BERT <ref type="bibr" target="#b8">[9]</ref>) to encode S into a fixed length feature vector f ? R d . As shown in <ref type="figure" target="#fig_3">Fig.6 (a)</ref>, a text detector is followed by a text recognizer predicting n = 5 words, and the 5 words are fed into the BERT to encode a feature vector f p ? R d . For each gallery image g, the same process is operated to get a feature vector f g ? R d , and sim t (f p , f g ) is calculated by the cosine similarity.</p><p>But this design does not perform well because errors from OCR models (especially from text recognizer) are propagated to the BERT causing poor feature encoding. Moreover, the positional information of text boxes is lost in the sequence. So we design a new method in <ref type="figure" target="#fig_3">Fig.6 (b)</ref>. Rather than using the n recognized words, we use the n intermediate feature vectors from the text recognizer to mitigate propagated errors. For example, CREAM is confused as CRE4w, but the intermediate feature should maintain information on A and M , which is more robust than the false prediction. Each feature is then added by a 2D positional encoding <ref type="bibr" target="#b34">[46,</ref><ref type="bibr" target="#b2">3]</ref> whose calculation is based on the location of the corresponding text. It encodes the spatial information into the feature and it is predefined to keep the same dimension as the intermediate feature. Finally, we get a sequence that contains the n encoded features f 1?n . As shown we get S p = {f 1 p , f 2 p , f 3 p , f 4 p , f 5 p } from a query product and S g = {f 1 g , f 2 g , f 3 g , f 4 g } from a gallery reference. Inspired by the Hungarian Algorithm [31], we design Eq.8 to directly calculate the similarity between two sequence with varying length:</p><formula xml:id="formula_5">sim t (p, g) = sim t (S p , S g ) = max X n i=1 m j=1 ( f i p ? f j g |f i p | ? |f j g | X ij )<label>(8)</label></formula><p>where the X is a n ? m boolean matrix where j X ij = 1, i X ij = 1. Eq.8 maximizes the summation of cosine similarities from assigned feature pairs, and the assignment is optimized by X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmarking The Unitail</head><p>The implementation is detailed in the supplementary materials.</p><p>Quadrilateral Product Detection Along with the proposed RetailDet, we build baselines by borrowing the existing detectors capable of detecting quadrilaterals, mainly from the textual and aerial scenes <ref type="bibr" target="#b9">[10]</ref>. These methods can be grouped into segmentation based methods <ref type="bibr" target="#b46">[76,</ref><ref type="bibr">62,</ref><ref type="bibr">61,</ref><ref type="bibr" target="#b21">34,</ref><ref type="bibr">21]</ref> and regression based methods <ref type="bibr" target="#b28">[41,</ref><ref type="bibr" target="#b35">65,</ref><ref type="bibr">47]</ref>. Segmentation methods consider QUADs as per-pixel classification masks for each region of interest, while regression methods directly regress the bounding boxes. <ref type="table" target="#tab_3">Table 1</ref> shows their performances. Overall, regression based methods outperform segmentation based methods because most products can be well aligned by quadrilaterals, and the learning on segmentation mask involves extra difficulties. The RetailDet outperforms other detectors by a large margin. All detectors achieve degraded results in the cross-domain as opposed to the origin-domain, confirming that domain shift exists among stores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Detection &amp; Text Recognition</head><p>We benchmark the tasks of text detection and text recognition in <ref type="table" target="#tab_5">Table 2</ref> and   PW: methods use public available weights. NED: Normalized Edit Distance, the lower the better. Acc: word top-1 Accuracy, the higher the better.</p><p>Product Matching The product matching results are shown in <ref type="table">Table 4</ref>. With just texts across all 1454 categories, the method in <ref type="figure" target="#fig_3">Fig.6(b)</ref> reaches 31.71% and 47.81% on the Hard Example Track and the General Track, respectively. The result is convincing that only textual information is a strong representation for the product, but this new direction clearly requires further exploration.</p><p>Moreover, the textual information improves the regular visual classifier using the method proposed in Sec.4.2. In the hard example track, the improvement of textual information is significant (+1.76 ? 2.75%) since the similar-looking products which are hard for regular classifier could be easier distinguished by texts. In the general track, the improvement drops (+0.56 ? 0.94%) due to the limited ability of the textual matching algorithm. <ref type="table">Table 4</ref>: Benchmarking on the product matching task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc (%) Hard Example: Only text ( <ref type="figure" target="#fig_3">Fig.6 (b)</ref>  <ref type="bibr" target="#b32">44]</ref> 58.37 ResNet101+Text 60.52 (+2.15) General: Only Text( <ref type="figure" target="#fig_3">Fig.6 (a)</ref>) 30. <ref type="bibr" target="#b24">37</ref> Only Text( <ref type="figure" target="#fig_3">Fig.6 (b</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discussion</head><p>RetailDet <ref type="table" target="#tab_9">Table 5</ref> shows that the RetailDet achieves state-of-the-art results on product detection benchmark SKU110k <ref type="bibr">[18]</ref>. The SKU110k and the Unitail-Det share the same training images but with different annotations (QUAD vs AABB), and the origin-domain mAP on the SKU110k is 59.0 and 61.3 on the Unitail-Det (not included in <ref type="table">Table.</ref>1). The difference is mainly for the reason that QUAD is a natural fit for products as a smoother regression target, and it avoids densely-packed overlaps which makes post-processing easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difficulty of The Text Based Product Matching</head><p>Since the proposed matching pipeline is not end-to-end trained, errors are accumulated by the text detector (0.773 recall and 0.871 precision for DBNet) and text recognizer (0.772 for ABINet). Some products in the dataset do not contain texts (2%), and many words are partially or fully illegible. This requires further study on topics such as: attention mechanism on text regions, end-to-end framework for text spotting based product recognition, and one-shot learning based sequence matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we introduce the United Retail Datasets (Unitail), a large-scale benchmark aims at supporting well-aligned textually enhanced scene product recognition. It involves quadrilateral product instances, on-product texts, product matching gallery, and testing suite. We also design two baselines that take advantages of the Unitail and provide comprehensive benchmark experiments on various state-of-the-art methods.</p><p>17. Georgiadis, K., Kordopatis-Zilos, G., Kalaganis, F., Migkotzidis, P., Chatzilari, E., Panakidou, V., Pantouvakis, K., Tortopidis, S., Papadopoulos, S., Nikolopoulos, S., Kompatsiaris, I. International Conference on Graphic and Image Processing (2015) 60. Wada, K.: labelme: Image polygonal annotation with python. https://github. com/wkentaro/labelme (2018) 61. Wang, W., Xie, E., Li, X., Hou, W., Lu, T., Yu, G., Shao, S.: Shape robust text detection with progressive scale expansion network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9336-9345 (2019) 62. Wang, W., Xie, E., Song, X., Zang, Y., Wang, W., Lu, T., Yu, G., Shen, C.: Efficient and accurate arbitrary-shaped text detection with pixel aggregation network. In: ICCV. pp. 8439-8448 (2019) 63. Wei, X., Cui, Q., Yang, L., Wang, P., <ref type="bibr">Liu</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Annotation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Unitail-Det</head><p>Annotation Method We consider quadrilaterals as proper fits to products. The bounding box is common for localizing objects and reflects their shape in detection tasks. Axis-aligned rectangles are popular because they satisfy the minimum requirement for learning targets with minimum labeling efforts. Annotations with more accurate localization and appearance alignment are needed in this task. While segmentation masks are another level of accuracy for annotating scene objects, they are not cost-friendly, and the direct regression on bounding boxes is easier for densely-packed products than the segmentation methods verified in the benchmark.</p><p>To ensure the quality of annotation, annotators follow a strictly defined guide. Illustrated in <ref type="figure" target="#fig_5">Fig.7</ref>, products localized far away from the camera and with a size less than 8 ? 8 pixels are not treated as positive. Instead, we annotate ignoring masks covering the distant regions for these products. Cuboid and cylinder (boxes, cans, bottles), as the majority in stores, enjoy normal quadrilaterals defined in the paper; spherical and cones whose corners are difficult to identify, along with irregularly shaped products and distorted bags are expected to be affine-transformed back into an upright position. In those cases, we first draw the minimum AABB to cover them and then adjust the four corners according to the camera perspective. Only the visible part is annotated if another product or tag blocks one product. Many products have multiple faces observable, and only the frontal face of the products is practically annotated. Labelme [60] is applied as an annotation tool. Although many existing methods <ref type="bibr" target="#b28">[41,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b43">73]</ref> re-order the regression targets (four corner points) to favour the loss convergence regardless of the original order in ground-truths, we define the first corner point (x1, y1) as the top-left corner.</p><p>Statistics <ref type="figure">Fig.8 (a)</ref> illustrates the number of QUADs in each image. The mean and standard deviation is 145 and 46, respectively. An image in QuadRetail may contain at least 5 QUADs and up to 744 QUADs. Despite the density, the overlap among QUADs is not severe due to the annotation standard. <ref type="figure">Fig.8 (b)</ref> illustrates the scales of QUADs. The average scale is 22393.7 (149.6 2 ) over the QuadRetail. The minimum and maximum are 17 2 and 1938 2 , respectively. The average image width is 2466.8, and the average height is 3288.1.</p><p>The aspect ratio of a rectangle is commonly defined as w h . We measure the aspect ratio of QUAD as in Eq.9</p><formula xml:id="formula_6">ratio = t ? b l ? r<label>(9)</label></formula><p>where t, b, l, r are lengths of top, bottom, left, and right edge, respectively. The interior angles of any convex QUAD add up to 360 degrees of arc. The standard deviation of these angles (std a ) is a qualified reflection of the QUAD shape. For rectangles, std a = 0. For a extreme QUAD looks close to a line segment, std a = 90. In the origin-domain of QuadRetail, the std a = 6.24. In the cross-domain, the std a = 12.73. This means the images from the cross-domain are from taken from tougher angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Unitail-OCR</head><p>Unitail-OCR consists of a gallery and a testing suite to support text detection, text recognition, and product recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation Method</head><p>We start by cropping products from the Unitail-Det cross-domain. Since the crops are in quadrilaterals, they are further transformed to form upright appearances. Products with scales larger than 15?15 are further selected. The categorization is organized by two hierarchical stages. In the first stage, ten supercategories (food, dairy, paper goods, canned, produce, clothing, Technology, Pharmacy, Care, other) are defined to classify the products coarsely. In the second stage, we first apply a strong pre-trained model trained with a bag of tricks to group the product into 6k clusters and correct them by human annotators. During the correction, we do not accept blurred products that are hard to identify. Many fine-grained categories rely on textual information, and the blurred ones that are difficult to read are filtered out. After the categorization, we label each product with word-level text boxes, following the annotation method in ICDAR2015 <ref type="bibr">[26]</ref>. We annotate 29681 text regions from 4466 products as quadrilateral text boxes. The bounding boxes are classified as legible or illegible. The alphanumeric transcriptions 0 ? 9, a ? z are annotated for the legible ones. A vocabulary covers all words in the gallery and the testing suite are attached with the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Baseline Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Off-the-shelf Algorithms</head><p>We conduct experiments using multiple codebases including mmdetection <ref type="bibr" target="#b4">[5]</ref> for FCOS, SAPD, ATSS; mmocr <ref type="bibr">[30]</ref> for DBNet, FCENet, PSENet, CRNN, NRTR, RobustScanner, SAR, SATRN, ABINet; AlphaRotate <ref type="bibr" target="#b37">[67]</ref> for RIDet and RS-Det; maskrcnn benchmark <ref type="bibr" target="#b26">[39]</ref> for maskrcnn and Gliding Vertex; timm [64] for efficientnet, ResNet, and ResNet-IBN. To be more specific, models for product detection tasks are trained on 4 NVIDIA Titan RTX GPUs with two images per GPU. For fair comparison, the training schedule is 12 epochs with an initial learning rate of 0.01 divided by 10 at the 9th and the 11th epoch. Unless otherwise specified, the input images are scaled to 1200 and randomly horizontally flipped without any other augmentation. The Convex Hull and Shoelace Formula are implemented in CUDA to calculate the exact IOU of QUADs. Up to 400 detections per image are allowed to evaluate. For text detection models, we respect their optimized training setting for ICDAR2015 based on their officially released paper or code, but change the input image size to 1333 ? 800. For text recognition, we finetune the publicly available weights on Unitial-OCR for 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 RetailDet and RetailDet++</head><p>Base Network Our design of base network applies prior-art DenseBox-style head [23] to multiple feature pyramid levels. The feature pyramid is generated via feature pyramid network (FPN) <ref type="bibr" target="#b22">[35]</ref> which utilizes a deep convolutional network as the backbone. As an image is fed into the backbone, several feature maps are extracted to compose the initial feature pyramid. This work adopts the ResNet family as the backbone, and the extracted feature maps are from C 3 to C 5 . The feature maps after FPN are denoted as P 3 , P 4 , P 5 . An anchor-free detection head is attached then. The head contains two branches. One is a binary classification branch to predict a heatmap for product/background. Another is a regression branch to predict the offset from the pixel location to the four corner points of the QUAD. Each branch consists of 3 stacks of convolutional layers followed by another c channel convolutional layer, where c equals to 1 for the classification branch and 8 for the regression branch.</p><p>Corner Refinement Module RetailDet++ is the RetailDet enhanced with Corner Refinement Module (CRM) and deeper backbone. Here, we introduce the CRM. For each predicted QUAD from the RetailDet, we get the locations of its four corners and center. Then we apply the bilinear interpolation to extract feature of 5 points (4 corners, one center) from the feature map generated by the 3rd stacked convolution in the regression branch. These features are concatenated and fed into a 1 ? 1 convolutional layer to predict the offsets between groundtruth and the former predictions. The same operation and convolution are also inserted in the classification branch to predict retail/background as a 2nd-stage classification. During testing, we combine the regression results from the two stages but only use the classification result from the first stage. CRM shares the spirits with Faster-RCNN[50], BorderDet[48] and Reppoints <ref type="bibr" target="#b38">[68]</ref>, but we find that the 5 points as mentioned above are enough for quadrilateral products, and the 2nd-stage classification helps training though not involved in testing.</p><p>Losses During training, we first shrink QUADs by a ratio ? = 0.3 according to the gravity centers. If one feature pixel locates inside the shrunk QUAD, the pixel is considered responsible for learning the ground-truth. We utilize f ocal loss <ref type="bibr" target="#b23">[36]</ref> for classification and SmoothL 1 loss for regression, and we reweight both losses by the production of quad-centerness and level reweighting factor F . The total loss is the summation of the classification and regression losses. If two-stage, additional f ocal loss and L 1 loss for CRM are added to the total loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Proof: Eq.2 Is Equivalent to Eq.2 on Rectangles</head><p>When QUAD is specialized to Rectangles, in Eq.2, d l g = d r g , d l p + d r p = 2d l g , so if d l p &lt;d l g , then d r p &gt;d l g = d r g ; if d l p &gt;d l g , then d r p &lt;d l g = d r g . Thus,</p><formula xml:id="formula_7">min(d l p ,d l g ) max(d l p ,d l g ) ? min(d r p ,d r g ) max(d r p ,d r g ) = min(d l p ,d r p ) d l g ? d r g max(d l p ,d r p ) = min(d l p ,d r p ) max(d l p ,d r p )</formula><p>, similarly to d t and d b , then, Eq.2 is mathematically equivalent to Eq.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Analysis on Soft Selection</head><p>Soft Selection is a loss-based strategy, where training losses of ground-truths indicate their pyramid level. It first assigns each object to all pyramid levels P 3 , P 4 , P 5 and calculates loss l for each level P l . l=3,4,5. Then, the level that produces the minimal loss is converted to a one-hot vector, i.e., (1,0,0) if the minimal loss is from P 3 ; and (0,1,0) if it is from P 4 , and so on. The vector is used as the ground-truth to train an auxiliary network that simultaneously predicts a vector (F 3 , F 4 , F 5 ). Each element F l is a down-weighting factor for loss l . The final loss of each object is l (F l ? loss l ).</p><p>By Soft Selection, the minimal loss from level l indicates that the auxiliary network is trained to generate a relatively larger F l , but we find the loss not independent of scales. On the contrary, object scale inherently determines which level will produce the minimal loss. We claim the reason as follows. First, when assigning objects (e.g. object A with size 8 ? 8 and B with size 16 ? 16) to pyramid, their regression targets (denoted as T A , T B ) are normalized by the level stride. Specifically, on a lower level like P 3 , the target is divided by stride 8, while on a higher level like P 4 , the target is divided by 16, and so on. Therefore, when assigning A to P 3 and P 4 , T A is 1 ? 1 and 0.5 ? 0.5, respectively; when assigning B, T B is 2 ? 2 and 1 ? 1, respectively. Note that all levels share the detection head. Apparently, the combination of T A = 1 ? 1 and T B = 1 ? 1  <ref type="table">Table 7</ref>: Detection performance on SKU110k-r.</p><p>leads to the smallest regression difficulty for the regression head. Naturally, it produces minimal regression losses, which means the smaller object is assigned to a lower level. Second, since A has a smaller scale, it requires more local finegrained information beneficial for classification, which is more available from high-resolution lower levels. In comparison, B has a larger scale and needs a larger receptive field, which is more available from higher levels. Therefore, the "loss-based" Soft Selection, in essence, follows the scale-based law. Nevertheless, why does Soft Selection outperforms scale-based strategies? We credit the improvement to its loss-reweighting mechanism. This mechanism involves multiple levels during training and reweights the loss in terms of the regression and classification difficulties, making optimization easier. Since the pyramid is discrete, if an object scale falls into the gap of two adjacent levels, both levels' difficulties will be similar. The auxiliary network has opportunities to learn to predict proper F l for both levels. The analysis motivates us to abandon the auxiliary network and design Soft Scale (SS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 RetailDet</head><p>On SKU110k-R The result of RetailDet on SKU110k-R is compared with other methods in <ref type="table">Table.</ref>7. RetailDet outperforms the state-of-the-art detectors CenterNet <ref type="bibr" target="#b42">[72]</ref>, DRN <ref type="bibr" target="#b33">[45]</ref> and CFA [19] on SKU110k-r where products are in RBOX style. Following CFA, we use multi-scale training.</p><p>Ablation <ref type="table">Study Table.</ref>8 shows the improvement of each component brings to RetailDet. The Quad-Centerness (QC), Soft-Scale (SS) and Corner Refinement Module (CRM) gradually improve the mAP by 2.1, 1.0, 2.4 in the origin domain and 1.8, 0.6, 3.2 in the cross domain. And the improvement is consistent under different IOU thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Inference Speed</head><p>We report the inference speed of the key models in <ref type="table">Table 9</ref> Base QC SS CRM mAP AP50 AP75 ? 58.   <ref type="table">Table 9</ref>: Speed tested on single 2080Ti. * pipeline is accelarated without losing accuracy by applying only text models on visually low-confidence products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Qualitative Results</head><p>We visualize the product detection results in <ref type="figure" target="#fig_7">Fig.9, Fig.10, and Fig.11</ref>. The detector is our RetailDet two-stage variant with ResNext101. The average testing speed is 2.8FPS. We only visualize the QUADs with confident scores higher than 0.3. We show some failure cases in the cross domain in <ref type="figure">Fig.12.</ref> (a)(b) Unseen categories. (c)(d) Tough camera angles.</p><p>We show the OCR results from DBNet and ABINet in <ref type="figure" target="#fig_0">Fig.13</ref>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Unitail-Det statistics. Left: instance density. Right: instance scale</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Unitail-OCR statistical graphic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>(a): C F COS on AABB. (b): C F COS on QUAD. (c) and (d): C QU AD .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>(a) Pipeline with BERT encoded features, (b) Pipeline with positional encoding and Hungarian Algorithm based textual similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>: Products-6k: A large-scale groceries product recognition dataset. In: The 14th PErvasive Technologies Related to Assistive Environments Conference. p. 1-7. PETRA 2021, Association for Computing Machinery, New York, NY, USA (2021). https://doi.org/10.1145/3453892.3453894, https://doi.org/10.1145/3453892.3453894 18. Goldman, E., Herzig, R., Eisenschtat, A., Goldberger, J., Hassner, T.: Precise detection in densely packed scenes. In: Proc. Conf. Comput. Vision Pattern Recognition (CVPR) (2019) 19. Guo, Z., Liu, C., Zhang, X., Jiao, J., Ji, X., Ye, Q.: Beyond bounding-box: Convexhull feature adaptation for oriented and densely packed object detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 8792-8801 (June 2021) 20. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in natural images. In: IEEE Conference on Computer Vision and Pattern Recognition (2016) 21. He, K., Gkioxari, G., Doll?r, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision. pp. 2961-2969 (2017) 22. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016) 23. Huang, L., Yang, Y., Deng, Y., Yu, Y.: Densebox: Unifying landmark localization with end to end object detection. arXiv preprint arXiv:1509.04874 (2015) 24. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and artificial neural networks for natural scene text recognition. In: Workshop on Deep Learning, NIPS (2014) 25. Jund, P., Abdo, N., Eitel, A., Burgard, W.: The freiburg groceries dataset. CoRR abs/1611.05799 (2016), http://arxiv.org/abs/1611.05799 26. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanov, A., Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F., Uchida, S., Valveny, E.: Icdar 2015 competition on robust reading. In: Proceedings of the 2015 13th International Conference on Document Analysis and Recognition (ICDAR). p. 1156-1160. ICDAR '15, IEEE Computer Society, USA (2015). https://doi.org/10.1109/ICDAR.2015.7333942, https://doi. org/10.1109/ICDAR.2015.7333942 27. Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., Bigorda, L.G.i., Mestre, S.R., Mas, J., Mota, D.F., Almaz?n, J.A., de las Heras, L.P.: Icdar 2013 robust reading competition. In: 2013 12th International Conference on Document Analysis and Recognition. pp. 1484-1493 (2013). https://doi.org/10.1109/ICDAR.2013.221 28. Kong, T., Sun, F., Liu, H., Jiang, Y., Shi, J.: Foveabox: Beyond anchor-based object detector. arXiv preprint arXiv:1904.03797 (2019) 29. Koubaroulis, D., Matas, J., Kittler, J.: Evaluating colour-based object recognition algorithms using the soil-47 database. In: in Asian Conference on Computer Vision. pp. 840-845 (2002) 30. Kuang, Z., Sun, H., Li, Z., Yue, X., Lin, T.H., Chen, J., Wei, H., Zhu, Y., Gao, T., Zhang, W., Chen, K., Zhang, W., Lin, D.: Mmocr: A comprehensive toolbox for text detection, recognition and understanding. arXiv preprint arXiv:2108.06543 (2021) 47. Qian, W., Yang, X., Peng, S., Yan, J., Guo, Y.: Learning modulated loss for rotated object detection. Proceedings of the AAAI Conference on Artificial Intelligence 35(3), 2458-2466 (May 2021), https://ojs.aaai.org/index.php/AAAI/article/ view/16347 48. Qiu, H., Ma, Y., Li, Z., Liu, S., Sun, J.: Borderdet: Border feature for dense object detection. CoRR abs/2007.11056 (2020), https://arxiv.org/abs/2007.11056 49. Redmon, J., Farhadi, A.: Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767 (2018) 50. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. pp. 91-99 (2015) 51. Risnumawan, A., Shivakumara, P., Chan, C.S., Tan, C.L.: A robust arbitrary text detection system for natural scene images. https://doi.org/https://doi.org/10.1016/j.eswa.2014.07.008, https://www. sciencedirect.com/science/article/pii/S0957417414004060 52. Rocha, A., Hauagge, D.C., Wainer, J., Goldenstein, S.: Automatic fruit and vegetable classification from images. Computers and Electronics in Agriculture 70(1), 96-104 (2010). https://doi.org/https://doi.org/10.1016/j.compag.2009.09.002, https://www.sciencedirect.com/science/article/pii/S016816990900180X 53. Rong, T., Zhu, Y., Cai, H., Xiong, Y.: A solution to product detection in densely packed scenes (2021) 54. Sheng, F., Chen, Z., Xu, B.: Nrtr: A no-recurrence sequence-to-sequence model for scene text recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 781-786. IEEE (2019) 55. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-basedsequence recognition and its application to scene text recognition. IEEE transactions on pattern analysis and machine intelligence (2016) 56. Singh, A., Pang, G., Toh, M., Huang, J., Galuba, W., Hassner, T.: TextOCR:Towards large-scale end-to-end reasoning for arbitrary-shaped scene text (2021) 57. Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. CoRR abs/2104.00298 (2021), https://arxiv.org/abs/2104.00298 58. Tian, Z., Shen, C., Chen, H., He, T.: Fcos: Fully convolutional one-stage object detection. arXiv preprint arXiv:1904.01355 (2019) 59. Varol, G., Kuzu, R.: Toward retail product recognition on grocery shelves. In:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Annotation examples of the Unitail-Det. Top-left: small products are ignored by masks (red bounded regions). Top-right: frontal faces represent products. Bottom: quadrilaterals on irregular-shaped products.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 (Fig. 8 :</head><label>88</label><figDesc>c) illustrates the aspect ratio. Most ratios are around 0.3 -0.6, which is in line Histograms. (a) Instance density. (b) Instance scales. (c) Instance aspect ratio with the practical observation. There are also QUADs with extreme AR (&lt;0.05) and (&gt;38).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Visualization of high difficulty detection result from the RetailDet.28 F. Chen et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Visualization of medium difficulty detection result from the RetailDet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Visualization of low difficulty detection result from the RetailDet. Visualization of failure cases in cross domain. (a)(b) Unseen categories. (c)(d) Tough shooting angles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 :</head><label>13</label><figDesc>Visualization of OCR results. Upper ones of each section are from models trained on ICDAR2015, and lower ones are on Unitail-OCR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>The United Retail Datasets (Unitail)</head><label></label><figDesc></figDesc><table><row><cell>Data</cell><cell>Unitail-Det</cell><cell></cell><cell>Unitail-OCR</cell><cell></cell></row><row><cell>Tasks</cell><cell>Product Detection</cell><cell>Text Detection</cell><cell>Text Recognition</cell><cell>Product Matching</cell></row><row><cell></cell><cell></cell><cell></cell><cell>pullups</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>fun</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>fast</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>easy</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>3T4T</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>5382600</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>pullups</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>fun</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>fast</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>easy</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>4T5T</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>5382800</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Multiple product recognition datasets have been built over the past decades. Early ones have limitations in the diversity of categories and amount of images, such as the SOIL-47 [29] containing 47 categories, the Supermarket Produce Datasets [52] with 15 categories, and the Freiburg Groceries [25] covering 25 categories. Recent collections like Products-6K[17] and Products-10K[1] focus on large-scale data, which satisfy the training of deep learning algorithms. AliProducts</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Text Detector Text Recognizer Intermediate Features Text Recognizer (b)</head><label></label><figDesc></figDesc><table><row><cell>Product P</cell><cell>Text Detector</cell><cell>Word Ground-truth</cell><cell>Text Recognizer</cell><cell>haagen d chocolate ice cre4w haagendazs chocolate SP Sg</cell><cell>BERT BERT</cell><cell>fg di m =d fP d im = d</cell><cell>Cosine Similarity</cell></row><row><cell>Gallery g</cell><cell></cell><cell></cell><cell></cell><cell>ice cream</cell><cell></cell><cell cols="2">(a)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Intermediate Features</cell><cell>add</cell><cell>SP (After Encoding)</cell><cell></cell></row><row><cell>Product P</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fP 1 fP 2 fP 3</cell><cell>Sim t</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2D</cell><cell>fP 4</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Positional</cell><cell>fP 5</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Encoding</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hungarian Algorithm</cell></row><row><cell></cell><cell>Text Location</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sg (After Encoding)</cell><cell></cell></row><row><cell></cell><cell>Ground-truth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fg 1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fg 2</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fg 3</cell><cell></cell></row><row><cell>Gallery g</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>fg 4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Benchmarking product detection on the Unitail. All methods are trained and tested under same setting. g-mAP is the geometric mean of mAPs.</figDesc><table><row><cell>Origin-Domain</cell><cell>Cross-Domain</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>, respectively. For each of the listed algorithms, it is trained under two setting: one is following a common setting that trained on SynthText[20] and ICDAR2015(IC15)[26]  for text detection and on Synth90K[24] and SynthText for text recognition, another is trained/finetuned on the Unitail. As shown all algorithms achieve better performance if trained on the Unitail, this verifies that texts in the retail domain are better handled by the proposed dataset.</figDesc><table><row><cell>Method</cell><cell>Training Set</cell><cell>R</cell><cell>P hmean</cell></row><row><cell cols="4">DBNet[34] SynthText+IC15 0.541 0.866 0.666</cell></row><row><cell>DBNet</cell><cell>Unitail</cell><cell cols="2">0.773 0.871 0.819</cell></row><row><cell cols="4">FCENet[76] SynthText+IC15 0.420 0.745 0.538</cell></row><row><cell>FCENet</cell><cell>Unitail</cell><cell cols="2">0.795 0.857 0.825</cell></row><row><cell cols="4">PSENet[61] SynthText+IC15 0.421 0.750 0.539</cell></row><row><cell>PSENet</cell><cell>Unitail</cell><cell cols="2">0.705 0.789 0.744</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Benchmarking</cell></row><row><cell>text detection on Uni-</cell></row><row><cell>tail. P and R stand for</cell></row><row><cell>Precision and Recall,</cell></row><row><cell>respectively. hmean is</cell></row><row><cell>the harmonic mean of</cell></row><row><cell>precision and recall.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>:</cell><cell>Bench-</cell></row><row><cell cols="2">marking text recog-</cell></row><row><cell cols="2">nition on Unitail.</cell></row><row><cell cols="2">S90k: Synth90k. ST:</cell></row><row><cell>SynthText.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Results on SKU110k. Re-</cell></row><row><cell cols="2">tailDet++ is an enhanced variant</cell></row><row><cell cols="2">where a box refinement module is</cell></row><row><cell cols="2">added (see supplementary for details)</cell></row><row><cell>Method</cell><cell>mAP</cell></row><row><cell>RetinaNet+EM[18]</cell><cell>49.2</cell></row><row><cell>FSAF [75]</cell><cell>56.2</cell></row><row><cell>CenterNet [11]</cell><cell>55.8</cell></row><row><cell>ATSS [71]</cell><cell>57.4</cell></row><row><cell>FCOS [58]</cell><cell>54.4</cell></row><row><cell>SAPD [74]</cell><cell>55.7</cell></row><row><cell>Faster-RCNN+FPN [50]</cell><cell>54.0</cell></row><row><cell>Reppoints [68]</cell><cell>55.6</cell></row><row><cell cols="2">Cascade-RCNN+Rong [53] 58.7</cell></row><row><cell>RetailDet++ (Ours)</cell><cell>59.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Comparison of related benchmarks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Ablation study on the Unitail-Det val set. QC: Quad-Centerness. SS: Soft Scale. CRM: Corner Refinement Module.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>Task</cell><cell>FPS</cell></row><row><cell>RetailDet</cell><cell cols="3">ResNet101 Product Detection 6.3</cell></row><row><cell>DBNet</cell><cell>ResNet50</cell><cell cols="2">Text Detection 27.6</cell></row><row><cell>ABINet</cell><cell cols="3">ResNet-ABI Text Recognition 41.4</cell></row><row><cell>Visual+Text*</cell><cell>-</cell><cell cols="2">Product Matching 65.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Products-10k: A large-scale product recognition dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/2008.10545</idno>
		<ptr target="https://arxiv.org/abs/2008.10545" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rethinking object detection in retail stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno>abs/2005.12872</idno>
		<ptr target="https://arxiv.org/abs/2005.12872" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ncms: Towards accurate anchor free object detection through l2 norm calibration and multi-feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="page">103050</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Mmdetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Weakly supervised learning with side information for noisy labeled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.11586" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">CoRR abs</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Total-text: A comprehensive dataset for scene text detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<idno>abs/1710.10400</idno>
		<ptr target="http://arxiv.org/abs/1710.10400" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">ABO: dataset and benchmarks for real-world 3d object understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luthra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dideriksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/2110.06199</idno>
		<ptr target="https://arxiv.org/abs/2110.06199" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>abs/1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Object detection in aerial images: A large-scale benchmark and challenges</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08189</idno>
		<title level="m">Centernet: Object detection with keypoint triplets</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-009-0275-4</idno>
		<ptr target="https://doi.org/10.1007/s11263-009-0275-4" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mvtec D2S: densely segmented supermarket dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Follmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>B?ttger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?rtinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>K?nig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<idno>abs/1804.08292</idno>
		<ptr target="http://arxiv.org/abs/1804.08292" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing products: A per-exemplar multi-label image classification approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Floerkemeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<editor>Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="440" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fine-grained product class recognition for assisted shopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mircic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>S?r?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Floerkemeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mattern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/ARXIV.1510.04074</idno>
		<ptr target="https://arxiv.org/abs/1510" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<idno type="DOI">https:/onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On recognizing texts of arbitrary shapes with 2d self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno>CoRR abs/1910.04396</idno>
		<ptr target="http://arxiv.org/abs/1910.04396" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time scene text detection with differentiable binarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11474" to="11481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Detecting curve text in the wild: New dataset and new solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1712.02170</idno>
		<ptr target="http://arxiv.org/abs/1712.02170" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/maskrcnn-benchmark" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Insert date here</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognizing groceries in situ using in vitro training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2007.383486</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2007.383486" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimization for arbitrary-oriented object detection via representation invariance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/LGRS.2021.3115110</idno>
		<ptr target="https://doi.org/10.1109/LGRS.2021.3115110" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scene Text Recognition using Higher Order Language Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.26.127</idno>
		<ptr target="https://hal.inria.fr/hal-00818183" />
	</analytic>
	<monogr>
		<title level="m">BMVC -British Machine Vision Conference</title>
		<meeting><address><addrLine>BMVA, Surrey, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Product verification using ocr classification and mondrian conformal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Oucheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>L?fstr?m</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2021.115942</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0957417421012963" />
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page">115942</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dynamic refinement network for oriented and densely packed object detection pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<idno>abs/1802.05751</idno>
		<ptr target="http://arxiv.org/abs/1802.05751" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06677</idno>
		<title level="m">Alpharotate: A rotation detection benchmark using tensorflow</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11490</idno>
		<title level="m">Reppoints: Point set representation for object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Detecting texts of arbitrary orientations in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2012.6247787</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2012.6247787" />
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1083" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robustscanner: Dynamically enhancing positional clues for robust text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno>abs/1904.07850</idno>
		<ptr target="http://arxiv.org/abs/1904.07850" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">EAST: an efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno>abs/1704.03155</idno>
		<ptr target="http://arxiv.org/abs/1704.03155" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Soft anchor-point object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="91" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for singleshot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Fourier contour embedding for arbitrary-shaped text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
