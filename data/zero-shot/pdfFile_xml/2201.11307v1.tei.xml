<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dissecting the impact of different loss functions with gradient surgery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><forename type="middle">Xuan</forename><surname>Microsoft</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Geroge Washington University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Geroge Washington University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
							<email>pless@gwu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Geroge Washington University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dissecting the impact of different loss functions with gradient surgery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pair-wise loss is an approach to metric learning that learns a semantic embedding by optimizing a loss function that encourages images from the same semantic class to be mapped closer than images from different classes. The literature reports a large and growing set of variations of the pair-wise loss strategies. Here we decompose the gradient of these loss functions into components that relate to how they push the relative feature positions of the anchor-positive and anchor-negative pairs. This decomposition allows the unification of a large collection of current pair-wise loss functions. Additionally, explicitly constructing pair-wise gradient updates to separate out these effects gives insights into which have the biggest impact, and leads to a simple algorithm that beats the state of the art for image retrieval on the CAR, CUB and Stanford Online products datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Metric Learning trains networks to map semantically related images to similar locations in an embedding space. Metric learning is useful in extreme classification settings when there are so many classes and limited embedding size that standard approaches fail, when there is a need to compare features extracted from images in unseen classes, or when there may be incomplete labels that allow a system to know that images come from the same or different classes without knowing what those classes are.</p><p>In this domain, one popular pair-wise loss to train a network is Triplet Loss. Triplets are three images comprising an anchor image, a positive image from the same class, and a negative image from a different class. The network is trained with a loss function that penalizes situations where the anchor-negative pair is closer than the anchor-positive pair. Many variations of this basic approach explore ways to choose which triplets should be included in the optimization or how much they should be weighted, whether the optimization should consider distances or angles between the embedded vectors, and what specific loss function should drive the scoring of a particular triplet.</p><p>One recent work gave a large-scale analysis of many of these variations <ref type="bibr" target="#b10">[11]</ref> and found that a substantial fraction of the reported performance variation disappears with careful matching of experimental conditions. In this work, we propose a further unifying analysis of these approaches, but explicitly consider <ref type="bibr">Figure 1</ref>. To realize a desired embedding space, a common method is to design a loss function which can be calculated on deep learning platforms such as PyTorch and TensorFlow(Red). The auto-grad mechanism on the platforms automatically calculates the gradient to update the model parameters to forming the desired embedding space(Blue). In practice, the goal of deep metric learning is about optimizing the separation or clustering of feature points extracted from imagery, and the loss function is a somewhat indirect approach to reach that goal, while the gradient more directly affects the update of the feature extraction. We propose the method to directly design the gradient to train models.</p><p>how the pair-wise loss function attempts to affect the embedding location of the anchor, positive, and negative examples. Different pair-wise loss functions have gradients that directly affect the desired locations of each embedded location in different ways. Those gradients are different in terms of the direction the anchor, positive and negative examples are pushed, the overall importance or weight given to different triplets, and the relative importance or weight given to the anchor-positive vs. the anchor negative pairs.</p><p>In addition to the analysis, we exploit the fact that PyTorch <ref type="bibr" target="#b11">[12]</ref> allows for the programmatic specification of gradients, allowing us to explicitly control the above gradient components, and then supports back-propagation to encourage the low-level features to move in this way. This flexibility allows us to explore the relative contributions of these components of the gradient and better understand what is and is not important in the optimization. Finally, we demonstrate the potential to directly modify the gradient components to train models for deep metric learning tasks instead of loss function modification.</p><p>The three main contributions 1 of this are:</p><p>? a direct gradient framework to create a unified analysis of many recent triplet and pair-wise loss functions in terms of their gradients, ? an experimental analysis showing how different choices for components of the gradient affects model performance, ? a deeper understanding of the practical effects of defining a loss based on the Euclidean metric compared with the cosine similarity metric, and ? an integration of the best choice of each component to create a new gradient rule that outperforms the current state-of-art result for image retrieval by a clear margin across multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>There are many loss functions that have been proposed to solve the deep metric learning problem. Pair-wise loss functions such as contrastive loss <ref type="bibr">[2]</ref>, binomial deviance loss <ref type="bibr" target="#b23">[24]</ref>, lifted structure loss <ref type="bibr" target="#b16">[17]</ref> and multi-similarity loss <ref type="bibr" target="#b18">[19]</ref> penalize pairs of same label instances if their distance is large and pairs of different label instances if their distance is small. The triplet loss function <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref> and its variants such as circle loss <ref type="bibr" target="#b17">[18]</ref> form a triplet that contains anchor, positive and negative instances, where the anchor and positive instance share the same label, and anchor and negative instance share different labels. These loss functions have losses encouraging the anchor-positive distance to be smaller than anchornegative distance. Other variants of triplet loss integrate more negative instances into a triplet, such as N-Pair loss <ref type="bibr" target="#b15">[16]</ref>. Proxy loss <ref type="bibr" target="#b9">[10]</ref> defines for each class a learnable anchor as a proxy. During the training, each instance is directly pulled to its proxy and pushed away from the proxy location for other classes.</p><p>Due to the explosion of many new loss functions, issues underlying the fair comparison for these loss functions have been raised in <ref type="bibr" target="#b10">[11]</ref>. This paper works hard to re-implement many works before 2019. It tries to fix settings such as network architecture, optimizer and image prepossessing and compares different methods apple to apple. This gives a relatively clear comparison of many loss functions but does not try to explore why some methods are superior to others.</p><p>Recent works such as Multi-Similarity Loss and Circle Loss <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref> have started with standard triplet loss formulations and adjust the gradient of loss functions to give clear improvements with very simple code modifications. These works all find an explicit loss function whose gradient creates the desired loss function. In some cases, like the current state-of-the-art approach across many datasets <ref type="bibr" target="#b18">[19]</ref>, the updated loss function for one triplet includes the relative similarity between the anchorpositive and the anchor and other examples for the anchor's class. This more complicated loss function and more complicated gradient may cause subtle challenges in the optimization process.</p><p>Other strategies start with a desired gradient weighting function and integrate the desired gradients to solve for a loss function whose gradient has the appropriate properties. This is often limited to simple weighting strategies, such as the simple linear form in <ref type="bibr" target="#b17">[18]</ref> and simple gradient removal for positive pairs when triplets contain hard negative in <ref type="bibr" target="#b21">[22]</ref>, because it may be hard to find the loss function whose gradient is consistent with complex weighting strategies.</p><p>The discussion of explicitly updating the direction of the gradient has been introduced in <ref type="bibr" target="#b8">[9]</ref>. They encourage the anchor-positive and anchor-negative directional updates to be orthogonal (so they don't cancel each other), but include this as a "direction regularization", which does not enforce orthogonality.</p><p>The most related work is P2Sgrad <ref type="bibr" target="#b26">[27]</ref>, the author analyzes the gradient in the family of margin-based softmax loss and directly modified the gradient with the cosine similarity for better optimization. Comparing to P2Sgrad, our work focuses on the triplet and pair-wise loss functions.</p><p>The framework in this paper directly explore the space of desired gradient updates as shown in <ref type="figure" target="#fig_5">Figure 1</ref>. By not limiting ourselves to designing a loss function with appropriate gradients, we can be more explicit in experimentally dissecting the effects of different parts of the gradient. Furthermore, we can recombine the gradient terms that are experimentally most useful in a form of gradient surgery <ref type="bibr" target="#b24">[25]</ref> that very slightly alters existing algorithms to give improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Role of the Gradient in Metric Learning</head><p>We define a collection of terms for how a batch of images affects a network. Let X be a batch of input images, f be the L2 normalized feature vectors of the images extracted by the network, l be loss value for the batch, ? be the parameters of the network, ? be the learning rate, f ? (?) be the mapping function of the network, and L(?) be loss function. In the forward training step, the expression is:</p><formula xml:id="formula_0">l =L(f), where f =f ? (X)<label>(1)</label></formula><p>.</p><p>The network weights are updated as:</p><formula xml:id="formula_1">? t+1 =? t ?? ?L ?f ?f ??<label>(2)</label></formula><p>This equation highlights that the gradient of the loss function (rather than the loss function itself) directly affects how the model updates its parameters. Therefore, explicitly exploring the gradient is a useful path to exploring network learning behavior.</p><p>We decompose the gradient into two terms, ?L ?f and ?f ?? . The first term represents how changing the embedded feature location affects the loss, and this is the term explored most in detail in this work. The second term represents how model parameter (network weight) changes affect the feature embedding. In a modern deep network with multiple layers, the second term is always expanded with the multiplication of multiple terms for each layer because of the derivative chain rule.</p><p>In the following discussion, we focus on the particular forms of the first term in many triplet and pair-wise loss functions and then proposed to directly set and design the first term for model training. In Section 3.1, as an example, two commonly used triplet losses are decomposed into components and then those components are categorized into three parts. Then, Section 3.2, 3.3 and 3.4 extend the analysis to more existing loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gradient of Triplet Losses</head><p>Given a triplet, (f a ,f p ,f n ), there are two commonly used triplet losses in the literature, a triplet loss based on Euclidean distance:</p><formula xml:id="formula_2">L euc =max(D 2 ap ?D 2 an +m,0),<label>(3)</label></formula><p>where D ap = f a ? f p , D an = f a ? f n are the distances between the anchor-positive and the anchor-negative pairs, and m is a distance margin. A second common triplet loss is the triplet loss based on cosine similarity with NCA <ref type="bibr">[1]</ref>:</p><formula xml:id="formula_3">L cos =?log( exp(?S ap ) exp(?S ap )+exp(?S an ) )<label>(4)</label></formula><p>where S ap =f a T f p , is the cosine similarity computed as the dotproduct of the normalized anchor feature and the normalized feature from the positive example, the anchor-negative is computed in the same way, S an =f a T f n and ? is the scaling parameter. When comparing these two loss functions, their substantial differences make it challenging to determine how the loss affects performance. One loss is based on the Euclidean distance combined with a hinge function, while the other uses cosine similarity along with a negative log softmax function to combine the anchor-positive and anchor-negative pairs. Looking at the gradients of these loss functions makes the difference more clear. In triplet loss based on Euclidean distance, if the loss is greater than 0, its gradient can be derived from Equation 3 as:</p><formula xml:id="formula_4">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?L euc ?f p =2 f p ?f a e euc p ?L euc ?f n =2 f a ?f n e euc n ?L euc ?f a =?2 f a ?f p e euc p ?2 f n ?f a e euc n<label>(5)</label></formula><p>Being explicit about this gradient allows us to name the direction that the positive example is being pulled to anchor example as e euc p , and these are unit vectors defined as: e euc p = fp?fa fp?fa , with corresponding directions for the negative example, e euc n = fa?fn fa?fn . The gradient of the triplet loss function based on cosine similarity can also be derived from Equation 4 to give a unit direction and magnitude:</p><formula xml:id="formula_5">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?L cos ?f p = 1 1+exp(?(S ap ?S an )) ?e cos p ?L cos ?f n = 1 1+exp(?(S ap ?S an )) ?e cos n ?L cos ?f a = 1 1+exp(?(S ap ?S an ))</formula><p>?(e cos ap +e cos an )</p><p>where e cos p =?f a , e cos n =f a , e cos ap =?f p and e cos an =f n are the unit gradient directions.</p><p>Though both L euc and L cos contain different gradient components, those components can be categorized into two major parts: unit gradient direction for moving the feature and a scalar weight that affects the length of the gradient in that direction. The weight itself can be divided into two sub-parts: the weight related to all three features in a triplet f a , f p and f n (Triplet Weight), and the weight related to the positive pair f a and f p or negative pair f a and f n in a triplet (Pair Weight). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unit Gradient Direction</head><p>The first gradient component is the unit vector in the direction of the gradient, derived from how the loss function moves the relative configuration of the anchor, positive and negative features. We refer to the unit gradient direction of the two most common metrics Euclidean distance and cosine similarity as Euclidean direction e euc and cosine direction e cos . Recent work <ref type="bibr" target="#b8">[9]</ref>, also suggests to other directions, Euclidean orthogonal direction e euc?orth and cosine orthogonal direction e cos?orth .</p><p>Euclidean Direction(e euc ): In equation 5, the geometric explanation of Euclidean direction is to move the positive feature directly towards the anchor and move the negative feature directly away from the anchor, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The vector direction of the anchor image (not shown in the <ref type="figure">Figure)</ref>, is a combination of these directions.</p><p>Cosine Direction(e cos ): In equation 6, the geometric explanation of cosine direction on positive pair is to move the positive feature in the anchor feature direction and move the anchor in positive feature direction, and on negative pair is to move negative in the opposite of the anchor feature direction and move the anchor in the opposite of the negative feature direction as shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Orthogonal Direction(e euc?orth &amp; e cos?orth ): A direct gradient modification function 7 can be applied to both the Euclidean and cosine directions. This requires the negative pair to move in a direction orthogonal to the direction the positive pair is moving. This is constrained as:</p><formula xml:id="formula_7">e n ?(f a ?f p )=0 (7)</formula><p>This gradient was realized in recent work by <ref type="bibr" target="#b8">[9]</ref> who implicitly encourage the negative examples to move orthogonally to the anchor positive direction by adding regularizer in their loss function. Our approach is directly understanding the gradient direction for each example highlights the impact of this loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pair Weight</head><p>We define the pair-weight P , for the anchor-positive pair P + and anchor-negative pair P ? . The pair weight of cosine similarity P cos based triplet loss is a constant scaling parameter. This is useful as a baseline for comparison. For this case where both pair weights are set with constant 1, as:</p><formula xml:id="formula_8">P con + =P con ? =1;<label>(8)</label></formula><p>In Euclidean distance based triplet loss, the pair weight P euc is different for the anchor-positive and anchor-negative pairs:</p><formula xml:id="formula_9">P euc + = f a ?f p P euc ? = f a ?f n<label>(9)</label></formula><p>and indicates the pair weight is proportional to the distance between the anchor and the other element of the pair. Recent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref> argue that the weight for anchornegative pair should be large when they are close to each other. Otherwise, as mentioned in <ref type="bibr" target="#b21">[22]</ref>, the optimization will quickly converge to bad local minima. The solution in Circle loss <ref type="bibr" target="#b17">[18]</ref> is to apply a linear pair weight P lin : for negative pairs, the weight is large if the similarity is large and small if the similarity is small; for positive pairs, the weight is large if the similarity is small and small if the similarity is large:</p><formula xml:id="formula_10">P lin + =1?S ap P lin ? =S an<label>(10)</label></formula><p>Early work binomial deviance loss <ref type="bibr" target="#b23">[24]</ref> uses a similar pair weight but with a nonlinear sigmoid form P sig :</p><formula xml:id="formula_11">? ? ? ? ? ? ? P sig + = 1 1+exp(?(S ap ??)) P sig ? = 1 1+exp(??(S an ??))<label>(11)</label></formula><p>where ?, ? and ? are three hyper-parameters.</p><p>Multi-similar(MS) loss <ref type="bibr" target="#b18">[19]</ref> combines ideas from the lifted structure loss <ref type="bibr" target="#b16">[17]</ref> and binomial deviance loss <ref type="bibr" target="#b23">[24]</ref>, which includes not only the self-similarity of a selected pair but also the relative similarity from other pairs. The MS paper <ref type="bibr" target="#b18">[19]</ref> tries to find a loss function whose derivative fits the proposed pair weight. Because the relative similarity term involves additional examples (outside the triplet), this creates additional gradients relative to those examples, even though the stated purpose is to weigh the selected pair. Therefore, it's difficult to understand if the performance gain is coming from the proposed pair weight or from the gradients affecting the feature location of these other examples. By casting their work within our framework, we can decouple the pair-weighting and explore the impact of this term in isolation.</p><p>We follow the MS paper to cast their weighting function P sig?ms in our framework. Given a triplet, the self-similarity of the selected positive pair and negative pair are S ap and S an . The similarity of other positives and negatives to the anchor is considered as relative-similarity, noted as R ap i and R an j . In addition, <ref type="bibr" target="#b18">[19]</ref> also defines P and N be the sets of selected R ap i and R an j , where</p><formula xml:id="formula_12">P ={R ap i : R ap i &lt;max{S an ,R an j }+ } N ={R an j : R an j &gt;min{S ap ,R ap i }? } (12) ? ? ? ? ? ? ? ? ? P sig?ms + = 1 m sig + +exp(?(S ap ??)) P sig?ms ? = 1 m sig ? +exp(??(S an ??))<label>(13)</label></formula><p>where</p><formula xml:id="formula_13">m sig + = 1 |P| P exp(?(S ap ?R ap i )) m sig ? = 1 |N | N exp(??(S an ?R an j ))</formula><p>When m sig + =m sig ? =1 the pair weights simplify back to the sigmoid form in equation <ref type="bibr" target="#b10">11</ref>.</p><p>In practice, training MS loss needs to tune four hyperparameters ?, ?, ? and to fit different datasets, making the training not convenient and not efficient. With analysis on relative-similarity terms m sig + and m sig ? in the appendix, we define a clearer and parameter free version of pair weight called linear MS pair weight P lin?ms , which behaves similar to the original MS weight:</p><formula xml:id="formula_14">P lin?ms + =(1?m lin + )(1?S ap ) P lin?ms ? =(1+m lin ? )S an<label>(14)</label></formula><p>where</p><formula xml:id="formula_15">m lin + = 1 |P| P (S ap ?R ap i ) m lin ? = 1 |N | N (S an ?R an j )</formula><p>The triplet weight contains the similarity of both positive and negative pairs of a triplet, measuring whether a triplet is well separated or not. In Euclidean distance based triplet loss, the triplet weight (denoted as T ) is a constant indicating that every triplet will be treated equally. For the fair comparison for other triplet weights, we set constant weight 0.5.</p><p>T con =0.5 (15) In cosine similarity based triplet loss, the triplet weight is:</p><formula xml:id="formula_16">T cos = 1 1+exp(?(S ap ?S an ))<label>(16)</label></formula><p>T cos is rely on the difference of S ap and S an . When a triplet in a correct configuration, S ap ?S an &gt; 0, the triplet weight is small. Otherwise, the triplet weight will be large. In Circle loss <ref type="bibr" target="#b17">[18]</ref>, the triplet weight is:</p><formula xml:id="formula_17">T cir = 1 1+exp(?(S ap (2?S ap )?S 2 an ))<label>(17)</label></formula><p>Because T cos only considers the similarity difference S ap ?S an , some corner cases such triplet with both large S ap and S an or both small S ap and S an are not well treated. The idea of T cir is to introduce a non-linear mapping for S ap and S an in the exponential term in order to weight more on the corner cases. <ref type="figure" target="#fig_1">Figure 3</ref> shows the triplet weight diagram, a triplet visualization tool from <ref type="bibr" target="#b21">[22]</ref>, for T cos and T cir with ? =1. The equal weight line in T cos is straight lines with form S ap ? S an = const.. And the equal weight line in T cir is circular lines with form (S ap ?1) 2 +S 2 an =const.. Selectively Contrastive Triplet(SCT) loss <ref type="bibr" target="#b21">[22]</ref> selects triplets with hard negatives (the negative example in a triplet is closer to anchor than the positive example) and applies only contrastive loss to the hard negative pairs during the batch training. At gradient level, this approach is to remove the gradients from the anchor-positive pairs for triplets with hard negatives. We treat the selection as a masking operator on positive pair weight:</p><formula xml:id="formula_18">T sc1 (P + )= 0 if S an &gt;S ap P + others<label>(18)</label></formula><p>Because the decision boundary of triplets selection S an =S ap is a 1st order straight line, we note this masking operator is noted as T sc1 . Besides, we continue to extend the selection idea with Circle loss. The triplets in the corner cases can be also selected to only separate the negative pairs. Then, the decision boundary of the selection operator becomes a 2nd order circular line. We note it as T sc2 , </p><formula xml:id="formula_19">T sc2 (P + )= 0 if S ap (2?S ap )?S 2 an &gt;0.5 P + others<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Metric Learning Gradient Summary</head><p>In this section, we have derived ways to represent many previous loss functions in terms of their gradients. We have explicitly defined the gradients in terms of how the anchor, positive and negative are moved, defined them in terms of a unit vector in the direction of motion, a weight of anchor-positive term</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Direction PairWeight TripletWeight Triplet (Euclidean) <ref type="bibr" target="#b14">[15]</ref> e euc P euc T con Triplet (cosine) <ref type="bibr" target="#b22">[23]</ref> e cos P con T cos Circle loss <ref type="bibr" target="#b17">[18]</ref> e cos P lin T cir Binomial deviance <ref type="bibr" target="#b23">[24]</ref> e cos P sig T con MS loss <ref type="bibr" target="#b18">[19]</ref> e cos P sig?ms T con DR-MS loss <ref type="bibr" target="#b8">[9]</ref> e cos?orth P sig?ms T con SC triplet loss <ref type="bibr" target="#b21">[22]</ref> e cos P con T cos , T sc1 <ref type="table" target="#tab_3">Table 1</ref>. Triplet loss functions define a gradient on the embedded feature locations of the anchor, positive, and negative examples of the triplet. A large collection of recently proposed triplet loss functions (left) can be put into a unified framework by decomposing the gradient into the (unit) directions they impose on the features, and the weight of that gradient due to the properties of the anchor-positive and anchor-negative pairs, and the overall configuration of the triplet. This decomposition gives some insight into why some approaches give improved results, and provides a design space for choosing particular combinations of weights to optimize overall performance. and the anchor negative term and weight of the triplet overall. <ref type="table" target="#tab_3">Table 1</ref> shows how to map different combinations of gradient components into currently proposed loss functions. Section 5 gives explicit experiments to understand the isolated effects of these three parts of gradient component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment Settings</head><p>We run a set of experiments on the CUB200 (CUB) <ref type="bibr" target="#b19">[20]</ref>, CAR196 (CAR) <ref type="bibr" target="#b6">[7]</ref>, Stanford Online Products (SOP) <ref type="bibr" target="#b16">[17]</ref> and In-shop Cloth (In-shop) <ref type="bibr" target="#b7">[8]</ref> dataset. All experiments are run on the PyTorch platform <ref type="bibr" target="#b11">[12]</ref> with Nvidia Tesla V100 GPU, using ResNet <ref type="bibr">[4]</ref> architectures, pre-trained on ILSVRC 2012-CLS data <ref type="bibr" target="#b13">[14]</ref>. Training images augmented using a standard scheme (random horizontal flip and random crops padded by 10 pixels on each side), and normalized using the channel means and standard deviations. The network is trained with stochastic gradient descent (SGD) with momentum 0, step 0.1 and milestone at 60% of the total epochs. We refer the Easy Positive with Hard Negative mining protocol <ref type="bibr" target="#b22">[23]</ref> to sample a batch with C classes and N images per class. On CUB, CAR, SOP and In-shop dataset, we sample 8, 16, 4 and 4 images per class in a mini-batch.</p><p>Small embedding size comparing to training classes size: We follow the early goal of deep metric learning works <ref type="bibr">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> which sets the embedding size to be smaller than the number of training classes. On CUB, CAR, SOP and In-shop dataset the embedding size is 64, 64, 512, 512.</p><p>Comparison of Gradient Components: To compare each component in the gradient, we train ResNet18 on CAR dataset and In-shop dataset for 60 epochs. The training is run with batch size 128. For a given test setting, we run the test 5 times to remove the effect caused by the randomness coming from the random sampling of the batch and random initialization of the final FC embedding layer which reducing the GAP feature to a target dimension (e.g. 64 or 512). Then, the mean and standard deviation of Recall@1 are calculated.</p><p>Comparison with the State-of-the-Art: To compare the re-Direction CAR In-shop e euc 69.5 ? 0.7 83.7 ? 0.1 e cos 75.5 ? 0.2 85.2 ? 0.3 e euc?orth 66.9 ? 0.5 84.1 ? 0.1 e cos?orth 77.0 ? 0.7 86.6 ? 0.2 PyTorch Implementation: In PyTorch platform, we use torch.autograd.Function module to customize both forward and backward functions for a loss module. The backward function is to generate our customized gradient for the optimizer. During the training, the gradient is directly starting from the backward function, replacing the gradient generated by AutoGrad of the forward function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparison Experiments</head><p>In this section, we give explicit experiment results to demonstrate the isolated effects contributed by unit gradient direction, pair weight and triplet weight. More raw results are shown in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Unit Gradient Direction</head><p>To understand the behavior of unit gradient directions in section 3.2, we set constant pair and triplet weight T con = 0.5 and P con = 1, and vary the choice of Euclidean, cosine, Euclidean-orthogonal and cosine-orthogonal direction.  In <ref type="table" target="#tab_0">Table 2</ref>, we find the following trends. First, the cosine and cosine-orthogonal direction have better Recall@1 accuracy than other directions. Second, the cosine-orthogonal gradient direction gives an improvement for both datasets compared to the cos direction. More analysis will be discussed in section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Pair Weight</head><p>To understand the behavior of the pair weights, we set triplet weights with constant form T con =0.5 and gradient direction with e cos and e euc for two sets of results respectively. As for baseline results, the pair weights are set with constant form P con =1.</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, all pair weights provide a clear performance gain to their baseline results. Also, the performance gap of gradient direction e euc and e cos after applying the pair weight is greatly reduced.</p><p>Both relative-similarity methods P lin?ms and P sig?ms perform better than the method with only self-similarity on CAR dataset across different learning rates. Due to the property of well separation on In-shop dataset as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the relative-similarity term will less likely exist during the train because few positive and negative examples will be in P and N set as mentioned in equation 12. P lin?ms is performance almost as same as P lin , but P sig?ms shows some computation instability effect. We put a further analysis of this effect in the appendix.</p><p>In summary, <ref type="table" target="#tab_4">Table 3</ref> shows several features related to the pair weight. First, pair weight causes substantial improvement in recall@1 accuracy. Second, in most cases, the linear and sigmoid pair weight outperforms the default Euclidean pair weight. Third, the linear version of the multi-similarity gradient direction is much more robust to different learning rates than the sigmoid version(see in appendix), and gives better performance and Recall@1 accuracy. <ref type="table">Table 4</ref>, we show two groups of experiments to compare the seven triplet weights. One group sets the pair-weight to be constant P con = 1. Another group uses the linear pair weight P lin . All experiments use cosine gradient direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Triplet Weight</head><p>Comparing to the baseline method where triplet weight T con = 0.5, T cos and T cir has minimal but slight boost in performance; T sc1 and T sc2 has bigger impact on CAR data set than In-shop dataset. This is due to the properties of these two datasets as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. CAR dataset has low inter-class variance(images from different classes may look similar) while In-shop dataset has high inter-class variance(images from different classes look not similar). The major challenge of CAR dataset is to distinguish similar images with different labels, and this is the purpose of triplet operators T sc1 and T sc2 because they concentrate on separating triplets with hard negative in training. And In-shop dataset is to relatively easy to separate images with different labels, the goal is to continue separating the images better, which is the impact of T cos and T cir Therefore, we can conclude that the performance gain in Circle loss is largely from the pair weight not from the triplet weight. Selective Contrastive operator benefits the training tasks which need to separate triplets with hard negative and is not helpful for training tasks which easily separate triplets during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Euclidean or Cosine Direction?</head><p>In Section 3.2 and <ref type="figure" target="#fig_0">Figure 2</ref>, the different gradient behaviors of e euc and e cos have been showed. But additional discussion will highlight the performance difference shown in Sections 5.1 and 5.2.</p><p>We first decompose the unit gradient to move positive and negative features into two directions: the direction along positive and negative feature d and the direction orthogonal to positive and negative feature d ? . As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, only the gradient component along d ? effectively contributes to the angle change of anchor-positive and anchor-negative pair which directly affect the similarity score. The effective gradient projection strength for e euc and e cos : <ref type="figure">Figure 5</ref>. Visualization of triplets mined with the nearest positive and the nearest negative strategy <ref type="bibr" target="#b22">[23]</ref> in training batches of the last 5 epochs (left), in the whole train set (middle) and in the whole test set (right) after training. First row: Euclidean gradient direction. Second row: Cosine gradient direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1+S 2</head><p>Euclidean direction ? 1?S 2 cosine direction <ref type="bibr" target="#b19">(20)</ref> where S is the similarity of a positive or negative pair. The derivation of the above projection length is shown in the appendix. <ref type="figure" target="#fig_0">Figure 2</ref> right shows the change of the effective gradient strength for e euc and e cos varying as a function of pair similarity. Because most pairs during the training have positive similarity, we focus on projection length when similarity is positive.</p><p>The Euclidean gradient has stronger force to pull positive close and push negative away than the cosine direction when two features are close to each other. Therefore, Euclidean gradient continues to force features together even when they are already relatively close, unlike the cosine gradient. In <ref type="figure">Figure 5</ref> left column, we show the triplet diagram plot of triplets extracted from the last 5 epochs of training (epoch 55-60) on CAR dataset. The Euclidean direction clusters the same label feature more tightly than the cosine direction because there are more triplets along the right edge of the triplet diagram comparing to the scatter of cosine direction.</p><p>However, the tight clustering behavior in training leads to even the triplet with nearest positive and the nearest negative to be compact. In the middle and right column of <ref type="figure">Figure 5</ref>, we plot just these triplets for the whole training set (middle), and testing set (right). The Euclidean gradient has more triplets very close to the top right corner, indicating that point have very similar same class and different class neighbors, while cosine gradient creates triplets that are more spread out. The spread out effect indicates the feature learned by the deep model is distinguishable <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. Because these Euclidean feature are more compressed (for both the anchorpositive and anchor-negative pairs), it is harder for the network to learn distinguishable features that if it is using the cosine gradient.</p><p>One more piece of evidence to support the analysis above is the pair weight result in section 5. direction, the performance gap between these two methods is almost disappeared. This is because the Euclidean pair weight P euc reduces the weight de-emphasizes positive pair when they are already close, and therefore avoid its tight clustering behavior, making Euclidean direction behave similarly as cosine direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Best combination of gradients</head><p>In the previous chapter, we separately consider the gradient terms that relate to the gradient directions, the pair weights applied to the gradients from the anchor-negative and anchor-positive pairs, and the overall weight of the triplets. In terms of the gradient direction, the e cos?orth gives the best performance and is relatively stable with respect to the learning rate. In terms of the pair-weighting, P lin?ms is consistently a top performer across datasets. Similarly, T cir shows stable improvement to both CAR and In-shop datasets. We combine these gradient components empirically to form the final gradient, and train a network by imposing this gradient combination. We compare the performance of the network trained this way with many latest state-of-the-art results.</p><p>To ensure a fair comparison, we also re-implement current related SOTA approaches, MS and DR-MS results (noted as MS* and DR-MS*) with our gradient method to create a comparison with the same network backbone, pre-processing and training settings. The implementation difference is shown in the appendix. The result is reported in <ref type="table">Table 5</ref>. In addition, we vary the batch size 128, 256, 384, 512 on all tests for four datasets and continue to improve the Recall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Limitations</head><p>We point out the following limitations of the paper: ? We do not exhaustively compute all possible combination of all the three gradient components, and instead focus on the isolated effect of single gradient components. There may be additional improvements in explicitly considering the interactions between the different gradient components.</p><p>? There are recent loss functions proposed in the deep metric learning literature such as Proxy loss <ref type="bibr" target="#b9">[10]</ref> and N-pair loss <ref type="bibr" target="#b15">[16]</ref>; and we currently are not able to put those loss function into our framework due to complex gradient computation for multiple negative pairs. ? Our experiments do not fully explore training optimizations.</p><p>We have fixed hyper-parameters in our sampling approach, we keep a constant step size, and we fix the hyper-parameters in gradient components such as P sig?ms for most experiments.</p><p>Our results are based on hyper-parameter selections from earlier papers, but the gradient based approach to learning embedding functions may be improved with additional search over the hyper-parameter space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We provide a new framework to train deep metric learning networks with direct gradient modification. In our framework, we disentangled gradient components of many loss functions into common components, and analyze the effects of each component. We find that the Euclidean gradient direction and the cosine gradient direction behave quite differently. In its default form, the Euclidean gradient creates embedding spaces that are very tightly clustered and the cosine gradient direction has a consistently big improvement over a large set of experimental conditions. Second, recently popular works define new loss functions that, in terms of their gradient, primarily change the pair weight term, which is consistent with our findings that the pair-weight term is very important. In contrast, we find the triplet weight term to have limited impact that was not consistent across datasets.</p><p>Finally, this study of the importance of different weighting functions and components of the gradient led to a simple approach that directly defines the desired gradients and gives improvements to state-of-the-art performance relative to recent work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Analysis of relative-similarity term in MS gradient</head><p>There are two terms in MS loss dynamically changing the pair weight. The self-similarity term has been discussed in sigmoid pair weight P sig from the main paper in Section 3.3 As for the relative-similarity term, the major effect is to increase or decrease the maximum magnitude of the pair weight.</p><p>Given a negative pair, when its relative-similarity term m sig ? &gt;1, this indicates the selected negative example is relatively closer to anchor compared to other negative examples. Then, the negative weight increases because the relative term decreases the denominator in P sig?ms ? . When its relative-similarity term </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Calculation instability of original MS weight</head><p>The training result in Appendix <ref type="table" target="#tab_0">Table 2</ref> reflects an apparent instability problem. This is caused by the exponential item m sig ? with a large ? which leads dramatically large value change that decreases the computation stability during the training. Appendix <ref type="figure" target="#fig_5">Figure 1</ref> shows the pair weight functions from Equation <ref type="bibr" target="#b12">13</ref> and Equation 14 in the main paper with two different sets of relative similarity terms. Because the Easy Positive and Hard Negative(EPHN) mining <ref type="bibr">[4]</ref> strategy is applied in training, each anchor has its positive and negative with the largest similarity. The relative similarity terms are always greater than zero, ?S i ap = S s ap ? S r ap i ? 0 and ?S j an = S s an ? S r an j ? 0 (If all ?S i ap = ?S j an = 0, the weight function degrade back to Equation 10 and 11 in the main paper). In addition, we set ?S i ap =?S j an =0.1 in Appendix <ref type="figure" target="#fig_5">Figure 1</ref>. We see that sig-ms weight changes dramatically on the negative pair due to the exponential with the large ?, generating an unstable gradient during the training. The problem occurs when the learning rate is large as shown in Appendix <ref type="table" target="#tab_0">Table 2</ref>. With a small learning rate, especially in CAR tests, the sig-ms pair weight performs better than sig pair weight. But in In-shop tests, the unstable computation problem occurs and the performance of sig-ms pair weight is much worse than sig pair weight.</p><p>In contrast, with the relative similarity term, the variation of our lin-ms pair weight is well controlled for both positive and negative pairs as shown in Appendix <ref type="figure" target="#fig_5">Figure 1</ref>. Also, in Appendix <ref type="table">Table ?</ref>?, the performance of lin-ms pair weight performs better and is more stable with respect to learning-rate than sig-ms pair weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation difference to the origin MS and DR-MS work</head><p>Because both original MS <ref type="bibr">[3]</ref> and DR-MS <ref type="bibr">[1]</ref> work have not, to our knowledge, released complete training details for their SOTA results, including batch size and learning rate. We try our best to re-implemente these two methods but also keep a fair   comparison with other algorithms. The re-implementation of sigmoid pair weight and MS pair weight uses hyper-parameters ?=2, ? =10, ?=0.5 and =0.1. This setting is similar to the origin MS paper and other implementations such as DR-MS <ref type="bibr">[1]</ref> paper and Reality Check <ref type="bibr">[2]</ref> paper. We set ?=0.5 to make the weight symmetric when similarity varies from 0 to 1 and support fair comparison with linear pair weight which are also symmetric when similarity varies from 0 to 1.</p><p>In <ref type="table">Table 5</ref> of the main paper, this re-implementation outperforms what is originally reported by the MS paper on CUB, CAR, and SOP with batch size 128. However, in MS paper, the best recall@1, 78.2% of SOP uses batch size 1000. In the DR-MS paper, the best recall@1 of In-shop, 91.7%, use batch size 600. When DR-MS uses batch size 160 (a comparable batch size to our setting), recall@1 is 88.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The gradient projection length along d ? for</head><p>e euc and e cos</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Left: Illustration of two types of gradient direction e euc (red) and e cos (blue), and two gradient decomposition directions (green). Right: Gradient projection strength along d ? of e euc and e cos varies with the similarity of positive/negative pairs. With the categorizations of the gradient components, it becomes easy to compare the effects of each component. Before the comparison, we first show how recently proposed loss functions can be characterized by computing the direction and weights of the different gradient terms in Sections 3.2, 3.3 and 3.4 and then perform comparisons of the isolated effects of each gradient component in Sections 5.1, 5.2 and 5.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>A triplet diagram characterizes the behavior of triplet weights as a function of the the similarity of the anchor-positive pair (along the x-axis) and the anchor-negative pair (along the y-axis). Triplets where the anchor, positive and negative features are all very similar will be in the top right of the right, and triplets where the anchor-positive are similar and the anchor-negative are not similar are in the bottom right corner. Using this diagram, (Left) shows the weight of the different triplets based on cosine similarity based triplet loss T cos , (Middle) shows the weights of different triplets for the Circle loss T cir . (Right) Shows the decision boundary for two forms of the selectively-contrastive triplet operators T sc1 and T sc2 (where the anchor-positive pair only has a positive weight below the boundary).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>right shows the difference decision boundaries of T sc1 and T sc2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Distribution of the nearest positive pairs(blue) and the nearest negative pairs(orange) over whole CAR(left) and In-shop(right) dataset after training. On the CAR datasets, the nearest positive pairs distribution is largely overlay on the nearest negative pair distribution, indicating the CAR set is not easily to be separated well among different classes. However, on the In-shop datasets, the overlay area of the nearest positive pairs distribution and the nearest negative pair is much less, indicating the In-shop dataset can be easily separated among different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>m sig ? &lt;1, indicating the selected negative example is relatively far away from anchor comparing to other negative examples, the negative weight decreases because the relative term increases the denominator in P sig?ms ? . The latter situation will not exist under the training with hard negative mining. Given a positive pair, when its relative-similarity term m sig + &gt;1, this indicates the selected positive example is relatively close to anchor compared to other positive examples, the positive weight decreases because the relative term increases the denominator in P sig?ms + . When its relative-similarity term m sig + &lt; 1, indicating the selected positive example is relatively far away from anchor comparing to other positive examples, the positive weight increases because the relative term decreases the denominator in P sig?ms + . The latter situation will not exist under the training with easy positive mining.In sum, the main effect caused by the relative-similarity term is to dynamically change the maximum penalty for positive and negative pairs as shown in AppendixFigure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of sig-ms pair weight and lin-ms pair weight with relative similarity term ?S i ap =?S j an =0 and 0.1 when ?=2, ? =10 and ?=0.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3 ? 9 Table 2 . 3 ? 2 ?</head><label>39232</label><figDesc>0.7 75.5 ? 0.2 71.0 ? 0.5 62.5 ? 0.5 85.2 ? 0.3 84.9 ? 0.2 84.3 ? 0.4 83.3 ? 0.3 P euc e cos 71.2 ? 0.4 76.6 ? 0.5 77.0 ? 0.4 71.8 ? 0.7 84.8 ? 0.2 84.6 ? 0.4 84.0 ? 0.2 83.3 ? 0.6 P lin e cos 67.2 ? 0.6 73.7 ? 0.3 77.2 ? 0.5 77.8 ? 0.9 86.2 ? 0.1 87.2 ? 0.3 87.3 ? 0.2 86.5 ? 0.4 P lin?ms e cos 69.8 ? 0.9 75.8 ? 0.3 78.2 ? 0.6 78.8 ? 0.9 86.5 ? 0.1 87.3 ? 0.1 87.0 ? 0.1 85.1 ? 0.5 P sig e cos 67.2 ? 0.6 72.8 ? 0.2 74.3 ? 0.2 74.0 ? 0.3 86.6 ? 0.1 87.3 ? 0.1 87.8 ? 0.2 87.2 ? 0.2 P sig?ms e cos 68.0 ? 0.6 73.0 ? 0.1 76.2 ? 0.5 76.0 ? 0.7 86.4 ? 0.2 86.0 ? 0.4 72.5 ? 0.9 57.0 ? 4e euc 69.5 ? 0.7 66.9 ? 0.6 59.2 ? 0.8 52.4 ? 0.6 83.7 ? 0.1 83.7 ? 0.2 83.3 ? 0.2 82.0 ? 0.2 P euc e euc 72.6 ? 0.9 75.2 ? 0.4 74.3 ? 0.8 67.2 ? 0.5 84.9 ? 0.2 85.2 ? 0.2 84.9 ? 0.3 83.7 ? 0.3 P lin e euc 67.5 ? 0.8 73.4 ? 0.7 76.6 ? 0.7 76.7 ? 0.5 86.4 ? 0.1 87.4 ? 0.1 87.4 ? 0.3 86.5 ? 0.4 P lin?ms e euc 69.7 ? 0.3 75.5 ? 0.6 77.9 ? 0.4 78.2 ? 0.4 86.4 ? 0.2 87.5 ? 0.2 87.5 ? 0.1 85.0 ? 0.3 P sig e euc 68.0 ? 0.2 71.9 ? 0.4 71.1 ? 0.3 69.2 ? 0.7 85.5 ? 0.2 85.9 ? 0.2 86.2 ? 0.1 85.6 ? 0.2 P sig?ms e euc 67.7 ? 1.0 73.3 ? 0.5 74.6 ? 0.8 74.0 ? 0.8 84.9 ? 0.4 72.4 ? 1.3 48.6 ? 3.2 35.8 ? 4.Comparing recall@1 performance of different pair weights on CAR and In-shop dataset with various learning rates(lr). 0.7 75.5 ? 0.2 71.0 ? 0.5 62.5 ? 0.5 85.2 ? 0.1 84.9 ? 0.3 84.3 ? 0.2 83.3 ? 0.4 T cos P con 73.5 ? 0.3 75.8 ? 0.2 74.1 ? 0.6 67.0 ? 0.5 85.9 ? 0.2 86.0 ? 0.3 85.7 ? 0.1 85.3 ? 0.3 T cos ,T sc1 P con 74.7 ? 0.3 77.0 ? 0.2 75.9 ? 0.7 69.2 ? 1.1 85.0 ? 0.2 85.1 ? 0.3 84.6 ? 0.1 82.7 ? 0.2 T cir P con 72.1 ? 0.6 75.5 ? 0.3 75.0 ? 1.0 70.6 ? 0.5 85.4 ? 0.1 86.0 ? 0.1 85.6 ? 0.2 85.3 ? 0.3 T cir ,T sc2 P con 70.9 ? 0.6 76.1 ? 0.7 76.6 ? 0.8 73.1 ? 0.1 83.9 ? 0.3 84.2 ? 0.1 83.8 ? 0.2 82.9 ? 00.5 77.8 ? 0.9 77.0 ? 0.6 76.3 ? 0.6 86.2 ? 0.1 87.2 ? 0.3 87.3 ? 0.2 86.5 ? 0.4 T cos P lin 76.6 ? 0.8 77.4 ? 0.5 77.5 ? 0.5 77.3 ? 0.1 86.0 ? 0.2 87.4 ? 0.2 87.9 ? 0.4 87.5 ? 0.3 T cos ,T sc1 P lin 76.2 ? 0.2 78.8 ? 0.6 78.6 ? 0.6 78.0 ? 0.3 85.8 ? 0.2 86.8 ? 0.2 87.3 ? 0.3 86.7 ? 0.3 T cir P lin 75.2 ? 0.2 77.2 ? 0.2 78.3 ? 0.2 76.9 ? 0.4 85.5 ? 0.1 86.9 ? 0.1 87.7 ? 0.2 87.5 ? 0.3 T cir ,T sc2 P lin 75.4 ? 0.6 78.0 ? 0.4 78.8 ? 0.3 78.5 ? 0.3 84.5 ? 0.1 86.2 ? 0.1 86.8 ? 0.2 86.9 ? 0.2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Comparing recall@1 performance of different gradient directions on CAR and In-shop dataset</figDesc><table><row><cell>CAR In-shop</cell><cell>PairWeight P con P euc P lin P lin?ms P sig P sig?ms P con P euc P lin P lin?ms P sig P sig?ms</cell><cell>e euc 69.5 ? 0.7 75.5 ? 0.2 e cos 75.2 ? 0.4 77.0 ? 0.4 76.7 ? 0.5 77.8 ? 0.9 78.2 ? 0.4 78.8 ? 0.9 71.9 ? 0.4 74.3 ? 0.2 74.6 ? 0.8 76.2 ? 0.5 83.7 ? 0.1 85.2 ? 0.3 85.2 ? 0.2 84.8 ? 0.2 87.4 ? 0.1 87.3 ? 0.2 87.5 ? 0.2 87.3 ? 0.1 86.2 ? 0.1 87.8 ? 0.2 84.9 ? 0.4 86.4 ? 0.2</cell></row></table><note>Table 3. Comparing recall@1 performance of different pair weights with Euclidean and cosine direction on CAR and In-shop dataset cent state-of-the-Arts results, we select ResNet50 as the backbone for 80 epochs training. The training is run with different batch sizes 128, 256, 384 and 512. Each test is run 3 times and mean Recall@K is calculated as the measurement for retrieval quality.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>TripletWeight P con P lin T con 75.5 ? 0.2 77.8 ? 0.9 T cos 75.8 ? 0.2 77.5 ? 0.5 T cos &amp; T sc1 77.0 ? 0.2 78.8 ? 0.6 T cos &amp; T sc2 77.2 ? 0.5 78.4 ? 0.5</figDesc><table><row><cell>CAR In-shop</cell><cell>T cir T cir &amp; T sc1 T cir &amp; T sc2 T con T cos T cos &amp; T sc1 T cos &amp; T sc2 T cir T cir &amp; T sc1 T cir &amp; T sc2</cell><cell>75.5 ? 0.3 78.3 ? 0.2 77.0 ? 0.4 78.1 ? 0.6 76.6 ? 0.8 78.8 ? 0.3 85.2 ? 0.1 87.3 ? 0.2 86.0 ? 0.3 87.9 ? 0.4 85.1 ? 0.3 87.3 ? 0.3 84.5 ? 0.2 86.9 ? 0.2 86.0 ? 0.1 87.7 ? 0.2 84.9 ? 0.2 87.2 ? 0.1 84.2 ? 0.1 86.9 ? 0.2</cell></row></table><note>Table 4. Comparing recall@1 performance of different triplet weights with constant and linear pair weight on CAR and In-shop dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2. When the Euclidean pair weight P euc is applied to Euclidean direction and cosine</figDesc><table><row><cell>Dataset Method LiftedStruct [17] ProxyNCA [10] SoftTriple [13] EasyPositive [23] 57.3 CUB(dim=64) R@1 R@2 R@4 R@1 R@2 R@4 R@1 R@10 R@100 R@1 R@10 R@20 CAR(dim=64) SOP(dim=512) In-shop(dim=512) 43.6 56.6 68.6 53.0 65.7 76.0 62.5 80.8 91.9 ---49.2 61.9 67.9 73.2 82.4 86.4 73.7 -----60.1 71.9 81.2 78.6 86.6 91.8 78.3 90.3 95.9 ---68.9 79.3 75.5 84.2 90.3 78.3 90.7 96.3 87.8 95.7 96.8 MS [19] 57.4 69.8 80.0 77.3 85.3 90.5 78.2 90.5 96.0 89.7 97.9 98.5 SCT [22] 57.7 69.8 79.6 73.4 82.0 88.0 81.9 92.6 96.8 90.9 97.5 98.2 DR-MS [9] 59.1 71.0 80.3 79.3 86.7 91.4 ---91.7 98.1 98.7 Proxy-anchor [6] 61.7 73.0 81.8 78.8 87.0 92.2 79.1 90.8 96.2 91.5 98.1 98.8 MS*(B128) 59.8 71.7 81.0 79.0 86.6 91.5 78.7 90.4 96.0 89.4 96.6 97.4 DR-MS*(B128) 60.7 71.9 81.3 79.9 87.0 91.7 78.8 90.4 96.1 89.6 96.4 97.4 Ours(B128) 63.5 74.8 83.6 82.5 89.1 93.3 79.9 90.5 95.5 91.4 97.7 98.4 Ours(B256) 63.8 74.8 83.7 85.5 91.0 94.6 82.0 92.3 96.8 92.2 97.8 98.4 Ours(B384) 63.8 75.0 84.2 86.5 91.6 94.8 82.2 92.5 96.8 92.0 97.8 98.3 Ours(B512) 63.1 74.6 83.2 85.7 91.2 94.7 82.3 92.5 96.9 90.8 97.2 97.9</cell></row></table><note>Table 5. Retrieval Performance on the CUB, CAR, SOP and In-shop datasets comparing to the best reported results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>? 0.4 69.5 ? 0.7 66.9 ? 0.6 59.2 ? 0.2 83.7 ? 0.1 83.7 ? 0.2 83.3 ? 0.2 82.0 ? 0.2 e cos 67.9 ? 0.4 74.3 ? 0.7 75.5 ? 0.2 71.0 ? 0.5 85.2 ? 0.3 84.9 ? 0.2 84.3 ? 0.4 83.3 ? 0.3 e euc?orth 58.9 ? 0.5 66.9 ? 0.5 65.0 ? 0.6 60.0 ? 0.9 84.0 ? 0.2 84.1 ? 0.1 83.7 ? 0.2 82.5 ? 0.3 e cos?orth 69.3 ? 0.4 75.2 ? 0.3 77.0 ? 0.7 73.7 ? 0.7 86.1 ? 0.2 86.3 ? 0.4 86.6 ? 0.2 85.3 ? 0.1 Comparing recall@1 performance of different gradient directions on CAR and In-shop dataset with various learning rates(lr).</figDesc><table><row><cell>Dataset Direction e euc</cell><cell>lr 0.025 66.8</cell><cell>lr 0.05</cell><cell>CAR</cell><cell>lr 0.1</cell><cell>lr 0.2</cell><cell>lr 0.5</cell><cell>lr 1.0</cell><cell>In-shop</cell><cell>lr 2.0</cell><cell>lr 4.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparing recall@1 performance of different triplet weights on CAR and In-shop dataset with various learning rates(lr).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Reject in CVPR2021, ICCV2021, CVPR2022 arXiv:2201.11307v1 [cs.CV] 27 Jan 2022</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 1. Raw data table of gradient component</head><p>Appendix <ref type="table">Table 1</ref> shows more raw data with various learning rates for isolated effect of gradient direction. Appendix <ref type="table">Table 2</ref> shows more raw data with various learning rates for isolated effect of pair weight. Appendix <ref type="table">Table 3</ref> shows more raw data with various learning rates for isolated effect of triplet weight.</p><p>Let Euclidean gradient projection length along positive feature vector be l euc pos , along negative feature vector be l euc neg .</p><p>The the orthogonal projection length l euc ?pos and l euc ?neg are:</p><p>Let cosine gradient projection length along positive feature vector be l cos pos , along negative feature vector be l cos neg .</p><p>l cos pos =e cos p f p =?f a f p =?S ap l cos neg =e cos n f n =f a f m =S ap </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>L. K. Saul, Y. Weiss, and L. Bottou</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2821" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Proxy anchor loss for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Moving in the right direction: A regularization for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Deen Dayal Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srirangaraj</forename><surname>Fedorishin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venu</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="681" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Softtriple loss: Deep metric learning without triplet sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baigui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multiclass n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="5022" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hard negative examples are hard, but useful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved embeddings with easy positive triplet mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 22nd International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Gradient surgery for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06782</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning spread-out local feature descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">P2sgrad: Refined gradients for optimizing deep face models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengya</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9906" to="9914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Moving in the right direction: A regularization for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Deen Dayal Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srirangaraj</forename><surname>Fedorishin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venu</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="681" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5022" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved embeddings with easy positive triplet mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
