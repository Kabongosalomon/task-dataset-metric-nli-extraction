<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-layered Semantic Representation Network for Multi-label Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Che</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchuan</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-layered Semantic Representation Network for Multi-label Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-label image classification (MLIC) is a fundamental and practical task, which aims to assign multiple possible labels to an image. In recent years, many deep convolutional neural network (CNN) based approaches have been proposed which model label correlations to discover semantics of labels and learn semantic representations of images. This paper advances this research direction by improving both the modeling of label correlations and the learning of semantic representations. On the one hand, besides the local semantics of each label, we propose to further explore global semantics shared by multiple labels. On the other hand, existing approaches mainly learn the semantic representations at the last convolutional layer of a CNN. But it has been noted that the image representations of different layers of CNN capture different levels or scales of features and have different discriminative abilities. We thus propose to learn semantic representations at multiple convolutional layers. To this end, this paper designs a Multi-layered Semantic Representation Network (MSRN) which discovers both local and global semantics of labels through modeling label correlations and utilizes the label semantics to guide the semantic representations learning at multiple layers through an attention mechanism. Extensive experiments on four benchmark datasets including VOC 2007, COCO, NUS-WIDE, and Apparel show a competitive performance of the proposed MSRN against state-of-the-art models. *</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-label image classification (MLIC) deals with assigning multiple labels to each image, and it has been applied in many fields, including multi-object recognition <ref type="bibr" target="#b11">[12]</ref>, medical diagnosis recognition <ref type="bibr" target="#b8">[9]</ref> and Person re-identification <ref type="bibr" target="#b20">[21]</ref>. The recent progress is mainly made by exploiting label correlations and learning semantic representations with deep learning models.</p><p>Modeling label correlations has been long studied in multi-label classification and has been demonstrated very effective because correlated labels are highly likely to co-occur <ref type="bibr" target="#b4">[5]</ref>. For an image recognition task, deep learning models, especially convolutional neural networks (CNNs), have been widely recognized as state-of-the-art models. Hence, many recent approaches to MLIC are based on deep learning models and exploiting label correlations, e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18]</ref>. In these approaches, an existing deep learning model is usually employed as a tool to transform a raw image into a high-level abstract representation. But objects of interest may only be in certain regions of an image. Therefore, some studies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29]</ref> utilize semantics of labels to guide the learning of semantic representations of images. The semantic representations are expected to only cover the regions of interest, and thereby improve the classification performance. This paper advances this research direction by improving both the modeling of label correlations and the learning of semantic representations. On the one hand, unlike existing approaches only learn local semantics of each label, we propose to further explore global semantics shared by multiple labels. The global semantics can effectively capture high-order correlations among multiple labels. On the other hand, existing approaches mainly learn the semantic representations at the last convolutional layer of a CNN. But it has been noted that the image representations at different layers of CNN capture different levels or scales of features and have different discriminative abilities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27]</ref>. Therefore, better performance might be achieved by simultaneously exploiting features at multiple layers of a CNN.</p><p>To realize the proposals mentioned above, we design a novel Multi-layered Semantic Representation Network (MSRN). MSRN generates both label-specific and group embeddings which capture local semantics and global semantics respectively, and then combines them with multiple layers of a CNN by an attention mechanism to learn label and group shared semantic representations of images. To be specific, first, we introduce LGE (Label-Group Embedding) module to capture both local semantics of each label and semantics of a group of labels in embeddings based on the label co-occurrence graph. Second, we propose SGA (Semantic Guided Attention) module to explicitly guide the CNN to focus on the regions of interest. Third, we design a framework to combine the LGE module with the multiple layers of a CNN through the attention mechanism built in the SGA module. We conduct experiments on four benchmark multi-label image datasets including COCO, VOC 2007, Apparel and NUS-WIDE. The experimental results show that our method outperforms state-of-theart approaches.</p><p>The contributions of this paper are summarized as follows:</p><p>? A Multi-layered Semantic Representation Network (MSRN) is designed for multi-label image classification. ? Second-order and high-order label correlations are considered simultaneously to improve the performance of multi-label image classification. ? Semantic representations are learned at multiple layers through an attention mechanism by modeling label correlations.</p><p>The rest of the paper is organized as follows. Section 2 introduces related work. Section 3 presents the proposed method. Section 4 presents empirical evaluation. Section 5 concludes this paper and introduces future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In MLIC, images are annotated with multiple labels simultaneously where labels usually have correlations. It has been demonstrated that exploiting label correlations can significantly improve the performance <ref type="bibr" target="#b30">[31]</ref>. Recent progress has been made by employing deep learning models, especially convolutional neural networks. Wang et al. <ref type="bibr" target="#b21">[22]</ref> extract label semantics and associate it to Recurrent Neural Network (RNN). In addition, Lee et al. <ref type="bibr" target="#b13">[14]</ref> apply knowledge graphs to exploit the label dependencies based on the label co-occurrence graph. ML-GCN <ref type="bibr" target="#b4">[5]</ref> learns the semantic label embeddings through Graph Convolution Network (GCN), and applies it as inter-dependent object classifiers at the prediction stage. In <ref type="bibr" target="#b22">[23]</ref>, a label graph superimposing framework is proposed to exploit label correlations. The label graph is constructed by superimposing statistical label graph into knowledge prior oriented graph, which, however, is usually unavailable in real applications.</p><p>Some studies further locate regions of interest because each class label might be determined by some specific regions of an image. Examples include <ref type="bibr" target="#b32">[33]</ref> [26] which apply bounding box to focus on the regions of proposal. To learn regions with arbitrary boundaries, more studies propose attention based methods where attention is a spatial weight map representing relative importance among pixels <ref type="bibr" target="#b10">[11]</ref>. SRN <ref type="bibr" target="#b31">[32]</ref> is an end-to-end CNN model which trains learnable convolutions on the attention maps of labels. In LGE module which can output the label and the group embeddings capturing label semantics and group semantics, respectively. Then, the SGA module produces label-level semantic representations and group-level semantic representations of the image by combining each image feature map from each branch with the learned label and group embeddings through an attention mechanism. At last, we concatenate the generated semantic representations and apply fully connected layers to perform the classification.</p><p>Recently, some methods <ref type="bibr" target="#b28">[29]</ref> [5] <ref type="bibr" target="#b3">[4]</ref> apply Graph Neural Network (GNN) techniques to generate semantic label embeddings which can be utilized as visual attention for multi-label image classification. You et al. <ref type="bibr" target="#b28">[29]</ref> propose a method of computing cosine similarity between label embeddings to exploit label dependencies. Chen et al. <ref type="bibr" target="#b3">[4]</ref> apply a GNN with graph propagation mechanism to exploit the interaction between DNN and label dependencies. Despite having achieved high performance on multi-label image classification, these methods do not explicitly consider high-order label dependencies which may result in semantics shared by a group of labels <ref type="bibr" target="#b30">[31]</ref>. Moreover, to the best of our knowledge, no existing approaches utilize image representations extracted from multiple layers of a CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>The architecture of the proposed MSRN is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We design an LGE module to generate label and group embeddings with the input of a graph G = {V, A}. V = {v i } n i=1 is the feature matrix of labels where v i is a vector of features and n is the number of labels, and A = {a ij } n i,j=1 is the adjacency matrix about label co-occurrence. The outputs of LGE module are label embeddings</p><formula xml:id="formula_0">E l = {e i l } n i=1 ? R n?d and group embeddings E g = {e i g } m i=1 ? R m?d ,</formula><p>where m is the number of groups, and d is the dimension of the embeddings.</p><p>The backbone in our framework can be any kind of CNNs, such as VGG <ref type="bibr" target="#b18">[19]</ref>, ResNet <ref type="bibr" target="#b9">[10]</ref> and DenseNet <ref type="bibr" target="#b7">[8]</ref>. In this paper, Resnet-101 is chosen for experiment. Given an input image,</p><formula xml:id="formula_1">F = {f b } B i=1 indicates the output image features of different branches, where B is the total number of branches, and f b ? R W b ?H b ?C b is image feature for the b-th branch with spatial resolution W b ? H b and channel C b .</formula><p>The branches marked as blue lines in <ref type="figure" target="#fig_0">Figure 1</ref> are used to receive the image feature map f b from corresponding layers in the CNN. We provide three branches in our work to receive the output image features from the last layer of the last three blocks of Resnet-101. Since the channel C b of image features from different layers of CNN are distinct, we use a convolutional layer with 1?1 kernel to project image features from</p><formula xml:id="formula_2">f b to f b = conv 1?1 (f b ) ? R W b ?H b ?d</formula><p>which has the same dimension d as the label embeddings E l and group embeddings E g .</p><p>Then we propose an SGA module to position-wisely combine the image feature maps</p><formula xml:id="formula_3">F = {f b } B i=1</formula><p>with label embeddings E l and group embeddings E g . The outputs of SGA module are label semantic</p><formula xml:id="formula_4">representations O = {o b } B b=1 and group shared semantic representations Q = {q b } B b=1 , where o b ? R n?d and q b ? R m?d .</formula><p>Some of the existing approaches utilize the label-specific representation for each label. But since in real applications the labeling results of a dataset usually have noisy or missing labels, the label-specific semantic representation might not be sufficient enough to predict correct labels. Therefore, in the final stage of our framework, we concatenate the generated label and group shared semantic representations into M = [O||Q], and apply fully connected layers to perform the prediction where the cross entropy loss function is adopted as follows:</p><formula xml:id="formula_5">L 1 = n i=1 y i log(?(? i )) + (1 ? y i ) log(1 ? ?(? i )),<label>(1)</label></formula><p>where y i is equal to 1 or 0 for image i in terms of a certain label,? i is the output of fully connected layer, and ?(?) is the sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Label-Group Embedding Module</head><p>Since label correlation is important information in multi-label image classification as we mentioned in section 1, we build a Label-Group Embedding (LGE) module to generate semantic label embeddings E l and group embeddings E g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Semantic Label Embeddings</head><p>Graph attention Networks (GAT) <ref type="bibr" target="#b19">[20]</ref> is a self-attention based model which is most frequently used for learning embeddings of graph-structured data. With GAT algorithm, we can obtain the semantic label embeddings E l from the label graph G. In our model, GAT first produces the attention coefficient ? ij between the i-th and j-th label as follows:</p><formula xml:id="formula_6">? ij = exp(LeakyReLU(P [U v i ||U v j ])) k?Ni exp(LeakyReLU(P [U v i ||U v k ])) ,<label>(2)</label></formula><p>where P ? R 1?2w and U ? R w?v are two learnable weight matrices, N i is the set of neighborhoods of label i in the graph, and || represents the concatenation operation. The negative input slope in LeakyReLU is set to be 0.2 in our work. Then, we can obtain label embeddings</p><formula xml:id="formula_7">E 1 l = {e i l } n i=1</formula><p>from the first GAT layer by linearly combining attention coefficients ? with the transformed label features:</p><formula xml:id="formula_8">e i l = ?( j?Ni ? ij U v j + U v i ),<label>(3)</label></formula><p>where the ?(?) is non-linear activation function which is ELU in our method. For simplicity, GAT t (?) is used to represent the t-th GAT layer that consists of equations <ref type="formula" target="#formula_6">(2)</ref> and <ref type="formula" target="#formula_8">(3)</ref>, and the semantic label embeddings E t l can be generated by the following equation:</p><formula xml:id="formula_9">E t l = GAT t (E t?1 l , A),<label>(4)</label></formula><p>where E 0 l = V is the original feature matrix of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Semantic Group Embeddings</head><p>Differentiable graph pooling (Diffpool) <ref type="bibr" target="#b27">[28]</ref> is a graph clustering algorithm that soft map graph nodes to a set of clusters. Once we capture the semantic label embeddings E l , we can apply Diffpool to generate semantic group embeddings E g as E g = Diffpool(E l , A).</p><p>Moreover, in order to learn more compact group embeddings, we try to minimize the distance between the group embeddings E g and the labels embeddings E l as follow</p><formula xml:id="formula_11">L 2 = m k=1 E i l ?C k E k g ? E i l 2 2 ,<label>(6)</label></formula><p>where C k indicates the k-th cluster of labels which are highly correlated labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semantic Guided Attention Module</head><p>The aim of SGA module is to utilize the semantic embeddings E l and E g to guide the learning of semantic representations of images at different branches. As the feature contained in each position (w, h) of an image feature map could be correlated to the semantics of the label embeddings, we propose a position-wise attention mechanism to fully combine the image feature space and the semantic embedding space. Similar to existing studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref>, we adopt the Hadamard product between each position (w, h) of an image feature map from the b-th branch and the label, group embeddings to calculate the attention weights as</p><formula xml:id="formula_12">sl b i w,h = f w,h b e i l , sg b j w,h = f w,h b e j g ,<label>(7)</label></formula><p>where the is Hadamard product, sl bw,h ? R 1?1?n?d and sg bw,h ? R 1?1?m?d . Then we apply normalization to the computed compatibility scores</p><formula xml:id="formula_13">al b ? R W b ?H b ?n?d and ag b ? R W b ?H b ?m?d al w,h b = exp(sl bw,h )</formula><p>x,y exp(sl bx,y )</p><p>, ag w,h b = exp(sg bw,h )</p><p>x,y exp(sg bx,y )</p><p>.</p><p>Once obtained the normalized compatibility scores, we apply the second Hadamard product to generate position-wise attention maps.</p><formula xml:id="formula_15">o b = w,h al w,h b f w,h b , q b = w,h ag w,h b f w,h b .<label>(9)</label></formula><p>Finally, we concatenate the local semantic representations O={o b } B b=1 ?R n?(Bd) and group shared semantic representations Q={q b } B b=1 ?R m?(Bd) and predict the labels b? y i =f c 2 (LeakeyReLU(f c 1 (tanh(M )))), where M =[O||Q], f c 1 and f c 2 are fully connected layers. The total training loss is L 1 + ?L 2 , where ? is a regularization parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>In this section, we will describe the implementation details of our proposed model MSRN and the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details and Evaluation Metrics</head><p>The input label features V are 300-dimensional Glove features pretrained on Wikipedia dataset. The backbone ResNet-101 is pretrained on ImageNet for accelerating training process. We remove the last average pooling layer and classifier of Resnet-101 and apply the MaxPooling with kernel size 2 ? 2 and stride 2 to obtain image features, F , from last three building blocks of ResNet-101. The output dimension of f c 1 is 2048, and the output dimension of f c 2 is the same as the number of labels. In addition, to reduce the impact of branches corresponding to lower layers of the backbone on gradients, we add a buffer convolutional layer <ref type="bibr" target="#b0">[1]</ref> with kernel size 1 ? 1 and stride 1 before we obtain image features from the last two branches. Both the output feature dimension of first GAT layer and input feature dimension of second layer are 300 and the output feature dimension of second GAT layer is 512. The number of groups of labels m is set as 4. The regularization parameter ? is set to 0.001. The input image is resized to 448x448 for both training and testing. We train our model on one Tesla V100-16GB GPU and set the batch size to 8. For optimization, we apply SGD as optimizer with momentum 0.9 and weight decay 10 ?4 . The initial learning rate is set to 0.01 and the learning rate decay by 0.1 each 30 epochs in total 90 epochs 1 .</p><p>The evaluation metrics we used in our experiments include mean average precision (mAP) over all categories, precision (CP, OP), recall (CR, OR), and F1 score (CF1, OF1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results</head><p>VOC2007 <ref type="bibr" target="#b6">[7]</ref>: We compare our method with ResNet-101 <ref type="bibr" target="#b9">[10]</ref>, CNN-RNN <ref type="bibr" target="#b21">[22]</ref>, AR <ref type="bibr" target="#b2">[3]</ref>, ML-GCN <ref type="bibr" target="#b4">[5]</ref>, FGCN <ref type="bibr" target="#b23">[24]</ref>, SSGRL <ref type="bibr" target="#b3">[4]</ref>. Following <ref type="bibr" target="#b3">[4]</ref>, we also pretrain our model on the MS-COCO dataset.  The results of all the methods are shown in <ref type="table" target="#tab_0">Table 1</ref>. We can see that the result of MSRN(pre) is 2%, 1.9%, and 1% better than ML-GCN <ref type="bibr" target="#b4">[5]</ref>, FGCN <ref type="bibr" target="#b23">[24]</ref>, SSGRL <ref type="bibr" target="#b3">[4]</ref> on mAP respectively. It should be noted that the input image size of SSGRL(pre) is 576?576 which is larger than ours. Our model also achieves the best AP score on 17 categories. The results definitely demonstrate the effectiveness of modeling multi-layered semantic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS-COCO [16]:</head><p>The comparison results on MS-COCO dataset are shown in <ref type="table" target="#tab_1">Table 2</ref>. We compare our method with ResNet-101 <ref type="bibr" target="#b9">[10]</ref>, ML-GCN <ref type="bibr" target="#b4">[5]</ref>, FGCN <ref type="bibr" target="#b23">[24]</ref> and CMA <ref type="bibr" target="#b28">[29]</ref>. Our method can achieve 83.4% mAP score which is in the first rank. Our model achieves comparable performance with the state-of-the-art methods. Specifically, MSRN wins the second place in terms of mAP, CP, and CF1. For the results on top-3 labels, MSRN obtains the best performance in terms of CR, CF1, OR and OF1.</p><p>NUS-WIDE <ref type="bibr" target="#b5">[6]</ref> contains 269,648 images and 81 concepts. The dataset is split by following <ref type="bibr" target="#b28">[29]</ref>. We compare MSRN with CNN-RNN <ref type="bibr" target="#b21">[22]</ref>, AT <ref type="bibr" target="#b29">[30]</ref>, S-CLs <ref type="bibr" target="#b16">[17]</ref> and CMA <ref type="bibr" target="#b28">[29]</ref>. As shown in <ref type="table" target="#tab_2">Table  3</ref>, our model achieves 0.1% better than MS-CMA on mAP. In addition, MSRN achieves the best performance in terms of CF1, OF1 and CF1-3. Both our model and MS-CMA <ref type="bibr" target="#b28">[29]</ref> extract image features from lower layers of CNNs, but our model outperforms it by 0.1%, 0.2%, 0.2% and 0.4% in terms of mAP, CF1, OF1 and CF1-3, respectively. Apparel 2 is a clothing dataset for multi-label image classification. We test our model on it and make comparisons with ResNet-101 <ref type="bibr" target="#b9">[10]</ref>, SSGRL <ref type="bibr" target="#b3">[4]</ref> and ML-GCN <ref type="bibr" target="#b4">[5]</ref>. In our experiment, we randomly select 50% images from the dataset for training, and other 50% images for testing. The result in <ref type="table" target="#tab_3">Table 4</ref> shows that our model achieves 99.65 mAP score. Our model is 0.18% better than ResNet-101 and 0.06% better than the current best model on mAP. Our model also achieves the best score on all metrics that we employ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>In this section, we perform ablation studies to evaluate the effectiveness of different components of our framework.</p><p>Label and Group Embeddings: To verify the effectiveness of label and group embeddings, we conduct experiments with three simplified versions of our proposed method MSRN, i.e., label-E (only using label embedding), group-E (only using group embedding), and no LGE module. The results shown in <ref type="table" target="#tab_4">Table 5</ref> clearly indicate the effectiveness of label and group embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Branches:</head><p>As ResNet-101 contains four blocks, we conduct experiments to validate whether the multi-branch architecture is better than the single-branch architecture and whether the model performs better with all branches. The experimental results are shown in <ref type="table" target="#tab_5">Table 6</ref>. We can find that the multi-branch architecture can improve at least 0.12% compared to the single-branch architecture, and achieves the best performance with the last 3 branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Sensitivity</head><p>In this section, we study the sensitivity of MSRN to two hyper-parameters, i.e., the number of groups of labels m and the regularization parameter ?. Due to the limitation of space, we only present the analyses on VOC2007 dataset. For the number of groups m, we conduct experiments of six different cases corresponding to 2, 4, 6, 8, 10 and 20, respectively, with ? fixed as 10 ?3 . The experimental results in <ref type="table" target="#tab_6">Table 7</ref> show that the performance in terms of mAP is not much sensitive to m. For ?, we study the values from {10 ?1 , 10 ?2 , ..., 10 ?6 }. The results with different values of ? are shown in <ref type="table" target="#tab_6">Table 7</ref>, which shows the performance is not much sensitive to ?.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper proposes a novel Multi-layered Semantic Representation Network (MSRN) for multilabel image classification. MSRN for the first time considers both local semantics and global semantics of labels through modeling label correlations, and learns semantic representations of images at multiple layers of a convolutional neural network through an attention mechanism. Extensive experiments show that MSRN outperforms many state-of-art methods on VOC2007, MS-COCO, NUS-WIDE and Apparel datasets. In the future, we will improve our method to explicitly utilize labels which exist but are unobservable due to lack of labeling efforts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc><ref type="bibr" target="#b1">[2]</ref>, Chen et al. propose an order-free RNN based model for multi-label image classification, which uniquely integrates the learning of visual attention and Long Short Term Memory (LSTM) layers to jointly learn the labels of interest and their co-occurrences. Overall architecture of our MSRN model. Given an image, a CNN network outputs the image features from different layers to different branches. At the same time, we input label correlation graph to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of mAP and AP (in %) of our method and state-of-the-art methods on Pascal VOC2007 dataset where numbers in bold indicate the best performance and numbers underlined indicate the second performance.</figDesc><table><row><cell>voc2007</cell><cell>CNN-RNN</cell><cell>ResNet-101</cell><cell>AR</cell><cell>ML-GCN</cell><cell>A-GCN</cell><cell>F-GCN</cell><cell>SSGRL</cell><cell>SSFRL(pre)</cell><cell>MSRN</cell><cell>MRSN(pre)</cell></row><row><cell>areo</cell><cell>96.7</cell><cell>99.5</cell><cell>98.6</cell><cell>99.5</cell><cell>99.4</cell><cell>99.5</cell><cell>99.5</cell><cell>99.7</cell><cell>100.0</cell><cell>99.7</cell></row><row><cell>bike</cell><cell>83.1</cell><cell>97.7</cell><cell>97.1</cell><cell>98.5</cell><cell>98.5</cell><cell>98.5</cell><cell>97.1</cell><cell>98.4</cell><cell>98.8</cell><cell>98.9</cell></row><row><cell>bird</cell><cell>94.2</cell><cell>97.8</cell><cell>97.1</cell><cell>98.6</cell><cell>98.6</cell><cell>98.7</cell><cell>97.6</cell><cell>98.0</cell><cell>98.9</cell><cell>98.7</cell></row><row><cell>boat</cell><cell>92.8</cell><cell>96.4</cell><cell>95.5</cell><cell>98.1</cell><cell>98.0</cell><cell>98.2</cell><cell>97.8</cell><cell>97.6</cell><cell>99.1</cell><cell>99.1</cell></row><row><cell>bottle</cell><cell>61.2</cell><cell>75.7</cell><cell>75.6</cell><cell>80.8</cell><cell>80.8</cell><cell>80.9</cell><cell>82.6</cell><cell>85.7</cell><cell>81.6</cell><cell>86.6</cell></row><row><cell>bus</cell><cell>82.1</cell><cell>91.8</cell><cell>92.8</cell><cell>94.6</cell><cell>94.7</cell><cell>94.8</cell><cell>94.8</cell><cell>96.2</cell><cell>95.5</cell><cell>97.9</cell></row><row><cell>car</cell><cell>89.1</cell><cell>96.1</cell><cell>96.8</cell><cell>97.2</cell><cell>97.2</cell><cell>97.3</cell><cell>96.7</cell><cell>98.2</cell><cell>98.0</cell><cell>98.5</cell></row><row><cell>cat</cell><cell>94.2</cell><cell>97.6</cell><cell>97.3</cell><cell>98.2</cell><cell>98.2</cell><cell>98.3</cell><cell>98.1</cell><cell>98.8</cell><cell>98.2</cell><cell>98.9</cell></row><row><cell>chair</cell><cell>64.2</cell><cell>74.2</cell><cell>78.3</cell><cell>82.3</cell><cell>82.4</cell><cell>82.5</cell><cell>78.0</cell><cell>82.0</cell><cell>84.4</cell><cell>86.0</cell></row><row><cell>cow</cell><cell>83.6</cell><cell>80.9</cell><cell>92.2</cell><cell>95.7</cell><cell>95.5</cell><cell>95.7</cell><cell>97.0</cell><cell>98.1</cell><cell>96.6</cell><cell>98.7</cell></row><row><cell>table</cell><cell>70.0</cell><cell>85.0</cell><cell>87.6</cell><cell>86.4</cell><cell>86.4</cell><cell>86.6</cell><cell>85.6</cell><cell>89.7</cell><cell>87.5</cell><cell>89.1</cell></row><row><cell>dog</cell><cell>92.4</cell><cell>98.4</cell><cell>96.9</cell><cell>98.2</cell><cell>98.2</cell><cell>98.2</cell><cell>97.8</cell><cell>98.8</cell><cell>98.6</cell><cell>99.0</cell></row><row><cell>horse</cell><cell>91.7</cell><cell>96.5</cell><cell>96.5</cell><cell>98.4</cell><cell>98.4</cell><cell>98.4</cell><cell>98.3</cell><cell>98.7</cell><cell>98.6</cell><cell>99.1</cell></row><row><cell>motor</cell><cell>84.2</cell><cell>95.9</cell><cell>93.6</cell><cell>96.7</cell><cell>96.7</cell><cell>96.7</cell><cell>96.4</cell><cell>97.0</cell><cell>97.2</cell><cell>97.3</cell></row><row><cell>person</cell><cell>93.7</cell><cell>98.4</cell><cell>98.5</cell><cell>99.0</cell><cell>98.9</cell><cell>99.0</cell><cell>98.8</cell><cell>99.0</cell><cell>99.1</cell><cell>99.2</cell></row><row><cell>plant</cell><cell>59.8</cell><cell>70.1</cell><cell>81.6</cell><cell>84.7</cell><cell>84.8</cell><cell>84.8</cell><cell>84.9</cell><cell>86.9</cell><cell>87.0</cell><cell>90.2</cell></row><row><cell>sleep</cell><cell>93.2</cell><cell>88.3</cell><cell>93.1</cell><cell>96.7</cell><cell>96.6</cell><cell>96.7</cell><cell>96.5</cell><cell>98.1</cell><cell>97.6</cell><cell>99.2</cell></row><row><cell>sofa</cell><cell>75.3</cell><cell>80.2</cell><cell>83.2</cell><cell>84.3</cell><cell>84.4</cell><cell>84.4</cell><cell>79.8</cell><cell>85.8</cell><cell>86.5</cell><cell>89.7</cell></row><row><cell>train</cell><cell>99.7</cell><cell>98.9</cell><cell>98.5</cell><cell>98.9</cell><cell>98.9</cell><cell>99.0</cell><cell>98.4</cell><cell>99.0</cell><cell>99.4</cell><cell>99.8</cell></row><row><cell>tv</cell><cell>78.6</cell><cell>89.2</cell><cell>89.3</cell><cell>93.7</cell><cell>93.7</cell><cell>93.7</cell><cell>92.8</cell><cell>93.7</cell><cell>94.4</cell><cell>95.3</cell></row><row><cell>mAP</cell><cell>84.0</cell><cell>89.9</cell><cell>92.0</cell><cell>94.0</cell><cell>94.0</cell><cell>94.1</cell><cell>93.4</cell><cell>95.0</cell><cell>94.9</cell><cell>96.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of our method with state-of-the-art methods on MS-COCO dataset where numbers in bold indicate the best performance and numbers underlined indicate the second performance.</figDesc><table><row><cell>Methods</cell><cell>mAP</cell><cell>CP</cell><cell>CR</cell><cell>CF1</cell><cell>OP</cell><cell>OR</cell><cell>OF1</cell></row><row><cell>CNN-RNN[22]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ResNet-101[10]</cell><cell>80.3</cell><cell>77.8</cell><cell>72.8</cell><cell>75.2</cell><cell>81.5</cell><cell>75.1</cell><cell>78.2</cell></row><row><cell>ML-GCN[5]</cell><cell>83.0</cell><cell>84.0</cell><cell>72.8</cell><cell>78.0</cell><cell>84.7</cell><cell>76.2</cell><cell>80.2</cell></row><row><cell>A-GCN[15]</cell><cell>83.1</cell><cell>84.7</cell><cell>72.3</cell><cell>78.0</cell><cell>85.6</cell><cell>75.5</cell><cell>80.3</cell></row><row><cell>F-GCN[24]</cell><cell>83.2</cell><cell>85.4</cell><cell>72.4</cell><cell>78.3</cell><cell>86.0</cell><cell>75.7</cell><cell>80.5</cell></row><row><cell>SSGRL[4]</cell><cell>83.8</cell><cell>89.9</cell><cell>68.5</cell><cell>76.8</cell><cell>91.3</cell><cell>70.8</cell><cell>79.7</cell></row><row><cell>CMA[29]</cell><cell>83.4</cell><cell>83.4</cell><cell>72.9</cell><cell>77.8</cell><cell>86.8</cell><cell>76.3</cell><cell>80.9</cell></row><row><cell>MS-CMA[29]</cell><cell>83.8</cell><cell>82.9</cell><cell>74.4</cell><cell>78.4</cell><cell>84.4</cell><cell>77.9</cell><cell>81.0</cell></row><row><cell>MSRN</cell><cell>83.4</cell><cell>86.5</cell><cell>71.5</cell><cell>78.3</cell><cell>86.1</cell><cell>75.5</cell><cell>80.4</cell></row><row><cell>Methods</cell><cell>CP-3</cell><cell>CR-3</cell><cell>CF1-3</cell><cell>OP-3</cell><cell>OR-3</cell><cell>OF1-3</cell><cell></cell></row><row><cell>CNN-RNN[22]</cell><cell>59.3</cell><cell>52.5</cell><cell>55.7</cell><cell>59.8</cell><cell>61.4</cell><cell>60.7</cell><cell></cell></row><row><cell>ResNet-101[10]</cell><cell>84.1</cell><cell>59.4</cell><cell>69.7</cell><cell>89.1</cell><cell>62.8</cell><cell>73.6</cell><cell></cell></row><row><cell>ML-GCN[5]</cell><cell>89.2</cell><cell>64.1</cell><cell>74.6</cell><cell>90.5</cell><cell>66.5</cell><cell>76.7</cell><cell></cell></row><row><cell>A-GCN[15]</cell><cell>89.0</cell><cell>64.2</cell><cell>74.6</cell><cell>90.5</cell><cell>66.3</cell><cell>76.6</cell><cell></cell></row><row><cell>F-GCN[24]</cell><cell>89.3</cell><cell>64.3</cell><cell>74.7</cell><cell>90.5</cell><cell>66.6</cell><cell>76.7</cell><cell></cell></row><row><cell>SSGRL[4]</cell><cell>91.9</cell><cell>62.5</cell><cell>72.7</cell><cell>93.8</cell><cell>64.1</cell><cell>76.2</cell><cell></cell></row><row><cell>CMA[29]</cell><cell>86.7</cell><cell>64.9</cell><cell>74.3</cell><cell>90.9</cell><cell>67.2</cell><cell>77.2</cell><cell></cell></row><row><cell>MS-CMA[29]</cell><cell>88.2</cell><cell>65.0</cell><cell>74.9</cell><cell>90.2</cell><cell>67.4</cell><cell>77.1</cell><cell></cell></row><row><cell>MSRN</cell><cell>84.5</cell><cell>72.9</cell><cell>78.3</cell><cell>84.3</cell><cell>76.8</cell><cell>80.4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparion with state-of-the-art methods on NUS-WIDE dataset where numbers in bold indicate the best performance and numbers underlined indicate the second performance.</figDesc><table><row><cell>Methods</cell><cell cols="5">mAP CF1 OF1 CF1-3 OF1-3</cell></row><row><cell>CNN-RNN[22]</cell><cell>56.1</cell><cell>-</cell><cell>-</cell><cell>34.7</cell><cell>55.2</cell></row><row><cell>AT[30]</cell><cell cols="3">57.6 55.2 70.3</cell><cell>51.7</cell><cell>68.8</cell></row><row><cell>S-CLs[17]</cell><cell cols="3">60.1 58.7 73.7</cell><cell>53.8</cell><cell>71.1</cell></row><row><cell>CMA[29]</cell><cell cols="3">60.8 60.4 73.3</cell><cell>55.5</cell><cell>70.0</cell></row><row><cell>MS-CMA[29]</cell><cell cols="3">61.4 60.5 73.8</cell><cell>55.7</cell><cell>69.5</cell></row><row><cell>MSRN</cell><cell cols="3">61.5 60.7 74.0</cell><cell>56.1</cell><cell>69.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison with state-of-the-art methods on Apparel dataset where numbers in bold indicate the best performance and numbers underlined indicate the second performance.</figDesc><table><row><cell>Methods</cell><cell>mAP</cell><cell>CF1</cell><cell>OF1</cell><cell cols="2">CF1-3 OF1-3</cell></row><row><cell cols="5">ResNet-101[10] 99.47 97.51 97.78 97.51</cell><cell>97.78</cell></row><row><cell>SSGRL[4]</cell><cell cols="4">99.57 97.77 98.01 97.77</cell><cell>98.01</cell></row><row><cell>ML-GCN[5]</cell><cell cols="4">99.56 97.68 97.88 97.68</cell><cell>97.87</cell></row><row><cell>MSRN</cell><cell cols="4">99.65 98.21 98.36 98.21</cell><cell>98.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison among different versions of MSRN.</figDesc><table><row><cell cols="5">Setting no LGE label-E group-E MSRN</cell></row><row><cell>mAP</cell><cell>91.75</cell><cell>94.42</cell><cell>94.20</cell><cell>94.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparison among different number of branches. Number of branches last 1 last 2 last 3 all 4 mAP 94.53 94.66 94.85 94.65</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison among different values of m and ?. mAP 94.43 94.74 94.85 94.69 94.66 94.55</figDesc><table><row><cell>m</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>20</cell></row><row><cell cols="7">mAP 94.59 94.85 94.66 94.56 94.63 94.48</cell></row><row><cell>?</cell><cell>10 ?1</cell><cell>10 ?2</cell><cell>10 ?3</cell><cell>10 ?4</cell><cell>10 ?5</cell><cell>10 ?6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source codes and pre-trained models of our method are publicly available at https://jiunhwang.github.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">www.kaggle.com/kaiska/apparel-dataset</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by NSFC: 61806005 and 61906003, and The University Synergy Innovation Program of Anhui Province:GXXT-2020-012.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Order-free rnn with visual attention for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recurrent attentional reinforcement learning for multilabel image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning semantic-specific graph representation for multi-label image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="522" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-label image recognition with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5172" to="5181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Laurens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Chest x-rays classification: A multi-label and fine-grained problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sedai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakravorty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learn to pay attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="817" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-label zero-shot learning with structured knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1576" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning label correlations for multi-label image recognition with graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="378" to="384" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-label image classification via knowledge distillation from weakly supervised detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rank-consistency multi-label deep hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification via multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10978" to="10987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CNN-RNN: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-label classification with label graph superimposing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12265" to="12272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast graph convolution network based multi-label image recognition via cross-modal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1575" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-label image recognition by recurrently discovering attentional regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploit bounding box annotations for multi-label object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-scale recognition with dag-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1215" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-modality attention with semantic graph embedding for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12709" to="12716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning spatial regularization with imagelevel supervisions for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2027" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
