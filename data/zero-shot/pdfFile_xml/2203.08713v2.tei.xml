<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeciWatch: A Simple Baseline for 10? Efficient 2D and 3D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
							<email>alzeng@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Ju</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Sensetime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyuan</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Sensetime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeciWatch: A Simple Baseline for 10? Efficient 2D and 3D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human Pose Estimation</term>
					<term>Video Analysis</term>
					<term>Efficiency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a simple baseline framework for videobased 2D/3D human pose estimation that can achieve 10? efficiency improvement over existing works without any performance degradation, named DeciWatch. Unlike current solutions that estimate each frame in a video, DeciWatch introduces a simple yet effective sample-denoiserecover framework that only watches sparsely sampled frames, taking advantage of the continuity of human motions and the lightweight pose representation. Specifically, DeciWatch uniformly samples less than 10% video frames for detailed estimation, denoises the estimated 2D/3D poses with an efficient Transformer architecture, and then accurately recovers the rest of the frames using another Transformer-based network. Comprehensive experimental results on three video-based human pose estimation, body mesh recovery tasks and efficient labeling in videos with four datasets validate the efficiency and effectiveness of DeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>2D/3D human pose estimation <ref type="bibr">[36,</ref><ref type="bibr">7,</ref><ref type="bibr">63]</ref> has numerous applications, such as surveillance, virtual reality, and autonomous driving. Various high-performance image-based pose estimators <ref type="bibr">[40,</ref><ref type="bibr">51,</ref><ref type="bibr">48,</ref><ref type="bibr" target="#b71">25,</ref><ref type="bibr" target="#b76">30,</ref><ref type="bibr" target="#b75">29]</ref> are proposed in the literature, but they are associated with substantial computational costs.</p><p>There are two main approaches to improving the efficiency of human pose estimators so that they can be deployed on resource-scarce edge devices (e.g., smart cameras). A straightforward way to improve the efficiency is designing more compact models, such as numerous light-weighted image-level pose estimators <ref type="bibr">[3,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b67">21,</ref><ref type="bibr">61,</ref><ref type="bibr">6,</ref><ref type="bibr">60,</ref><ref type="bibr">42,</ref><ref type="bibr">62,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b64">18,</ref><ref type="bibr">54</ref>] (see <ref type="figure" target="#fig_1">Fig. 1</ref>(a)(i)) and video-level pose estimators <ref type="bibr">[41,</ref><ref type="bibr">9]</ref> (see <ref type="figure" target="#fig_1">Fig. 1</ref>(a)(ii)) introduced in previous literature. However, when estimating on a video, such approaches inevitably lead to a sub-optimal solution for efficiency improvement due to the frame-by-frame estimation scheme. In contrast, a promising but rarely explored direction is leveraging the semantic redundancy among frames of videos, where we can feed only keyframes to heavy  (i) shows single-frame efficient methods <ref type="bibr">[3,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b67">21,</ref><ref type="bibr">61,</ref><ref type="bibr">6,</ref><ref type="bibr">60,</ref><ref type="bibr">42,</ref><ref type="bibr">62,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b64">18]</ref> that use lightweight models to reduce the costs of each frame; (ii) presents some temporal efficient strategies <ref type="bibr">[9,</ref><ref type="bibr">41]</ref> that utilize feature similarities among consecutive frames via RNNs to decrease feature extraction cost. (b) is the keyframe-based efficient framework <ref type="bibr">[59,</ref><ref type="bibr">10]</ref>. They first select about 30%?40% keyframes in a video by watching all frames, then recover the whole sequence based on features of selected keyframes. (c) is the proposed efficient sample-denoise-recover framework DeciWatch with 5%?10% frames watched.</p><p>and high-performance modules and recover or estimate the rest of the frames with light-weighted modules <ref type="bibr">[59,</ref><ref type="bibr">10]</ref> (see <ref type="figure" target="#fig_1">Fig. 1(b)</ref>). While the computational efficiency of these works is improved due to the use of keyframes, they still need to conduct costly feature extraction on each frame for keyframe selection, making it hard to further reduce their computational complexity. To achieve highly efficient 2D/3D pose estimation without the need of watching every frame in a video, we propose a novel framework based on the continuity of human motions, which conducts pose estimation only on sparsely sampled video frames. Since these detected poses ineluctably contain various noises, they will affect the effectiveness of the recovery. Subsequently, poses of those sampled frames should be denoised before recovered, where we formulate the three-step sample-denoise-recover framework. By doing so, the problem in the recover stage is similar to the long-standing motion completion task in the computer graphics literature <ref type="bibr" target="#b77">[31,</ref><ref type="bibr" target="#b70">24,</ref><ref type="bibr">46,</ref><ref type="bibr" target="#b74">28,</ref><ref type="bibr">2,</ref><ref type="bibr">8]</ref>. However, there are two main differences: (i). our objective is to achieve highly efficient pose estimation, and hence we could only afford lightweight models for pose recovery on frames that are not processed by pose estimators; (ii). most existing motion completion works assume groundtruth poses on the given keyframes. In contrast, the visible frames in our task could have untrustworthy poses with challenging occlusion or rarely seen actions.</p><p>This work proposes a simple yet effective baseline framework (see <ref type="figure" target="#fig_1">Fig. 1</ref>(c)) that watches sparsely sampled frames for highly efficient 2D and 3D video-based human pose estimation. We empirically show that we could maintain and even improve the pose estimation accuracy, with less than 10% frames calculated with the costly pose estimator. We name the proposed framework DeciWatch, and the contributions of this work include:</p><p>-To the best of our knowledge, this is the first work that considers sparsely sampled frames in video-based pose estimation tasks. DeciWatch is compatible with any given single-frame pose estimator, achieving 10? efficiency improvement without any performance degradation. Moreover, the pose sequence obtained by DeciWatch is much smoother than existing solutions as it naturally models the continuity of human motions. -We propose a novel sample-denoise-recover pipeline in DeciWatch. Specifically, we uniformly sample less than 10% of video frames for estimation, denoise the estimated 2D/3D poses with an efficient Transformer architecture named DenoiseNet, and then accurately recover the poses for the rest of the frames using another Transformer network called RecoverNet. Thanks to the lightweight pose representation, the two subnets in our design are much smaller than the costly pose estimator. -We verify the efficiency and effectiveness of DeciWatch on three human pose estimation, body recovery tasks, and efficient labeling in videos with four widely-used datasets and five popular single-frame pose estimators as backbones. We also conduct extensive ablation studies and point out future research directions to further enhance video-based tasks' efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Efficient Human Pose Estimation</head><p>Efficient attempts at human pose estimation can be divided into image-based and video-based. Image-based efficient pose estimators <ref type="bibr">[3,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b67">21,</ref><ref type="bibr">61,</ref><ref type="bibr">6,</ref><ref type="bibr">60,</ref><ref type="bibr">42,</ref><ref type="bibr">62,</ref><ref type="bibr">4,</ref><ref type="bibr">54]</ref> mainly focus on employing well-designed network structures <ref type="bibr">[3,</ref><ref type="bibr">60,</ref><ref type="bibr">42,</ref><ref type="bibr">62,</ref><ref type="bibr">4,</ref><ref type="bibr">54,</ref><ref type="bibr">56]</ref>, knowledge distillation <ref type="bibr">[35,</ref><ref type="bibr" target="#b67">21,</ref><ref type="bibr" target="#b64">18]</ref>, or low-resolution features <ref type="bibr">[61,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b79">33]</ref> to reduce model capacity and decrease spatial redundancies, where they may suffer from accuracy reduction, especially in the cases of complex and rare poses. Moreover, when dealing with videos, these methods reveal their limitations for having to estimate poses frame-by-frame. Their outputs also suffer from unavoidable jitters because they lack the capability of using temporal information.</p><p>To cope with video inputs, other attempts exploit temporal co-dependency among consecutive frames to decrease unnecessary calculations. However, only a few video-based efficient estimation methods <ref type="bibr">[9,</ref><ref type="bibr">41,</ref><ref type="bibr">59,</ref><ref type="bibr">10]</ref> are proposed in the literature, and they mainly target on 2D pose estimation. In particular, DKD <ref type="bibr">[41]</ref> introduces a lightweight distillator to online distill the pose knowledge via leveraging temporal cues from the previous frame. In addition to using local information of adjacent frames, KFP <ref type="bibr">[59]</ref> designs a keyframe proposal network that selects informative keyframes after estimating the whole sequence, and then applies a learned dictionary to recover the entire pose sequence. Lastly, MAPN <ref type="bibr">[10]</ref> exploits the readily available motion and residual information stored in the compressed streams to dramatically boost the efficiency, and all the residual frames will be calculated by a dynamic gate.</p><p>These proposed methods reduce computation costs by employing adaptive operations on different frames, i.e., complex operations on indispensable frames and simple ones on the rest. Despite obtaining efficiency improvement, they still fail to push the efficiency to a higher level since they ignore the fact that it is not necessary to watch each frame. What's more, relying on image features as intermediate representation is heavy for calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motion Completion</head><p>Motion completion is widely explored in the area of computer graphics, generally including motion capture data completion <ref type="bibr" target="#b66">[20,</ref><ref type="bibr">44,</ref><ref type="bibr" target="#b78">32,</ref><ref type="bibr">12,</ref><ref type="bibr">50,</ref><ref type="bibr">1,</ref><ref type="bibr" target="#b77">31,</ref><ref type="bibr" target="#b70">24,</ref><ref type="bibr">46]</ref> and motion in-filling <ref type="bibr">[11,</ref><ref type="bibr">14,</ref><ref type="bibr">15,</ref><ref type="bibr">14,</ref><ref type="bibr">17,</ref><ref type="bibr" target="#b74">28,</ref><ref type="bibr">2,</ref><ref type="bibr">8,</ref><ref type="bibr">55]</ref>, which has great significance in the film, animation, and game applications. To be specific, points or sequences missing often occur in motion capture due to technical limitations and occlusions of markers. Accordingly, existing approaches include traditional methods (e.g. linear, Cubic Spline, Lagrange, and Newton's polynomial interpolation, low-rank matrix completion) <ref type="bibr" target="#b66">[20,</ref><ref type="bibr">44,</ref><ref type="bibr" target="#b78">32,</ref><ref type="bibr">12]</ref> and learning-based methods (e.g., Recurrent Neural Networks (RNNs)) <ref type="bibr" target="#b77">[31,</ref><ref type="bibr" target="#b70">24]</ref>. Motion in-filling aims to complete the absent poses with specific keyframe constraints. RNNs <ref type="bibr">[11,</ref><ref type="bibr">14,</ref><ref type="bibr">15,</ref><ref type="bibr">14,</ref><ref type="bibr">52]</ref> and convolutional models <ref type="bibr">[53,</ref><ref type="bibr" target="#b74">28]</ref> are commonly used in motion in-filling. Recently, Generative adversarial learning <ref type="bibr">[17,</ref><ref type="bibr" target="#b73">27]</ref> and autoencoder <ref type="bibr" target="#b74">[28,</ref><ref type="bibr">2]</ref> are also introduced for realistic and naturalistic output. Some recent works <ref type="bibr">[8,</ref><ref type="bibr" target="#b65">19]</ref> also introduce selfattention models to infill the invisible frames.</p><p>Although both general motion completion and our target are to recover the full pose sequence, there are two main differences. On the one hand, the objective of motion completion is to generate diverse or realistic motions under certain assumptions, e.g., a recurring or repeated motion like walking. They may fail when motions are aperiodic and complex. In contrast, our goal is to achieve high efficiency in video-based pose estimation, where the benchmarks are usually from real-life videos. On the other hand, motion completion assumes having ground-truth poses as inputs rather than estimated poses. Current designs may not be able to handle unreliable and noisy poses generated from deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition and Overview</head><p>Given an input video I = {I t } T t=1 of length T , a pose estimation framework computes the corresponding sequence of posesP = {P t } T t=1 , aiming to minimize the distance between the estimated posesP and the ground-truth poses P.P t could be any human pose representation, including 2D keypoint position, 3D keypoint position, and 6D rotation matrix.</p><p>The main target of this work is to set a baseline for efficient video-based pose estimation without compromising accuracy. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>(c), we devise a three-step sample-denoise-recover flow to process video-based pose estimation efficiently and effectively. As adjacent frames usually contain redundant information and human motion is continuous, DeciWatch first samples a small percentage of frames (e.g., 10%) I sampled and applies existing pose estimators <ref type="bibr">[51,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b71">25,</ref><ref type="bibr" target="#b76">30,</ref><ref type="bibr" target="#b75">29]</ref> thereon to obtain the corresponding poses. However, recovering the full pose sequence from sparsely observed poses is challenging, especially when the poses are estimated by networks and often contain noise. Relying on a few poses to recover the entire sequence, the quality of sampled poses is the key. To tackle the challenge, we introduce two subnets, DenoiseNet and RecoverNet. Specifically, DenoiseNet refines sparse poses from pose estimator. Then RecoverNet performs motion recovery based on the refined sparse poses to recover the whole pose sequence, with the intuition that humans can perceive complete motion information through a small number of keyframes. With this new mechanism, the computation cost can be reduced significantly by watching only a small number of frames, which replaces high-cost image feature extraction and pose estimation with a low-cost pose recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Getting Sampled Poses</head><p>Different from the previous keyframe-based efficient frameworks <ref type="bibr">[59,</ref><ref type="bibr">10]</ref> using each frame's feature to select keyframes, we use a uniform sampling that watches one frame in every N frame to select sparse frames I sampled as a baseline strategy. Due to the redundancy in consecutive frames and continuity of human poses, a uniform sampling strategy under a certain ratio is capable of keeping enough information for recovery. Then we can estimate I sampled by any existing pose estimators, such as SimplePose <ref type="bibr">[51]</ref> for 2D poses, FCN <ref type="bibr">[39]</ref> for 3D poses, and PARE <ref type="bibr" target="#b75">[29]</ref> for 3D body mesh recovery, to get sparse posesP sampled noisy ? R T N ?(K?D) . K is the number of keypoints, and D is the dimensions for each keypoint. Notably, we experimentally show that uniform sampling can surpass complex keyframe selection methods from both efficiency and accuracy in Sec   by a transformer-based DenoiseNet to handle the dynamically various noises. Then, after a preliminary pose recovery, we embed the sequence into temporal semantic pose tokens and put them into another transformer-based RecoverNet that can leverage spatio-temporal correlations to recover realistic and natural poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Denoising the Sampled Poses</head><p>Motion completion often resorts to ground-truth sparse poses for infilling the whole sequence. However, in our scenario, the sampled poses are obtained by single-frame pose estimators, inevitably leading to noisy, sparse poses. Consequently, the quality of sparse poses is crucial for motion recovery. Before recovering the full motion, we develop a denoising network to refine the sampled pose? P sampled noisy to clean posesP sampled clean . Due to the temporal sparseness and noisy jitters, the key designs of DenoiseNet lie in two aspects: (i) A dynamic model for handling diverse possible pose noises; (ii) Global temporal receptive fields to capture useful Spatio-temporal information while suppressing distracting noises. Based on these two considerations, local operations, like convolutional or recurrent networks, are not well suited. Intuitively, Transformer-based models <ref type="bibr">[49]</ref> are capable of capturing the global correlations among discrete tokens, so we use Transformer-based encoder modules to relieve noises from the sparse poses. The denoise process can be formulated as: The learnable parameters in DenoiseNet are trained by minizingP sampled clean with sampled ground-truth poses P sampled .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Recovering the Sampled Poses</head><p>After getting the sparse clean posesP sampled clean ? R T N ?(K?D) , we use another Spatio-temporal subnet, RecoverNet, to recover the absent poses. In order to learn the consistent temporal correlations, a simple temporal upsampling (e.g., a linear transformation W P R ? R T ? T N ) is applied to perform preliminary sequence recovery to getP preliminary clean ? R T ?(K?D) .</p><formula xml:id="formula_0">P preliminary clean = W P RP sampled clean (3)</formula><p>To make the recovery more realistic and accurate, we adopt another transformerbased network for detailed poses recovery. Unlike the previous pose transformers <ref type="bibr">[64]</ref>, we bring temporal semantics into pose encoding to encode the neighboring D frames' poses into pose tokens via a temporal 1D convolutional layer. The main architecture of RecoverNet is also the same as Transformer, which employs M multi-head self-attention blocks. P = TransformerDecoder Conv1d P preliminary clean + Epos,F sampled clean WRD, <ref type="bibr">(4)</ref> where the pose decoder is W RD ? R C?(K?D) . As illustrated in the second block marked as RecoverNet in <ref type="figure" target="#fig_0">Fig.2</ref>, we draw key information in the Cross-Attention block by leveraging denoised featuresF sampled clean . Efficiency calculation. The computational costs of DeciWatch is from three parts: (i) using existing backbones to estimate the sampled posesP sampled noisy , (ii) using DenoiseNet to get clean sampled posesP sampled clean , and (iii) using Recover-Net to recover the clean sampled poses to the complete pose sequenceP t . To summarize, FLOPs of DeciWatch is:</p><formula xml:id="formula_1">F LOP s = 1 T (T /N * f (E) + T * (f (D) + f (R))),<label>(5)</label></formula><p>where f (?) calculates the model's per frame FLOPs. f (E), f (D) and f (R) represent per frame FLOPs of pose estimators, DenoiseNet and RecoverNet, respectively. Using poses instead of image features as representation makes two subnets computational efficient. Notably, (f (D) + f (R)) f (E) (more than 10 4 ?. Details can be find in <ref type="table" target="#tab_1">Table 1</ref> and 2). Since DeciWatch samples very few frames in step 1, the mean FLOPs can be reduced to 1/N compared with those watch-every-frame methods, resulting in a 10? speedup overalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Loss Function</head><p>We follow recent 3D pose estimation methods <ref type="bibr">[43,</ref><ref type="bibr">57]</ref> to apply a simple L1 regression loss to minimize the errors between P t andP t for 2D or 3D pose estimation. Particularly, to learn the noisy patterns from sampled estimated poses, we further add an L1 loss between sparse estimated posesP sampled clean and the corresponding ground-truth poses P sampled . Therefore, the objective function is defined as follows.</p><formula xml:id="formula_2">L = ?( 1 T T t=1 |P t ? P t |) + 1 (T /N ) T /N n=1 |P sampled(n) clean ? P sampled(n) |,<label>(6)</label></formula><p>where ? is a scalar to balance the losses between RecoverNet and DenoiseNet. We set ? = 5 by default.</p><p>4 Experiments 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets. We verify our baseline framework on three tasks. For 2D pose estimation, we follow existing video-based efficient methods <ref type="bibr">[59,</ref><ref type="bibr">10]</ref> using dataset Sub-JHMDB <ref type="bibr" target="#b69">[23]</ref>. For 3D pose estimation, we choose the most commonly used dataset Human3.6M <ref type="bibr" target="#b68">[22]</ref>. For 3D body recovery, we evaluate on an in-the-wild dataset 3DPW <ref type="bibr">[38]</ref> and a dance dataset AIST++ <ref type="bibr" target="#b80">[34]</ref> with fast-moving and diverse actions. Evaluation metrics. For 2D pose estimation, we follow previous works <ref type="bibr">[41,</ref><ref type="bibr">59,</ref><ref type="bibr">10]</ref> adopting the Percentage of the Correct Keypoints (PCK ), where the matching threshold is set as 20% of the bounding box size under pixel level. For 3D pose estimation and body recovery, following <ref type="bibr" target="#b72">[26,</ref><ref type="bibr" target="#b76">30,</ref><ref type="bibr" target="#b75">29,</ref><ref type="bibr">39,</ref><ref type="bibr">58]</ref>, we report Mean Per Joint Position Error (MPJPE ) and the mean Acceleration error (Accel ) to respectively measure the localization precision and smoothness. Besides, we report efficiency metrics mean FLOPs (G) per frame, the number of parameters and the inference time tested on a single TITAN Xp GPU. Implementation details. To facilitate the training and testing in steps 2 and 3, we first prepare the detected poses on both training and test sets offline. The uniform sampling ratio is set to 10% by default, which means watching one frame in every N = 10 frames in videos. To deal with different input video lengths, we input non-overlapping sliced windows with fixed window sizes. It is important to make sure the first and last frames are visible, so the input and output window sizes are both (N * Q + 1), where Q is the average number of visible frames in a window. We set Q = 1 for 2D poses due to the short video length of the 2D dataset and Q = 10 for others. We change embedding dimension C and video length T to adapt different datasets and estimators, which influence FLOPs slightly. For DenoiseNet, we apply M = 5 transformer blocks with embedding dimension C = 128 by default. For RecoverNet, we use the same settings as DenoiseNet. The temporal kernel size of the semantic pose encoder is 5. For more details, please refer to the supplementary material. All experiments can be conducted on a single TITAN Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Efficient Video-based Methods</head><p>Existing efficient video-based pose estimation methods <ref type="bibr">[41,</ref><ref type="bibr">59,</ref><ref type="bibr">10]</ref> only validate on 2D poses. In this section, we compare the accuracy and the efficiency of DeciWatch with SOTAs. We follow their experiment settings for fair comparisons and use the same pose estimator SimplePose <ref type="bibr">[51]</ref>. As shown in <ref type="table" target="#tab_1">Table 1</ref>, our approach shows significantly increased accuracy with the highest efficiency, achieving more than 20? improvement in the computation cost on the Sub-JHMDB dataset. Compared to the SOTA method <ref type="bibr">[10]</ref>, we surpass them by 4.3% and 4.4% on average PCK (Avg.) with 55.7% and 77.9% reduction in FLOPs. Our improvement mainly comes from elbows (Elb.) (from 91.7% to 99.6%) and ankles (Ank.) (from 92.2% to 96.6%) under a 8.3% ratio. These outer joints usually move faster than inner joints (e.g., Hips), which may cause motion blur and make estimators hard to detect precisely. However, previous efficient video-based pose estimation methods did not consider a denoising or refinement strategy. DeciWatch uses DenoiseNet to reduce noises. Then, RecoverNet interpolates the sparse poses using the assumption of continuity of motion without watching blurry frames, showing the superiority of DeciWatch.</p><p>To further verify the effectiveness of the denoise scheme in DeciWatch, we input the full sequence of outputs from SimplePose, which means the Ratio is 100%, and the result of P CK is 99.3. The additional improvement in accuracy shows that DeciWatch can also be used as an effective denoise/refinement model to further calibrate the output positions. Based on the above observations, using a lightweight DeciWatch in a regression manner to further refine heatmap-based 2D pose estimation methods can be a promising refinement strategy. For efficiency, the total number of parameters in DenoiseNet and RecoverNet is 0.60M and the inference time is about 0.58ms/frame.</p><p>Besides, we argue that PCK@0.05 with lower thresholds will be better to reflect the effectiveness of the methods since the commonly used metric PCK@0.2 appears saturated. We report PCK@0.05 and PCK@0.1 of DeciWatch in Supp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Boosting Single-frame Methods</head><p>The used single-frame pose estimators: We compare DeciWatch with the following single-frame pose estimators <ref type="bibr">[39,</ref><ref type="bibr" target="#b76">30,</ref><ref type="bibr" target="#b71">25,</ref><ref type="bibr" target="#b75">29]</ref> that watch each frame when estimating a video. We first introduce these methods as follows.</p><p>-FCN <ref type="bibr">[39]</ref> is one of the most important 2D-to-3D methods with multiple fully connected layers along the spatial dimension.</p><p>-SPIN <ref type="bibr" target="#b76">[30]</ref> is one of the most commonly used methods, which combines SMPL optimization in the training process.</p><p>-EFT <ref type="bibr" target="#b71">[25]</ref> is trained on augmented data compared with SPIN <ref type="bibr" target="#b76">[30]</ref> to get better performance and generalization ability.</p><p>-PARE <ref type="bibr" target="#b75">[29]</ref> proposes a part-guided attention mechanism to handle partial occlusion scenes, achieving the state-of-the-art on many benchmarks. The comparisons: We demonstrate the comparison results in <ref type="table" target="#tab_2">Table 2</ref> at sampling ratios of 10% (N = 10) and 6.7% (N = 15). To be specific, when the sampling ratio is 10%, DeciWatch can reduce MPJPE by about 2% to 3% for most estimators and reduce Accel by about 73% to 92%, indicating DeciWatch achieves higher precision and smoothness with about 10% FLOPs. Moreover, with 6.7% watched frames, DeciWatch still has the capability to recover the complete pose sequence with competitive results. For the AIST++ dataset, we surprisingly find that training on sparse poses and recovering them can significantly improve output qualities by 33.8% and 23.6% with a sampling ratio of 10% and 6.7% respectively. This indicates that our method is capable of datasets with fast movements and difficult actions, such as Hip-hop or Ballet dances.</p><p>In general, we attribute the high efficiency of DeciWatch to the use of lightweight and temporal continuous poses representation rather than the heavy features used by previous works <ref type="bibr">[41,</ref><ref type="bibr">59,</ref><ref type="bibr">10]</ref>. Meanwhile, the superior effectiveness, especially for motion smoothness, comes from its ability to capture spatiotemporal dynamic relations in the denoising and recovery process and the welldesigned sample-denoise-recover steps. Additionally, the inference speeds in step 2 and 3 are about 0.1ms/frame, significantly faster than image feature extraction. All estimation results are re-implemented or tested by us for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with Motion Completion Techniques</head><p>The third step of DeciWatch is similar to motion completion/interpolation as introduced in Sec. 2.2. To assess existing interpolation methods quantitatively, we compare our model with four traditional methods and one of the latest learning-based interpolation methods <ref type="bibr">[2]</ref> based on Conditional Variational Auto-Encoder(CVAE). The original experiments in the CAVE-based model are based on the ground-truth of the Human3.6M dataset <ref type="bibr" target="#b68">[22]</ref> (marked as CVAE [2]-R. ). We compare two additional settings on the same dataset: (i). CVAE [2]-R. inputs estimated 3D poses rather than ground-truth 3D poses and uses Random sampling; (ii). CVAE [2]-U. inputs estimated 3D poses and use Uniform sampling, which is the same setting as DeciWatch. For a fair comparison, we adjusted the sampling ratio of training and testing to be consistent as 20%, 10%, and 5%.</p><p>In <ref type="table" target="#tab_10">Table 3</ref>, DeciWatch outperforms all methods. Specifically, we find that the results of the CVAE-based model are even twice as bad as the traditional methods at all ratios, especially with estimated poses inputs and uniform sampling. This is because CVAE-based methods try to encode a long sequence of motion into an embedding and then recover them, which is practically difficult to embed well and recover precisely for a specific video. Instead, our method and <ref type="table" target="#tab_10">Table 3</ref>. Comparison of MPJPE with existing motion completion methods on Human3.6M dataset <ref type="bibr" target="#b68">[22]</ref> for 3D pose estimation. Noted that <ref type="bibr">[2]</ref> is originally trained and tested on ground-truth 3D poses (noted by ) with random sampling (CVAE [2]-R.), we retrain their model with detected 3D poses to keep the same uniform sampling as us (CVAE [2]-U.). We use FCN <ref type="bibr">[39]</ref> as the single-frame estimator to generate the sparse detected results, and its MPJPE is 54.6mm. the traditional interpolation strategies directly utilize the continuity of human poses as a priori, making the interpolation process easier.</p><p>Owing to the relatively low MPJPE and the slow motions in the Human3.6M dataset, DeciWatch only have limited improvement over traditional methods. We evaluate on a more challenging dance dataset AIST++ <ref type="bibr" target="#b80">[34]</ref> in <ref type="table">Table 4</ref>. A tremendous lift is revealed to over 30% under a 10% ratio. The improvement of DeciWatch is from: DeciWatch can learn to minimize errors with data-driven training, especially poses with high errors, while traditional methods have no such prior knowledge to decrease the errors from both visible and invisible poses. <ref type="table">Table 4</ref>. Comparison of MPJPE with traditional interpolation methods on AIST++ dataset <ref type="bibr" target="#b80">[34]</ref>. We use 3D pose estimator SPIN <ref type="bibr" target="#b76">[30]</ref> as the single-frame estimator, and its MPJPE is 107.7mm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ratio</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>As a baseline framework, we do not emphasize the novelty of network design but provide some possible designs in each step for further research. We have explored the impact of different pose estimators in <ref type="table" target="#tab_1">Table 1</ref> and 2 in previous sections. This section will explore how designs in steps 2 and 3 influence the results. All experiments use the same input window length at 101 and a 10% sampling ratio by default. We keep the same setting in both training and testing. Impact of sampling ratio and input window size. Due to space limitation, we discuss this part in Supp.. We summarize the key observations as follows :(i). With the increase in sampling ratio, the MPJPEs first drop before rising, and they are at the lowest when the sampling ratio is about 20%. Accels will decrease constantly. These observations give us a new perspective that in pose estimation, it is not essential to watch all frames for achieving a better and smoother performance. (ii). Besides, the MPJPE of DeciWatch surpasses the original baseline even when the sampling ratio is about 8%. (iii). Lastly, DeciWatch is robust to different window sizes from 11 to 201 frames. Impact of sampling strategies.</p><p>Although we use uniform sampling one frame for every N frame, there are other sampling strategies that can be adopted without watching each frame, such as (i). uniform sampling 2 or 3 frames for every N frames, which contains velocity and acceleration information (named as U.-2 and U.-3 ); (ii). random sampling (R.); (iii). combining uniform sampling with random sampling (U.-R.). From <ref type="table" target="#tab_5">Table 5</ref>, U.-2 and U.-3 get the worse results compared to U.(Ours) because intervals between visible frames become longer, and the information in two or three adjacent frames is too similar to be helpful for the recovery. Moreover, random sampling shows is capable of recovery since a long invisible period may appear, which is hard for model learning. Combining uniform sampling (half of the frames) to avoid long invisible periods can slightly decrease the error in random sampling. In summary, uniform sampling one frame for every N frame (U.(Ours)) surpasses all other sampling strategies under the same model. Impact of denoise and recovery subnets. In <ref type="table" target="#tab_6">Table 6</ref>, we comprehensively verify the effectiveness of DenoiseNet and RecoverNet at 10% sampling ratio on three datasets. When we remove any part of the two subnets, the results deteriorate to various degrees. Removing RecoverNet means we only use a preliminary recovery via a temporal linear layer, which leads to unsatisfying results as discussed in Sec. 4.4. In fact, RecoverNet is very important for the whole framework since it is supervised by the entire sequence's ground-truth, especially for the fast-moving dance dataset AIST++ <ref type="bibr" target="#b80">[34]</ref>. DenoiseNet can remove noises in advance while giving a better initial pose sequence to RecoverNet, which can reduce the burden in the recovery stage. In summary, the two subnets are both essential and effectively improve the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">An application: Efficient Pose Labeling in Videos</head><p>A large amount of labeled data leads to the success of deep models. However, labeling each frame in videos is labor-intensive and high cost. It is also hard to guarantee continuity among adjacent frames, especially for 3D annotations. Due to the efficiency and smoothness of the pose sequences recovered by DeciWatch, reducing the need for dense labeling could be a potential application. We verify </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This work proposes a sample-denoise-recover flow as a simple baseline framework for highly efficient video-based 2D/3D pose estimation. Thanks to the lightweight representation and continuity characteristics of human poses, this method can watch one frame in every 10 frame and achieve nearly 10? improvement in efficiency while maintaining competitive performance, as validated in the comprehensive experiments across various video-based human pose estimation and body mesh recovery tasks. There are many opportunities to further improve the proposed baseline solution:</p><p>Adaptive sampling and dynamic recovery. In DeciWatch, we use a simple uniform sampling strategy for all the joints. In practice, the movements of different joints under different actions vary greatly. Consequently, an adaptive sampling strategy has the potential to further boost the efficiency of video-based pose estimators. For instance, combining multi-modality information (e.g., WIFI, sensors) to relieve visual computation can be interesting. Correspondingly, how to design a dynamic recovery network that can handle non-uniformly sampled poses is an interesting yet challenging problem to explore. High-performance pose estimator design. While this work emphasizes the efficiency of pose estimators, our results show that watching fewer frames with our framework could achieve better per-frame precision compared with watching each frame. This is in line with the recent findings on multi-view pose estimation methods <ref type="bibr">[5,</ref><ref type="bibr">13,</ref><ref type="bibr">45]</ref>, showing better results without calculating every possible view simultaneously. We attribute the above phenomena to the same intrinsic principle that it is likely to achieve better results by discarding some untrustworthy estimation results. Therefore, designing such a strategy to achieve the best pose estimation performance is an interesting problem to explore.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Dataset Descriptions</head><p>-Sub-JHMDB JHMDB[23] is a video-based dataset for 2D human pose estimation. For a fair comparison, we only conduct our experiments on a subset of JHMDB called sub-JHMDB. It contains 316 videos and the average duration is 35 frames. For each frame, it provides 15 annotated body keypoints. We use the bounding box calculated from the puppet mask provided by <ref type="bibr">[37]</ref>. Following the settings <ref type="bibr">[59,</ref><ref type="bibr">10]</ref>, we mix 3 original splitting schemes for training and testing together in 2D pose estimation experiments.</p><p>-Human3.6M <ref type="bibr" target="#b68">[22]</ref> Human3.6M is a large-scale indoor video dataset with 15 actions from 4 camera viewpoints. It has 3.6 million frames and a frame rate of 50 fps. 3D human joint positions are captured accurately from a high-speed motion capture system. Following previous works <ref type="bibr">[56,</ref><ref type="bibr">39,</ref><ref type="bibr">43,</ref><ref type="bibr">58]</ref>, we use the standard protocol with 5 actors (S1, S5, S6, S7, S8) as the training set and another 2 actors (S9, S11) as the testing set.</p><p>-3DPW <ref type="bibr">[38]</ref> 3DPW is a challenging in-the-wild dataset consisting of more than 51k frames with accurate 3D poses and shapes annotation. The sequences are 30fps. This dataset is usually used to validate the performance of model-based body recovery methods <ref type="bibr" target="#b72">[26,</ref><ref type="bibr" target="#b76">30,</ref><ref type="bibr" target="#b71">25,</ref><ref type="bibr" target="#b75">29</ref>].</p><p>-AIST++ [34] is a challenging dataset with diverse and fast-moving dances that comes from the AIST Dance Video DB <ref type="bibr">[?]</ref>. It contains 3D human motion annotations of 1, 408 video sequences at 60 fps, which is 10.1M frames in total. The 3D human keypoint annotations and SMPL parameters it provides cover 30 different actors in 9 views. We follow the original settings to split the training and testing sets based on actors and actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">An application: Efficient Pose Labeling in Videos</head><p>Due to the efficiency and smoothness of the pose sequences recovered by Deci-Watch, reducing the need for dense labeling could be a potential application. We verify the effectiveness of this application on the Human3.6M and AIST++ dataset by directly inputting the sparse ground-truth 3D positions into the RecoverNet of DeciWatch. In <ref type="table" target="#tab_1">Table 1</ref>, we compare DeciWatch with the most used spline interpolation, linear interpolation, and quadratic interpolation. Our method has a slower error growth as the interval N gets larger. To be specific, it is possible to label one frame every 10 frames with only 2.89mm position errors in slow movement videos (e.g., in Human3.6M <ref type="bibr" target="#b68">[22]</ref>) and label one frame every 5 frames with only 4.03mm position errors in fast-moving videos (e.g., in AIST++ <ref type="bibr" target="#b80">[34]</ref>). This application can improve annotation efficiency by more than 10?. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Additional Evaluation Metrics for 2D Pose Estimation</head><p>As is shown in Tab. 1 in the main paper, the results of DeciWatch have achieved nearly 99% accuracy on PCK@0.2. However, qualitative visualization shows that an awful lot of errors still exist in the recovery results. We attribute it to the fact that PCK@0.2 is quite loose for accuracy measurement, which only requires the detected keypoints to be within 20% of the bounding box size under pixel level. As a result, we use two additional evaluation metrics, PCK@0.1, and PCK@0.05, for better localization evaluation. More specifically, PCK@0.1 and PCK@0.05 restrict the matching threshold to 10% and 5% of the bounding box size. Tab. 2 shows the results of DeciWatch and SimplePose <ref type="bibr">[51]</ref> on these three metrics. In future work, we recommend using PCK@0.05 as the main metrics for 2D pose estimation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generalization Ability</head><p>DeciWatch learns the patterns of noisy human motions since motion distribution could be overlapped among some datasets, making it has potential generalization ability. We further test DeciWatch trained on 3DPW-PARE across various backbones and datasets in Tab. 3, where DeciWatch still achieves competitive pose estimation results with 10x efficiency. We attribute it to the fact that DeciWatch effectively learns the continuity of motions, which is applicable for different sorts of motions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study</head><p>Impact of sampling ratio and input window size. Both input window size and sampling ratio will affect inference efficiency and performance of DeciWatch. With the same input window size, the lower the sampling ratio is, the more efficient the inference process will be. We present the comparison of original pose estimator (Ori.)and DeciWatch with sampling ratio from 100% to 5% (sampling interval N changes from 1 to 20) in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>. When the sampling ratio is 100%, DeciWatch can be regarded as a denoise model. As shown in <ref type="figure" target="#fig_1">Fig. 1(a)</ref>, the changing trends of MPJPE are similar for all three estimation methods (PARE <ref type="bibr" target="#b75">[29]</ref>, EFT <ref type="bibr" target="#b71">[25]</ref>, SPIN <ref type="bibr" target="#b76">[30]</ref>). Surprisingly, we find that MPJPEs first drop before rising, and they are smallest when the sampling ratio is about 20%, with improvements of 4.9%, 3.4%, and 4.9% for PARE, EFT, and SPIN respectively. This gives us a new perspective that in pose estimation, not every frame has to be watched to achieve better performance. The reason behind this is the different degrees of noise in estimated poses. It may be harder to eliminate these diverse degrees of errors in all frames than only denoise some of the frames and recover the rest by temporal continuity. Besides, the MPJPEs of DeciWatch is worse than that of the original pose estimator when the sampling ratio is larger than 8% due to too limited input information.</p><p>With the sampling ratio fixed at 10%, we further explore the influence of window size. In <ref type="figure" target="#fig_1">Fig. 1(b)</ref>, we test window sizes from 11 to 201. Results indicate that our framework is robust to different window sizes.   <ref type="bibr" target="#b75">[29,</ref><ref type="bibr" target="#b71">25,</ref><ref type="bibr" target="#b76">30]</ref> to our framework on the 3DPW dataset.</p><p>To serve for future research, we report results, including MPJPEs and Accels, of 3D pose and body estimation on 3DPW, Human3.6M, and AIST++ datasets in <ref type="table">Table 4</ref>. All results show similar trends in the change of precision (MPJPEs) and smoothness (Accels). In addition, DeciWatch utilizes the natural smoothness of human motions to recover the detected poses. As a result, the Accels decreases steadily when the interval N increases, indicating DeciWatch can enhance the smoothness of the existing backbone methods. Analyses on the phenomenon: fewer samples with better performance. When the inputs of DeciWatch are ground-truth poses, the performance deteriorates with decreased sampling ratio (see <ref type="table" target="#tab_1">Table 1</ref> above). However, in practice, the inputs of DeciWatch are noisy detected poses, and some of them have high errors (e.g., due to occlusion). Consequently, considering the detected poses' errors, two factors affect the recovered poses.</p><p>1) On the one hand, not considering/aggregating the highly noisy poses can improve performance by reducing the impact of noisy poses on both DenoiseNet <ref type="table">Table 4</ref>. Results of original (Ori.) estimators <ref type="bibr" target="#b75">[29,</ref><ref type="bibr" target="#b71">25,</ref><ref type="bibr" target="#b76">30,</ref><ref type="bibr">39]</ref> and DeciWatch under different sampling ratios. Ori. is the watch-every-frame pose estimator. Sampling interval N is set from 1 to 20. The best results are in bold.</p><p>Metrics/N Ori <ref type="table" target="#tab_1">.  1  3  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20</ref> and RecoverNet. 2) On the other hand, dropping too many frames would lead to performance degradation due to information insufficiency. Generally speaking, when the sampling ratio is high (e.g., &gt;20%), we could easily recover intermediate poses thanks to the continuity of motions. And for those dropped intermediate poses, the denoised and recovered poses via Deci-Watch obtain lower error compared to their original noisy estimation. Consequently, the overall MPJPEs would drop with the increase of intervals (i.e., the decrease of sampling ratio) in the beginning. However, when the sampling ratio becomes too low (e.g., &lt;5%), the highly sparse poses do not provide sufficient information for motion recovery, and the MPJPEs would go up under such circumstances. In other words, there would be a "sweet spot" for the sampling ratio with the minimum MPJPEs. This phenomenon is also present in the traditional interpolation method, where MPJPE first drops (from 107.7mm to 105.8mm) before rising. Study on different denoise networks. As a baseline framework, we try to explore the performance of different network designs of the two subnets, De-noiseNet and RecoverNet. In the second step, we use DenoiseNet with Transformer architecture to relieve noises from single-frame estimators. We first remove this network to validate its effectiveness. <ref type="table" target="#tab_5">Table 5</ref> shows a 2.1mm reduction of MPJPE without DenoiseNet, indicating this step is essential to the recovery of more precise poses. Then, we try to simply replace the Transformer with TCNs <ref type="bibr">[43]</ref>, with zero paddings to make the input and output length the same, and MLPs <ref type="bibr">[58]</ref> along temporal axes. Results show these models are incapable of handling the discrete diverse noises, making the final recovery results worse than the original result. Study on different recovery methods. Lastly, we analyze possible designs of the recovery process in the third step. First, we try the simple Linear interpolation, which shows more significant errors compared with the original PARE since it loses the non-linear motion dynamics. Then, we adopt TCNs <ref type="bibr">[43]</ref>, which have local temporal receptive fields (e.g., 3) in each layer to recover the missing values with the interval N as 10, and it leads to the worst results. After adding MLPs <ref type="bibr">[58]</ref> at the last layer to enhance long-term temporal coherence, the error reduces from 172.3mm to 99.5mm (by 42.3% improvement), but the error is still far from satisfactory. MLPs <ref type="bibr">[58]</ref> can utilize the continuity of temporal dimension to learn non-linear fitting curves from sampled points. Still, they do not aggregate spatial information, which makes them get a slightly worse result. Study on different hyper-parameters. We also show the effects of hyperparameters in DeciWatch. ? is used in the loss function to balance the losses between RecoverNet and DenoiseNet. Results in <ref type="table" target="#tab_14">Table 7</ref> show that MPJPEs are robust to diverse loss values. Therefore, we set it to 5 by default. Moreover, we use the same embedding dimension C and block number M in transformer blocks. We show the results of different C in <ref type="table">Table 8</ref> and M in <ref type="table">Table 9</ref>. Fewer parameters, such as C = 12 and M = 1, will lead to performance degradation. As the model becomes deeper (larger M ) and wider (larger C), the performance will meet saturation. By default, we set C = 64 and M = 5 for all experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Qualitative Results</head><p>We demonstrate three typical successful cases of DeciWatch to understand why DeciWatch uses fewer frames with higher efficiency but gets better performance than existing single-frame methods. First, cases in <ref type="figure" target="#fig_0">Fig. 2</ref> show that DeciWatch can improve not only efficiency but also effectiveness on the 3D body recovery task. The estimated body in the yellow boxes are inputs of DeciWatch, where the interval N is set to 10. Existing SOTA models, like PARE <ref type="bibr" target="#b75">[29]</ref>, will fail (illustrated in red boxes) when the frames have heavy body occlusions, human interactions, or poor image quality. Interestingly, DeciWatch skips some frames (inputs are in yellow boxes) to avoid the negative effect. Therefore, compared with the watch-every-frame model <ref type="bibr" target="#b75">[29]</ref>, DeciWatch may reduce the effects of unreliable and noisy estimated poses by a temporal recovery scheme to obtain the rest of the results.</p><p>What happens if there are mistakes in the visible frames? In <ref type="figure" target="#fig_8">Fig. 3</ref>, we show the impact of denoising scheme in DeciWatch on 2D pose estimation. Given a sliding window of 31 frames, we mainly demonstrate the visible four frames (highlighted in yellow boxes) with their detected 2D poses by SimplePose <ref type="bibr">[51]</ref>. We observe that there are left-right flipped keypoint detection in the 1 th and 21 th frames of <ref type="figure" target="#fig_8">Fig. 3(a)</ref>, which sometimes happens when the input image is the back of the person. In the 31 th frame of <ref type="figure" target="#fig_8">Fig. 3(d)</ref>, high errors occur due to heavy self-occlusion. Our method utilizes long temporal effective receptive fields to denoise the noisy input poses and then recover the clean sparse poses to get the final sequence poses, making the output poses smooth and precise in an efficient way.</p><p>In addition to being able to do better motion sequence recovery, can Deci-Watch still learn motion prior? In some cases, even if all visible frames are inaccurate, DeciWatch can still recover accurate poses by learning motion prior. As shown in <ref type="figure" target="#fig_9">Fig. 4, (a)</ref> shows the original video frames of AIST++ with an interval of 10, which is all the visible frames in one slide window (a sliding window with the length of 101 has 11 visible frames). (b) is the corresponding SMPL pose detected by SPIN <ref type="bibr" target="#b76">[30]</ref>. Large errors occur in the actor's occluded right arm and hand. In <ref type="figure" target="#fig_9">Fig. 4(c)</ref>, DeciWatch can successfully correct the errors and outputs smooth poses leveraging dancing action prior and human motion continuity, which are hard for existing single-frame estimators to estimate occluded body parts. (d) shows the ground truth poses of the video frames.</p><p>For more visualization of 2D pose estimation, 3D pose estimation as well as body recovery, please refer to our website 1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Failure Case Analyses</head><p>There are two types of failure cases in DeciWatch, which motivates the two corresponding future directions.</p><p>-When the sampling rate is lower than the motion frequency of some body parts, it will be difficult to supplement the actual motion. Human body is articulated. Thus different body parts have different movement frequencies and distribution. For example, the moving frequency and amplitude of hands and feet will be greater than that of the trunk. Our method adopts the same sampling rate for the whole body without considering that the motion distribution of different keypoints is different. In some actions, such as playing the guitar, only the hand will move at high frequency, but most other joints will not move, so the detailed information recovery of hand movement will be lost. Therefore, adaptive sampling strategies, especially on different body parts or keypoints, will be beneficial. -If the estimated poses of most visible frames in the sliding window are in large errors, it is hard for DeciWatch to recover the correct poses. As shown in <ref type="figure" target="#fig_8">Fig. 3</ref>, although our method can correct the noisy poses to some extent, this is the advantage of learnable methods. That is, the traditional interpolation method can not fix them. However, if most of the visible poses are noisy, our output may also tend to have similar (but smooth) errors. Thus, it is still essential to continuously improve the performance and robustness of pose estimation methods, especially in extreme scenes. At the same time, we can also consider using additional lightweight information, such as IMUs, to help improve performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 A.</head><label>2</label><figDesc>Zeng et al.    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The workflows of three types of efficient pose estimation frameworks. (a) is compact model designs. The (green) pose estimation module has two design strategies:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of the denoise and recovery subnets. First, we denoise the sparsely sampled posesP sampled noisy into a clean posesP sampled clean</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )</head><label>1</label><figDesc>As demonstrated in the left block of Fig. 2, after being encoded through a linear projection matrix W DE ? R (K?D)?C , summed with a positional embedding E pos ? R T N ?C , and then processed by the TransformerEncoder composed of M Multi-head Self-Attention blocks as in [49], input noisy posesP sampled noisy are embedded into a clean featureF sampled clean ? R T N ?C , where C is the embedding dimension. Dropout, Layer Normalization, and Feedforward layers are the same as the original Transformer. Lastly, we use another linear projection matrix W DD ? R C?(K?D) to obtain refined sparse poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Acknowledgement. This work is supported in part by Shenzhen-Hong Kong-Macau Science and Technology Program (Category C) of Shenzhen Science Technology and Innovation Commission under Grant No. SGDX2020110309500101, and Shanghai AI Laboratory. 18. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015) 19. Ho, H.I., Chen, X., Song, J., Hilliges, O.: Render in-between: Motion guided video synthesis for action interpolation. arXiv preprint arXiv:2111.01029 (2021) 20. Howarth, S.J., Callaghan, J.P.: Quantitative assessment of the accuracy for three interpolation techniques in kinematic analysis of human movement. Computer methods in biomechanics and biomedical engineering 13(6), 847-855 (2010) 21. Hwang, D.H., Kim, S., Monet, N., Koike, H., Bae, S.: Lightweight 3d human pose estimation network training using teacher-student learning. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 479-488 (2020) 22. Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence 36(7), 1325-1339 (2013) 23. Jhuang, H., Gall, J., Zuffi, S., Schmid, C., Black, M.J.: Towards understanding action recognition. In: Proceedings of the IEEE international conference on computer vision. pp. 3192-3199 (2013) 24. Ji, L., Liu, R., Zhou, D., Zhang, Q., Wei, X.: Missing data recovery for human mocap data based on a-lstm and ls constraint. In: 2020 IEEE 5th International Conference on Signal and Image Processing (ICSIP). pp. 729-734. IEEE (2020) 25. Joo, H., Neverova, N., Vedaldi, A.: Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. In: 2021 International Conference on 3D Vision (3DV). pp. 42-52. IEEE (2021) 26. Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J.: End-to-end recovery of human shape and pose. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7122-7131 (2018) 27. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 4401-4410 (2019) 28. Kaufmann, M., Aksan, E., Song, J., Pece, F., Ziegler, R., Hilliges, O.: Convolutional autoencoders for human motion infilling. In: 2020 International Conference on 3D Vision (3DV). pp. 918-927. IEEE (2020) 29. Kocabas, M., Huang, C.H.P., Hilliges, O., Black, M.J.: Pare: Part attention regressor for 3d human body estimation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 11127-11137 (2021) 30. Kolotouros, N., Pavlakos, G., Black, M.J., Daniilidis, K.: Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2252-2261 (2019) 31. Kucherenko, T., Beskow, J., Kjellstr?m, H.: A neural network approach to missing marker reconstruction in human motion capture. arXiv preprint arXiv:1803.02665 (2018) 32. Lai, R.Y., Yuen, P.C., Lee, K.K.: Motion capture data completion and denoising by singular value thresholding. In: Eurographics (Short Papers). pp. 45-48 (2011) 33. Li, J., Bian, S., Zeng, A., Wang, C., Pang, B., Liu, W., Lu, C.: Human pose regression with residual log-likelihood estimation. In: ICCV (2021) 34. Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance generation with aist++. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 1 .</head><label>1</label><figDesc>Comparing effects of different (a) sampling ratios and (b) window sizes. Sampling interval N is from 1 to 20. We compare MPJPEs of the three original (Ori.) pose estimators</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 2 .</head><label>2</label><figDesc>Visualization results of estimated body recovery from two video sequences with eleven frames in (a) and (e) rows. (b) and (f) are estimated poses from the existing SOTA model PARE [29]. We highlight the input poses of DeciWatch in the yellow boxes and the high-error poses in the red boxes. (c) and (g) are output poses of our proposed DeciWatch, the sampling ratio is 10% in this framework. (d) and (h) show the ground truth of the corresponding poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 .</head><label>3</label><figDesc>Visualization the impact of denoising scheme in DeciWatch on calibrating the wrong detected poses on four visible frames from the single-frame backbone. We demonstrate the cases via two video sequences and simply ignore the invisible frames.(a) and (d) rows show estimated poses from the popular model SimplePose [51], where the sampling interval is 10. Inputs of DeciWatch are highlighted in the yellow boxes. (b) and (e) are output poses of our proposed DeciWatch, which can denoise and smooth the input poses by the proposed DenoiseNet and RecoverNet. (c) and (f) show the ground truth of the corresponding poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of the recovery results on high-error estimated poses from AIST++ dataset. Only visible frames are shown, which are sampled with an interval of 10. Images in the row (a) are the original input frames at the 1 th , 11 th , 21 th ,...,101 th frame. (b), (c), (d) show the poses detected by SPIN [30], poses recovered by Deci-Watch, and the corresponding ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>4.2.</figDesc><table><row><cell></cell><cell></cell><cell>DenoiseNet</cell><cell></cell><cell cols="2">Preliminary Recovery</cell><cell></cell><cell>RecoverNet</cell><cell></cell><cell></cell><cell></cell><cell>Visible Pose</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Invisible Pose</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Self-Attn</cell><cell>Mult-head</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Self Attention</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cross-Attn</cell><cell>Mult-head</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cross Attention</cell></row><row><cell>. . .</cell><cell>FC</cell><cell>Self-Attn Transformer</cell><cell>FC</cell><cell>. . .</cell><cell>. . .</cell><cell>Conv1d</cell><cell>Self-Attn Cross-Attn</cell><cell>FC</cell><cell>. . .</cell><cell>FC Conv1d</cell><cell>Fully Connected Layer 1D Convolution Layer</cell></row><row><cell></cell><cell></cell><cell>Encoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Transformer</cell><cell></cell><cell></cell><cell></cell><cell>Pose Encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Decoder</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pose Decoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Semantic</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pose Encoder</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Positional Encoding</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>99.7 99.7 98.7 99.4 96.5 98.8 1.196+0.0005 ? 10.0% DeciWatch 99.9 99.8 99.6 99.7 98.6 99.6 96.6 98.9 0.997+0.0005 ? 8.3% DeciWatch 98.4 98.3 98.2 98.7 97.5 98.3 95.2 97.5 0.598+0.0005 ? 5.0% ? The results are recalculated according to Ratio and their tested FLOPs for SimplePose (i.e., 11.96G). ? Tested with ptflops v0.5.2[47].</figDesc><table><row><cell>Luo et al. [37]</cell><cell>98.2 96.5 89.6 86.0 98.7 95.6 90.0 93.6</cell><cell>70.98</cell><cell>100%</cell></row><row><cell cols="2">DKD (R50) [41] 98.3 96.6 90.4 87.1 99.1 96.0 92.9 94.0</cell><cell>8.65</cell><cell>100%</cell></row><row><cell>KFP (R50) [59]</cell><cell>95.1 96.4 95.3 91.3 96.3 95.6 92.6 94.7</cell><cell>10.69  ?</cell><cell>44.5%</cell></row><row><cell>KFP (R18) [59]</cell><cell>94.7 96.3 95.2 90.2 96.4 95.5 93.2 94.5</cell><cell>7.19  ?</cell><cell>40.8%</cell></row><row><cell cols="2">MAPN (R18) [10] 98.2 97.4 91.7 85.2 99.2 96.7 92.2 94.7</cell><cell>2.70</cell><cell>35.2%</cell></row><row><cell>SimplePose [51]</cell><cell>97.5 97.8 91.1 86.0 99.6 96.8 92.6 94.4</cell><cell>11.96</cell><cell>100%</cell></row><row><cell>DeciWatch</cell><cell>99.8 99.5</cell><cell></cell><cell></cell></row></table><note>Comparison on Sub-JHMDB [23] dataset with existing video-based efficient methods [41,59,10] for 2D pose estimation. R stands for ResNet back- bone [16]. Ratio represents the sampling ratio. The pose estimator of DeciWatch is the single-frame model SimplePose (R50) [51]. Best results are in bold. Sub-JHMDB dataset -2D Pose Estimation Methods Head Sho. Elb. Wri. Hip Knee Ank. Avg. ? FLOPs(G) ? Ratio</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparing DeciWatch with existing single-image 3D pose estimators on Human3.6M<ref type="bibr" target="#b68">[22]</ref>, 3DPW[38], and AIST++<ref type="bibr" target="#b80">[34]</ref> datasets. Pose estimators used in DeciWatch keep the same as the corresponding methods. 8? 1.8(3.3%) 1.5? 17.7(92.2%) 0.621+0.0007 10.0% DeciWatch 53.5? 1.1(2.0%) 1.4? 17.8(92.7%) 0.414+0.0007 6.7% 3? 3.6(3.7%) 7.1? 27.6(79.5%) 0.413+0.0004 10.0% DeciWatch 96.7? 0.2(0.2%) 6.9? 27.8(80.1%) 0.275+0.0004 6.7% 0? 1.3(1.4%) 6.8? 26.0(79.3%) 0.413+0.0004 10.0% DeciWatch 92.3? 2.0(2.2%) 6.6? 26.2(79.9%) 0.275+0.0004 6.7% 2? 1.7(2.2%) 6.9? 18.8(73.2%) 1.551+0.0004 10.0% DeciWatch 80.7? 1.8(2.2%) 6.7? 18.6(73.9%) 1.034+0.0004 6.7% DeciWatch 71.3? 36.4(33.8%) 5.7? 28.1(83.1%) 0.413+0.0007 10.0% DeciWatch 82.3? 25.4(23.6%) 5.5? 28.3(83.7%) 0.275+0.0007 6.7%</figDesc><table><row><cell>Methods</cell><cell cols="2">MPJPE ?</cell><cell>Accel ?</cell><cell>FLOPs(G)?</cell><cell>Ratio</cell></row><row><cell cols="4">Human3.6M [22] -3D Pose Estimation</cell><cell></cell></row><row><cell>FCN [39]</cell><cell></cell><cell>54.6</cell><cell>19.2</cell><cell>6.21</cell><cell>100.0%</cell></row><row><cell cols="4">DeciWatch 52.3DPW [38] -3D Body Recovery</cell><cell></cell></row><row><cell>SPIN [30]</cell><cell></cell><cell>96.9</cell><cell>34.7</cell><cell>4.13</cell><cell>100.0%</cell></row><row><cell cols="2">DeciWatch 93.EFT [25]</cell><cell>90.3</cell><cell>32.8</cell><cell>4.13</cell><cell>100.0%</cell></row><row><cell cols="2">DeciWatch 89.PARE [29]</cell><cell>78.9</cell><cell>25.7</cell><cell>15.51</cell><cell>100.0%</cell></row><row><cell cols="4">DeciWatch 77.AIST++ [34] -3D Body Recovery</cell><cell></cell></row><row><cell>SPIN [30]</cell><cell></cell><cell>107.7</cell><cell>33.8</cell><cell>4.13</cell><cell>100.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Ratio Nearest Linear Quadratic Cubic-Spline CVAE-R. CVAE-R. CVAE-U. DeciWatch</figDesc><table><row><cell>20%</cell><cell>54.4</cell><cell>54.4</cell><cell>54.3</cell><cell>54.5</cell><cell>87.4</cell><cell>114.1</cell><cell>119.4</cell><cell>52.8? 1.8(3.2%)</cell></row><row><cell>10%</cell><cell>54.7</cell><cell>54.3</cell><cell>55.2</cell><cell>54.4</cell><cell>99.1</cell><cell>119.2</cell><cell>121.5</cell><cell>52.8? 1.8(3.2%)</cell></row><row><cell>5%</cell><cell>57.6</cell><cell>57.5</cell><cell>57.2</cell><cell>57.3</cell><cell>134.9</cell><cell>140.5</cell><cell>123.1</cell><cell>54.4? 0.2(0.3%)</cell></row></table><note>All estimation results are re-implemented or tested by us for fair comparisons.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Nearest Linear Quadratic Cubic-Spline DeciWatch</figDesc><table><row><cell cols="2">20% 106.7 104.6</cell><cell>105.8</cell><cell>106.8</cell><cell>67.6? 39.7(37.0%)</cell></row><row><cell cols="2">10% 108.3 106.3</cell><cell>108.2</cell><cell>108.9</cell><cell>71.3? 36.0(33.6%)</cell></row><row><cell>5%</cell><cell>123.2 120.7</cell><cell>119.9</cell><cell>121.2</cell><cell>90.8? 16.5(15.4%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Comparison of MPJPE</cell></row><row><cell cols="3">with different sampling strategies on</cell></row><row><cell cols="3">3DPW dataset with EFT [25] pose es-</cell></row><row><cell cols="3">timator (MPJPE is 90.3mm). U.-2 and</cell></row><row><cell cols="3">U.-3 are uniform sampling 2 or 3 frames for</cell></row><row><cell cols="3">every N frames. U.-R. conducts both uni-</cell></row><row><cell cols="3">form sampling and random sampling. R. is</cell></row><row><cell cols="3">random sampling.</cell></row><row><cell cols="3">Ratio U.(Ours) U.-2 U.-3 U.-R. R.</cell></row><row><cell>20%</cell><cell>87.2</cell><cell>89.3 91.5 94.2 97.1</cell></row><row><cell>10%</cell><cell>89.0</cell><cell>96.3 103.4 101.3 104.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Exploring impacts of the two DenoiseNet and RecoverNet subnets with 10% sampling ratio on three dataset and the corresponding backbones. Ori. means the original estimator (watching all frames) with 100% sampling ratio. No RecoverNet is preliminarily recovered via a temporal linear layer.</figDesc><table><row><cell>Dataset w/Backbone</cell><cell cols="4">Ori. (100%) No DenoiseNet No RecoverNet DeciWatch</cell></row><row><cell>Human3.6M w/FCN [39]</cell><cell>54.6</cell><cell>54.5</cell><cell>54.7</cell><cell>52.8</cell></row><row><cell>3DPW w/PARE [29]</cell><cell>78.9</cell><cell>79.8</cell><cell>81.0</cell><cell>77.2</cell></row><row><cell>AIST++ w/SPIN [25]</cell><cell>107.7</cell><cell>91.5</cell><cell>95.3</cell><cell>71.3</cell></row></table><note>the effectiveness of this application on the Human3.6M and AIST++ dataset by directly inputting the sparse ground-truth 3D positions into the RecoverNet of DeciWatch. Due to limited pages, please see Supp. for details.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Ailing Zeng 1 , Xuan Ju 1 , Lei Yang 2 , Ruiyuan Gao 1 , Xizhou Zhu 2 , Bo Dai 3 , and Qiang Xu 1 The Chinese University of Hong Kong, 2 Sensetime Group Limited, 3 Shanghai AI Laboratory {alzeng, qxu}@cse.cuhk.edu.hk In Sec. 1, we present dataset descriptions. Next, we present results of efficient labeling in Sec. 2 and the generalization ability of DeciWatch in Sec. 4. Then, we show more ablation studies on different sampling ratios, model designs of DenoiseNet and RecoverNet, and hyper-parameters in Sec. 5. Moreover, we show qualitative comparison results in Sec. 6 to demonstrate why DeciWatch works. Last, in Sec. 7, we discuss some failure cases in this method to motivate further research.</figDesc><table><row><cell>-Supplementary Materials-</cell></row><row><cell>DeciWatch: A Simple Baseline for 10? Efficient</cell></row><row><cell>2D and 3D Pose Estimation</cell></row><row><cell>). pp. 13401-13412 (October 2021)</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 1 .</head><label>1</label><figDesc>Comparison of MPJPE on efficient pose labeling that labels one frame in every N frames on Human3.6M<ref type="bibr" target="#b68">[22]</ref> and AIST++<ref type="bibr" target="#b80">[34]</ref> dataset. .55  10.81 24.15 35.20 7.21 21.31 27.72 73.69 99.04 Quadratic 1.26 4.31 10.05 17.22 22.85 2.04 8.33 23.59 43.13 61.16 Cubic-Spline 0.18 0.99 5.36 18.42 29.21 0.89 5.12 18.31 45.32 77.39 DeciWatch 0.25 1.33 2.89 6.21 10.59 0.83 4.03 11.25 20.12 41.25</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Human3.6M</cell><cell></cell><cell></cell><cell></cell><cell cols="2">AIST++</cell><cell></cell></row><row><cell>Interval N</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>2</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell></row><row><cell>Linear</cell><cell>2.21 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2 .</head><label>2</label><figDesc>Comparison of DeciWatch and SimplePose[51]  on PCK@0.2, PCK@0.1, and PCK@0.05. In future work, we recommend using PCK@0.05 as the main metrics for 2D pose estimation.</figDesc><table><row><cell cols="3">Sub-JHMDB dataset -2D Pose Estimation</cell><cell></cell><cell></cell></row><row><cell cols="5">Sampling Ratio Evaluation Metric PCK@0.2 ? PCK@0.1 ? PCK@0.05 ? SimplePose 93.92% 81.25% 56.88% 20% DeciWatch 99.11% 95.43% 82.66%</cell></row><row><cell>10%</cell><cell>SimplePose DeciWatch</cell><cell>93.94% 98.75%</cell><cell>81.61% 94.05%</cell><cell>57.30% 79.44%</cell></row><row><cell>5%</cell><cell>SimplePose DeciWatch</cell><cell>92.38% 97.50%</cell><cell>82.79% 91.76%</cell><cell>58.95% 73.02%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 3 .</head><label>3</label><figDesc>Cross-backbone and cross-dataset results from DeciWatch checkpoints trained on 3DPW-PARE.</figDesc><table><row><cell>Dataset/Estimator</cell><cell>MPJPE?</cell><cell>Accel.?</cell></row><row><cell>3DPW/EFT</cell><cell cols="2">Estimator (100%) DeciWatch (10%) 87.2? 3.1(3.4%) 7.2? 25.6(77.9%) 90.3 32.8</cell></row><row><cell>3DPW/SPIN</cell><cell cols="2">Estimator (100%) DeciWatch (10%) 98.3? 1.4(1.4%) 7.1? 27.5(79.5%) 96.9 34.6</cell></row><row><cell>AIST++/SPIN</cell><cell cols="2">Estimator (100%) DeciWatch (10%) 101.8? 5.9(5.5%) 6.2? 27.6(81.7%) 107.7 33.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 .</head><label>5</label><figDesc>Comparison results with different denoise network designs on 3DPW dataset with the state-of-the-art pose estimator Pare<ref type="bibr" target="#b75">[29]</ref> (MPJPE is 78.9mm).</figDesc><table><row><cell>Metrics</cell><cell>No DenoiseNet</cell><cell>TCNs [43]</cell><cell>MLPs [58]</cell><cell>Ours</cell></row><row><cell>MPJPE</cell><cell>79.8</cell><cell>80.5</cell><cell>79.5</cell><cell>77.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 .</head><label>6</label><figDesc>Comparison results with different recovery network designs on 3DPW dataset with the state-of-the-art pose estimator PARE<ref type="bibr" target="#b75">[29]</ref>(MPJPE is 78.9mm).</figDesc><table><row><cell>Metrics</cell><cell>Linear</cell><cell>TCNs [43]</cell><cell>TCNs w/MLPs</cell><cell>MLPs [58]</cell><cell>Ours</cell></row><row><cell>MPJPE</cell><cell>79.8</cell><cell>172.3</cell><cell>99.5</cell><cell>78.0</cell><cell>77.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 .</head><label>7</label><figDesc>Comparison results of different loss weight ? on 3DPW dataset with the state-of-the-art pose estimator Pare<ref type="bibr" target="#b75">[29]</ref>(MPJPE is 78.9mm).</figDesc><table><row><cell>?</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>10</cell></row><row><cell>MPJPE</cell><cell>78.0</cell><cell>77.6</cell><cell>77.2</cell><cell>77.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 .Table 9 .</head><label>89</label><figDesc>Comparison results of different embedding dimension C on 3DPW dataset with the state-of-the-art pose estimator Pare<ref type="bibr" target="#b75">[29]</ref>. Comparison results of different block number M on 3DPW dataset with the state-of-the-art pose estimator Pare<ref type="bibr" target="#b75">[29]</ref>.</figDesc><table><row><cell>C</cell><cell>12</cell><cell>32</cell><cell></cell><cell>64</cell><cell>128</cell><cell>256</cell></row><row><cell>MPJPE</cell><cell>78.0</cell><cell cols="2">77.2</cell><cell>77.4</cell><cell cols="2">77.6</cell><cell>77.4</cell></row><row><cell>?</cell><cell>1</cell><cell></cell><cell>3</cell><cell>5</cell><cell></cell><cell>10</cell></row><row><cell>MPJPE</cell><cell cols="2">79.3</cell><cell>77.5</cell><cell cols="2">77.2</cell><cell>77.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Due to the limit of pages, we present data description, comprehensive results of different sampling ratios, the effect of hyper-parameters, generalization ability, qualitative results, and failure cases analyses in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">arXiv:2203.08713v2 [cs.CV] 20 Jul 2022</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Website: https://ailingzeng.site/deciwatch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Estimating missing marker positions using low dimensional kalman smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lasenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomechanics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1854" to="1858" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A unified 3d human motion synthesis model via conditional variational auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11645" to="11655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Openpose: realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mobilehumanpose: Toward real-time 3d human pose estimation in mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2328" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Part-aware measurement for robust multi-view multi-human 3d pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="1472" to="1481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03215</idno>
		<title level="m">Fasterpose: A faster simple baseline for human pose estimation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A review of 3d human pose estimation algorithms for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Desmarais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mottet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Slangen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">103275</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Single-shot motion completion with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00776</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive computationally efficient network for monocular 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="127" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Motion adaptive pose estimation from compressed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11719" to="11728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting missing marker trajectories in human motion data using marker intercorrelations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Gl?ersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Federolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">152616</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured aleatoric uncertainty in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent transition networks for character locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2018 Technical Briefs</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust motion inbetweening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yurick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="60" to="61" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human motion prediction via spatiotemporal inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Online knowledge distillation for efficient pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11740" to="11750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11536</idno>
		<title level="m">Recent advances in monocular 2d and 3d human pose estimation: A deep learning perspective</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lstm pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5207" to="5215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic kernel distillation for efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6942" to="6950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Real-time 2d multi-person pose estimation on cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Osokin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12004</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Lightweight openpose. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mocap systems and hand movement reconstruction using cubic spline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Benaoumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Zoubir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Control, Decision and Information Technologies (CoDIT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adaptively multi-view and temporal fusing transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05092</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gap reconstruction in optical motion capture sequences using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Skurowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pawlyta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">6115</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Flops counter for convolutional networks in pytorch framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sovrasov</surname></persName>
		</author>
		<idno>original- date: 2018-08-17T09:54:59Z</idno>
		<ptr target="https://github.com/sovrasov/flops-counter.pytorch" />
		<imprint>
			<date type="published" when="2022-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time estimation of missing markers for reconstruction of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boulanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 XIII Symposium on Virtual Reality</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring versatile prior for human motion via motion frequency guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="606" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional sequence generation for skeleton-based action synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4394" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lite-hrnet: A lightweight high-resolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10440" to="10450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Glamr: Global occlusion-aware human mesh recovery with dynamic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01524</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C F</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning skeletal graph neural networks for hard 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13715</idno>
		<title level="m">Smoothnet: A plug-and-play network for refining human poses in videos</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Key frame proposal network for efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="609" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Simple and lightweight human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10346</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Estimating human pose efficiently by parallel pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6785" to="6800" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A lightweight graph transformer network for human mesh reconstruction from 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12696</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13392</idno>
		<title level="m">Deep learning-based human pose estimation: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">3d human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11656" to="11665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Estimating missing marker positions using low dimensional kalman smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lasenby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomechanics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1854" to="1858" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A unified 3d human motion synthesis model via conditional variational auto-encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11645" to="11655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Openpose: realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mobilehumanpose: Toward real-time 3d human pose estimation in mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2328" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Part-aware measurement for robust multi-view multi-human 3d pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="1472" to="1481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03215</idno>
		<title level="m">Fasterpose: A faster simple baseline for human pose estimation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A review of 3d human pose estimation algorithms for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Desmarais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mottet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Slangen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">103275</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Single-shot motion completion with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00776</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Adaptive computationally efficient network for monocular 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="127" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Motion adaptive pose estimation from compressed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11719" to="11728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4346" to="4354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Predicting missing marker trajectories in human motion data using marker intercorrelations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Gl?ersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Federolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">152616</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Structured aleatoric uncertainty in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Recurrent transition networks for character locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2018 Technical Briefs</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Robust motion inbetweening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yurick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="60" to="61" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Human motion prediction via spatiotemporal inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Render in-between: Motion guided video synthesis for action interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">I</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.01029</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Quantitative assessment of the accuracy for three interpolation techniques in kinematic analysis of human movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Howarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Callaghan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods in biomechanics and biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="847" to="855" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Lightweight 3d human pose estimation network training using teacher-student learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Missing data recovery for human mocap data based on a-lstm and ls constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 5th International Conference on Signal and Image Processing (ICSIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="42" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Convolutional autoencoders for human motion infilling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aksan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pare: Part attention regressor for 3d human body estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11127" to="11137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">A neural network approach to missing marker reconstruction in human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kucherenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellstr?m</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02665</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Motion capture data completion and denoising by singular value thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics (Short Papers)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Human pose regression with residual log-likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Ai choreographer: Music conditioned 3d dance generation with aist++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="13401" to="13412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Online knowledge distillation for efficient pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11740" to="11750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11536</idno>
		<title level="m">Recent advances in monocular 2d and 3d human pose estimation: A deep learning perspective</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Lstm pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5207" to="5215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Dynamic kernel distillation for efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6942" to="6950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Real-time 2d multi-person pose estimation on cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Osokin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12004</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Lightweight openpose. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Mocap systems and hand movement reconstruction using cubic spline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Benaoumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Zoubir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Control, Decision and Information Technologies (CoDIT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Adaptively multi-view and temporal fusing transformer for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05092</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Gap reconstruction in optical motion capture sequences using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Skurowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pawlyta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">6115</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Flops counter for convolutional networks in pytorch framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sovrasov</surname></persName>
		</author>
		<idno>original- date: 2018-08-17T09:54:59Z</idno>
		<ptr target="https://github.com/sovrasov/flops-counter.pytorch" />
		<imprint>
			<date type="published" when="2022-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Real-time estimation of missing markers for reconstruction of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boulanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 XIII Symposium on Virtual Reality</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Exploring versatile prior for human motion via motion frequency guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="606" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Convolutional sequence generation for skeleton-based action synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4394" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Lite-hrnet: A lightweight high-resolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10440" to="10450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Glamr: Global occlusion-aware human mesh recovery with dynamic cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01524</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C F</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Learning skeletal graph neural networks for hard 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13715</idno>
		<title level="m">Smoothnet: A plug-and-play network for refining human poses in videos</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Key frame proposal network for efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="609" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Simple and lightweight human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10346</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Estimating human pose efficiently by parallel pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6785" to="6800" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">A lightweight graph transformer network for human mesh reconstruction from 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12696</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13392</idno>
		<title level="m">Deep learning-based human pose estimation: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">3d human pose estimation with spatial and temporal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendieta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11656" to="11665" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
