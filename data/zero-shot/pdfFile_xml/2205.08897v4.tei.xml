<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-16">16 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Zhou</surname></persName>
							<email>tian.zt@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqing</forename><surname>Ma</surname></persName>
							<email>maziqing.mzq@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xue</surname></persName>
							<email>xue.w@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Qingsong</surname></persName>
							<email>qingsong.wen@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Liang</surname></persName>
							<email>liang.sun@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Tao</surname></persName>
							<email>tao.yao@libaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Wotao</surname></persName>
							<email>wotao.yin@libaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Rong</surname></persName>
							<email>jinrong.jr@libaba-inc.com</email>
						</author>
						<title level="a" type="main">FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-16">16 Sep 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have shown that deep learning models such as RNNs and Transformers have brought significant performance gains for long-term forecasting of time series because they effectively utilize historical information. We found, however, that there is still great room for improvement in how to preserve historical information in neural networks while avoiding overfitting to noise presented in the history. Addressing this allows better utilization of the capabilities of deep learning models. To this end, we design a Frequency improved Legendre Memory model, or FiLM: it applies Legendre Polynomials projections to approximate historical information, uses Fourier projection to remove noise, and adds a low-rank approximation to speed up computation. Our empirical studies show that the proposed FiLM significantly improves the accuracy of stateof-the-art models in multivariate and univariate long-term forecasting by (20.3%, 22.6%), respectively. We also demonstrate that the representation module developed in this work can be used as a general plug-in to improve the long-term prediction performance of other deep learning modules.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Long-term forecasting refers to making predictions based on the history for a long horizon in the future, as opposed to short-term forecasting. Long-term time series forecasting has many key applications in energy, weather, economics, transportation, and so on. It is more challenging than ordinary time series forecasting. Some of the challenges in long-term forecasting include long-term time dependencies, susceptibility to error propagation, complex patterns, and nonlinear dynamics. These challenges make accurate predictions generally impossible for traditional learning methods such as ARIMA. Although RNN-like deep learning methods have made breakthroughs in time series forecasting <ref type="bibr">Rangapuram et al. (2018)</ref>; <ref type="bibr">Salinas et al. (2020)</ref>, they often suffer from problems such as gradient vanishing/exploding <ref type="bibr">Pascanu et al. (2013)</ref>, which limits their practical performance. Following the success of Transformer <ref type="bibr">Vaswani et al. (2017)</ref> in the NLP and CV communities <ref type="bibr">Vaswani et al. (2017)</ref>; <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>; <ref type="bibr" target="#b5">Dosovitskiy et al. (2021)</ref>; <ref type="bibr">Rao et al. (2021)</ref>, it also shows promising performance in capturing long-term dependencies <ref type="bibr" target="#b21">Zhou et al. (2021)</ref>; <ref type="bibr" target="#b19">Wu et al. (2021)</ref>; <ref type="bibr" target="#b18">Zhou et al. (2022)</ref>; <ref type="bibr" target="#b18">Wen et al. (2022)</ref> for time series forecasting. We provide an overview of this line of work (including deep recurrent networks and efficient Transformers) in the appendix A.</p><p>In order to achieve accurate predictions, many deep learning researchers increase the complexity of their models, hoping that they can capture critical and intricate historical information. These methods, however, cannot achieve it. <ref type="figure" target="#fig_0">Figure 1</ref> compares the ground truth time series of the real-world ETTm1 dataset with the predictions of the vanilla Transformer method and the LSTM model <ref type="bibr">Vaswani et al. (2017)</ref>; <ref type="bibr" target="#b10">Hochreiter &amp; Schmidhuber (1997a)</ref>  <ref type="bibr" target="#b21">Zhou et al. (2021)</ref>. It is observed that the prediction is completely off from the distribution of the ground truth. We believe that such errors come from these models miscapturing noise while attempting to preserve the true signals. We conclude that two keys in accurate forecasting are: 1) how to capture critical historical information as complete as possible; and 2) how to effectively remove the noise. Therefore, to avoid a derailed forecast, we cannot improve a model by simply making it more complex. Instead, we shall consider a robust representation of the time series that can capture its important patterns without noise. This observation motivates us to switch our view from long-term time series forecasting to long-term sequence compression.</p><p>Recursive memory model <ref type="bibr">Voelker et al. (2019)</ref>; <ref type="bibr" target="#b7">Gu et al. (2021a</ref><ref type="bibr">Gu et al. ( ,b, 2020</ref> has achieved impressive results in function approximation tasks. <ref type="bibr">Voelker et al. (2019)</ref> designed a recursive memory unit (LMU) using Legendre projection, which provides a good representation for long time series. S4 model <ref type="bibr" target="#b7">Gu et al. (2021a)</ref> comes up with another recursive memory design for data representation and significantly improves state-ofthe-art results for Long-range forecasting benchmark (LRA) <ref type="bibr">Tay et al. (2020)</ref>. However, when coming to long-term time series forecasting, these approaches fall short of the Transformer-based methods' state-of-the-art performance. A careful examination reveals that these data compression methods are powerful in recovering the details of historical data compared to LSTM/Transformer models, as revealed in <ref type="figure" target="#fig_1">Figure 2</ref>. However, they are vulnerable to noisy signals as they tend to overfit all the spikes in the past, leading to limited long-term forecasting performance. It is worth noting that, Legendre Polynomials employed in <ref type="bibr">LMP Voelker et al. (2019)</ref> is just a special case in the family of Orthogonal Polynomials (OPs). OPs (including Legendre, Laguerre, Chebyshev, etc.) and other orthogonal basis (Fourier and Multiwavelets) are widely studied in numerous fields and recently brought in deep learning <ref type="bibr" target="#b17">Wang et al. (2018)</ref>; <ref type="bibr" target="#b9">Gupta et al. (2021)</ref>; <ref type="bibr" target="#b18">Zhou et al. (2022)</ref>; <ref type="bibr">Voelker et al. (2019)</ref>; <ref type="bibr" target="#b6">Gu et al. (2020)</ref>. A detailed review can be found in Appendix A. The above observation inspires us to develop methods for accurate and robust representations of time series data for future forecasting, especially longterm forecasting. The proposed method significantly outperforms existing long-term forecasting methods on multiple benchmark datasets by integrating those representations with powerful prediction models. As the first step towards this goal, we directly exploit the Legendre projection, which is used by <ref type="bibr">LMU Voelker et al. (2019)</ref> to update the representation of time series with fixed-size vectors dynamically. This projection layer will then be combined with different deep learning modules to boost forecasting performance. The main challenge with directly using this representation is the dilemma between information preservation and data overfitting, i.e., the larger the number of Legendre projections is, the more the historical data is preserved, but the more likely noisy data will be overfitted. Hence, as a second step, to reduce the impact of noisy signals on the Legendre projection, we introduce a layer of dimension reduction by a combination of Fourier analysis and low-rank matrix approximation. More specifically, we keep a large dimension representation from the Legendre projection to ensure that all the important details of historical data are preserved. We then apply a combination of Fourier analysis and low-rank approximation to keep the part of the representation related to low-frequency Fourier components and the top eigenspace to remove the impact of noises. Thus, we can not only capture the long-term time dependencies, but also effective reduce the noise in long-term forecasting. We refer to the proposed method as Frequency improved Legendre Memory model, or FiLM for short, for long-term time series forecasting.</p><p>In short, we summarize the key contributions of this work as follows:</p><p>1. We propose a Frequency improved Legendre Memory model (FiLM) architecture with a mixture of experts for robust multiscale time series feature extraction.</p><p>2. We redesign the Legendre Projection Unit (LPU) and make it a general tool for data representation that any time series forecasting model can exploit to solve the historical information preserving problem.</p><p>3. We propose Frequency Enhanced Layers (FEL) that reduce dimensionality by a combination of Fourier analysis and low-rank matrix approximation to minimize the impact of noisy signals from time series and ease the overfitting problem. The effectiveness of this method is verified both theoretically and empirically.</p><p>4. We conduct extensive experiments on six benchmark datasets across multiple domains (energy, traffic, economics, weather, and disease). Our empirical studies show that the proposed model improves the performance of state-of-the-art methods by 19.2% and 26.1% in multivariate and univariate forecasting, respectively. In addition, our empirical studies also reveal a dramatic improvement in computational efficiency through dimensionality reduction.</p><p>2 Time Series Representation in Legendre-Fourier Domain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Legendre Projection</head><p>Given an input sequence, the function approximation problem aims to approximate the cumulative history at every time t. Using Legendre Polynomials projection, we can project a prolonged sequence of data onto a subspace of bounded dimension, leading to compression, or feature representation, for evolving historical data. Formally, given a smooth function f observed online, we aim to maintain a fixed size compressed representation of its history f (x) <ref type="bibr">[t??,t]</ref> , where ? specifies the window size. At every time point t, the approximation function g (t) (x) is defined with respect to the measure ? (t) = 1 ? ? [t??,t] (x). In this paper, we use Legendre Polynomials of degree at most N ? 1 to build the function g (t) (x), i.e.</p><formula xml:id="formula_0">g (t) (x) = N n=1 c n (t)P n 2(x ? t) ? + 1 ,<label>(1)</label></formula><p>where P n (?) is the n-th order Legendre Polynomials. Coefficients c n (t) are captured by the following dynamic equation:</p><formula xml:id="formula_1">d dt c(t) = ? 1 ? Ac(t) + 1 ? Bf (t).<label>(2)</label></formula><p>where the definition of A and B can be found in <ref type="bibr">Voelker et al. (2019)</ref>. Using Legendre Polynomials as a basis allows us to accurately approximate smooth functions, as indicated by the following theorem.</p><p>Theorem 1 (Similar to Proposition 6 in <ref type="bibr" target="#b6">Gu et al. (2020)</ref>).</p><formula xml:id="formula_2">If f (x) is L-Lipschitz, then f [t??,t] (x) ? g (t) (x) ? (t) ? O(?L/ ? N ). Moreover, if f (x) has k-th order bounded derivatives, we have f [t??,t] (x) ? g (t) (x) ? (t) ? O(? k N ?k+1/2 ).</formula><p>According to Theorem 1, without any surprise, the larger the number of Legendre Polynomials basis, the more accurate the approximation will be, which unfortunately may lead to the overfitting of noisy signals in the history. As shown in Section 4, directly feeding deep learning modules, such as MLP, RNN, and vanilla Attention without modification, with the above features will not yield state-ofthe-art performance, primarily due to the noisy signals in history. That is why we introduce, in the following subsection, a Frequency Enhanced Layer with Fourier transforms for feature selection. . LPU: Legendre Projection Unit. LPU_R: reverse recovery of Legendre Projection. FEL: Frequency Enhanced Layer. RevIn: data normalization block. The input data is first normalized and then project to Legendre Polynomials space (LPU memory C). The LPU memory C is processed with FEL and generates the memory C ? of output. Finally, C ? is reconstructed and denormalized to output series with LPUR. A multiscale structure is employed to process the input with length {T, 2T, ... nT }.</p><p>Before we close this subsection, we note that unlike <ref type="bibr" target="#b7">Gu et al. (2021a)</ref>, a fixed window size is used for function approximation and feature extraction. This is largely because a longer history of data may lead to a larger accumulation of noises from history. To make it precise, we consider an autoregressive setting with random noise. Let {x t } ? ? d be the time sequence with x t+1 = Ax t + b + ? t for t = 1, 2, ..., where A ? ? d?d , b ? ? d , and ? t ? ? d is random noise sampled from N (0, ? 2 I).</p><p>As indicated by the following theorem, given x t , the noise will accumulate in x t?? over time at the rate of ? ?, where ? is the window size.</p><p>Theorem 2. Let A be an unitary matrix and ? t be ? 2 -subgaussian random noise. We have</p><formula xml:id="formula_3">x t = A ? x t?? + ??1 i=1 A i b + O(? ? ?).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fourier Transform</head><p>Because white noise has a completely flat power spectrum, it is commonly believed that the time series data enjoys a particular spectral bias, which is not generally randomly distributed in the whole spectrum. Due to the stochastic transition environment, the real output trajectories of the forecasting task contain large volatilises and people usually only predict the mean path of them. The relative more smooth solutions thus are preferred. According to Eq. (1), the approximation function g (t) (x) can be stabilized via smoothing the coefficients c n (t) in both t and n. This observation helps us design an efficient data-driven way to tune the coefficients c n (t). As smoothing in n can be simply implemented via multiplying learnable scalars to each channel, we mainly discuss smoothing c n (t) in t via Fourier transformation. The spectral bias implies the the spectrum of c n (t) mainly locates in low frequency regime and has weak signal strength in high frequency regime. To simplify our analysis, let us assume the Fourier coefficients of c n (t) as a n (t). Per spectral bias, we assume that there exists a s, a min &gt; 0, such that for all n we have t &gt; s, |a n (t)| ? a min . An idea to sample coefficients is to keep the the first k dimensions and randomly sample the remaining dimensions instead of the fully random sampling strategy. We characterize the approximation quality via the following theorem:</p><p>Theorem 3. Let A ? ? d?n be the Fourier coefficients matrix of an input matrix X ? ? d?n , and ?(A), the coherence measure of matrix A, is ?(k/n). We assume there exist s and a positive a min such that the elements in last d ? s columns of A is smaller than a min . If we keep first s columns selected and randomly choose O(k 2 /? 2 ?s) columns from the remaining parts, with high probability</p><formula xml:id="formula_4">A ? P (A) F ? O (1 + ?)a min ? (n ? s)d ,</formula><p>where P (A) denotes the matrix projecting A onto the column selected column space.</p><p>Theorem 3 implies that when a min is small enough, the selected space can be considered as almost the same as the original one.  The overall structure of FiLM is shown in <ref type="figure" target="#fig_2">Figure 3</ref> The FiLM maps a sequence X ? Y , where X, Y ? R T ?D , by mainly utilizing two sub-layers: Legendre Projection Unit (LPU) layer and Fourier Enhanced Layer (FEL). In addition, to capture history information at different scales, a mixture of experts at different scales is implemented in the LPU layer. An optional add-on data normalization layer RevIN <ref type="bibr" target="#b12">Kim et al. (2021)</ref> is introduced to further enhance the model's robustness. It is worth mentioning that FiLM is a simple model with only one layer of LPU and one layer of FEL.</p><p>LPU: Legendre Projection Unit LPU is a state space model: C t = AC t?1 + Bx t , where x t ? R is the input signal, C t ? R N is the memory unit, and N is the number of Legendre Polynomials. LPU contains two untrainable prefixed matrices A and B defined as follows:</p><formula xml:id="formula_5">A nk = (2n + 1) (?1) n?k if k ? n 1 if k ? n , B n = (2n + 1)(?1) n .<label>(3)</label></formula><p>LPU contains two stages, i.e., Projection and Reconstruction. The former stage projects the original signal to the memory unit: C = LPU(X). The later stage reconstructs the signal from the memory unit: X re = LPU_R(C). The whole process in which the input signal is projected/reconstructed to/from memory C is shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEL: Frequency Enhanced Layer</head><p>Low-rank Approximation The FEL is with a single learnable weight matrix (W ? R M ? ?N ?N ) which is all we need to learn from the data. However, this weight could be large. We can decompose W into three matrices W 1 ? R M ? ?N ? ?N ? , W 2 ? R N ? ?N and W 3 ? R N ? ?N to perform a lowrank approximation (N ? &lt;&lt; N ). Take Legendre Polynomials number N = 256 as default, our model's learnable weight can be significantly reduced to 0.4% with N ? = 4 with minor accuracy deterioration as shown in Section 4. The calculation mechanism is described in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>Mode Selection A subset of the frequency modes is selected after Fourier transforms to reduce the noise and boost the training speed. Our default selection policy is choosing the lowest M mode. Various selection policies are studied in the experiment section. Results show that adding some random high-frequency modes can give extra improvement in some datasets, as supported by our theoretical studies in Theorem 3.</p><p>Implementation source code for LPU and FEL are given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mixture of Multiscale Experts Mechanism</head><p>Multiscale phenomena is a unique critical data bias for time series forecasting. Since we treat history sequence points with uniform importance, our model might lack such prior. Our model implemented a simple mixture of experts strategy that utilizes the input sequence with various time horizons {T, 2T, ... nT } to forecast the predicted horizon T and merge each expert prediction with linear layer as show in <ref type="figure" target="#fig_2">Figure 3</ref>. This mechanism improves the model's performance consistently across all datasets, as shown in <ref type="table" target="#tab_6">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Normalization</head><p>As <ref type="bibr" target="#b19">Wu et al. (2021)</ref>; <ref type="bibr" target="#b18">Zhou et al. (2022)</ref> point out, time series seasonal-trend decomposition is a crucial data normalization design for long-term time series forecasting. We find that our LMU projection can inherently play a normalization role for most datasets, but lacking an explicate normalization design still hurt the robustness of performance in some cases. A simple reversible instance normalization (RevIN) <ref type="bibr" target="#b12">Kim et al. (2021)</ref> is adapted to act as an add-on explicate data normalization block. The mean and standard deviation are computed for every instance x</p><formula xml:id="formula_6">(i) k ? R T of the input data as E t x (i) kt = 1 T T j=1 x (i) kj and Var x (i) kt = 1 T T j=1 x (i) kj ? E t x (i) kt 2 .</formula><p>Using these statistics, we normalize the input data x (i) asx (i)</p><formula xml:id="formula_7">kt = ? k ? ? x (i) kt ?Et x (i) kt Var x (i) kt +? ? ? + ? k , where ?, ? ? R K are learnable affine</formula><p>parameter vectors. Then the normalized input data is sent into the model for forecasting. In the end, we denormalize the model output by applying the reciprocal of the normalization performed at the beginning.</p><p>RevIN slows down the training process by 2-5 times, and we do not observe consistent improvement on all datasets by applying RevIn. Thus, it can be considered an optional stabilizer in model training. Its detailed performance is shown in the ablation study in <ref type="table" target="#tab_6">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate the proposed FiLM, we conduct extensive experiments on six popular real-world benchmark datasets for long-term forecasting, including traffic, energy, economics, weather, and disease. Since classic models such as ARIMA and simple RNN/TCN models perform inferior as shown in <ref type="bibr" target="#b21">Zhou et al. (2021)</ref> and <ref type="bibr" target="#b19">Wu et al. (2021)</ref>, we mainly include five state-of-the-art (SOTA) Transformerbased models, i.e., FEDformer, Autoformer <ref type="bibr" target="#b19">Wu et al. (2021)</ref>, Informer <ref type="bibr" target="#b21">Zhou et al. (2021)</ref>, <ref type="bibr">Log-Trans Li et al. (2019)</ref>, Reformer <ref type="bibr" target="#b14">Kitaev et al. (2020)</ref>, and one recent state-space model with recursive memory S4 <ref type="bibr" target="#b7">Gu et al. (2021a)</ref>, for comparison. FEDformer is selected as the main baseline as it achieves SOTA results in most settings. More details about baseline models, datasets, and implementations are described in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Result</head><p>For better comparison, we follow the experiment settings of Informer <ref type="bibr" target="#b21">Zhou et al. (2021)</ref> where the input length is tuned for best forecasting performance, and the prediction lengths for both training and evaluation are fixed to be 96, 192, 336, and 720, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multivariate Results</head><p>In multivariate forecasting tasks, FiLM achieves the best performance on all six benchmark datasets at all horizons, as shown in <ref type="table" target="#tab_0">Table 17</ref>. Compared with SOTA work (FEDformer), our proposed FiLM yields an overall 20.3% relative MSE reduction. It is worth noting that the improvement is even more significant on some of the datasets, such as Exchange (over 30%).  <ref type="figure">Figure 6</ref>: LPU boosting effect. LPU can serve as a plug-in block in various backbones, e.g., FEL, MLP, LSTM, CNN, and Attention. Replacing LPU with a comparable-sized linear layer will always lead to degraded performance.</p><p>The Exchange dataset does not exhibit apparent periodicity, but FiLM still achieves superior performance. The improvements made by FiLM are consistent with varying horizons, demonstrating its strength in long-term forecasting. More results on the ETT full benchmark are provided in Appendix C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Univariate Results</head><p>The benchmark results for univariate time series forecasting are summarized in Appendix C.4, <ref type="table" target="#tab_0">Table 10</ref>. Compared with SOTA work (FEDformer), FiLM yields an overall 22.6% relative MSE reduction. And on some datasets, such as Weather and Electricity, the improvement can reach more than 40%. It again proves the effectiveness of FiLM in long-term forecasting.  <ref type="figure">Figure  6</ref>. In the experiment, we compare the LPU and the comparable-sized linear layer. It is worth mentioning that LPU does not contain any learnable parameter. The results are shown in <ref type="table" target="#tab_0">Table 19</ref>. For all the modules, LPU significantly improves their average performance for long-term forecasting: MLP: 119.4%, LSTM: 97.0%, CNN: 13.8%, Attention: 8.2%. Vanilla Attention has a relative poor performance when combining the LPU, which worth further digging. Low-rank Approximation for FEL Low-rank approximation of learnable matrix in Frequency Enhanced Layer can significantly reduce our parameter size to 0.1%?0.4% with minor accuracy deterioration. The experiment details are shown in <ref type="table" target="#tab_0">Table  13</ref>. Compared to Transformer-based baselines, FiLM enjoys 80% learnable parameter reduction and 50% memory usage reductions, as shown in Appendix I, J. Mode selection policy for FEL Frequency mode selection policy is studied in <ref type="table" target="#tab_3">Table 4</ref>. The Lowest mode selection method shows the most robust performance. The results in Low random column shows that randomly adding some high-frequency signal gives extra improvement in some datasets, as our theoretical studies in Theorem 3 support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>This subsection presents the ablation study of the two major blocks (FEL &amp; LPU) employed, the multiscale mechanism, and data normalization (RevIN). Ablation of LPU To prove the effectiveness of LPU layer, in <ref type="table" target="#tab_4">Table 5</ref>, we compare the original LPU layer with six variants. The LPU layer consists of two sets of matrices (Projection matrices &amp; Reconstruction matrices). Each has three variants: Fixed, Trainable, and Random Init. In Variant 6, we use comparable-sized linear layers to replace the LPU layer. We observe that Variant 6 leads to a 32.5% degradation on average, which confirms the effectiveness of Legendre projection. The projection matrix in LPU is recursively called N times (N is the input length). So if the projection matrix is randomly initialized (Variant 5), the output will face the problem of exponential explosion. If the projection matrix is trainable (Variants 2, 3, 4), the model suffers from the exponential explosion as well and thus needs to be trained with a small learning rate, which leads to a plodding training speed and requires more epochs for convergence. Thus, the trainable projection version is not recommended, considering the trade-off between speed and performance. The variant with trainable reconstruction matrix (Variant 1) has comparable performance and less difficulty in convergence.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions and Conclusion</head><p>In long-term forecasting, the critical challenge is the trade-off between historical information preservation and noise reduction for accurate and robust forecasting. To address this challenge, we propose a Frequency improved Legendre Memory model, or FiLM, to preserve historical information accurately and remove noisy signals. Moreover, we theoretically and empirically prove the effectiveness of the Legendre and Fourier projection employed in out model. Extensive experiments show that the proposed model achieves SOTA accuracy by a significant margin on six benchmark datasets. In particular, we would like to point out that our proposed framework is rather general and can be used as the building block for long-term forecasting in future research. It can be modified for different scenarios. For example, the Legendre Projection Unit can be replaced with other orthogonal functions such as Fourier, Wavelets, Laguerre Polynomials, Chebyshev Polynomials, etc. In addition, based on the properties of noises, Fourier Enhanced Layer is proved to be one of the best candidate in the framework. We plan to investigate more variants of this framework in the future.</p><p>Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.-X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In Advances in Neural Information Processing Systems, volume 32, 2019.</p><p>Li, Z., Kovachki, N. B., Azizzadenesheli, K., Liu, B., Bhattacharya, K., Stuart, A. M., and Anandkumar, A. Fourier neural operator for parametric partial differential equations. CoRR, abs/2010.08895, 2020.</p><p>Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and Dustdar, S. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations, 2022.</p><p>Makridakis, S., Andersen, A., Carbone, R., Fildes, R., Hibon, M., Lewandowski, R., Newton, J., Parzen, E., and Winkler, R. The accuracy of extrapolation (time series) methods:</p><p>Results of a forecasting competition.</p><p>Journal of Forecasting, 1 <ref type="formula" target="#formula_1">(2)</ref> Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020.</p><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ?., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p><p>Voelker, A., Kaji?, I., and Eliasmith, C. Legendre memory units: Continuous-time representation in recurrent neural networks. In Advances in Neural Information Processing Systems, volume 32, 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>In this section, we will give an overview of the related literature in time series forecasting.</p><p>Traditional Time Series Models The first generation of well-discussed time series model is the autoregressive family. ARIMA <ref type="bibr" target="#b0">Box &amp; Jenkins (1968)</ref>; <ref type="bibr" target="#b1">Box &amp; Pierce (1970)</ref> follows the Markov process and build recursive sequential forecasting. However, a plain autoregressive process has difficulty in dealing non-stationary sequences. Thus, ARIMA employed a pre-process iteration by differencing, which transforms the series to stationary. Still, ARIMA and related models have the linear assumption in the autoregressive process, which limits their usages in complex forecasting tasks.</p><p>Deep Neural Network in Forecasting With the bloom of deep neural networks, recurrent neural networks (RNN) was designed for tasks involving sequential data. However, canonical RNN tends to suffer from gradient vanishing/exploding problem with long input because of its recurrent structure. Among the family of RNNs, <ref type="bibr">LSTM Hochreiter &amp; Schmidhuber (1997b)</ref> and <ref type="bibr">GRU Chung et al. (2014)</ref> proposed gated structure to control the information flow to deal with the gradient vanishing/exploration problem. Although recurrent networks enjoy fast inference, they are slow to train and not parallelizable. Temporal convolutional network (TCN) Sen et al. <ref type="formula" target="#formula_0">(2019)</ref> is another family for sequential tasks. However, limited to the reception field of the kernel, the long-term dependencies are hard to capture. Convolution is a parallelizable operation but expensive in inference.  <ref type="formula" target="#formula_1">2020)</ref>, based on LMU, proposes a novel mechanism (Scaled Legendre), which involves the function's full history (LMU uses rolling windowed history). In the subsequent work of HiPPO, the authors propose S4 model <ref type="bibr" target="#b7">Gu et al. (2021a)</ref> and gives the first practice on time series forecasting tasks. However, LMU and HiPPO share the same backbone (LSTM), which may limit their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithm Implementation</head><p>Algorithm 1 Frequency Enhanced Layer class Freq_enhanced_layer(nn.Module): def __init__(self, in_channels, out_channels, modes1, modes2, compression=0): super(Freq_enhanced_layer, self).__init__() self.in_channels = in_channels self.out_channels = out_channels self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1 self.modes2 = modes2 self.compression = compression self.scale = (1 / (in_channels * out_channels)) self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1)) if compression&gt;0: ## Low-rank approximation self.weights0 = nn.Parameter(self.scale * torch.rand(in_channels, self.compression, dtype= torch.cfloat)) self.weights1 = nn.Parameter(self.scale * torch.rand(self.compression, self.compression, len (self.index), dtype=torch.cfloat)) self.weights2 = nn.Parameter(self.scale * torch.rand(self.compression, out_channels, dtype= torch.cfloat)) def forward <ref type="formula">(</ref> In this subsection, we summarize the details of the datasets used in this paper as follows: 1) ETT <ref type="bibr" target="#b21">Zhou et al. (2021)</ref> dataset contains two sub-dataset: ETT1 and ETT2, collected from two separated counties. Each of them has two versions of sampling resolutions (15min &amp; 1h). ETT </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Implementation Details</head><p>Our model is trained using ADAM <ref type="bibr" target="#b13">Kingma &amp; Ba (2017)</ref> optimizer with a learning rate of 1e ?4 to 1e ?3 . The batch size is set to 32 (It depends on the GPU memory used in the experiment. In fact, a batch size up to 256 does not deteriorate the performance but with faster training speed if larger memory GPU or multiple GPUs is used). The default training process is 15 epochs without any early stopping. We save the model with the lowest loss on the validation set for the final testing. The mean square error (MSE) and mean absolute error (MAE) are used as metrics. All experiments are repeated 5 times and the mean of the metrics is reported as the final results. All the deep learning networks are implemented using PyTorch <ref type="bibr">Paszke et al. (2019)</ref> and trained on NVIDIA V100 32GB GPUs/NVIDIA V100 16GB GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Experiment Error Bars</head><p>We train our model 5 times and calculate the error bars for FiLM and SOTA model FEDformer to compare the robustness, which is summarized in <ref type="table" target="#tab_9">Table 9</ref>. It can be seen that the overall performance of the proposed FiLM is better than that of the SOTA FEDformer model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Univariate Forecasting Results</head><p>The univariate benchmark results are summarized in <ref type="table" target="#tab_0">Table 10</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 ETT Full Benchmark</head><p>We present the full-benchmark on four ETT datasets <ref type="bibr" target="#b21">Zhou et al. (2021)</ref> in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Low-rank Approximation for FEL</head><p>With the low-rank approximation of learnable matrix in Fourier Enhanced Layer significantly reducing our parameter size, here we study its effect on model accuracy on two typical datasets as shown in <ref type="table" target="#tab_0">Table 13</ref>.  The proof is a simple extension of the Proposition 6 in <ref type="bibr" target="#b6">Gu et al. (2020)</ref>. We omit it for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Theorem 2</head><p>As we have x t = Ax t?1 + b ? ? t?1 for t = 2, 3, ...?, we recursively use them and the following result holds:</p><formula xml:id="formula_8">x t = Ax t?1 + b + ? t?1 = A(Ax t?2 + b + ? t?2 ) + b + ? t?1 = A 2 x t?2 + Ab + b + A? t?2 + ? t?1 ? ? ? = A ? x t?? + ??1 i=1 A i b + ??1 i=1 A i ? t?i ( * )</formula><p>.</p><p>Following Hoeffding inequality, for ? &gt; 0 we have</p><formula xml:id="formula_9">? (|( * )| ? ?) ? exp ? 2? 2 ??1 i=1 A i ? t?1 2 ?2 ,<label>(4)</label></formula><p>where ? ?2 is the Orlicz norm defined as</p><formula xml:id="formula_10">X ?2 := inf c ? 0 : [exp(X 2 /c 2 )] ? 2 .</formula><p>Since we require A being unitary, we will have A? 2 2 = ? 2 2 and it implies A i ? t?1 2 ?2 = ? t?1 2 ?2 = O(? 2 ) for i = 1, 2, ..., ?. The desirable result follows by setting ? = O( ? ??).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Theorem 3</head><p>As we keep first s columns selected, P (A) ? A has all 0 elements in first s columns. We thus ignore them and consider the approximation quality on? ? R d?(n?s) with the sampled columns. Via the similar analysis in Appendix C of <ref type="bibr" target="#b18">Zhou et al. (2022)</ref>, with high probability we have ? ?P (A) F ? (1 + ?) ? ?? k F , where? k is the "best" rank-k approximation provided by truncating the singular value decomposition (SVD) of?, and where ? F is the Frobenius norm. As we assume the element in last n ? s columns of A is smaller than a min , one can verify ? ? P (A) F ? d ? (n ? s)a min and desirable result follows immediately. Influence of Legendre Polynomial number N and Frequency mode number M The experimental results on three different datasets (ETTm1, Electricity, and Exchange) in <ref type="figure" target="#fig_8">Figure 7</ref> show the optimal choice of Legendre Polynomials number (N ) when we aim to minimize the reconstruction error (in MSE) on the historical data. The MSE error decreases sharply at first and saturates at an optimal point, where N is in proportion to the input length. For input sequence with lengths of 192, 336, and 720, N ? 40, 60, and 100 gives the minimal MSE, respectively. <ref type="figure" target="#fig_9">Figure 8</ref> shows the MSE error of time series forecasting on Electricity dataset, with different Legendre Polynomials number (N ), mode number and input length. We observe that, when enlarging N , the model performance saturates at an optimal point. For example, in <ref type="figure" target="#fig_9">Figure 8</ref> Left (input length=192), the best performance is reached when N &gt; 64. While in <ref type="figure" target="#fig_9">Figure 8</ref> Right (input length=720), the best performance is reached when N is larger than 128. Another influential parameter is the mode number. From <ref type="figure" target="#fig_9">Figure 8</ref> we observe that a small mode number will lead to better performance, as module with small mode number works as a denoising filter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Noise Injection Experiment</head><p>Our model's robustness in long-term forecasting tasks can be demonstrated using a series of noise injection experiments as shown in <ref type="table" target="#tab_0">Table 14</ref>. As we can see, adding Gaussian noise in the training/test stage has limited effect on our model's performance, since the deterioration is less than 1.5% in the worst case. The model's robustness is consistent across various forecasting horizons. Note that adding the noise in testing stage other than the training stage will even improve our performance by 0.4%, which further supports our claim of robustness. We adopt the Kolmogorov-Smirnov (KS) test to check whether our model's input and output sequences come from the same distribution. The KS test is a nonparametric test for checking the the equality of probability distributions. In essence, the test answers the following question "Are these two sets of samples drawn from the same distribution?". The Kolmogorov-Smirnov statistic is defined as:</p><formula xml:id="formula_11">D n,m = sup x |F 1,n (x) ? F 2,m (x)| ,</formula><p>where sup is the supremum function, F 1,n and F 2,m are the empirical distribution functions of the two compared samples. For samples that are large enough, the null hypothesis would be rejected at level ? if</p><formula xml:id="formula_12">D n,m &gt; ? 1 2 ln ? 2 ? n + m n ? m ,</formula><p>where n and m are the first and second sample sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 Distribution Analysis</head><p>In this section, we evaluate the distribution similarity between models' input and output sequences using the KS test. In <ref type="table" target="#tab_0">Table 15</ref>, we applied the Kolmogrov-Smirnov test to check if output sequence of various models that trained on ETTm1/ETTm2 are consistent with the input sequence. On both datasets, by setting the standard P-value as 0.01, various existing baseline models have much smaller P-values except FEDformer and Autoformer, which indicates their outputs have a high probability of being sampled from a different distributions compared to their input signals. Autoformer and FEDformer have much larger P-values mainly due to their seasonal-trend decomposition mechanism. The proposed FiLM also has a much larger P-value compared to most baseline models. And its null hypothesis can not be rejected in most cases for these two datasets. It implies that the output sequence generated by FiLM shares a similar pattern as the input signal, and thus justifies our design motivation of FiLM as discussed in Section 1. Though FiLM gets a smaller P-value than FEDformer, it is close to the actual output, which indicates that FiLM makes a good balance between recovering and forecasting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Learnable Parameter Size</head><p>Compared to Transformer-based baseline models, FiLM enjoys a lightweight property with 80% learnable parameter reduction as shown in <ref type="table" target="#tab_0">Table 16</ref>. It has the potential to be used in mobile devices, or, in some situations where a lightweight model is preferred.  has a quasi constant memory usage. Note that the memory usage of FiLM is only linear to input length. Furthermore, FiLM enjoys a much smaller memory usage than others because of the simple architecture and compressed parameters with low-rank approximation as discussed in Appendix I.</p><p>Training Speed Experiments are performed on one NVIDIA V100 32GB GPU. As shown in <ref type="figure">Figure 9</ref> (Right), FiLM has faster training speed than others with the prolonging output length. For fair comparison, we fix the experimental setting of Xformer, where we fix the input length as 96 and prolong the output length. However, in the real experiment settings, we use longer input length (much longer than 96). Thus, the experiment in <ref type="figure">Figure 9</ref> (Right) is merely a toy case to show the tendency. In <ref type="figure" target="#fig_0">Figure 10</ref>, we show the average epoch time vs average performance under the settings of benchmarks. The experiment is performed on ETTm2 dataset with output length = 96, 192, 336, and 720. Because of the extreme low memory usage of FiLM, it can be trained with larger batch size (batch size = 256) on only one GPU compared with baselines (batch size = 32). In <ref type="figure" target="#fig_0">Figure 10</ref>, FiLM-256 is the FiLM models trained with batch size = 256, it exhibits significant advantages on both speed and accuracy. Furthermore, due to the shallow structure and smaller amount of trainable parameters, FiLM is easy to converge and enjoys smaller performance variation and smaller performance degradation when using large batch size. It is observed that the models with Fourier enhanced block (FiLM &amp; FEDformer) have better robustness. It also worth noting that vanilla Transformer has good training speed because of the not-so-long sequence length. Only a sequence length over one thousand will distinguish the advantage of efficient Transformers. As N-HiTS is the latest development from the research group, which also published N-BEATS, we add N-HiTS to our empirical comparison. Here, we adopt the results in the N-HiTS paper to prevent inappropriate parameter tuning problems. We also add a seasonal-naive model in the comparison. FiLM outperforms N-HiTS in most cases(33/48). Moreover, Simple Seasonal-naiveMakridakis et al. <ref type="formula" target="#formula_0">(1982)</ref> is a solid baseline on exchange datasets better than N-hits, Fedformer, and Autoformer, but FiLM still surpasses its performance, as shown in <ref type="table" target="#tab_0">Table 17</ref>. As shown in <ref type="table" target="#tab_0">Table 18</ref>, although LPU+MLP combining all boosting tricks has slightly better performance than FiLM for the ETTm1 dataset, FiLM remains the most consistent and effective model among all variants across all six datasets. FEL is a much better backbone structure than MLP, LSTM, CNN, and vanilla attention modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.3 Boosting experiments of LPU with common deep learning backbones for all six datasets</head><p>As shown in <ref type="table" target="#tab_0">Table 19</ref>, LPU shows a consistent boosting effect across all selected common deep learning backbones for most datasets. It can be used as a simple and effective build add-on block for long-term time series forecasting tasks. Although without data normalization, pure LPU negatively boosts performance for some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.4 Ablation univariate forecasting experiments for Low rank approximation with all six datasets</head><p>As shown in <ref type="table" target="#tab_1">Table 20</ref>, with the low-rank approximation of learnable matrix in Fourier Enhanced Layer significantly reducing our parameter size, and even improve our model's performance for in some datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.5 Ablation univariate forecasting experiments for frequency mode selection with all six datasets</head><p>Three different mode selection policies are studied for frequency enhanced layer: 1) lowest mode selection: we select m lowest frequency modes to retain. 2) random model selection: we select m frequency modes randomly to retain. 3) lowest with extra high mode selection: we select 0.8 ? m lowest frequency modes and 0.2 ? m high frequency modes randomly to retain. The experimental results are summarized in <ref type="table" target="#tab_0">Table 21</ref> with m = 64 for both experiments. Lowest mode selection is the most stable frequency mode selection policy through adding some randomness mode can improve the results for some datasets.     Parameter size 100% 6.4% 1.6% 0.4%</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The discrepancy between ground truth and forecasting output from vanilla Transformer and LSTM on a real-world ETTh1 dataset Left: trend shift. Right: seasonal shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Data recovery with Autoencoder structure: recovery a 1024-length data with a bottleneck of 128 parameters. Left: Legendre Projection Unit. Right: LSTM and vanilla Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Overall structure of FilM (Frequency improved Legendre Memory Model)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Structure of Legendre Projection Layer (LPU). LPU contains two states: Projection &amp; Reconstruction. C(t) is the compressed memory for historical input up to time t. x(t) is the original input signal at time t. A, B are two pre-fixed projection matrices. C(t) is reconstructed to original input by multiplying a discrete Legendre Polynomials matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Structure of Frequency Enhanced Layer (FEL): Original version (use weights W ) and Low-rankApproximation version (use weights W = W1 ? W2 ? W3), where N is Legendre Polynomials number, M is Fourier mode number, and T is sequence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>With the innovation of Transformers in natural language processing Vaswani et al. (2017); Devlin et al. (2019) and in computer vision tasks Dosovitskiy et al. (2021); Rao et al. (2021), they are also discussed, renovated, and applied in time series forecasting Wen et al. (2022), especially the main attention module. Some works use temporal attention Qin et al. (2017) to capture long-range dependencies for time series. Others use the backbone of Transformer. Transformers usually employ an encoder-decoder architecture, with the self-attention and cross-attention mechanisms serving as the core layers. Li et al. (2019) invents a logsparse attention module to deal with the memory bottleneck for Transformer model. Kitaev et al. (2020) uses locality-sensitive hashing to replace the attention module for less time complexity. Zhou et al. (2021) proposes a probability sparse attention mechanism to deal with long-term forecasting. Wu et al. (2021) designs a decomposition Transformer architecture with an Auto-Correlation mechanism as an alternative for attention module. Liu et al. (2022) designs a low-complexity Pyramidal Attention for the long-term time forecasting tasks. Zhou et al. (2022) proposes two attention modules which operate in frequency domain using Fourier or wavelet transformation.Orthogonal Basis and Neural Network Orthogonal basis project arbitrary functions onto a certain space and thus enable the representation learning in another view. Orthogonal basis family is easy to be discretized and to serve as an plug-in operation in neural networks. Recent studies began to realize the efficiency and effectiveness of Orthogonal basis, including the polynomial family and others (Fourier basis &amp; Multiwavelet basis). Fourier basis is first introduced for acceleration due to Fast Fourier algorithm, for example, acceleration of computing convolution<ref type="bibr" target="#b6">Gu et al. (2020)</ref> or Auto-correlation function<ref type="bibr" target="#b19">Wu et al. (2021)</ref>. Fourier basis also serves as a performance boosting block: Fourier with Recurrent structure<ref type="bibr" target="#b20">Zhang et al. (2018)</ref>, Fourier with MLP Li et al. (2020); Lee-<ref type="bibr" target="#b16">Thorp et al. (2021)</ref> and Fourier in Transformer<ref type="bibr" target="#b18">Zhou et al. (2022)</ref>. Multiwavelet transform is a more local filter (compared with Fourier) and a frequency decomposer. Thus, neural networks which employ multiwavelet filter usually exhibit a hierarchical structure and treat different frequency in different tunnels, e.g.,<ref type="bibr" target="#b17">Wang et al. (2018)</ref>;<ref type="bibr" target="#b9">Gupta et al. (2021)</ref>;<ref type="bibr" target="#b18">Zhou et al. (2022)</ref>. Orthogonal Polynomials are naturally good selections of orthogonal basis. Legendre Memory Unit (LMU) Voelker et al. (2019) uses Legendre Polynomials for an orthogonal projection of input signals for a memory strengthening with the backbone of LSTM. The projection process is mathematically derived from delayed linear transfer function. HiPPOGu et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>self, x): B, H,E, N = x.shape # Compute Fourier coefficients up to factor of e^(-something constant) x_ft = torch.fft.rfft(x) # Multiply relevant Fourier modes out_ft = torch.zeros(B, H, self.out_channels, x.size(-1)//2 + 1) if self.compression == 0: a = x_ft[:, :, :, :self.modes1] out_ft[:, :, :, :self.modes1] = torch.einsum("bjix,iox-&gt;bjox", a, self.weights1) else: a = x_ft[:, :, :, :self.modes2] a = torch.einsum("bjix,ih-&gt;bjhx", a, self.weights0) a = torch.einsum("bjhx,hkx-&gt;bjkx", a, self.weights1) out_ft[:, :, :, :self.modes2] = torch.einsum("bjkx,ko-&gt;bjox", a, self.weights2) # Return to physical space x = torch.fft.irfft(out_ft, n=x.size(-1)) return x Algorithm 2 LPU layer from scipy import signal from scipy import special as ss class LPU(nn.Module): def __init__(self, N=256, dt=1.0, discretization='bilinear'): # N: the order of the Legendre projection # dt: step size -can be roughly inverse to the length of the sequence super(LPU,self).__init__() self.N = N A,B = transition(N) ### LMU projection matrix A,B, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=discretization) B = B.squeeze(-1) self.register_buffer('A', torch.Tensor(A)) self.register_buffer('B', torch.Tensor(B)) def forward(self, inputs): # inputs: (length, ...) # output: (length, ..., N) where N is the order of the Legendre projection c = torch.zeros(inputs.shape[:-1] + tuple([self.N])) cs = [] for f in inputs.permute([-1, 0, 1]): f = f.unsqueeze(-1) new = f @ self.B.unsqueeze(0) # [B, D, H, 256] c = F.linear(c, self.A) + new cs.append(c) return torch.stack(cs, dim=0) def reconstruct(self, c): a = (self.eval_matrix @ c.unsqueeze(-1)).squeeze(-1) return (self.eval_matrix @ c.unsqueeze(-1)).squeeze(-1) C Dataset and Implementation Details C.1 Dataset Details</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>0022 0.037 0.015 0.091 0.0085 0.070 0.0074 0.0736 0.0031 0.042 0.0071 0.063 0.0041 0.049</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>The reconstruction error (MSE) vs. Legendre Polynomial number (N ) on three datasets with three different input lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>The MSE error of univariate time series forecasting task on Electricity dataset with different Legendre Polynomials number (N ), mode number and input length. Left: input length = 192. Mid: input length = 336. Right: input length = 720.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>(Left) the memory usage of FiLM and baseline models. (Right) training speed of FiLM and baseline models. The input length is fixed to 96 and the output length is768, 1536, 3072, 4608, and 7680.    Memory Usage As shown inFigure 9(Left), FiLM has a good memory usage with the prolonging output length. For fair comparison, we fix the experimental settings of Xformer, where we fix the input length as 96 and prolong the output length. FromFigure 9(Left), we can observe that FiLM 21 Comparison of training speed and performance of benchmarks. The experiment is performed on ETTm2 with output length = 96, 192, 336, and 720. The performance of the models is measured with Score, where Score = 1/M SE. The radius of the circle measured the STD of the performance. A higher Score indicates better performance, same for Speed. A smaller circle indicates better robustness. The Speed and Score are presented on relative value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>long-term series forecasting with extra baseline modelsFor the additional benchmarks for multivariate experiments, we add some non-Transformer methods for comparison.N-BEATSOreshkin et al. (2019) and N-HiTSChallu et al. (2022) are two recent proposed powerful non-Transformer methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>0.481 0.487 0.533 0.508 0.561 0.570 0.612 336 0.557 0.584 0.553 0.565 0.703 0.696 0.722 0.706 720 0.641 0.644 0.648 0.641 0.900 0.780 1.493 1.032</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>multivariate long-term series forecasting results on six datasets with various input length and prediction length O ? {96, 192, 336, 720} (For ILI dataset, we set prediction length O ? {24, 36, 48, 60}). A lower MSE indicates better performance. All experiments are repeated 5 times.</figDesc><table><row><cell>Methods</cell><cell>FiLM</cell><cell>FEDformer</cell><cell>Autoformer</cell><cell>S4</cell><cell>Informer</cell><cell>LogTrans</cell><cell>Reformer</cell></row></table><note>Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>LPU Boosting Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A set of experiments are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>conducted to measure</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>the boosting effect of LPU when combined with</cell><cell cols="2">Methods Compare LPU</cell><cell>FEL Linear</cell><cell>LPU</cell><cell>MLP Linear</cell><cell cols="2">LSTM LPU Linear</cell><cell>lagged-LSTM LPU Linear LPU Linear LPU CNN Attention Linear</cell></row><row><cell>various common deep learning modules (MLP, LSTM, CNN, and Atten-</cell><cell>ETTm1</cell><cell cols="5">96 0.030 +38% 0.034 +8.0% 0.049 192 0.047 +9.5% 0.049 +30% 0.174 336 0.063 +5.8% 0.061 +64% 0.119 720 0.081 +1.4% 0.082 +62% 0.184</cell><cell>+73% +32% +84% +32%</cell><cell>0.093 -21% 0.116 -50% 0.243 0.331 -48% 0.101 +20% 0.387 0.214 -19% 0.122 +25% 1.652 +12% -81% -86% 0.303 -6.5% 0.108 +13% 4.782 -61%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>436</cell><cell>-6.7%</cell><cell>0.517 +23% 0.359 +29% 2.043</cell><cell>-54%</cell></row><row><cell></cell><cell></cell><cell cols="5">720 0.321 +37% 0.339 +196% 0.636</cell><cell>-11%</cell><cell>0.492 +28% 0.424 +18% 9.115 +298%</cell></row></table><note>Boosting effect of LPU layer for common deep learning backbones: MLP, LSTM, CNN and Attention.'+' indicates degraded performance.Electricity 96 0.213 +136% 0.431 +121% 0.291 +55.6% 0.739 -33% 0.310 +43% 0.805 +23% 192 0.268 +32% 0.291 +239% 0.353 +17% 0.535 +15% 0.380 +12% 0.938 +14% 336 0.307 +0.1% 0.296 +235% 0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Low-rank Approximation (LRA) study for Frequency Enhanced Layer (FEL): Comp. K=0 means default version without LRA, 1 means the largest compression using K=1.</figDesc><table><row><cell></cell><cell>Comp. K</cell><cell>0</cell><cell>16</cell><cell>4</cell><cell>1</cell></row><row><cell></cell><cell>Metric</cell><cell cols="4">MSE MAE MSE MAE MSE MAE MSE MAE</cell></row><row><cell>ETTh1</cell><cell>96 192 336 720</cell><cell cols="4">0.371 0.394 0.371 0.396 0.371 0.398 0.400 0.421 0.414 0.423 0.411 0.423 0.414 0.426 0.435 0.444 0.442 0.445 0.443 0.446 0.443 0.444 0.492 0.478 0.454 0.451 0.464 0.474 0.468 0.478 0.501 0.499</cell></row><row><cell>Weather</cell><cell>96 192 336 720</cell><cell cols="4">0.199 0.262 0.199 0.263 0.197 0.262 0.198 0.263 0.228 0.288 0.225 0.285 0.226 0.285 0.225 0.286 0.267 0.323 0.266 0.321 0.263 0.314 0.264 0.316 0.319 0.361 0.314 0.355 0.315 0.354 0.318 0.357</cell></row><row><cell cols="2">Parameter size</cell><cell>100%</cell><cell>1.95%</cell><cell>0.41%</cell><cell>0.10%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">Policy</cell><cell>Lowest</cell><cell>Random</cell><cell>Low random</cell></row><row><cell cols="2">Metric</cell><cell cols="3">MSE MAE MSE MAE MSE MAE</cell></row><row><cell>Exchange</cell><cell cols="4">96 0.086 0.204 0.086 0.208 0.087 0.210 192 0.188 0.292 0.187 0.318 0.207 0.340 336 0.356 0.433 0.358 0.437 0.353 0.461 720 0.727 0.669 0.788 0.680 0.748 0.674</cell></row><row><cell>Weather</cell><cell cols="4">96 0.199 0.262 0.197 0.256 0.196 0.254 192 0.228 0.288 0.234 0.300 0.234 0.301 336 0.267 0.323 0.266 0.319 0.263 0.316 720 0.319 0.361 0.317 0.356 0.316 0.354</cell></row></table><note>Mode selection policy study for frequency en- hanced layer. Lowest: select the lowest m frequency mode; Random: select m random frequency mode; Low random: select the 0.8 * m lowest frequency mode and 0.2 * m random high frequency mode.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The experiments are performed on ETTm1 and Electricity with different input lengths. The metric of variants is presented in relative value ('+' indicates degraded performance, '-' indicates improved performance).</figDesc><table><row><cell></cell><cell>Index</cell><cell>Original</cell><cell cols="2">Variant 1</cell><cell cols="2">Variant 2</cell><cell cols="2">Variant 3</cell><cell cols="2">Variant 4</cell><cell>Variant 5</cell><cell cols="2">Variant 6</cell></row><row><cell cols="2">Projection</cell><cell cols="2">Fixed</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Trainable</cell><cell></cell><cell></cell><cell>Random Init</cell><cell cols="2">Linear</cell></row><row><cell cols="2">Reconstruction</cell><cell>Fixed</cell><cell cols="2">Trainable</cell><cell cols="2">Fixed</cell><cell cols="2">Trainable</cell><cell cols="2">Random Init</cell><cell>-</cell><cell cols="2">Linear</cell></row><row><cell></cell><cell>Metric</cell><cell>MSE MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>-</cell><cell>MSE</cell><cell>MAE</cell></row><row><cell>ETTm1</cell><cell>96 192 336 720</cell><cell cols="9">0.030 0.128 +6.0% +3.1% +4.6% +3.1% +4.7% +2.7% +0.7% +0.6% 0.047 0.160 -2.6% -1.3% +2.5% +1.8% -0.4% +0.4% -3.0% -1.0% 0.063 0.185 +4.2% +1.8% -2.4% -0.4% -6.3% -2.5% +2.2% +1.4% 0.081 0.215 -0.4% -0.6% +24% +12% +11% +5.9% NaN NaN</cell><cell>NaN NaN NaN NaN</cell><cell cols="2">+38% +9.5% +8.7% +22% +5.8% +5.0% +1.4% +2.2%</cell></row><row><cell>Electricity</cell><cell>96 192 336 720</cell><cell cols="9">0.213 0.328 +15% +6.8% +11% +5.2% +11% +5.1% +11% +4.8% 0.268 0.373 -4.6% -3.8% +7.8% +3.9% +6.8% +3.5% -5.3% -3.9% 0.307 0.417 -4.2% -5.0% -3.9% -6.0% -7.2% -8.0% -8.5% -9.0% 0.321 0.423 +2.9% +2.8% +10% +6.8% +3.0% +1.7% +207% +85%</cell><cell>NaN NaN NaN NaN</cell><cell cols="2">+136% +58% +32% +16% +0.1% -5.0% 37% 22%</cell></row></table><note>Ablation studies of LPU layer. The original LPU block (whose projection and reconstruction matrix are fixed) is replaced with 6 variants (Fixed means the matrix are not trainable. Trainable means the matrix is initialized with original parameters and trainable. Random Init means the matrix is initialized randomly and trainable).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies of FEL layer. The FEL layer is replaced with 4 different variants: MLP, LSTM, CNN, and Transformer. S4 is also introduced as a variant. The experiments are performed on ETTm1 and Electricity. The metric of variants is presented in relative value ('+' indicates degraded performance, '-' indicates improved performance). To prove the effectiveness of the FEL, we replace the FEL with multiple variants (MLP, LSTM, CNN, and Transformer). S4 is also introduced as a variant since it has a version with Legendre projection memory. The experimental results are summarized inTable 6. FEL achieves the best performance compared with its LSTN, CNN, and Attention counterpart. MLP has comparable performance when input length is 192, 336, and 720. However, MLP suffers from insupportable memory usage, which is N 2 L (while FEL is N 2 ). S4 achieves similar results as our LPU+MLP variant. Among all, the LPU+CNN variant shows the poorest performance.</figDesc><table><row><cell cols="2">Methods</cell><cell>FilM</cell><cell cols="2">LPU+MLP</cell><cell cols="2">LPU+LSTM</cell><cell cols="2">LPU+CNN</cell><cell cols="2">LPU+attention</cell><cell>S4</cell></row><row><cell cols="2">Metric</cell><cell>MSE MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE MAE</cell></row><row><cell>ETTm1</cell><cell cols="10">96 0.029 0.127 +0.0% +0.0% +12.1% +7.1% +13.5% +9.5% 192 0.041 0.153 -1.5% -0.6% +12.2% +8.5% +10.8% +7.8% 336 0.053 0.175 -1.7% -1.7% +4.5% +4.0% +10.8% +6.3% 720 0.071 0.205 -0.9% -1.0% +5.5% +3.4% +8.6% +4.9% +13.8% +7.8% +1.7% +1.6% +2.0% +3.3% +4.5% +2.9%</cell><cell>----</cell><cell>----</cell></row><row><cell>Electricity</cell><cell cols="11">96 0.154 0.247 +155% +81% +160% +84% +330% +155% +242% +119% 128% 83% 192 0.166 0.258 +59% +39% +121% +67% +224% +117% +264% +131% 124% 76% 336 0.188 0.283 +55% +35% +150% +74% +128% +71% +183% +95% 117% 69% 720 0.249 0.341 +33% +25% +154% +73% +192% +95% +312% +138% 89% 51%</cell></row><row><cell cols="2">Ablation of FEL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation studies of Normalization and Multiscale. Multiscale use 3 branches with: T , 2T , and 4T as input sequence length. T is the predicted sequence length The multiscale module leads to significant improvement on all datasets consistently. However, the data normalization achieves mixed performance, leading to improved performance on Traffic and Illness but a slight improvement on the rest. The Ablation study of RevIN data normalization and the mixture of multiscale experts employed is shown inTable 7.4.3 Other StudiesOther supplemental studies are provided in Appendix. 1. A parameter sensitivity experiment (in Appendix F) is carried out to discuss the choice of hyperparameter for M and N, where M is the frequency mode number and N is the Legendre Polynomials number. 2. A noise injection experiment (in Appendix G) is conducted to show the robustness of our model. 3. A Kolmogorov-Smirnov (KS) test (in Appendix H) is conducted to discuss the similarity between the output distribution and the input. Our proposed FiLM has the best results on KS test, which supports our motivation in model design. 4. At last, (in Appendix J), though failing short in training speed compared to some MLP based models likeN-HiTSChallu et al. (2022), FiLM is an efficient model with shorter per-step training time and smaller memory usage compared to baseline models in univariate forecasting tasks. However, it is worth mentioning that the training time will considerably prolong for multivariate forecasting tasks with many target variables.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>ETTm2</cell><cell>Electricity</cell><cell>Exchange</cell><cell cols="2">Traffic</cell><cell>Weather</cell><cell>Illness</cell></row><row><cell></cell><cell>Metric</cell><cell cols="6">MSE Relative MSE Relative MSE Relative MSE Relative MSE Relative MSE Relative</cell></row><row><cell>Methods</cell><cell cols="3">Original Normalization 0.268 0.301 Multiscale 0.259 + 0.19% 0.187 +16% 0.195 +4.8% +3.6% 0.199 +6.4% +0% With both 0.271 +4.7% 0.189 +1.5%</cell><cell>0.534 + 60% 0.456 +36% 0.335 0% 0.398 +19%</cell><cell>0.528 0.656 0.541 0.442</cell><cell>+19% +48% +22% +0%</cell><cell>0.264 +4.9% 0.256 +1.5% 0.253 +0.50% 2.41 3.55 3.36 0.253 +0.4% 1.97</cell><cell>+ 80% +70% +22% +0%</cell></row><row><cell cols="6">Ablation of Multiscale and Data Normalization (RevIN)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>/onlinelibrary.wiley.com/doi/abs/10.1002/for.3980010202. Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. N-BEATS: Neural basis expansion analysis for interpretable time series forecasting. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.</figDesc><table><row><cell>:111-153, 1982.</cell><cell>doi:</cell><cell>https://doi.org/10.1002/for.3980010202.</cell><cell>URL</cell></row><row><cell>https:/</cell><cell></cell><cell></cell><cell></cell></row></table><note>Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28, pp. 1310-1318, 2013. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chil- amkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high- performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8024-8035. 2019. Qin, Y., Song, D., Chen, H., Cheng, W., Jiang, G., and Cottrell, G. W. A dual-stage attention- based recurrent neural network for time series prediction. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI), Melbourne, Australia, August 19-25, 2017, pp. 2627-2633. ijcai.org, 2017. Rangapuram, S. S., Seeger, M. W., Gasthaus, J., Stella, L., Wang, Y., and Januschowski, T. Deep state space models for time series forecasting. In Advances in Neural Information Processing Systems, volume 31, 2018. Rao, Y., Zhao, W., Zhu, Z., Lu, J., and Zhou, J. Global filter networks for image classification. Advances in Neural Information Processing Systems, 34, 2021. Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T. DeepAR: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181-1191, 2020. Sen, R., Yu, H., and Dhillon, I. S. Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting. In Advances in Neural Information Processing Systems (NeurIPS), December 8-14, 2019, Vancouver, BC, Canada, pp. 4838-4847, 2019.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Details of benchmark datasets.</figDesc><table><row><cell>DATASET</cell><cell cols="3">LENGTH DIMENSION FREQUENCY</cell></row><row><cell>ETTM2</cell><cell>69680</cell><cell>8</cell><cell>15 MIN</cell></row><row><cell>EXCHANGE</cell><cell>7588</cell><cell>9</cell><cell>1 DAY</cell></row><row><cell>WEATHER</cell><cell>52696</cell><cell>22</cell><cell>10 MIN</cell></row><row><cell>ELECTRICITY</cell><cell>26304</cell><cell>322</cell><cell>1H</cell></row><row><cell>ILI</cell><cell>966</cell><cell>8</cell><cell>7 DAYS</cell></row><row><cell>TRAFFIC</cell><cell>17544</cell><cell>863</cell><cell>1H</cell></row><row><cell cols="4">dataset contains multiple time series of electrical loads and one time sequence of oil temperature.</cell></row><row><cell cols="4">2) Electricity 2 dataset contains the electricity consumption for more than three hundred clients with</cell></row><row><cell cols="4">each column corresponding to one client. 3) Exchange Lai et al. (2018) dataset contains the current</cell></row><row><cell cols="4">exchange of eight countries. 4) Traffic 3 dataset contains the occupation rate of freeway systems in</cell></row><row><cell cols="4">California, USA. 5) Weather 4 dataset contains 21 meteorological indicators for a range of one year</cell></row><row><cell cols="4">in Germany. 6) Illness 5 dataset contains the influenza-like illness patients in the United States. Table</cell></row><row><cell cols="4">8 summarizes all the features for the six benchmark datasets. They are all split into the training set,</cell></row><row><cell cols="3">validation set and test set by the ratio of 7:1:2 during modeling.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>MSE with error bars (Mean and STD) for FiLM and FEDformer baseline for multivariate long-term forecasting. All experiments are repeated 5 times.</figDesc><table><row><cell cols="2">MSE</cell><cell>ETTm2</cell><cell>Electricity</cell><cell>Exchange</cell><cell>Traffic</cell></row><row><cell>FiLM</cell><cell cols="3">96 192 0.222 ? 0.0038 0.165? 0.0023 0.165 ? 0.0051 0.153? 0.0014 336 0.277 ? 0.0021 0.186? 0.0018</cell><cell>0.079? 0.002 0.159? 0.011 0.270? 0.018</cell><cell>0.416? 0.010 0.408? 0.007 0.425? 0.007</cell></row><row><cell></cell><cell cols="3">720 0.371 ? 0.0066 0.236? 0.0022</cell><cell>0.536? 0.026</cell><cell>0.520? 0.003</cell></row><row><cell>FED-f</cell><cell cols="5">96 192 0.269 ? 0.0023 0.201? 0.0015 0.203 ? 0.0042 0.194 ? 0.0008 0.148 ? 0.002 0.217 ? 0.008 0.270? 0.008 0.604 ? 0.004 336 0.325 ? 0.0015 0.215? 0.0018 0.460? 0.016 0.621 ? 0.006 720 0.421 ? 0.0038 0.246? 0.0020 1.195? 0.026 0.626 ? 0.003</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Univariate long-term forecasting results on six datasets with various input length and prediction horizon O ? {96, 192, 336, 720}. A lower MSE indicates better performance. All experiments are repeated 5 times.</figDesc><table><row><cell>Methods</cell><cell cols="2">FiLM</cell><cell cols="2">FEDformer</cell><cell cols="2">Autoformer</cell><cell>S4</cell><cell></cell><cell cols="2">Informer</cell><cell cols="2">LogTrans</cell><cell cols="2">Reformer</cell></row><row><cell>Metric</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell></row><row><cell>ET T m2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11</head><label>11</label><figDesc>(multivariate forecasting) and Table 12 (univariate forecasting). The ETTh1 and ETTh2 are recorded hourly while ETTm1 and ETTm2 are recorded every 15 minutes. The time series in ETTh1 and ETTm1 follow the same pattern, and the only difference is the sampling rate, similarly for ETTh2 and ETTm2. On average, our FiLM yields a 14.0% relative MSE reduction for multivariate forecasting, and a 16.8% reduction for univariate forecasting over the SOTA results from FEDformer.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Multivariate long-term forecasting results on ETT full benchmark. The best results are highlighted in bold. A lower MSE indicates better performance. All experiments are repeated 5 times.</figDesc><table><row><cell cols="2">Methods</cell><cell>FiLM</cell><cell>FEDformer</cell><cell>Autoformer</cell><cell>S4</cell><cell>Informer</cell><cell>LogTrans</cell><cell>Reformer</cell></row><row><cell cols="2">Metric</cell><cell cols="7">MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE</cell></row><row><cell>ET T h1</cell><cell cols="8">96 0.371 0.394 0.376 0.419 0.449 0.459 0.949 0.777 0.865 0.713 0.878 0.740 0.837 0.728 192 0.414 0.423 0.420 0.448 0.500 0.482 0.882 0.745 1.008 0.792 1.037 0.824 0.923 0.766 336 0.442 0.445 0.459 0.465 0.521 0.496 0.965 0.75 1.107 0.809 1.238 0.932 1.097 0.835 720 0.465 0.472 0.506 0.507 0.514 0.512 1.074 0.814 1.181 0.865 1.135 0.852 1.257 0.889</cell></row><row><cell>ET T h2</cell><cell cols="8">96 0.284 0.348 0.346 0.388 0.358 0.397 1.551 0.968 3.755 1.525 2.116 1.197 2.626 1.317 192 0.357 0.400 0.429 0.439 0.456 0.452 2.336 1.229 5.602 1.931 4.315 1.635 11.12 2.979 336 0.377 0.417 0.482 0.480 0.482 0.486 2.801 1.259 4.721 1.835 1.124 1.604 9.323 2.769 720 0.439 0.456 0.463 0.474 0.515 0.511 2.973 1.333 3.647 1.625 3.188 1.540 3.874 1.697</cell></row><row><cell>ET T m1</cell><cell cols="8">96 0.302 0.345 0.378 0.418 0.505 0.475 0.640 0.584 0.672 0.571 0.600 0.546 0.538 0.528 192 0.338 0.368 0.426 0.441 0.553 0.496 0.570 0.555 0.795 0.669 0.837 0.700 0.658 0.592 336 0.373 0.388 0.445 0.459 0.621 0.537 0.795 0.691 1.212 0.871 1.124 0.832 0.898 0.721 720 0.420 0.420 0.543 0.490 0.671 0.561 0.738 0.655 1.166 0.823 1.153 0.820 1.102 0.841</cell></row><row><cell>ET T m2</cell><cell cols="8">96 0.165 0.256 0.203 0.287 0.255 0.339 0.705 0.690 0.365 0.453 0.768 0.642 0.658 0.619 192 0.222 0.296 0.269 0.328 0.281 0.340 0.924 0.692 0.533 0.563 0.989 0.757 1.078 0.827 336 0.277 0.333 0.325 0.366 0.339 0.372 1.364 0.877 1.363 0.887 1.334 0.872 1.549 0.972 720 0.371 0.389 0.421 0.415 0.422 0.419 2.074 1.074 3.379 1.338 3.048 1.328 2.631 1.242</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Univariate long-term forecasting results on ETT full benchmark. The best results are highlighted in bold. A lower MSE indicates better performance. All experiments are repeated 5 times. MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE</figDesc><table><row><cell>Methods</cell><cell>FiLM</cell><cell>FEDformer</cell><cell>Autoformer</cell><cell>S4</cell><cell>Informer</cell><cell>LogTrans</cell><cell>Reformer</cell></row><row><cell>Metric</cell><cell>MSE MAE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Low-rank Approximation (LRA) study for frequency enhanced layer: Comp. K=0 means default version without LRA, 1 means the largest compression using K=1.</figDesc><table><row><cell></cell><cell>Comp. K</cell><cell>0</cell><cell>16</cell><cell>4</cell><cell>1</cell></row><row><cell></cell><cell>Metric</cell><cell cols="4">MSE MAE MSE MAE MSE MAE MSE MAE</cell></row><row><cell>ET T h1</cell><cell>96 192 336 720</cell><cell cols="4">0.371 0.394 0.371 0.397 0.373 0.399 0.391 0.418 0.414 0.423 0.414 0.425 0.413 0.426 0.437 0.445 0.442 0.445 0.452 0.451 0.445 0.444 0.460 0.458 0.454 0.451 0.460 0.472 0.461 0.471 0.464 0.476</cell></row><row><cell>W eather</cell><cell>96 192 336 720</cell><cell cols="4">0.199 0.262 0.200 0.266 0.199 0.263 0.198 0.261 0.228 0.288 0.232 0.298 0.227 0.287 0.226 0.285 0.267 0.323 0.266 0.320 0.253 0.314 0.264 0.316 0.319 0.361 0.314 0.352 0.319 0.361 0.314 0.354</cell></row><row><cell cols="2">Parameter size</cell><cell>100%</cell><cell>6.4%</cell><cell>1.6%</cell><cell>0.4%</cell></row><row><cell cols="2">E Theoretical Analysis</cell><cell></cell><cell></cell><cell></cell></row><row><cell>E.1 Theorem 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>Noise injection studies. A 0.3*N (0, 1) Gaussian noise is introduced into our training/testing. We conduct 4 sets of experiments with/without noise in training and test phases. The experiments are performed on ETTm1 and Electricity with different output lengths. The metric of variants is presented in relative value ('+' indicates degraded performance, and '-' indicates improved performance).</figDesc><table><row><cell cols="2">Training</cell><cell cols="2">noise</cell><cell></cell><cell></cell><cell cols="2">with noise</cell></row><row><cell cols="2">Testing</cell><cell>without noise</cell><cell cols="2">with noise</cell><cell cols="2">without noise</cell><cell>with noise</cell></row><row><cell cols="2">Metric</cell><cell>MSE MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE MAE</cell></row><row><cell>ET T h1</cell><cell cols="7">96 0.371 0.394 -1.6% -2.0% -0.0% -0.0% -1.6% -2.0% 192 0.414 0.423 -0.5% -1.4% +0.5% -0.0% -0.5% -1.4% 336 0.442 0.445 -1.8% -0.9% -0.9% +1.3% -3.2% -1.6% 720 0.465 0.472 +0.2% -0.2% +0.9% +0.9% -0.6% -0.4%</cell></row><row><cell cols="6">H Distribution Analysis of Forecasting Output</cell><cell></cell></row><row><cell cols="3">H.1 Kolmogorov-Smirnov Test</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 15 :</head><label>15</label><figDesc>P-values of Kolmogrov-Smirnov test of different Transformer models for long-term forecasting output on ETTm1 and ETTm2 dataset. Larger value indicates the hypothesis (the input sequence and forecasting output come from the same distribution) is less likely to be rejected. The largest results are highlighted.</figDesc><table><row><cell cols="6">Methods Transformer Informer Autoformer FEDformer</cell><cell>FiLM</cell><cell>True</cell></row><row><cell>ETTm1</cell><cell>96 192 336 720</cell><cell>0.0090 0.0052 0.0022 0.0023</cell><cell>0.0055 0.0029 0.0019 0.0016</cell><cell>0.020 0.015 0.012 0.008</cell><cell>0.048 0.028 0.015 0.014</cell><cell cols="2">0.016 0.0123 0.013 0.023 0.0046 0.010 0.0024 0.004</cell></row><row><cell>ETTm2</cell><cell>96 192 336 720</cell><cell>0.0012 0.0011 0.0005 0.0008</cell><cell>0.0008 0.0006 0.00009 0.0002</cell><cell>0.079 0.047 0.027 0.023</cell><cell>0.071 0.045 0.028 0.021</cell><cell cols="2">0.022 0.020 0.012 0.0081 0.023 0.087 0.060 0.042</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Parameter size of baseline models and FiLM with different low-rank approximations: the models are trained and tested on ETT dataset; the subscript number denotes k in low-rank approximation.</figDesc><table><row><cell cols="3">Methods</cell><cell cols="7">Transformer Autoformer FEDformer FiLM FiLM16 FiLM4</cell><cell>FiLM1</cell></row><row><cell cols="3">Parameter(M)</cell><cell>0.0069</cell><cell>0.0069</cell><cell cols="2">0.0098</cell><cell>1.50</cell><cell>0.0293</cell><cell cols="2">0.0062 0.00149</cell></row><row><cell cols="6">J Training Speed and Memory Usage</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Memory Usage (GB)</cell><cell>2 4 6 8 10 12 14</cell><cell cols="2">FiLM FEDformer-f Autoformer Informer Transformer</cell><cell></cell><cell>Running Time per Iteration (MS)</cell><cell>16 2 4 6 8 10 12 14</cell><cell>FiLM FEDformer-f Autoformer Informer Transformer</cell><cell></cell><cell></cell></row><row><cell></cell><cell>768</cell><cell>1536</cell><cell>3072</cell><cell>4608</cell><cell>7680</cell><cell>768</cell><cell>1536</cell><cell>3072</cell><cell>4608</cell><cell>7680</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Output Length</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Output Length</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 :</head><label>17</label><figDesc>multivariate long-term series forecasting results on six datasets with various input length and prediction length O ? {96, 192, 336, 720} (For ILI dataset, we set prediction length O ? {24, 36, 48, 60}). Supplementary results of non-Transformer baselines (N-Hits and a seasonal-naive model).</figDesc><table><row><cell>Methods</cell><cell>FiLM</cell><cell>N-Hits</cell><cell>FEDformer</cell><cell>Autoformer</cell><cell>Seasonal-naive</cell></row><row><cell>Metric</cell><cell cols="5">MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE</cell></row></table><note>K.2 Ablation univariate forecasting experiments for FEL layers with all six datasets</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 18 :</head><label>18</label><figDesc>: (Full Benchmark)Ablation studies of FEL layer. The FEL layer is replaced with 4 different variants: MLP, LSTM, CNN, and Transformer.The experiments are performed on ETTm1 and Electricity. The metric of variants is presented in relative value ('+' indicates degraded performance, '-' indicates improved performance).</figDesc><table><row><cell cols="2">Methods</cell><cell cols="2">FilM</cell><cell cols="2">LPU+MLP</cell><cell cols="2">LPU+LSTM</cell><cell cols="2">LPU+CNN</cell><cell cols="2">LPU+attention</cell></row><row><cell cols="2">Metric</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell></row><row><cell>ETTm1</cell><cell cols="11">96 192 0.041 0.153 -1.5% 0.029 0.127 +0.0% +0.0% +12.1% +7.1% +13.5% +9.5% -0.6% +12.2% +8.5% +10.8% +7.8% 336 0.053 0.175 -1.7% -1.7% +4.5% +4.0% +10.8% +6.3% 720 0.071 0.205 -0.9% -1.0% +5.5% +3.4% +8.6% +4.9% +13.8% +7.8% +1.7% +1.6% +2.0% +3.3% +4.5% +2.9%</cell></row><row><cell>Electricity</cell><cell cols="11">96 192 0.166 0.258 +59% 0.154 0.247 +155% +81% +160% +84% +330% +155% +242% +119% +39% +121% +67% +224% +117% +264% +131% 336 0.188 0.283 +55% +35% +150% +74% +128% +71% +183% +95% 720 0.249 0.341 +33% +25% +154% +73% +192% +95% +312% +138%</cell></row><row><cell>Exchange</cell><cell cols="5">96 192 0.207 0.352 +7.2% +0.0% 0.110 0.247 -13% -12% 336 0.327 0.461 +48% +13% 720 0.811 0.708 +29% +14%</cell><cell>+51% +69% +62% +24%</cell><cell>+17% +32% +20% +9.6%</cell><cell>+4.6% +29% +68% +38%</cell><cell>-1.2% +12% +24% +12%</cell><cell>-4.6% +22% +72% +64%</cell><cell>-5.8% +11% +23% +27%</cell></row><row><cell>Traffic</cell><cell cols="5">96 192 0.120 0.199 +17% +7.5% 0.144 0.215 +69% +47% 336 0.128 0.212 +6.2% +7.6%</cell><cell>+13% +31% +16%</cell><cell cols="5">+15% +300% +176% +271% +161% +24% +258% +149% +1572% +355% +15% +151% +102% +1514% +368%</cell></row><row><cell></cell><cell cols="4">720 0.153 0.252 +38%</cell><cell>+28%</cell><cell>+11%</cell><cell cols="5">+7.9% +250% +126% +1048% +349%</cell></row><row><cell>Weather</cell><cell cols="5">96 0.0012 0.026 +17% +6.2% 192 0.0014 0.029 -1.4% -2.4% 336 0.0015 0.030 +0.0% -0.6% 720 0.0022 0.037 +4.6% -0.3%</cell><cell cols="3">+16% +5.0% +1.7% +6.9% +3.3% +1.3% +2.0% +19% 0.7% +4.1% -1.6% 3.6%</cell><cell>+8.1% -0.7% +0.0% 0.0%</cell><cell>+21% +4.3% +3.3% +0.0%</cell><cell>+8.9% +1.4% +1.3% -3.8%</cell></row><row><cell></cell><cell>96</cell><cell cols="3">0.629 0.538 +51%</cell><cell>+45%</cell><cell>-2.5%</cell><cell>9.5%</cell><cell>+20%</cell><cell>+29%</cell><cell>+112%</cell><cell>+59%</cell></row><row><cell>ILI</cell><cell cols="4">192 0.444 0.481 +99% 336 0.557 0.584 +33%</cell><cell>+58% +31%</cell><cell>+25% +21%</cell><cell>+24% +16%</cell><cell>+84% +58%</cell><cell>+56% +30%</cell><cell cols="2">+360% +142% +702% +94%</cell></row><row><cell></cell><cell cols="5">720 0.641 0.644 +8.4% +5.4%</cell><cell>+23%</cell><cell>+18%</cell><cell>+42%</cell><cell>+22%</cell><cell>+74%</cell><cell>+34%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 21 :</head><label>21</label><figDesc>Mode selection policy study for frequency enhanced layer. Lowest: select the lowest m frequency mode; Random: select m random frequency mode; Low random: select the 0.8 * m lowest frequency mode and 0.2 * m random high frequency mode.</figDesc><table><row><cell cols="2">Policy</cell><cell cols="2">Lowest</cell><cell cols="2">Random</cell><cell cols="2">Low random</cell></row><row><cell cols="2">Metric</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell></row><row><cell>ETTm2</cell><cell cols="4">96 192 0.094 0.233 0.096 0.065 0.189 0.066 336 0.124 0.274 0.125 720 0.173 0.323 0.173</cell><cell>0.189 0.235 0.270 0.322</cell><cell cols="2">0.066 0.190 0.096 0.235 0.128 0.275 0.173 0.323</cell></row><row><cell>Electricity</cell><cell cols="7">96 192 0.166 0.258 0.177 0.154 0.247 0.175 0.0.260 0.176 0.262 0.266 0.168 0.273 336 0.188 0.283 0.199 0.289 0.192 0.299 720 0.249 0.341 0.269 0.364 0.270 0.362</cell></row><row><cell>Exchange</cell><cell cols="4">96 192 0.207 0.352 0.196 0.11 0.259 0.110 336 0.327 0.461 0.451 720 0.811 0.708 0.835</cell><cell>0.256 0.351 0.522 0.714</cell><cell cols="2">0.106 0.249 0.207 0.357 0.373 0.484 0.604 0.628</cell></row><row><cell>Traffic</cell><cell cols="4">96 192 336 0.128 0.212 0.122 0.144 0.215 0.145 0.12 0.199 0.119</cell><cell>0.216 0.198 0.207</cell><cell cols="2">0.145 0.217 0.118 0.197 0.122 0.209</cell></row><row><cell></cell><cell cols="4">720 0.153 0.252 0.142</cell><cell>0.238</cell><cell cols="2">0.155 0.259</cell></row><row><cell>Weather</cell><cell cols="4">96 0.0012 0.026 0.0012 192 0.0014 0.029 0.0014 336 0.0015 0.03 0.0015 720 0.0022 0.037 0.0023</cell><cell>0.027 0.029 0.030 0.037</cell><cell cols="2">0.0012 0.026 0.0014 0.029 0.0015 0.030 0.0023 0.037</cell></row><row><cell></cell><cell>96</cell><cell cols="3">0.629 0.538 0.639</cell><cell>0.542</cell><cell cols="2">0.626 0.537</cell></row><row><cell>ILI</cell><cell cols="4">192 0.444 0.481 0.448 336 0.557 0.584 0.560</cell><cell>0.490 0.590</cell><cell cols="2">0.447 0.494 0.557 0.587</cell></row><row><cell></cell><cell cols="4">720 0.641 0.644 0.641</cell><cell>0.647</cell><cell cols="2">0.643 0.650</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 19 :</head><label>19</label><figDesc>(Full Benchmark) Boosting effect of LPU layer for common deep learning backbones: MLP, LSTM, CNN and Attention.'+' indicates degraded performance.</figDesc><table><row><cell cols="2">Methods</cell><cell></cell><cell>FEL</cell><cell cols="2">MLP</cell><cell cols="2">LSTM</cell><cell cols="2">lagged-LSTM</cell><cell cols="2">CNN</cell><cell cols="2">Attention</cell></row><row><cell cols="2">Compare</cell><cell>LPU</cell><cell>Linear</cell><cell>LPU</cell><cell>Linear</cell><cell>LPU</cell><cell>Linear</cell><cell>LPU</cell><cell>Linear</cell><cell>LPU</cell><cell>Linear</cell><cell>LPU</cell><cell>Linear</cell></row><row><cell>ETTm1</cell><cell cols="2">96 192 0.047 0.030 336 0.063 720 0.081</cell><cell cols="3">+38% 0.034 +8.0% +9.5% 0.049 +30% +5.8% 0.061 +64% +1.4% 0.082 +62%</cell><cell>0.049 0.174 0.119 0.184</cell><cell>+73% +32% +84% +32%</cell><cell>0.093 0.331 0.214 0.303</cell><cell>-21% -48% -19% -6.5%</cell><cell>0.116 0.101 0.122 0.108</cell><cell>-50% +20% +25% +13%</cell><cell>0.243 0.387 1.652 4.782</cell><cell>-81% -86% +12% -61%</cell></row><row><cell>Electricity</cell><cell cols="6">96 192 0.268 0.213 +136% 0.431 +121% 0.291 +32% 0.291 +239% 0.353 336 0.307 +0.1% 0.296 +235% 0.436 720 0.321 +37% 0.339 +196% 0.636</cell><cell>+56% +17% -6.7% -11%</cell><cell>0.739 0.535 0.517 0.492</cell><cell>-33% +15% +23% +28%</cell><cell>0.310 0.380 0.359 0.424</cell><cell>+43% +12% +29% +18%</cell><cell>0.805 0.938 2.043 9.115</cell><cell>+23% +14% -54% +298%</cell></row><row><cell>Exchange</cell><cell cols="2">96 192 0.205 0.130 336 0.467 720 1.003</cell><cell cols="2">+7.5% 0.110 +39% 0.257 +9.2% 0.461 +26% 1.981</cell><cell>-18% -36% -33% -61%</cell><cell cols="3">0.224 +6.0% 0.521 0.787 -35% 1.742 0.964 +24% 2.281 2.703 -29% 1.457</cell><cell>-58% -66% -38% +34%</cell><cell>0.244 0.630 3.231 5.531</cell><cell>-18% +2.1% -85% +9.7%</cell><cell cols="2">0.338 0.930 1.067 0.631 +1831% +872% +278% +69%</cell></row><row><cell>T raf f ic</cell><cell cols="2">96 192 0.141 0.312 336 0.143 720 0.215</cell><cell cols="6">+18% 0.376 +277% 0.215 +1.2% 0.216 +9.6% 0.199 +598% 0.177 +19% 0.186 +2.5% 0.195 +613% 0.192 +19% 0.190 +30% 0.240 +475% 0.234 -1.7% 0.250</cell><cell>+10% +17% +11% +15%</cell><cell>0.543 0.451 0.346 0.348</cell><cell>-33% +9.0% +44% +47%</cell><cell>0.429 0.476 0.377 0.773</cell><cell>+210% +176% +260% +171%</cell></row><row><cell>W eather</cell><cell cols="12">96 0.0073 192 0.0106 336 0.0079 720 0.0063 +0.4% 0.006 +7.6% 0.0062 +5.3% 0.103 -38% 0.006 -33% 0.006 -23% 0.0070 -17% 0.0022 +167% 0.0065 -64% 0.007 -14% 0.0074 -11% 0.0063 -19% 0.007 -24% 0.0075 -37% 0.006 +4.9% 0.0056 +12% 0.0055 +12% 0.0056 +0.5% 0.222 -36% 0.006 +4.2% 0.037</cell><cell>-11% -12% -69% -81%</cell></row><row><cell></cell><cell>24</cell><cell>1.393</cell><cell cols="3">+6.1% 1.220 +36%</cell><cell>2.306</cell><cell>+66%</cell><cell>4.189</cell><cell>-9.2%</cell><cell>2.264</cell><cell>-22%</cell><cell>2.249</cell><cell>+217%</cell></row><row><cell>ILI</cell><cell>36 48</cell><cell>1.242 1.448</cell><cell>-22% -28%</cell><cell cols="2">1.185 +56% 1.079 +79%</cell><cell>2.950 3.385</cell><cell>+44% +38%</cell><cell>2.516 3.501</cell><cell>+42% +16%</cell><cell>1.841 1.654</cell><cell>-3.0% +23%</cell><cell>5.026 2.838</cell><cell>+45% +115%</cell></row><row><cell></cell><cell>60</cell><cell>2.089</cell><cell>-18%</cell><cell cols="2">0.986 +96%</cell><cell>4.031</cell><cell>+18%</cell><cell>4.258</cell><cell>+10%</cell><cell cols="3">1.290 +176% 4.978</cell><cell>+250%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 20 :</head><label>20</label><figDesc>Low-rank Approximation (LRA) univariate forecasting study for frequency enhanced layer: Comp. K=0 means default version without LRA, 1 means the largest compression using K=1.</figDesc><table><row><cell>Comp. K</cell><cell>0</cell><cell></cell><cell>16</cell><cell>4</cell><cell>1</cell></row><row><cell>Metric</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell cols="2">MAE MSE MAE MSE MAE</cell></row><row><cell>ET T m2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams 20112014 3 http://pems.dot.ca.gov 4 https://www.bgc-jena.mpg.de/wetter/ 5 https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some recent advances in forecasting and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E P</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Distribution of residual autocorrelations in autoregressive-integrated moving average time series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pierce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<publisher>Taylor &amp; Francis</publisher>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1509" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Challu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Olivares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mergenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubrawski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12886</idno>
		<title level="m">Neural hierarchical interpolation for time series forecasting</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1474" to="1487" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00396</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state space layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiwavelet-based operator learning for differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021-12-06" />
			<biblScope unit="page" from="24048" to="24062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>0899-7667</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1530" to="888" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reversible instance normalization for accurate time-series forecasting against distribution shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>arXiv: 1412.6980</idno>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fnet</surname></persName>
		</author>
		<idno>abs/2105.03824</idno>
		<title level="m">Mixing tokens with fourier transforms. CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multilevel wavelet decomposition network for interpretable time series analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2437" to="2446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07125</idno>
		<title level="m">Transformers in time series: A survey</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoformer: Decomposition transformers with autocorrelation for long-term series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning long term dependencies via fourier recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
		<meeting>the 35th International Conference on Machine Learning (ICML)<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5810" to="5818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Conference</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11106" to="11115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">39th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
