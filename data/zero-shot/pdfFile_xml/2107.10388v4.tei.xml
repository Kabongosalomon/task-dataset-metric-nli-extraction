<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">JS Fake Chorales: a Synthetic Dataset of Polyphonic Music with Human Annotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">A</forename><surname>Peracha</surname></persName>
							<email>omar@humtap.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Humtap, Inc. London</orgName>
								<address>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">JS Fake Chorales: a Synthetic Dataset of Polyphonic Music with Human Annotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-quality datasets for learning-based modelling of polyphonic symbolic music remain less readily-accessible at scale than in other domains, such as language modelling or image classification. Deep learning algorithms show great potential for enabling the widespread use of interactive music generation technology in consumer applications, but the lack of large-scale datasets remains a bottleneck for the development of algorithms that can consistently generate high-quality outputs. We propose that models with narrow expertise can serve as a source of high-quality scalable synthetic data, and open-source the JS Fake Chorales, a dataset of 500 pieces generated by a new learning-based algorithm, provided in MIDI form. We take consecutive outputs from the algorithm and avoid cherry-picking in order to validate the potential to further scale this dataset on-demand. We conduct an online experiment for human evaluation, designed to be as fair to the listener as possible, and find that respondents were on average only 7% better than random guessing at distinguishing JS Fake Chorales from real chorales composed by JS Bach. Furthermore, we make anonymised data collected from experiments available along with the MIDI samples. Finally, we conduct ablation studies to demonstrate the effectiveness of using the synthetic pieces for research in polyphonic music modelling, and find that we can improve on state-of-the-art validation set loss for the canonical JSB Chorales dataset, using a known algorithm, by simply augmenting the training set with the JS Fake Chorales.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representation learning for application to music has become an increasingly active field of research in recent years, as <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates. Despite growth in the domain, certain bottlenecks have persisted which hinder the rate of breakthroughs, particularly regarding problems related to symbolic music. The most obvious of these concerns the volume of high-quality datasets available for researchers to work with cheaply and with low effort; datasets comprising millions of samples <ref type="bibr" target="#b0">[1]</ref> or even tens of billions of canonical input/output pairs <ref type="bibr" target="#b1">[2]</ref> have been available for many years in the fields of computer vision and natural language processing. Some of these are even licensed with sufficient permissiveness as to allow commercial use of any technology whose development relied upon said data <ref type="bibr" target="#b2">[3]</ref>, a factor which plays an important role in determining the ultimate viability of deploying these algorithms into the real world where they may generate value for businesses and customers alike, naturally promoting further investment. Unsurprisingly, the aforementioned two domains continue to see incredible progress in research and increasingly wide adoption of the resulting technology in consumer-facing products.</p><p>A second bottleneck more pertinent to generative music modelling is the lack of automated methods of assessing outputs which strongly correspond to good qualitative performance. This difficulty in measuring success naturally leads to slower progress. Few alternatives have been proposed to human evaluation, which is typically slow or costly, for assessing the quality of algorithmicallygenerated music. The process of acquiring human feedback itself tends to vary in implementation details between different published works <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7]</ref>, which further convolutes the process of reliably comparing results in the literature.</p><p>We propose a dataset of synthetic polyphonic music, generated by our neural network-based KS_Chorus algorithm, in symbolic form, and aim to validate both its usefulness for aiding in symbolic music modelling tasks and its ability to scale on demand. We achieve the latter by building the dataset from 500 consecutive generated outputs of KS_Chorus, with zero cherry-picking of samples, and performing an experiment designed to solicit human evaluation in a context entirely free from curation, which may otherwise serve to skew results in the algorithm's favour. 500 samples is small enough that conducting such an experiment remains fairly manageable, but is large enough relative to the set of 382 chorales by Johann Sebastian Bach <ref type="bibr" target="#b6">[8]</ref>, whose style the JS Fake Chorales seek to imitate, that the results may be deemed significant. After 6,810 responses collected from an estimated 446 unique respondents, our experiment shows that human listeners are only 7.3% better than a coin flip at telling apart our synthesised samples from chorales composed by Bach himself. We facilitate scalability by making a web app freely available for researchers to generate new samples from KS_Chorus, 1 enabling them to grow the dataset, which is further described in Section 3.3.</p><p>We then conduct experiments using the JS Fake Chorales as training data or auxiliary data on a common benchmark task, namely modelling the canonical JS Bach Chorales dataset. Using the current state-of-the-art model, TonicNet <ref type="bibr" target="#b7">[9]</ref>, without any tuning of parameters, we show that training purely on JS Fake Chorales achieves performance close to state-of-the-art on the JS Bach Chorales in terms of validation set loss, even without training on any pieces from the actual JS Bach training set. By combining the JS Bach Chorales training set with the JS Fake Chorales, using the latter as a form of dataset augmentation, we can improve on state-of-the-art results. We make all code to run our ablation studies publicly available, for reproducibility. <ref type="bibr" target="#b1">2</ref> In evidencing the scalability of the JS Fake Chorales and their usefulness in common music modelling tasks, we hope to highlight the possibility for synthetic data to help alleviate the current bottleneck of low data volume in this domain. Finally, to help address the bottleneck of weak methods for measuring qualitative performance, we make anonymised metadata for each of the 6,810 human responses available as part of the dataset, <ref type="bibr" target="#b2">3</ref> which includes information about the evaluator's level of music education and which specific pieces were guessed correctly or incorrectly by which evaluator, among other data further detailed in Section 3. Future work may see attempts at modelling these data as a means to automatically infer qualities which make music more readily-perceived as human-composed.</p><p>Ours is the first dataset in this domain that provides such data for such a large number of samples without cherry-picking, to our knowledge. Similarly, Section 4.1 shows that KS_Chorus is the first deep learning algorithm proven to consistently approach, though not yet achieve, a productionlevel consistency of high-quality unconditioned outputs in Bach's, or indeed any clearly-defined <ref type="bibr" target="#b0">1</ref> Web app to generate new samples is available at https://omarperacha.github.io/make-js-fake/ 2 Code to reproduce results from experiments is available at https://github.com/omarperacha/ TonicNet 3 Dataset and usage information are available at https://github.com/omarperacha/js-fakes polyphonic, style. The value of this is that the generated samples can already be very useful as auxiliary training data in downstream tasks, as shown in Section 4.2.</p><p>In a data-starved domain, models with narrower focus are able to squeeze performance on specific genres from the lack of a requirement to generalise to other styles, for example by making stylespecific adjustments in the representation. As a concrete example, KS_Chorus does not need to handle individual voices capable of producing more than one note simultaneously, allowing for a more efficient representation of Bach's chorales than cross-genre polyphonic music models, such as MuseNet <ref type="bibr" target="#b16">[18]</ref>. We propose that narrow-focussed models, by generating high quality synthetic data in a single or small number of styles, may play a role in significantly increasing the availability of training data, thus empowering future generative algorithms to generalise better across styles while improving on both quality and consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Synthetic datasets in the music domain have so far seen wider use in audio contexts rather than symbolic music. For example, both <ref type="bibr" target="#b10">[12]</ref> and <ref type="bibr" target="#b11">[13]</ref> trained algorithms on synthetic data to learn how to accurately detect the fundamental frequency of monophonic musical material. While algorithms which process audio have seen more benefit from synthetic data up until now, examples do exist of such data playing a role in symbolic music contexts; in <ref type="bibr" target="#b12">[14]</ref>, extra training data is synthesised by performing a conditional nonlinear transform on the original training corpus, in a naive 1-dimensional token embedding space. This data is used to augment the training set and is fed to the generator and discriminator in a generative adversarial learning context, with an input label corresponding to the intensity of the transform performed on the original data sample so as to allow both networks to learn features expressed in the extra data which are also present in the original data. It is worth noting that the intent in this case was not to create perfect stylistic imitations of the original dataset, but to introduce novelties in perceivably-systematic ways. Nonetheless, <ref type="bibr" target="#b12">[14]</ref> serves as a precursor for using synthetic data to improve the quality of generated symbolic music.</p><p>With 10,854 complete pieces of solo piano music in MIDI form, <ref type="bibr" target="#b13">[15]</ref> is among the larger symbolic music datasets available. It could be considered a synthetic dataset in that the authors used an automated method of transcription <ref type="bibr" target="#b14">[16]</ref> to convert over 1000 hours of audio recordings into a symbolic representation. The authors report validation onset and offset F1 score of 0.825, and a score of 0.967 for onsets alone. These are impressive results in the current landscape, yet clearly this may translate to many transcription errors across a dataset comprising millions of note events. The severity of these inconsistencies with respect to the stylistic integrity of the original human-composed piece can be hard to measure, and further work is required to determine the areas where this dataset will ultimately enable progress in the field.</p><p>Both <ref type="bibr" target="#b3">[5]</ref> and <ref type="bibr" target="#b4">[6]</ref> propose algorithms intended to generate stylistic imitations of Bach's chorales, and use human evaluation to support claims of the effectiveness of their solutions. The experiment conducted by <ref type="bibr" target="#b3">[5]</ref> first takes 50 pieces from the validation set of the Bach chorales and uses their DeepBach algorithm to reharmonise them. They repeat the process with other baseline algorithms. They then take 12-second extracts of each and play these samples one-by-one, asking listeners to determine if they think each is by Bach or is a generated piece. In contrast, we generate 500 pieces entirely from scratch with no conditioning and always play the entire piece to listeners, which can be over one minute in length. We also make a ground truth chorale by Bach available to the listener to play back at any time alongside the sample currently being evaluated.</p><p>The experiment conducted by <ref type="bibr" target="#b4">[6]</ref> is similar to ours in that it allows to listen to two samples simultaneously. Unlike ours, however, the listener must simply pick which of the two samples they feel is more Bach-like. In our case, the evaluator knows that one sample is definitely composed by Bach, and must decide if the second sample is also by Bach or if it is generated by KS_Chorus. A second significant difference is that <ref type="bibr" target="#b4">[6]</ref> only use a set of 12 compositions fully-generated by their BachBot algorithm in their experiments. This is too small a sample to substantiate the model's general performance. In our case, we generate 500 samples in a row and include all of these in the human evaluation experiment without cherry-picking. Neither <ref type="bibr" target="#b3">[5]</ref> nor <ref type="bibr" target="#b4">[6]</ref> make a substantial dataset of generated compositions available, while we make these initial 500 available as the JS Fake Chorales dataset, and offer evidence that the dataset can be further scaled while maintaining the same quality consistently as a result of avoiding curation during human evaluation.</p><p>Perhaps the most relevant work to ours is the Bach Doodle Dataset <ref type="bibr" target="#b15">[17]</ref>, obtained by allowing users to interact with the CoCoNet algorithm <ref type="bibr" target="#b5">[7]</ref>; the user could enter a melody via a web interface, and the algorithm would harmonise this melody in the style of Bach. Comprising 21.6 million samples, the size far exceeds any that is commonly-used in symbolic music, to our knowledge. The web interface also collects a user rating on a three-point scale to reflect their satisfaction with the output. This rating can not be said to reflect stylistic consistency with any intended target, however, but simply provides a general measure of the user's experience. This rating is provided as part of the dataset, along with more metadata about the user's session. Because the melodies are user-entered, only constrained to fit within a selected key and within a 2-bar duration, the quality of the final samples in the Bach Doodle Dataset will be inherently unpredictable. Furthermore, all samples are fixed to 2 bars in duration, and melodies are limited to quaver resolution. Our samples consist only of pieces in their entirety, with a median bar duration of 11.375 and maximum of 35.5, and rhythmic resolution can fully capture that seen in the original Bach chorales.</p><p>Several style-agnostic polyphonic models have been proposed in recent years. Three examples notable for being particularly high-performing are MuseNet <ref type="bibr" target="#b16">[18]</ref>, MMM <ref type="bibr" target="#b17">[19]</ref> and MusicBERT <ref type="bibr" target="#b18">[20]</ref>. While MMM and MusicBERT are highly promising, MuseNet is the only one of these to be evaluated on full song generation. This evaluation is not performed quantitatively, and provided samples are cherry-picked, though an interface for novel song generation is available. It is likely that all three models have been trained on the Bach Chorales, though not known for certain as not all the datasets are public. In any case, none of these models are able to demonstrate the capability to generate a dataset with quality matching the JS Fake Chorales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Synthesising 4-part Chorales</head><p>To create the JS Fake Chorales dataset, we first develop a learning-based sequence modelling algorithm, KS_Chorus, and train it on the Bach chorales as made available by the music21 toolkit <ref type="bibr" target="#b19">[21]</ref>. Bach's chorales are a good choice as the basis for a synthetic dataset mainly because they are widely-cited in the literature regarding algorithmic modelling of polyphonic music <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b7">9]</ref>, and so downstream quantitative analysis against good benchmarks is readily possible.</p><p>KS_Chorus is a generative algorithm for polyphonic music of any number of instruments, where each instrument is itself monophonic. It consists of an ensemble of deep RNN-based networks with some domain-specific architectural additions, totalling roughly 130 million parameters, and a sampling routine idiosyncratic both to this architecture and the data representation utilised. We do not detail the specifics of the architecture, input representation or training process here, and instead leave this to a separate work for the future.</p><p>Once trained, we then sample 500 compositions from the model consecutively and fully unconditionally, i.e using absolutely no priming or similar input to specify parameters such as the length of the compositions. The only exception to this relates to metre; while a small handful of pieces exist in the original training corpus which are not in 4/4 time, we found during development that KS_Chorus falls somewhat below par when generating compositions in other time signatures. We therefore manually restrict all generated samples to be in 4/4, because the likelihood of being correctly identified as algorithmically-composed seemed to be consistently higher in other time signatures.</p><p>Each generated composition is compared to each of the 344 Bach chorales in the training set seen by the model during development to ensure originality; in particular, the edit distance to every single sample in the training corpus must be greater than 50%, measured separately across both the entire piece and across the opening few bars. An edit distance below 50% in either case would invalidate the sample and trigger the algorithm to run once more. Furthermore, all samples in the original dataset were transposed as far as possible in either direction while maintaining a singable range for each voice, and the edit distance was similarly validated to these transpositions. A single composition takes roughly 10 minutes on average to generate when running on CPU. The algorithm was implemented and trained using the PyTorch library <ref type="bibr" target="#b20">[22]</ref>, and sampling was also performed in Python.</p><p>While we did not measure the edit distance of generated pieces to all other generated pieces at sample time, we perform post-hoc analysis of this metric. Since we do not detail the representation used by KS_Chorus in this work, for the sake of reproducibility we perform these comparisons using a standard representation seen in the literature <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b21">23]</ref>; the piece is split into 16th-note time-steps and represented as a matrix x ? Z 4?T , where T is the number of time-steps in that sample, the four rows represent the four different voice parts and the value of each element is the pitch observed in the respective voice at the respective time-step. This matrix is typically converted into a linear sequence before being input to a learning algorithm, by end-concatenating the columns. We also make the JS Fake Chorales available in this representation.</p><p>Using the above representation, we compare the intra-set edit distance (i.e. comparing each sample with every sample from the same dataset) for both the Bach chorales and the JS Fake Chorales, and we repeat the comparison of each JS Fake chorale with each Bach chorale. <ref type="table" target="#tab_0">Table 1</ref> shows the results of taking the smallest edit distance from each sample in dataset 1 to each sample in dataset 2 for the three different dataset configurations, and computing the mean of these smallest distances. We also provide the minimum in each case. We can see that there is in fact more intra-set similarity among the Bach chorales than among the JS Fakes. Of course, the edit distance between the two was enforced at generation time, hence the distribution of inter-set least edit distances trending much higher than that of either intra-set least edit distances. We can deduce from this post-hoc analysis that 50% was perhaps an unnecessarily strict threshold for edit distance during the process of sampling from KS_Chorus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Human Annotation</head><p>We conduct a human evaluation experiment hosted on the internet to investigate how these generated compositions are perceived in comparison to Bach's chorales. We first present the experiment design in this section and describe the data obtained and provided as part of the JS Fake Chorales dataset, while the actual results of this experiment will be discussed in Section 4. We use G to notate the set of 500 pieces which comprise the JS Fake Chorales dataset, and C to refer to the training corpus of 344 chorales by J.S. Bach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Listening Experiment</head><p>Participants are first prompted to self-report their highest level of musical education, selecting from a set of six coarse pre-defined options: 0, no musical education; 1, some musical education, but no formal qualifications; 2, high School level qualifications directly related to music; 3, undergraduate degree directly related to music; 4, postgraduate degree directly related to music; and 5, postgraduate degree specialising in the music of J.S. Bach.</p><p>Once they have confirmed their choice, they are taken to a page with instructions describing how to complete the experiment. On this page, the participants are provided with a reference chorale by Bach to listen to, chosen randomly per session from the 344 pieces [C 1 , ..., C 344 ] ? C. Participants are informed that the given chorale is a genuine example of Bach's music, and they must play it through to the end in order to advance further. Upon doing so, a second piece will appear, which is selected at random from the combined set of all pieces G ? C (excluding whichever Bach chorale was used as the reference), with equal probability of any sample in this set being selected.</p><p>Participants are not informed whether this second piece is composed by Bach or KS_Chorus, but are tasked to determine this. Once more they can not proceed without listening to the given sample in its entirety, at which point a menu will appear through which they can provide their response and submit. All pieces are rendered using the same piano synthesiser, and can be played as many times as the participant wants before submitting a response, including the reference track. KS_Chorus does not currently include tempo data as part of the representation it learns to model, therefore all piece are played back at a fixed tempo of 120bpm.</p><p>Upon submitting a response, participants are taken to a new page where they are immediately shown the correct answer, and given the option to try another round. They are also shown a running score which increments by 1 for each correct response in the session, which carries over if they choose to try a new sample. Should they choose to do so, they will be taken back to the previous page where the same reference piece will be available for them to play back as desired, and a new sample will be loaded for the participant to identify as belonging to G or C, ensuring this new sample has not previously been seen in the current session. In this case, participants are not forced to replay the reference track, but must once more listen through the new sample entirely before proceeding.</p><p>Responses were solicited both organically via social media, and through Amazon MTurk <ref type="bibr" target="#b22">[24]</ref>. In the latter case, participants were required to obtain a score of 10 in order to fulfil the conditions for payment, and were given a 30 minute limit to do so. This was done to motivate participants to try their utmost to provide correct responses; mandating that the sample be played through entirely for each round vastly increases the time cost for every wrong answer, meaning that random guessing is not an effective strategy for MTurk respondents to earn the available reward. The mean time taken by MTurk workers to complete the task was 19m45s, and we estimate these individuals make up roughly 66% of all unique respondents. Overall we received 6,810 responses from 446 unique IP address hashes. Responses were collected throughout the month of February 2021, and the website remains live for anyone to interact with, 4 while data is no longer being collected from sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Metadata</head><p>Besides a simple binary value denoting the accuracy of a participant's response, we collect several other data points designed to offer insight on how complex a given sample was to identify by the participant. Examples include how many times the sample was played before submitting a response and how long the participant took to submit once they had heard the sample for the first time. We aggregate these and make them available as part of the dataset in the hope that this information might provide value to researchers.</p><p>For each generated chorale G i ? G, we make four additional pieces of metadata available: (1) Total responses concerning sample G i ; (2) responses which correctly identified G i as composed by an algorithm; (3) mean number of times the sample was played before a response was submitted; and (4) mean time in seconds between hearing the sample and submitting a response. We provide these data separately for each skill level, to provide further granularity on how the samples were perceived by different demographics. This allows such custom analysis as examining which pieces were likely to be misidentified by people with university degrees in music, and which pieces did people with limited musical education find simplest to correctly identify. We also provide the data aggregated over all skill levels. Further usage explanation is provided in the documentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Web Application for Generating New JS Fake Chorales</head><p>We make a web application freely available for anyone to sample new chorales with KS_Chorus.</p><p>The application consists of a single page, including text and a button to trigger sample generation.</p><p>Once the sampling process is complete, the button is replaced with a MIDI player where the user can preview the newly-generated sample, rendered with a piano synthesiser. The MIDI file will also be automatically downloaded to the user's local hard drive from their browser at that time. The main JS Fake Chorales repository will be periodically updated to include all samples generated from the web app, and users are informed of this. No user data of any kind is captured from this interface, besides the generated sample itself. While we commit to the persistency of the JS Fake Chorales dataset, and all samples generated from this web app in future included therein, we may in due course choose to discontinue the availability of this app as we see fit, for example if the cost of upkeep is deemed no longer sustainable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Sample Category Identification Task</head><p>We compare the likelihood of participants determining a sample to be human-composed, and present results across skill levels for samples by both Bach and KS_Chorus. Given an ideal model whose distribution of generated compositions G exactly equals the distribution of training samples C, we would expect the likelihood of any G i ? G and C i ? C being deemed as human-composed to also be equal. Given a fair experiment which does not incur bias, for example by disproportionately highlighting certain samples of Bach's pieces or suppressing specific qualities present in C from being observed by the evaluators, this expected likelihood should be 50%. <ref type="table" target="#tab_1">Table 2</ref> shows the proportion of all responses which correctly identified samples from G and C, broken down by skill level. We see that the total rate of correct responses for C is 47.6%, while for G it is 57.3%. These values, though unequal, are close to each other, and in particular they suggest evaluators did not differ greatly from random chance regarding the likelihood of a correct guess for either corpus. This is despite receiving instant feedback after each round and a reference C i ? C being made accessible at all times while prompting for a response, which should both aid in increasing the participants' success rate.</p><p>We perform Fisher's exact test on a contingency table consisting of rows C, G and columns voted Bach, voted A.I. across all skill levels to interpret whether samples from G did indeed have a similar likelihood to those from C of being considered human-composed. We test each skill group separately, the combination of all skill groups together, and two subgroups dividing the respondents into those who self-reported holding a formal qualification in music, and those who did not. We also compute the F1 score for each group as a measure of overall performance on the classification task. We report these results in <ref type="table" target="#tab_2">Table 3</ref>, and see that at the 99.9% critical value we can accept the null hypothesis that C and G are equally likely to be voted as being written by Bach, for every individual skill group and for the two larger subgroups. However, the value of p varies greatly across these groups, and for the overall sample comprising all responses p = 6.432e ?5 , so we must reject the null hypothesis. Therefore while at a high level the JS Fakes are certainly difficult to distinguish from Bach Chorales, performing further tests makes it clear there is room for improvement in the quality of the generated samples. Closing this gap is precisely an example of an application which this dataset might serve in future.</p><p>We can also see from <ref type="table" target="#tab_2">Table 3</ref> that there is no apparent trend in F1 score across respondent skill levels, which seems surprising. One obvious candidate for this is the inherent noise and unreliability introduced by enabling respondents to self-report their skill. A stronger method for a future experiment would involve assessing user skill level via some short test. However, the fact that p &gt; 0.001 consistently across all skill groups suggests participants within each group did in fact respond similarly to their peers. Another criticism we can make is that while likelihood of considering a piece from either sample as human-composed was similar, this observed value was not 50%, but 55.7% as measured by weighted average recall. While we have theories for why this may be the case, we must conclude that there are likely some aspects of the experimental design which are not free from introducing small bias. We leave the experiment fully accessible online for transparency and leave it for future work to analyse and improve on the fairness of designing such an experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Ranking</head><p>One possibility offered by the data captured is the ability to rate samples with respect to the likelihood that a listener might perceive the pieces as human-composed. This could prove a useful feature to researchers whose goal is to understand the qualities of music which people tend to perceive as "human", or to implicitly model them as part of a generative system designed to output increasingly higher-quality symbolic music. Moreover, this can be done in a manner stratified by listener music education level, which may provide further insight into which qualities are preferred or more telling for which people. We use the following equation to obtain such a rating R for any given sample S i ? G ? C.</p><formula xml:id="formula_0">R Si = N ?0.5 Si * (? Si + N Si )<label>(1)</label></formula><p>Equation 1 is designed to balance the absolute ratio of responses deeming the sample to belong to C with the number of responses received for that particular sample in total, since this ratio becomes more reliably indicative of quality as the number of responses increases. We use ? Si to denote the number of responses categorising a sample as Bach's music, and N Si for the total number of responses for that sample. We use 0.001 for , which serves to preserve a logical ranking for any samples where ? Si = 0, by giving those which received more negative responses overall a lower score.</p><p>We plot R as histograms for both G and C in <ref type="figure" target="#fig_2">Figure 2</ref>. We can see that samples from C trend towards slightly higher scores than those from G, suggesting that pieces by Bach are indeed still more likely to be perceived as human-composed than those by KS_Chorus. Having said that, the distributions of R for the two sets show strong similarities on aggregate. The maximum R for the generated pieces G is 2.89, while the mean and standard deviation are 1.19 and 0.55 respectively. For C, the maximum, mean and standard deviation are 2.94, 1.33 and 0.57 respectively.</p><p>We perform a Mann-Whitney U test on the R scores for the two sets and find p = 0.0493, which supports our claims that there are similarities between the way the two sets are perceived. Furthermore, we perform the same test on a naive scoring method, for transparency, where the rating is given by the number of "Bach" votes for each sample divided by the total number of votes for that sample. In this case, p = 0.303. We believe this motivates the use of Equation 1 to calculate rating, as it implicitly factors in a form of confidence by multiplying by a term proportional to N Si , offering a more nuanced impression of listener perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithmic Evaluation</head><p>We conduct experiments to ascertain the benefit of using the JS Fakes Dataset to help with music modelling tasks. We use the canonical JSB Chorales dataset to perform ablation studies, and measure performance in terms of negative log likelihood on the validation set, using the same dataset split as <ref type="bibr" target="#b6">[8]</ref>. The task involves element-wise autoregressive modelling of chorales represented in sequence form, as described in Section 3.1. We use the TonicNet_Z algorithm to perform experiments, using the same hyperparameters, training schedule and specific data representation as <ref type="bibr" target="#b7">[9]</ref>. We explore the   effect of training only on the JS Fake Chorales and of combining them with the training set of the JSB Chorales as a form of data augmentation. We show how the JS Fake Chorales impact results both with and without augmentation by transposition, and also investigate the impact of limiting the set of JS Fake Chorales seen by the model to those samples with the highest R ratings. <ref type="table" target="#tab_3">Table 4</ref> shows the results of these experiments, alongside two further strong-performing algorithms.</p><p>In the first column of <ref type="table" target="#tab_3">Table 4</ref>, we denote the combinations and subsets of datasets used to train TonicNet_Z. Bach here refers to the 229 samples in the JSB Chorales training set and JSF refers to the full set of 500 pieces in the JS Fake Chorales datatet. We also use two special subsets of JSF in experiments, namely JSF-top and JSF-top-s; these represent the subset of JSF obtained by ordering the samples by their R scores and taking the top 229. In the case of JSF-top-s, we first consider only responses by participants with a skill level of 3 or higher before calculating R, to perform an initial investigation into whether insights specifically from individuals with higher levels of musical education might impact downstream results.</p><p>The third column shows whether any augmentation techniques were performed on the datasets in question. Trnsps refers to transposing the training set only, as far as possible in both direction while all voices in the sample remain within a singable range for their respective voice type. MM refers to a crude technique of converting pieces in major keys to minor, and vice versa, as described in <ref type="bibr" target="#b7">[9]</ref>, roughly doubling the dataset size. The remaining columns show experimental results on the validation set, where NCL denotes evaluation on the notes only, as is typical in experiments on the JSB Chorales, omitting elements containing chord tokens specifically included as part of the TonicNet representation when averaging the loss for each sample.</p><p>We can see that even without fine-tuning parameters, training the model only on the JS Fake Chorales obtains results close to training on the original set Bach chorales, whether augmented or not. Remarkably, we are even able to perform better than strong baselines <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b21">23]</ref> without using any real training data. Naturally, there are far more samples in JSF than Bach, so we use JSF-top for a more balanced comparison and find that the model trained on this dataset indeed performs worse, but still within a close range compared to training on the canonical dataset. We see that there is no significant difference between using JSF-top and JSF-top-s during training, but that using either in combination with Bach provides better results than augmenting via MM. The dataset size is roughly doubled from the original in all three cases when using JSF-top, JSF-top-s and MM, meaning that the final training set size is close to equal for these cases, in turn suggesting that using the JS Fake Chorales is a higher quality form of augmentation than MM. We are therefore able to set a new state-of-the-art result of 0.208 validation set NLL on the JSB Chorales by combining the original training set with the JS Fake Chorales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We develop an algorithm capable of generating chorales in the style of Bach and sample 500 pieces from it consecutively with no human intervention. We devise an experiment for human evaluation designed to be fair and free from bias, and through this find that these samples are perceived to be composed by Bach almost as readily as pieces which are in fact by Bach. In so doing, we collect data about human interaction with these pieces and create a dataset of 500 synthetic chorales with human annotation. By avoiding cherry-picking when conducting human evaluation, we demonstrate that the number of pieces in the JS Fake Chorales dataset could be readily scaled while expecting quality to be maintained. We further provide a web interface to readily enable such scaling of the dataset.</p><p>We run experiments to demonstrate the effectiveness of this dataset in a common MIR task. We show that training an algorithm on our dataset can be nearly as effective at modelling Bach's music than training on his actual pieces, outperforming several strong baselines. Finally, by combining both the JS Fake Chorales and Bach's chorales into a training corpus, we achieve state-of-the-art results on JSB Chorales dataset.</p><p>In general, the motivation for any field of research is to see it lead to real-world applications where it enables people to do what they previously could not, or to to do things with fewer resources than was previously possible. In order to see widespread adoption of generative music algorithms in consumer applications, they must consistently perform well in at least one relevant task. Such tasks in the symbolic music domain include, but are not limited to, melody generation, melody continuation, accompaniment generation, music in-painting and music style transfer.</p><p>Today, no products which enable the aforementioned tasks could truly be described as being widelyused, with the exception of arpeggiators, a family of rule-based tools which perform a specific and narrow kind of style transfer in the symbolic domain. Ultimately this is due to a lack of algorithms which perform with sufficient consistency and quality to provide meaningful value to potential users. One of the most promising ways to overcome this is to break the two bottlenecks mentioned in Section 1 and leverage learning algorithms.</p><p>Large datasets of music in varying styles, especially containing polyphonic music, will be needed to create learning algorithms that perform all of the above tasks reliably, but these are not easily or cheaply obtainable. Hence we proposed a synthetic dataset with human evaluation data and on-demand scalability as one way to alleviate this problem. Conducting a listening experiment on hundreds of consecutive output allowed us to obtain a good picture of where samples meet or fall short of typical consumer expectation. Newly-generated samples from the same algorithm can be expected to be similarly distributed in this respect, and the function of distance between these samples and consumer expectation, measured as described in Sections 3 and 4, can be modelled in a semi-supervised way in future works. The ability to model this gap gives us a quantitative way to measure it for new algorithms, ultimately allowing researchers to close it more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>This work was supported by the Polish National Centre for Research and Development under Grant POIR.01.01.01-00-0322/20 titled "Humtap -research and development of the technology for audiovideo content generation with machine learning and social sharing".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Number of results returned by [4] when filtering for papers in the Computer Science, Mathematics or Statistics categories with both "Music" and "Learning" in their abstracts for the years 2010-2020 inclusive. For years before 2015, we further filter out any irrelevant results manually.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Histogram of scores R for pieces in the JS Fakes Dataset (a) and by Bach in the training dataset (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of taking the smallest edit distance from each sample in dataset 1 to each sample in dataset 2 and computing both the mean and the minimum of these smallest distances.</figDesc><table><row><cell cols="3">Dataset 1 Dataset 2 Minimum Mean</cell></row><row><cell>JSF</cell><cell>JSB</cell><cell>55.078% 72.818%</cell></row><row><cell>JSF</cell><cell>JSF</cell><cell>25.625% 66.657%</cell></row><row><cell>JSB</cell><cell>JSB</cell><cell>12.755% 57.792%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Number of samples by Bach and by KS_Chorus seen over all participant sessions during human evaluation (columns 2 &amp; 5), the number of times these were correctly identified (columns 3 &amp; 6) and the resulting rate of correct responses (columns 4 &amp; 7), shown across all skill levels.</figDesc><table><row><cell>Skill</cell><cell></cell><cell>Bach</cell><cell></cell><cell>A.I.</cell><cell>Total Samples</cell></row><row><cell></cell><cell cols="2">Samples Correct Ratio</cell><cell cols="2">Samples Correct Ratio</cell><cell></cell></row><row><cell>0</cell><cell>705</cell><cell>339 48.1%</cell><cell>1081</cell><cell>633 58.6%</cell><cell>1786</cell></row><row><cell>1</cell><cell>943</cell><cell>445 47.2%</cell><cell>1351</cell><cell>748 55.4%</cell><cell>2294</cell></row><row><cell>2</cell><cell>515</cell><cell>270 52.4%</cell><cell>732</cell><cell>409 55.9%</cell><cell>1247</cell></row><row><cell>3</cell><cell>264</cell><cell>98 37.1%</cell><cell>366</cell><cell>225 61.5%</cell><cell>630</cell></row><row><cell>4</cell><cell>215</cell><cell>97 45.1%</cell><cell>348</cell><cell>223 64.1%</cell><cell>563</cell></row><row><cell>5</cell><cell>120</cell><cell>65 54.2%</cell><cell>170</cell><cell>83 48.8%</cell><cell>290</cell></row><row><cell>ALL</cell><cell>2762</cell><cell>1314 47.6%</cell><cell>4048</cell><cell>2321 57.3%</cell><cell>6810</cell></row><row><cell cols="3">4 Experimental Results</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">4.1 Human Evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of performing Fisher's exact test and computing F1 score for each skill group, all skill groups combined, and two subgroups dividing the respondents into those who do and do not hold a formal qualification in music.</figDesc><table><row><cell cols="2">SKILL 0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>NO QUAL QUAL ALL</cell></row><row><cell>p</cell><cell cols="6">0.006 0.233 0.004 0.740 0.033 0.635</cell><cell>0.006</cell><cell>0.003 &lt; 0.001</cell></row><row><cell>F1</cell><cell cols="6">0.609 0.576 0.590 0.594 0.647 0.540</cell><cell>0.591</cell><cell>0.600</cell><cell>0.594</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results when modelling the J.S. Bach Chorales dataset with two strong baselines and TonicNet_Z trained on various combinations and subsets of the JS Fake Chorales and J.S. Bach Chorales datasets. Rows marked with asterisks are taken directly from<ref type="bibr" target="#b7">[9]</ref>, while rows marked with a cross are taken from<ref type="bibr" target="#b21">[23]</ref>.</figDesc><table><row><cell>Datasets</cell><cell>Model</cell><cell>Aug.</cell><cell cols="2">Val. NLL NCL Val. NLL</cell></row><row><cell>Bach  ?</cell><cell cols="2">CoCoNet (ordered) None</cell><cell>-</cell><cell>0.436</cell></row><row><cell>Bach  ?</cell><cell>Music Transformer</cell><cell>None</cell><cell>-</cell><cell>0.335</cell></row><row><cell>JSF-top</cell><cell>TonicNet_Z</cell><cell>None</cell><cell>0.474</cell><cell>0.363</cell></row><row><cell>JSF</cell><cell>TonicNet_Z</cell><cell>None</cell><cell>0.424</cell><cell>0.319</cell></row><row><cell>Bach *</cell><cell>TonicNet_Z</cell><cell>None</cell><cell>0.422</cell><cell>-</cell></row><row><cell>JSF-top-s</cell><cell>TonicNet_Z</cell><cell>Trnsps</cell><cell>0.364</cell><cell>0.272</cell></row><row><cell>JSF-top</cell><cell>TonicNet_Z</cell><cell>Trnsps</cell><cell>0.364</cell><cell>0.269</cell></row><row><cell>JSF</cell><cell>TonicNet_Z</cell><cell>Trnsps</cell><cell>0.328</cell><cell>0.234</cell></row><row><cell>Bach *</cell><cell>TonicNet_Z</cell><cell>Trnsps</cell><cell>0.321</cell><cell>0.224</cell></row><row><cell>Bach *</cell><cell>TonicNet_Z</cell><cell>Trnsps+MM</cell><cell>0.317</cell><cell>0.220</cell></row><row><cell>Bach+JSF-top</cell><cell>TonicNet_Z</cell><cell>Trnsps</cell><cell>0.309</cell><cell>0.215</cell></row><row><cell cols="2">Bach+JSF-top-s TonicNet_Z</cell><cell>Trnsps</cell><cell>0.307</cell><cell>0.213</cell></row><row><cell>Bach+JSF</cell><cell>TonicNet_Z</cell><cell>Trnsps</cell><cell>0.300</cell><cell>0.208</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Listening test available at https://omarperacha.github.io/listening-test</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DeepBach: a steerable model for Bach chorales generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pachet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/hadjeres17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>Research, D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1362" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bachbot: Automatic composition in the style of bach chorale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Counterpoint by convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Society for Music Information Retrieval</title>
		<meeting>the 18th International Society for Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the29th International Conference on Machine Learning, ICML 2012</title>
		<meeting>the29th International Conference on Machine Learning, ICML 2012</meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving polyphonic music models with feature-rich encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peracha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Society for Music Information Retrieval Conference</title>
		<meeting>the 21st International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="169" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural audio synthesis of musical notes with wavenet autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1704.01279" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ddsp: Differentiable digital signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Hantrakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1x1ma4tDr" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pyin: A fundamental frequency estimator using probabilistic threshold distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mauch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP. IEEE</title>
		<meeting>ICASSP. IEEE</meeting>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="659" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crepe: A convolutional representation for pitch estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gankyoku: a generative adversarial network for shakuhachi music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Peracha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Head</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Computer Music Conference. ICMA</title>
		<meeting>the International Computer Music Conference. ICMA</meeting>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Giantmidi-piano: A large-scale MIDI dataset for classical piano music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2010.07061</idno>
		<ptr target="https://arxiv.org/abs/2010.07061" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-resolution piano transcription with pedals by regressing onsets and offsets times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2010.01815</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Bach Doodle: Approachable music composition with machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Howcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">April) Musenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<ptr target="Available:openai.com/blog/musenet" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mmm : Exploring conditional multi-track music generation with the transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasquier</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2008.06048" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Musicbert: Symbolic music understanding with large-scale pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.05630" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Music21: A toolkit for computer-aided musicology and symbolic music data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Cuthbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ariza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Society for Music Information Retrieval Conference</title>
		<editor>J. S. Downie and R. C. Veltkamp</editor>
		<meeting>the 11th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="637" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Music transformer: Generating music with long-term structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Amazon mechanical turk: A research tool for organizations and information systems scholars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crowston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shaping the Future of ICT Research. Methods and Approaches</title>
		<editor>A. Bhattacherjee and B. Fitzgerald</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="210" to="221" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
