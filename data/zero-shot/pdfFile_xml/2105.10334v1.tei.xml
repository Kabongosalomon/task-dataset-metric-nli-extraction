<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fact-driven Logical Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siru</forename><surname>Ouyang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
							<email>zhangzs@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
							<email>zhaohai@cs.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fact-driven Logical Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Logical reasoning, which is closely related to human cognition, is of vital importance in human's understanding of texts. Recent years have witnessed increasing attentions on machine's logical reasoning abilities. However, previous studies commonly apply ad-hoc methods to model pre-defined relation patterns, such as linking named entities, which only considers global knowledge components that are related to commonsense, without local perception of complete facts or events. Such methodology is obviously insufficient to deal with complicated logical structures. Therefore, we argue that the natural logic units would be the group of backbone constituents of the sentence such as the subject-verb-object formed "facts", covering both global and local knowledge pieces that are necessary as the basis for logical reasoning. Beyond building the ad-hoc graphs, we propose a more general and convenient fact-driven approach to construct a supergraph on top of our newly defined fact units, and enhance the supergraph with further explicit guidance of local question and option interactions. Experiments on two challenging logical reasoning benchmark datasets, ReClor and LogiQA, show that our proposed model, FOCAL REASONER, outperforms the baseline models dramatically. It can also be smoothly applied to other downstream tasks such as MuTual, a dialogue reasoning dataset, achieving competitive results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Logical reasoning is one of the most important skills of human intelligence <ref type="bibr" target="#b0">[1]</ref>, which accounts for human intuition about entailment of sentences and reflects the semantic relations between sentential constituents such as determiner, noun, adjective, adverb, preposition, and verb phrases <ref type="bibr" target="#b1">[2]</ref>. Recently, there is a surging trend of research into logical reasoning ability, among which ReClor <ref type="bibr" target="#b2">[3]</ref> and LogiQA <ref type="bibr" target="#b0">[1]</ref> are two representative datasets introduced to promote the development of logical reasoning, where logical reasoning questions are selected from standardized exams such as GMAT and LSAT, requiring models to read and comprehend the complicated logical relationships. Similar to the standard question-answering (QA)-based MRC tasks in form, our concerned logical reasoning QA tasks contain three elements: passage, question and the candidate options as examples shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The major challenge of logical reasoning is to uncover logical structures, and reasoning with the candidate options and questions to predict the correct answer. However, it is difficult for PrLMs to capture the logical structure inherent in the texts since logical supervision is rarely available during pre-training. An increasing interest is using graph networks to model the entity-aware relationships in the passages <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. However, these methods only consider global knowledge components that are related to entity-aware commonsense, without local perception of complete facts or events.</p><p>In addition, the importance of logical units has not been well uncovered as the reasoning paths were modeled by either the annotated entities <ref type="bibr" target="#b7">[8]</ref>, or the sentence-level elements <ref type="bibr" target="#b5">[6]</ref>. We argue that the more natural logic units would be the group of backbone constituents of the sentence such as subject, verb and object that cover both global and local knowledge pieces. For example, these units may reflect the meaning of who did what to whom, or who is what. An example is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Such groups can be defined as "fact unit" following <ref type="bibr" target="#b8">[9]</ref> in Definition 1. The fact units are further organized into a super graph following Definition 2.</p><p>Definition 1 (Fact Unit) Given an triplet T = {E 1 , R, E 2 }, where E 1 and E 2 are entities, P is the predicate between them, a fact unit F is the set of all entities in T and their corresponding relations.  In contrast, the dotted vertices and edges are focused in most existing studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>In terms of the concerned logical reasoning QA tasks, existing studies mainly focus on the understanding of the passage, with little attention paid to the question and candidate options. Especially when taking the PrLMs as the model backbone, an important concern is the degraded ability to recognize the negations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, which would easily fail in the understanding of questions and options for the logical reasoning tasks. For one thing, there might be negated expressions in questions, such as "weakens the argument" in Example 2 in <ref type="figure" target="#fig_0">Figure 1</ref>. For another, some of the answer options may contradict to each other. Co-attention Interaction <ref type="figure">Figure 3</ref>: The framework or our model. It consists of an encoder (left), reasoning modules (middle) and a prediction module (right). For supergraph reasoning, in each iteration, each node selectively receives the message from the neighboring nodes to update its representation. The dashed circle means zero vector. two challenging logical reasoning benchmark datasets including LogiQA, ReClor, and one dialogue reasoning dataset Mutual for generalizability, achieving new state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Machine Reading Comprehension Recent years have witnessed massive researches on Machine Reading Comprehension (MRC), which has become one of the most important areas of NLP. Despite the success of MRC models on various datasets such as CNN/Daily Mail <ref type="bibr" target="#b14">[15]</ref>, SQuAD <ref type="bibr" target="#b15">[16]</ref>, RACE <ref type="bibr" target="#b16">[17]</ref> and so on, researchers began to rethink to what extent does the problem be solved. Nowadays, there is an increasing trend of research into the reasoning ability of machines. According to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, reasoning abilities can be broadly categorized into (1) commonsense reasoning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>;</p><p>(2) numerical reasoning <ref type="bibr" target="#b24">[25]</ref>; (3) multi-hop reasoning <ref type="bibr" target="#b25">[26]</ref> and (4) logical reasoning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>, among which logical reasoning is essential in human intelligence but has merely been delved into. Natural Language Inference (NLI) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> is a task closely related to logical reasoning. However, it has two obvious drawbacks in measuring logical reasoning abilities. One is that it only has three logical types which are entailment, contradiction and neutral. The other is its limitation on sentence-level reasoning. Hence, it is important to research more comprehensive and deep logical reasoning abilities.</p><p>Logical Reasoning in MRC There are two main kinds of features in language data that would be the necessary basis for logical reasoning: 1) knowledge: global facts that keep consistency regardless of the context, such as commonsense, mostly derived from named entities; 2) non-knowledge: local facts or events that may be sensitive to the context, mostly derived from linguistics. Existing works have made progress in improving logical reasoning ability <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30]</ref>, however, these approaches are barely satisfactory as they mostly focus on the global facts such as typical entity or sentence-level relations and use ad-hoc graphs to model them, which are obviously not sufficient. In this work, we strengthen the basis for logical reasoning by unifying both types of the features as facts. Different from previous studies that focus on the knowledge components, we propose a fact-driven logical reasoning framework that builds supergraphs on top of fact units to capture both global connections between entity-aware facts and the local concepts or events inside the fact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches</head><p>In this section, we will describe our method in detail. The overall architecture of the model is shown in <ref type="figure">Figure 3</ref> . We first construct a fact chain from the raw text based on the logical triplets extracted. Then we conduct reasoning over the fact chain with question-option guided approaches to learn and update the features, which are further incorporated in answer prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preprocessing</head><p>Triplets Extraction. Triplets can well represent the logical facts inherent in the context. To keep the framework generic, we use a fairly simple fact extractor based on the dependency parsing of each sentence. After that, we extract the subject, the predicate, and the object tokens to get the "Entity-Predicate-Entity" triplets corresponding to each sentence in the context.</p><p>Question Reformulation. The questions in logical reasoning vary. We believe that handling with question type may compensate for the weakness of PrLM in dealing with negations, thus conducing to the prediction performance. The negation detection is composed of two parts. Firstly, we use TextBlob 2 to do sentiment analysis. If the question is detected to be negative, we directly consider the question to have negations. Secondly, if any pre-defined negative words are among or immediately before the extracted "NP" or "VP", we identify it as a negation connective. And the pre-defined negative words include {"not", "n't", "unable", "no", "few", "little", "neither", "none of "}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supergraph Construction</head><p>With these fact triplets obtained from 3.1, we are able to construct the supergraph. Firstly, the fact units are organized in the form of Levi graph <ref type="bibr" target="#b30">[31]</ref>, which turns entities and predicates all into nodes. An original fact unit in the form of F = (V, E, R), where V is the set of the entities, E is the set of edges connected between entities, and R is the relations of each edge which are predicates here. The corresponding Levi graph is denoted as</p><formula xml:id="formula_0">F l = (V L , E L , R L ) where V L = V ? R,</formula><p>which makes the originally directly connected entities be intermediately connected via relations. As for R L , previous works such as <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> designed three types of edges R L = {def ault, reverse, self } to enhance information flow. Here in our settings, we extend it into five types: default-in, default-out, reverse-in, reverse-out, self, corresponding to the edges towards the predicates.</p><p>After getting fact units F l , we extract the coreference relations E C among entities in fact units using Huggingface neuralcoref 3 . Fact units with the same pronoun references are connected using undirected edges. To better encode the structure information of questions and options (details are specified in Section 3.3.2), a global atom is added and connected with all the other atoms. The final supergraph is denoted as S = (F l , E C ). An example is shown in <ref type="figure" target="#fig_3">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Context Encoder</head><p>We initialize our context encoder F C (.) with a transformer-based pre-trained encoder, i.e., RoBERTalarge <ref type="bibr" target="#b33">[34]</ref>. Question, context and option are concatenated and then fed into the encoder. If the question is detected to contain negative meanings, we add a special token &lt;pos&gt; before the question, else we add &lt;neg&gt;. In a whole, we get the hidden representation as following:</p><formula xml:id="formula_1">{h c,0 , ..., h c,lc+1 , h q,1 , ..., h q,lq , h o,1 , ..., h o,lo+1 } = F C ({x c,0 , ..., x c,lc+1 , x q,0 , ..., x q,lq , x o,1 , ..., x o,lo+1 }), (1) where x c,0 =&lt;s&gt;, x c,lc+1 = x o,lo+1 =&lt;/s&gt;, x q,0 =&lt;pos&gt;/&lt;neg&gt; and h i ? R d , d is the hidden size.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Super Graph Encoder</head><p>Graph Initialization We first utilize F C (.) to encode each token in nodes V L , and then we use the averaged hidden state as the initial representation of the original word of each atom, because Which one of the following ......, most seriously weakens the argument? Various species of animals from the same era as dinosaurs and similar to them ... did not become extinct when the dinosaurs did. PrLMs like RoBERTa take subwords as input while our triplets extraction performs in word-level.</p><p>For the global QA-context node, we also averaged the embeddings of tokens in question and option for initialization. We also use a one-hot embedding layer to encode the relations between two nodes.</p><p>Graph Attention Network Based on the relational graph convolutional network <ref type="bibr" target="#b34">[35]</ref> and given the initial representation h 0 i for every node v i , the feed-forward or the message-passing process can be written as: h</p><formula xml:id="formula_2">(l+1) i = ReLU( r?R L vj ?Nr(vi) 1 c i,r w (l) r h (l) j ),<label>(2)</label></formula><p>where N r (v i ) denotes the neighbors of node v i under relation r and c i,r is the number of those nodes. w (l) r is the learnable parameters of layer l. For information passing control, we introduce the gating mechanism <ref type="bibr" target="#b31">[32]</ref>, which calculates a value between 0 and 1. g</p><formula xml:id="formula_3">(l) i = ?(h (l) i W r,g ),<label>(3)</label></formula><p>where W (l) r,g is a learnable parameter under relation type r of the l-th layer. Finally, the forward process of gated GCN can be represented as:</p><formula xml:id="formula_4">h (l+1) i = ReLU( r?R L vj ?Nr(vi) g (l) q 1 c i,r w (l) r h (l) j ),<label>(4)</label></formula><p>Through the graph encoder F G (.), we then obtain the hidden representations of nodes in fact units as:</p><formula xml:id="formula_5">{h F 0 , ...h F m } = F G ({v L,0 , ...v L,m }, E L ).<label>(5)</label></formula><p>These features are further concatenated to get the final node representation of the super graph:</p><formula xml:id="formula_6">{h S 0 , ...h S m } = F G ({h F 0 , ...h F m }, E C ).<label>(6)</label></formula><p>For node features on the super graph, it is fused via the attention and gating mechanism with the original representations of the context encoder. Specifically, denote the original whole sequence representation after context encoder as H C , we apply attention mechanism to append the super graph representation to the original one:H</p><formula xml:id="formula_7">= Attn(H c , K F , V f ),<label>(7)</label></formula><p>where {K F , V f } are packed from the learned representations of the super graph. Intuitively, this information may play an auxiliary effect during the answer prediction. Therefore, we compute ? ? [0, 1] to weigh the expected importance of super graph representation of each source word:</p><formula xml:id="formula_8">? 1 = ?(W ?H + U ? H C ),<label>(8)</label></formula><p>where W ? and U ? are model parameters. We then fuse H C andH to learn an effective representation:</p><formula xml:id="formula_9">H = H C + ?H ? R 4?d .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Question-Option-aware Interaction</head><p>Since options have their inherent logical relations, contradictions may dwell in different options, which can be leveraged to aid answer prediction. Inspired by <ref type="bibr" target="#b35">[36]</ref>, we use an attention-based mechanism to gather option correlation information, and then the correlated option representation again interacts with question and passage representations.</p><p>Firstly, we define the attention operation which is frequently used in the following formulae. Given input matrices U ? R d?N and V ? R d?M , the attention weight function Attn(.) is defined as:</p><formula xml:id="formula_10">A = Attn(U, V ; v) = [ exp(s ij ) i exp(s ij ) ] i,j ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_11">s ij = v T [U :i ; V :j ; U :i ? V :j ], v ? R 3d</formula><p>is a learnable hyperparameter. d is the hidden dimension size. We will first fix an option and let it interact with all other options one-by-one to collect the pairwise correlation information. Then all the information will be fused to get an option-wise representation via gating mechanism. Specifically for an option O i , the information it get by interaction with option O j is calculated as:</p><formula xml:id="formula_12">O (j) i = [O q i ? O q i Attn(O q i , O q j ; v); O q i ? O q i Attn(O q i , O q j ; v)],<label>(11)</label></formula><p>where O q i is the representation of the concatenation for the i-th option and question after the context encoder. Then the option-wise information are gathered to fuse the option correlation information, which is defined as:</p><formula xml:id="formula_13">? i = tanh(W c [O q i ; {O (j) i } i =j ] + b c ),<label>(12)</label></formula><p>where W c ? R d?7d and b c ? R d . Finally, a gating mechanism is used to fuse the option features with the obtained option correlation features to produce the advanced option features:</p><formula xml:id="formula_14">O q i,:k = g i,:k ? O q i,:k + (1 ? g i,:k ) ?? i,:k ,<label>(13)</label></formula><p>where the g i,: </p><formula xml:id="formula_15">k = ?(W g [O i,:k ;? q i,:k ;Q] + b g ) ? R d is the i-th column of gate g.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hierarchical Decoder</head><p>To better incorporate the information obtained above, apart from getting the original pooled contextattended representation h C ? R 4?d , we combine the attended vectors O f and H from the previous encoder through a fusing layer.</p><formula xml:id="formula_16">E 1 = ReLU(FC([h C , H, h C ? H, h C ? H])), E 2 = ReLU(FC([h C , H, h C ? O f , h C ? O f ])), P = ?(FC([E 1 , E 2 ])), C = P ? H + (1 ? P ) ? O f ? R 4?d .<label>(14)</label></formula><p>Then another linear layer is applied for final prediction</p><formula xml:id="formula_17">z = W z C + b z ? R 4 .<label>(15)</label></formula><p>We seek to minimize the cross entropy loss by</p><formula xml:id="formula_18">L ans = ? log softmax(z) l ,<label>(16)</label></formula><p>where l indicate the correct decision.</p><p>Logical Fact Regularization Inspired by <ref type="bibr" target="#b36">[37]</ref>, the embedding of the tail entity should be close to the embedding to the head entity plus a relation-related vector in the hidden representation space. Without loss of generality, we assume that in our settings, the summation of the subject vector and the relation vector should be close to the object vector as much as possible, i.e.,</p><formula xml:id="formula_19">v subject + v relation ? v object .<label>(17)</label></formula><p>In order to make the logical facts more of factual correctness, we introduce a regularization for the extracted logical facts based on the hidden states of the sequence h i where i = 1, . . . , L and L is the total length of the sequence. The regularization is defined as:</p><formula xml:id="formula_20">L lf r = m k=1 (1 ? cos(h sub k + h rel k , h obj k )),<label>(18)</label></formula><p>where m is the total number of logical fact triplets extracted from the context as well as the option and k indicates the k ? th fact triplet.</p><p>Training Objective. During training, the overall loss for answer prediction is:</p><formula xml:id="formula_21">L = ?L ans + ?L lf r ,<label>(19)</label></formula><p>where ? and ? are two parameters. In our implementation, we set ? = 1.0 and ? = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conducted the experiments on three datasets. Two for specialized logical reasoning ability testing: Reading Comprehension dataset requiring logical reasoning (ReClor) <ref type="bibr" target="#b2">[3]</ref> and LogiQA <ref type="bibr" target="#b0">[1]</ref> and one for logical reasoning in dialogues: Multi-Turn Dialogue Reasoning (MuTual) <ref type="bibr" target="#b37">[38]</ref>.</p><p>ReClor ReClor contains 6,138 multiple-choice questions modified from standardized tests such as GMAT and LSAT, which are randomly split into train/dev/test sets with 4,638/500/1,000 samples respectively. It contains multiple logical reasoning types. The held-out test set is further divided into EASY and HARD subsets based on the performance of BERT-based model <ref type="bibr" target="#b38">[39]</ref>.</p><p>LogiQA LogiQA consists of 8,678 multiple-choice questions collected from National Civil Servants Examinations of China and are manually translated into English by experts. The dataset is randomly split into train/dev/test sets with 7,376/651/651 samples correspondingly. LogiQA also contains various logical reasoning types.</p><p>MuTual MuTual has 8,860 dialogues annotated by linguist experts and high-quality annotators from Chinese high school English listening comprehension test data. It is randomly split into train/dev/test sets with 7,088/886/886 samples respectively. There more than 6 types of reasoning abilities reflected in MuTual. MuTual plus is an advanced version, where one of the candidate responses is replaced by a safe response (e.g., "could you repeat that?") for each example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We fine-tune RoBERTa-large as the backbone pre-trained language model for FOCAL REASONER, which contains 24 hidden layers with hidden size 1,024. The overall model is end-to-end trained and updated by Adam <ref type="bibr" target="#b39">[40]</ref> optimizer with an overall learning rate 8e-6 for ReClor and LogiQA, and 4e-6 for MuTual. The weight decay is 0.01. We set the warm-up proportion during training to 0.1. For graph encoders, we implement it using DGL <ref type="bibr" target="#b3">4</ref> , an open-source lib of python. The layer number of the graph encoder is 2 for ReClor and 3 for LogiQA. The maximum sequence length is 256 for LogiQA and MuTual, and 384 for ReClor. The model is trained for 10 epochs with a total batch size is 16 and an overall dropout rate 0.1 on NVIDIA Tesla V100 GPU, which takes around 5 hours for ReClor and 10 hours for LogiQA. <ref type="table" target="#tab_2">Table 1</ref> and <ref type="table" target="#tab_3">Table 2</ref> show the results on ReClor, LogiQA, and MuTual, respectively. All the best results are shown in bold. Based on our implemented baseline models (basically consistent with public results), we observe dramatic improvements on both of the logical reasoning benchmarks,  FOCAL REASONER also outperforms the prior best system DAGN 5 , reaching 77.05% on the EASY subset, and 44.64% on the HARD subset. The boost suggests that FOCAL REASONER makes better use of logical structure inherent in the given context to perform reasoning than existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>In addition, <ref type="table">Table 5</ref> lists the accuracy of our model on the dev set of ReClor of different question types. Results show that our model can perform well on most of the question types, especially "Strengthen" and "Weaken". This means that our model can well interpret the question type from the question statement and make the correct choice corresponding to the question.</p><p>On the dialogue reasoning dataset MuTual, we also achieve quite a jump compared with the RoBERTabase LM <ref type="bibr" target="#b5">6</ref> . This verifies our model's generalizability on other downstream task settings. Our model can be smoothly applied to other tasks that require reasoning to boost performance.   <ref type="table" target="#tab_8">Table 4</ref> summarizes the ablation study conducted on each of our model's components using the ReClor dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>Supergraph reasoning: The first key component is the supergraph reasoning. We ablate the global atom and erase all the edges connected with it. The results suggest that the global atom indeed provides a better message propagation effect, leveraging performance from 64.6% to 66.8%. We also find that replaceing the initial <ref type="bibr" target="#b4">5</ref> For a fair comparison, we only compare to public literatures with the same PrLM RoBERTa-large. The test results can be found at https://eval.ai/web/challenges/challenge-page/503/ leaderboard/1347 <ref type="bibr" target="#b5">6</ref> Since there are no official results on RoBERTa-large LM, we use RoBERTa-base LM instead for consistency.  <ref type="table">Table 5</ref>: Accuracy on the dev set of ReClor corresponding to several representative question types. "S": "Strengthen", "W": "Weaken", "I": "Implication", "CMP": "Conclusion/Main Point", "ER": "Explain or Resolve", "D": "Dispute", "R": "Role", "IF": Identify a Flaw, "MS": Match Structures.</p><p>QA pair representation of the global atom with only question representation hurts the performance. In addition, without the logical fact regularization, the performance drops from 66.8% to 64.2%, indicating its usefulness. For edge analysis, when (1) all edges are regarded as a single type rather than the original designed 6 types and (2) coreference edges are removed, the accuracy drops to 63.7% and 64.8% respectively. It is proved that in our supergraph, edges link the fact units in reasonable manners, which properly indicates the logical structures.</p><p>Fact Units V.S. Named Entities We further replace the fact units with named entities which are used in the previous works. The statistics of fact units and named entities of ReClor and LogiQA are stated in <ref type="table" target="#tab_5">Table 3</ref>, from which we can infer that there are indeed more fact units than named entities. Thus using fact units can better incorporate the logical information within the context. When replacing all the fact units with named entities, we can see from <ref type="table" target="#tab_8">Table 4</ref> that it significantly drops the performance.  Question reformulation: We find that removing the reformulation process in the preprocessing stage stated in Section 3.1 hurts the performance. This is quite intuitive as reformulated questions provide constructive guidance in choosing the right option.</p><p>Interactions: We further experimented with the query-option-interactions setting to see how it affects the performance. The results suggest that the features learned from the interaction process improve the model performance. Our intuition is that the logical relations between different options are a strong indication of the right answer, which means that the model learns from comparative reasoning strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of Fact Units Numbers</head><p>To inspect how the proposed FOCAL REASONER helps with logical reasoning, we investigate the effects of the number of fact units. Based on this, we split the original dev set of ReClor and LogiQA into 5 subsets. The statistics of the fact unit distribution on the datasets are shown in   the difference in style of the two datasets. However, all the models include ours struggle when the number of fact units is above some certain thresholds, i.e., the logical structure is more complicated, calling for better mechanisms to cope with complex logical structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Interpretability: a Case Study</head><p>We aim to interpret FOCAL REASONER's reasoning process by analyzing the node-to-node attention weights induced in the supergraph in <ref type="figure">Figure.</ref>6. We can see that our FOCAL REASONER can well bridge the reasoning between context, question and option. Specifically, in the graph, "students rank 30%" attends strongly to "playing improve performance". Under the guidance of question to select the option that weakens the statement and option interaction, our model is able to tell that "students rank 30% can play" mostly undermines the conclusion that "playing improves performance".</p><p>A recent survey in a key middle school showed that high school students in this school have a special preference for playing football, and it far surpasses other balls.The survey also found that students who regularly play football are better at academic performance than students who do not often play football.This shows that often playing football can improve students' academic performance.</p><p>Which of the following can weaken the above conclusion most?</p><p>A. Only high school students who are ranked in the top 30% of grades can often play football. B. Regular football can exercise and maintain a strong learning energy. C. Often playing football delays the study time. D. Research has not proved that playing football can contribute to intellectual development.</p><p>1. students have preferences 2. preference playing football 3. it surpasses balls 4. who play football 5. students better performance 6. who !play football 7. playing improve performance 8. students rank 30% 9. students play football </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose a novel method named FOCAL REASONER for logical reasoning in the machine reading comprehension task. Our method not only better uncovers the logical structures within the context, which can be a general method for other sophisticated reasoning tasks but also better captures the logical interactions between context and options. The experimental results verify the effectiveness of our method. In the future, we intend to design more elaborate mechanisms to cope with different question types and logical types as well as combine the symbolic and neural approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two examples from LogiQA and ReClor respectively are illustrated. There are entities and relations between entities. Both are emphasized by different colors: entities, relations. Key words in questions are highlighted in blue. Key options are highlighted in gray.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 2 (</head><label>2</label><figDesc>Supergraph) A supergraph is a structure made of fact units (regarded as subgraphs) as the vertices, and the coreference relations as undirected edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An example of constructed supergraph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The process of constructing the fact chain and its corresponding Levi graph form of an example inFigure.1. Entities and relations are illustrated in its corresponding color. In the update, the QA context and the other atoms are jointly learned to conduct the reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The obtained option feature is further correlated with document feature via co-attention. The final representation corresponding to the i-th option is denoted as O f i ? R 4?d .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Accuracy of models on number of fact units on dev set of ReClor (left) and LogiQA (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>An example of how our model reasons to get the final answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>A large enough comet colliding with Earth could have caused a cloud of dust that enshrouded the planet and cooled the climate long enough to result in the dinosaurs' demise.</figDesc><table><row><cell>comet</cell><cell>Earth</cell></row><row><cell>comet colliding ? Earth</cell><cell></cell></row><row><cell>comet caused ? dust</cell><cell></cell></row><row><cell>dust enshrouded ? planet</cell><cell></cell></row><row><cell>dust cooled ? climate</cell><cell></cell></row><row><cell>comet result ? demise</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>58.30 75.91 44.46 36.87 39.32 FOCAL REASONER 66.80 58.90 77.05 44.64 41.01 40.25 Experimental results of our model compared with baseline models on ReClor and LogiQA dataset. Test-E and Test-H denote Test-Easy and Test-Hard respectively. We performed Pitman's permutation test<ref type="bibr" target="#b40">[41]</ref> and found that our model significantly outperformed the baseline (p&lt;0.05).</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell>ReClor</cell><cell></cell><cell>LogiQA</cell></row><row><cell></cell><cell></cell><cell>Dev</cell><cell cols="3">Test Test-E Test-H Dev</cell><cell>Test</cell></row><row><cell cols="2">Human [3]</cell><cell>-</cell><cell cols="2">63.00 57.10 67.20</cell><cell>-</cell><cell>86.00</cell></row><row><cell cols="2">BERT-Large [3]</cell><cell cols="4">53.80 49.80 72.00 32.30 34.10 31.03</cell></row><row><cell cols="2">XLNet-Large [3]</cell><cell cols="3">62.00 56.00 75.70 40.50</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">RoBERTa-Large [3] 62.60 55.60 75.50 40.00 35.02 35.33</cell></row><row><cell cols="2">DAGN [6]</cell><cell cols="4">65.20 58.20 76.14 44.11 35.48 38.71</cell></row><row><cell cols="3">DAGN (Aug) [6] 65.80 Model MuTual Dev Set</cell><cell>Test Set</cell><cell>Dev Set</cell><cell>MuTual plus</cell><cell>Test Set</cell></row><row><cell cols="6">R4@1 R4@2 MRR R4@1 R4@2 MRR R4@1 R4@2 MRR R4@1 R4@2 MRR</cell></row><row><cell cols="6">RoBERTa base [38] 69.5 87.8 82.4 71.3 89.2 83.6 62.2 85.3 78.2 62.6 86.6 78.7</cell></row><row><cell>-MC [38]</cell><cell cols="5">69.3 88.7 82.5 68.6 88.7 82.2 62.1 83.0 77.8 64.3 84.5 79.2</cell></row><row><cell cols="6">FOCAL REASONER 73.4 90.3 84.9 72.7 91.0 84.6 63.7 86.1 79.1 65.5 84.3 79.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Experimental results of our model compared with baseline PrLM on MuTual dataset. e.g., on ReClor test set, FOCAL REASONER achieves +4.2% on dev set and +3.3.% on the test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Statistics for fact unit entities and traditional named entities in datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>47.79 39.13 63.89 58.33 50.77 50.00 56.25 61.54 56.67 DAGN [6] 63.83 46.02 39.13 69.44 57.14 53 85 46.67 62.50 62.39 56.67 FOCAL REASONER 65.96 51.33 43.48 72.22 67.86 53.85 50.00 62.50 62.39 60.0</figDesc><table><row><cell>Model</cell><cell>S</cell><cell>W</cell><cell>I</cell><cell>CMP</cell><cell>ER</cell><cell>P</cell><cell>D</cell><cell>R</cell><cell>IF</cell><cell>MS</cell></row><row><cell>RoBERTa large [3]</cell><cell>61.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablation results on the dev set of ReClor.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Table.6. We can see that most numbers of fact units of the contexts in ReClor are in<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6)</ref> while it is [0, 3) for LogiQA.</figDesc><table><row><cell cols="2">Dataset [0, 3) [3, 6) [6, 9) [9, 12) [12, ?)</cell></row><row><cell>ReClor 37.2% 48.6% 12.6% 0.6%</cell><cell>1.2%</cell></row><row><cell>LogiQA 47.5% 37.5% 10.9% 3.5%</cell><cell>0.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Distribution of fact unit number on dev set of the training datasets.</figDesc><table><row><cell>We display the accuracy of RoBERTa-large,</cell></row><row><cell>DAGN and our proposed FOCAL REASONER</cell></row><row><cell>in Figure5.2. We find that our model outper-</cell></row><row><cell>forms baseline models on all the divided sub-</cell></row><row><cell>sets, which demonstrates the effectiveness of</cell></row><row><cell>our model on different numbers of fact units.</cell></row><row><cell>Specifically, for ReClor, FOCAL REASONER</cell></row><row><cell>performers better when there are more fact units</cell></row><row><cell>in the context, while for LogiQA, FOCAL REA-</cell></row></table><note>SONER works better when the number of fact units locates in [0, 3) and [9, 12). The reason may lie in</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/sloria/TextBlob 3 https://github.com/huggingface/neuralcoref</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.dgl.ai/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yile</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<editor>Christian Bessiere</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3622" to="3628" />
		</imprint>
	</monogr>
	<note>Main track</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucja</forename><surname>Iwa?ska</surname></persName>
		</author>
		<title level="m">Logical reasoning in natural language: It is all about knowledge. Minds and Machines</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="475" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reclor: A reading comprehension dataset requiring logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Qa-gnn: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beta embeddings for multi-hop logical reasoning in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DAGN: Discourse-aware graph network for logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Local reasoning for global graph properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Programming</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="308" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06598</idno>
		<title level="m">AR-LSAT: Investigating Analytical Reasoning of Text. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language-aware truth assessment of fact candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1009" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamically fused graph network for multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6140" to="6150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cognitive graph for multi-hop reading comprehension at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2694" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-hop question answering via reasoning chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shih-Ting Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02610</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probing neural network comprehension of natural language arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Niven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="4658" to="4664" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A primer in bertology: What we know about how bert works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How much reading does reading comprehension require? a critical investigation of popular benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="5010" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<title level="m">Progress in neural nlp: Modeling, learning, and reasoning. Engineering</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="275" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A thorough examination of the CNN/Daily Mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2358" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Commonsense reasoning and commonsense knowledge in artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="92" to="103" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abductive commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cosmos qa: Machine reading comprehension with contextual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2391" to="2401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial nli: A new benchmark for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Logic-driven context extension and data augmentation for logical reasoning of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03659</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Finite geometrical systems: six public lectues delivered in February</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levi</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1940" />
		</imprint>
		<respStmt>
			<orgName>at the University of Calcutta. University of Calcutta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph-to-sequence learning using gated graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="273" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A Robustly Optimized BERT Pretraining Approach. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2019-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rianne Vanden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web</title>
		<editor>Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Rapha?l Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Option comparison network for multiple-choice reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mutual: A dataset for multi-turn dialogue reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Conference of the Association for Computational Linguistics</title>
		<meeting>the 58th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The hitchhiker&apos;s guide to testing statistical significance in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rotem</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gili</forename><surname>Baumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1383" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
