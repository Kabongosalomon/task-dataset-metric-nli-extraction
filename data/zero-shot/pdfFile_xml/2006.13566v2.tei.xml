<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DISK: Learning local features with policy gradient</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><forename type="middle">J</forename><surname>Tyszkiewicz</surname></persName>
							<email>michal.tyszkiewicz@epfl.chpascal.fua@epfl.chtrulls@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">?cole Polytechnique F?d?rale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">?cole Polytechnique F?d?rale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Google Research</orgName>
								<address>
									<settlement>Zurich</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DISK: Learning local features with policy gradient</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Local feature frameworks are difficult to learn in an end-to-end fashion, due to the discreteness inherent to the selection and matching of sparse keypoints. We introduce DISK (DIScrete Keypoints), a novel method that overcomes these obstacles by leveraging principles from Reinforcement Learning (RL), optimizing end-to-end for a high number of correct feature matches. Our simple yet expressive probabilistic model lets us keep the training and inference regimes close, while maintaining good enough convergence properties to reliably train from scratch. Our features can be extracted very densely while remaining discriminative, challenging commonly held assumptions about what constitutes a good keypoint, as showcased in <ref type="figure">Fig. 1</ref>, and deliver state-of-the-art results on three public benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Local features have been a key computer vision technology since the introduction of SIFT [20], enabling applications such as Structure-from-Motion (SfM) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36]</ref>, SLAM [27], re-localization [23], and many others. While not immune to the deep learning "revolution", 3D reconstruction is one of the last bastions where sparse, hand-crafted solutions remain competitive with or outperform their dense, learned counterparts <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b15">16]</ref>. This is due to the difficulty of designing end-to-end methods with a differentiable training objective that corresponds well enough with the downstream task.</p><p>While patch descriptors can be easily learned on predefined keypoints <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b12">13]</ref>, joint detection and matching is harder to relax in a differentiable manner, due to its computational complexity. Given two images A and B with feature sets F A and F B , matching them is O(|F A |?|F B |). As each image pixel may become a feature, the problem quickly becomes intractable. Moreover, the "quality" of a given feature depends on the rest, because a feature that is very similar to others is less distinctive, and therefore less useful. This is hard to account for during training.</p><p>We address this issue by bridging the gap between training and inference to fully leverage the expressive power of CNNs. Our backbone is a network that takes images as input and outputs keypoint 'heatmaps' and dense descriptors. Discrete keypoints are sampled from the heatmap, and the descriptors at those locations are used to build a distribution over feature matches across images. We then use geometric ground truth to assign positive or negative rewards to each match, and perform gradient descent to maximize the expected reward E (i,j)?M A?B r(i ? j), where M A?B is the set of matches and r is per-match reward. In effect, this is a policy gradient method <ref type="bibr" target="#b43">[44]</ref>.</p><p>Probabilistic relaxation is powerful for discrete tasks, but its applicability is limited by the fact that the expected reward and its gradients usually cannot be computed exactly. Therefore, noisy Monte Carlo approximations have to be used instead, which harms convergence. We overcome this difficulty by careful modeling that yields analytical expressions for the gradients. As a result, we can benefit from the expressiveness of policy gradient, narrowing the gap between training and inference and ultimately outperforming state-of-the-art methods, while still being able to train models from scratch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(a) Upright Root-SIFT <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref> (b) DISK (ours) 179k landmarks, 22.3 observations/landmark 190k landmarks, 30.0 observations/landmark <ref type="figure">Figure 1</ref>: SIFT vs. DISK in SfM. We reconstruct "Sacre Coeur" from 1179 images <ref type="bibr" target="#b15">[16]</ref> with COLMAP. For Upright Root-SIFT (left) and DISK (right) we show a point cloud and one image with its keypoints. Landmarks, and their respective keypoints, are drawn in blue. Keypoints which do not create landmarks are drawn in red. Our features can be extracted (and create associations) on seemingly textureless regions where SIFT fails to, producing more landmarks with more observations.</p><p>Our contribution therefore is a novel, end-to-end-trainable approach to learning local features that relies on policy gradient. It yields considerably more accurate matches than earlier methods, and this results in better performance on downstream tasks, as illustrated in <ref type="figure">Fig. 1</ref> and Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The process of extracting local features usually involves three steps: finding a keypoint, estimating its orientation, and computing a description vector. In traditional methods such as SIFT <ref type="bibr" target="#b19">[20]</ref> or SURF <ref type="bibr" target="#b3">[4]</ref>, this involves many hand-crafted heuristics. The first wave of local features involving deep networks featured descriptors learned from patches extracted on SIFT keypoints <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38]</ref> and some of their successors, such as HardNet <ref type="bibr" target="#b24">[25]</ref>, SOSNet <ref type="bibr" target="#b39">[40]</ref>, and LogPolarDesc <ref type="bibr" target="#b12">[13]</ref>, are still state-of-the-art. Other learning-based methods focus on keypoints <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b17">18]</ref> or orientations <ref type="bibr" target="#b46">[47]</ref>, or merge the two notions entirely <ref type="bibr" target="#b7">[8]</ref>.</p><p>These methods attack a single element of this process. Others have developed end-to-end-trainable pipelines <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref> that can optimize the whole process and, hopefully, improve performance. However, they either use inexact approximations to the true objective <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref>, break differentiability <ref type="bibr" target="#b28">[29]</ref> or make big assumptions, such as extrema in descriptor space making good features <ref type="bibr" target="#b10">[11]</ref>.</p><p>Three recent approaches are attempting to bridge the gap between training and inference in a spirit close to ours. GLAMpoints <ref type="bibr" target="#b40">[41]</ref> seeks to estimate homographies between retinal images and use Reinforcement Learning (RL) methods to find keypoints that are correctly matched by SIFT descriptors. Since matching is deterministic, Q-learning can be used to regress for the expected reward of each keypoint, rather than optimize directly in policy space. Using hand-crafted descriptors and only addressing the detection problem was motivated by domain-specific requirements of strong rotation equivariance, which most learned models lack. While it makes sense in the specific scenario it was developed for, it limits what the method can do. Similarly, <ref type="bibr" target="#b8">[9]</ref> also uses handcrafted descriptors and learns to predict the probability that each pixel would be successfully matched with those. Their approach therefore inherits many of the limitations of GLAMpoints.</p><p>Reinforced Feature Points <ref type="bibr" target="#b5">[6]</ref> address the more difficult issue of learning with a general nondifferentiable objective for the purpose of camera pose estimation, with RANSAC in the loop. Unfortunately, supervising all detection and matching decisions with a single reward means that this approach suffers from weak training signal, an endemic RL problem, and has to rely on pre-trained models from <ref type="bibr" target="#b9">[10]</ref> that can only be fine-tuned. Our method can be seen as a relaxation of their approach, where we train for a surrogate objective: finding many correct feature matches. This allows for substantially more robust training from scratch and yields better downstream results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Given images A and B, our goal is first to extract a set of local features F A and F B from each and then match them to produce a set of correspondences M A?B . To learn how to do this through reinforcement learning, we redefine these two steps probabilistically. Let P (F I |I, ? F ) be a distribution over sets of features F I , conditional on image I and feature detection parameters ? F , and P (M A?B |F A , F B , ? M ) be a distribution over matches between features in images A and B, conditional on features F A , F B , and matching parameters ? M . Calculating P (M A?B |A, B, ?) and its derivatives requires integrating the product of these two probabilities over all possible F A , F B , which is clearly intractable. However, we can estimate gradients of expected reward ? ? E M A?B ?P (M A?B |A,B,?) R(M A?B ) via Monte Carlo sampling and use gradient ascent to maximize that quantity.</p><p>Feature distribution P (F I |I, ? F ). Our feature extraction network is based on a U-Net <ref type="bibr" target="#b31">[32]</ref>, with one output channel for detection and N for description. We denote these feature maps as K and D, respectively, from which we extract features F = {K, D}. We pick N =128, for a direct comparison with SIFT and nearly all modern descriptors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>The detection map K is subdivided into a grid with cell size h ? h, and we select at most one feature per grid cell, similarly to SuperPoint <ref type="bibr" target="#b9">[10]</ref>. To do so, we crop the feature map corresponding to cell u, denoted K u , and use a softmax operator to normalize it. Our probabilistic framework samples a pixel p in cell u with probability P s (p|K u ) = softmax(K u ) p . This detection proposal p may still be rejected: we accept it with probability P a (accept p |K u ) = ?(K u p ), where K u p is the (scalar) value of the detection map K at location p in cell u, and ? is a sigmoid. Note that P s (p|K u ) models relative preference across a set of different locations, whereas P a (accept p |K u ) models the absolute quality for location p. The total probability of sampling a feature at pixel p is thus P (p|K u ) = softmax(K u ) p ? ?(K u p ). Once feature locations {p 1 , p 2 , ...} are known, we associate them with the l 2 -normalized descriptors at this location, yielding a set of features F I = {(p 1 , D(p 1 )), (p 2 , D(p 2 )), ...}. At inference time we replace softmax with arg max, and ? with the sign function. This is again similar to <ref type="bibr" target="#b9">[10]</ref>, except that we retain the spatial structure and interpret cell K u in both a relative and an absolute manner, instead of creating an extra reject bin.</p><p>Match distribution P (M A?B |F A , F B , ? M ). Once feature sets F A and F B are known, we compute the l 2 distance between their descriptors to obtain a distance matrix d, from which we can generate matches. In order to learn good local features it is crucial to refrain from matching ambiguous points due to repeated patterns in the image. Two solutions to this problem are cycle-consistent matching and the ratio test. Cycle-consistent matching enforces that two features be nearest neighbours of each other in descriptor space, cutting down on the number of putative matches while increasing the ratio of correct ones. The ratio test, introduced by SIFT <ref type="bibr" target="#b19">[20]</ref>, rejects a match if the ratio of the distances between its first and second nearest neighbours is above a threshold, in order to only return confident matches. These two approaches are often used in conjunction and have been shown to drastically improve results in matching pipelines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref>, but they are not easily differentiable.</p><p>Our solution is to relax cycle-consistent matching. Conceptually, we draw forward (A B) matches for features F A,i from categorical distributions defined by the rows of distance matrix d, and reverse (A B) matches for features F B,j from distributions based on its columns. We declare F A,i to match F B,j if both the forward and reverse matches are sampled, i.e., if the samples are consistent. The forward distribution of matches is given by P A B (j|d, i) = softmax (?? M d(i, ?)) j , where ? M is the single parameter, the inverse of the softmax temperature. P A B is analogously defined by d T .</p><p>It should be noted that, given features F A and F B , the probability of any particular match can be computed exactly: P (i ? j) = P A B (i|d, j) ? P A B (j|d, i). Therefore, as long as reward R factorizes over matches as R(M A?B ) = (i,j)?M A?B r(i ? j), given F A and F B , we can compute exact gradients ? D,? M E R(M A?B ), without resorting to sampling. This means that the matching step does not contribute to the overall variance of gradient estimation, unlike in <ref type="bibr" target="#b5">[6]</ref>, which we believe to be key to the good convergence properties of our model. Finally, one can also replace our matching relaxation with a non-probabilistic loss like in <ref type="bibr" target="#b24">[25]</ref>. While it may be superior for descriptors alone, our solution upholds the probabilistic interpretation of the pipeline, making the hyperparameters (? tp , ? f p , ? kp ) easy to tune and naturally integrating with the gradient estimation in keypoint detection. <ref type="figure">Figure 2</ref>: Non-Maxima Suppression vs Grid-based sampling. We demonstrate the benefits of replacing the 1-per-cell sampling approach used during training with simple NMS at inference time. For a small region of an image (left), marked by the red box, we show the features chosen through NMS (middle) and the 'heatmap' K (right), overlaid by the grid. Notice how maxima can be cut by cell boundaries. Keypoints are sorted by "score" and color-coded: the top third are drawn in red, the next third in orange, and the rest in yellow. Each cell contains at most two very salient (red) features.</p><p>Reward function R(M A?B ). As stated above, if the reward R(M A?B ) can be factorized as a sum over individual matches, the formulation of P (M A?B |F A , F B , ? M ) allows for the use of closed-form formulas while training. For this reason we use a very simple reward, which rewards correct matches with ? tp points and penalizes incorrect matches with ? fp points. Let's assume we have ground-truth poses and pixel-to-pixel correspondences in the form of depth maps. We declare a match correct if depth is available at both p A,i and p B,j , and both points lie within pixels of their respective reprojections. We declare a match plausible if depth is not available at either location, but the epipolar distance between the points is less than pixels, in which case we neither reward nor penalize it. We declare a match incorrect in all other cases.</p><p>Gradient estimator. With R factorized over matches and P (i ? j|F A , F B , ? M ) given as a closed formula, the application of the basic policy gradient <ref type="bibr" target="#b43">[44]</ref> </p><formula xml:id="formula_0">is fairly simple: with F A , F B sampled from their respective distributions P (F A |A, ? F ), P (F B |B, ? F ) we have ? ? E M A?B R(M A?B ) = E F A ,F B i,j [P (i ? j|F A , F B , ? M ) ? r(i ? j) ? ? ? ? ij ] ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">? ij = log P (i ? j|F A , F B , ? M ) + log P (F A,i |A, ? F ) + log P (F B,j |B, ? F ).</formula><p>The summation above is non-exhaustive, missing the case of i not being matched with any j: since we award non-matches 0 reward, they can be safely ommited from the gradient estimator. Having a closed formula for P (i ? j|F A , F B , ? M ) along with R being a sum over individual matches allows us to compute the sum in equation 1 exactly, which in the general case of REINFORCE <ref type="bibr" target="#b43">[44]</ref> would have to be replaced with an empirical expectation over sampled matches, introducing variance in the gradient estimates. In our formulation, the only sources of gradient variance are due to mini-batch effects and approximating the expectation w.r.t. choices of F A , F B with an empirical sum.</p><p>It should also be noted that our formulation does not provide the feature extraction network with any supervision other than through the quality of matches those features participate in, which means that a keypoint which is never matched is considered neutral in terms of its value. This is a very useful property because keypoints may not be co-visible across two images, and should not be penalized for it as long as they do not create incorrect associations. On the other hand, this may lead to many unmatchable features on clouds and similar non-salient structures, which are unlikely to contribute to the downstream task but increase the complexity in feature matching. We address this by imposing an additional, small penalty on each sampled keypoint ? kp , which can be thought of as a regularizer.</p><p>Inference. Once the models have been trained we discard our probabilistic matching framework in favor of a standard cycle-consistency check, and apply the ratio test with a threshold found empirically on a validation set. Another consideration is that our method is confined to a grid, illustrated in <ref type="figure">Fig. 2</ref>. This has two drawbacks. Firstly, it can sample at most one feature per cell. Secondly, each cell is blind to its neighbours. Our method may thus select two contiguous pixels as distinct keypoints. At inference time we can work around this issue by applying non-maxima suppression on the feature map K, returning features at all local maxima. This addresses both issues at the cost of a misalignment between training and inference, which is potentially sub-optimal. We discuss this further in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first describe our specific implementation and the training data we rely on. We then evaluate our approach on three different benchmarks, and present two ablation studies.</p><p>Training data. We use a subset of the MegaDepth dataset <ref type="bibr" target="#b18">[19]</ref>, from which we choose 135 scenes with 63k images in total. They are posed with COLMAP, a state-of-the-art SfM framework that also provides dense depth estimates we use to establish pixel-to-pixel correspondences. We omit scenes that overlap with the test data of the Image Matching Challenge (Sec. 4.1), and apply a simple co-visibility heuristic to sample viable pairs of images. See the supplementary material for details.</p><p>Feature extraction network. We use a variation of the U-Net <ref type="bibr" target="#b31">[32]</ref> architecture. Our model has 4 down-and up-blocks which consist of a single convolutional layer with 5 ? 5 kernels, unlike the standard U-Net that uses two convolutional layers per block. We use instance normalization instead of batch normalization, and PReLU non-linearities. Our models comprise 1.1M parameters, with a formal receptive field of 219 ? 219 pixels. Training and inference code is available at https://github.com/cvlab-epfl/disk.</p><p>Optimization. Although the matching stage has a single learnable parameter, ? M , we found that gradually increasing it with a fixed schedule works well, leaving just the feature extraction network to be learned with gradient descent. Since the training signal comes from matching features, we process three co-visible images A, B and C per batch. We then evaluate the summation part of equation 1 for pairs A ? B, A ? C, B ? C and accumulate the gradients w.r.t. ?. While matching is pair-wise, we obtain three image pairs per image triplet. By contrast, two pairs of unrelated scenes would require four images. Our approach provides more matches while reducing GPU memory for feature extraction. We rescale the images such that the longer edge has 768 pixels, and zero-pad the shorter edge to obtain a square input; otherwise we employ no data augmentation in our pipeline. Grid cells are square, with each side h = 8 pixels.</p><p>Rewards are ? tp = 1, ? fp = ?0.25 and ? kp = ?0.001. Since a randomly initialized network tends to generate very poor matches, the quality of keypoints is negative on average at first, and the network would cease to sample them at all, reaching a local maximum reward of 0. To avoid that, we anneal ? fp and ? kp over the first 5 epochs, starting with 0 and linearly increasing to their full value at the end.</p><p>We use a batch of two scenes, with three images in each. Since our model uses instance normalization instead of batch normalization, it is also possible to accumulate gradients over multiple smaller batches, if GPU memory is a bottleneck. We use ADAM <ref type="bibr" target="#b16">[17]</ref> with learning rate of 10 ?4 . To pick the best checkpoint, we evaluate performance in terms of pose estimation accuracy in stereo, with DEGENSAC <ref type="bibr" target="#b6">[7]</ref>. Specifically, every 5k optimization steps we compute the mean Average Accuracy (mAA) at a 10 o error threshold, as in <ref type="bibr" target="#b15">[16]</ref>: see Sec. 4.1 and the appendix for details.</p><p>Finally, our method produces a variable number of features. To compare it to others under a fixed feature budget, we subsample them by their "score", that is, the value of heatmap K at that location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation on the 2020 Image Matching Challenge (IMC) [16] -Table 1, Figures 3 and 4</head><p>The Image Matching Challenge provides a benchmark that can be used to evaluate local features for two tasks: stereo and multi-view reconstruction. For the stereo task, features are extracted across every pair of images and then given to RANSAC, which is used to compute their relative pose. The multiview task uses COLMAP to generate SfM reconstructions from small subsets of 5, 10, and 25 images. The differentiating factor for this benchmark is that both tasks are evaluated downstream, in terms of the quality of the reconstructed poses, which are compared to the ground truth, by using the mean Average Accuracy (mAA) up to a 10-degree error threshold. While this requires carefully tuning components extraneous to local features, such as RANSAC hyperparameters, it measures performance on real problems, rather than intermediate metrics.</p><p>Hyperparameter selection. We rely on a validation set of two scenes: "Sacre Coeur" and "St. Peter's Square". We resize the images to 1024 pixels on the longest edge, generate cycle-consistent matches with the ratio test, with a threshold of 0.95. For stereo we use DEGENSAC <ref type="bibr" target="#b6">[7]</ref>, which outperforms vanilla RANSAC <ref type="bibr" target="#b15">[16]</ref>, with an inlier threshold of 0.75 pixels.  <ref type="table">Table 1</ref>: Image Matching Challenge results. The primary metric is (mAA), the mean Average Accuracy in pose estimation, up to 10 o . We also report (NM) the number of matches (given to RANSAC for stereo, and to COLMAP for multiview). For stereo, we also report (NI) the number of RANSAC inliers. For multiview, we also report (NL) number of landmarks (3D points), and (TL) track length (observations per landmark). The top 3 results are highlighted in red, green and blue.</p><p>Results. We extract DISK features for the nine test scenes, for which the ground truth is kept private, and submit them to the organizers for processing. The challenge has two categories: up to 2k or 8k features per image. We participate in both. We report the results in <ref type="table">Table 1</ref>, along with baselines taken directly from the leaderboards, computed in <ref type="bibr" target="#b15">[16]</ref>. We consider several descriptors on DoG keypoints: RootSIFT [20, 2] L2-Net <ref type="bibr" target="#b38">[39]</ref>, HardNet <ref type="bibr" target="#b24">[25]</ref>, GeoDesc <ref type="bibr" target="#b21">[22]</ref>, SOSNet <ref type="bibr" target="#b39">[40]</ref> and LogPolarDesc <ref type="bibr" target="#b12">[13]</ref>. For brevity, we show only their upright variants, which perform better than their rotation-sentitive counterparts on this dataset. For end-to-end methods, we consider SuperPoint <ref type="bibr" target="#b9">[10]</ref>, LF-Net <ref type="bibr" target="#b28">[29]</ref>, D2-Net <ref type="bibr" target="#b10">[11]</ref>, and R2D2 <ref type="bibr" target="#b30">[31]</ref>. All of these methods use DEGENSAC <ref type="bibr" target="#b6">[7]</ref> as a RANSAC variant for stereo, with their optimal hyperparameters. We also list the top 3 user submissions for each category, taken from the leaderboards on June 5, 2020 (the challenge concluded on May 31, 2020).</p><p>On the 2k category, we outperform all methods by 9.4% relative in stereo, and 6.7% relative in multiview. On the 8k category, averaging stereo and multiview, we outperform all baselines, but place slightly below the top three submissions. Our method can find many more matches than any other, easily producing 2-3x the number of RANSAC inliers or 3D landmarks. Our features used for the 2k category are a subset of those used for 8k, which indicates a potentially sub-optimal use of the increased budget, which may be solved training with larger images or smaller grid cells. We show qualitative images in Figs. 3 and 4. Further results are available in the supplementary material.</p><p>Note that we only compare with submissions using the built-in feature matcher, based on the l 2 distance between descriptors, instead of neural-network based matchers <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b32">33]</ref>, which combined with state-of-the-art features obtain the best overall results. Even so, DISK places #2 below only SuperGlue <ref type="bibr" target="#b32">[33]</ref> on the 2k category, outperforming all other solutions using learned matchers.</p><p>Rotation invariance. We observe our models break under large in-plane rotations, which is to be expected. We evaluate their performance with an additional test using synthetic data. We pick 36 images randomly from the IMC 2020 validation set, match them with their copies, rotated by ?, and calculate the ratio of correct matches, defined as those below a 3-pixel reprojection threshold. In <ref type="figure">Fig. 6</ref> we report it for different state-of-the-art methods that, like ours, bypass orientation detection, and overlay a histogram of the differences in in-plane rotation in the dataset. We find that DISK is exceptionally robust to the range of rotations it was exposed to, and loses performance outside of this range, suggesting that failure modes such as in <ref type="figure" target="#fig_0">Fig. 3</ref> can be remedied with data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on HPatches [3] -Fig. 5</head><p>HPatches contains 116 scenes with 6 images each. These scenes are strictly planar, containing only viewpoint or illumination changes (not both), and use homographies as ground truth. Despite  <ref type="bibr" target="#b24">[25]</ref>. Bottom: DISK. We extract cycle-consistent matches with optimal parameters and feed them to DEGENSAC <ref type="bibr" target="#b6">[7]</ref>. We plot the resulting inliers, from green to yellow if they are correct (0 to 5 pixels in reprojection error), in red if they are incorrect (above 5), and in blue if ground truth depth is not available. Our approach can match many more points and produce more accurate poses. It can deal with large changes in scale (4th and 5th columns) but not in rotation (6th column), which is discussed further in section 4.1 and <ref type="figure">Fig. 6</ref>.  <ref type="bibr" target="#b24">[25]</ref>. Bottom: DISK. COLMAP is used to reconstruct the "London Bridge" scene with 25 images. We show three of them and draw their keypoints, in blue if they are registered by COLMAP, and red otherwise. Our method generates evenly distributed features, producing 76% more landmarks with 30% more observations per landmark than HardNet. Keypoints on water or trees have low scores and are rare among the top 2k features, but appear more often when taking 8k. This suggests that our method can reach near-optimal performance on a small budget.   <ref type="figure">Figure 5</ref>: Results on HPatches. On the left, we report Mean Matching Accuracy (MMA) at 10 pixel thresholds. On the right, we summarize MMA by its AUC, up to 5 pixels. Results for RFP <ref type="bibr" target="#b5">[6]</ref> were kindly provided by the authors, which explains why keypoint/match counts are missing.</p><p>its limitations, it is often used to evaluate low-level matching accuracy. We follow the evaluation methodology and source code from <ref type="bibr" target="#b10">[11]</ref>. The first image on every scene is matched to the remaining five, omitting 8 scenes with high-resolution images. Cyclic-consistent matches are computed, and performance is measured in terms of the Mean Matching Accuracy (MMA), i.e., the ratio of matches with a reprojection error below a threshold, from 1 to 10 pixels, and averaged across all image pairs.</p><p>We report MMA in <ref type="figure">Fig. 5</ref>, and summarize it by its Area under the Curve (AUC), up to 5 pixels. Baselines include RootSIFT <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref> on Hessian-Affine keypoints <ref type="bibr" target="#b23">[24]</ref>, a learned affine region detector (HAN) <ref type="bibr" target="#b25">[26]</ref> paired with HardNet++ descriptors <ref type="bibr" target="#b24">[25]</ref>, DELF <ref type="bibr" target="#b27">[28]</ref>, SuperPoint <ref type="bibr" target="#b9">[10]</ref>, D2-Net <ref type="bibr" target="#b10">[11]</ref>, R2D2 <ref type="bibr" target="#b30">[31]</ref>, and Reinforced Feature Points (RFP) <ref type="bibr" target="#b5">[6]</ref>. For D2-Net we include both single-(SS) and multi-scale (MS) models. We consider DISK with number of matches restricted to 2k and 8k, for a fair comparison with different methods.</p><p>We obtain state-of-the-art performance on this dataset, despite the fact that our models are trained on non-planar data without strong affine transformations. We use the same models and hyperparameters used in the previous section to obtain 2k and 8k features, without any tuning. Our method is #1 on the viewpoint scenes, followed by R2D2, and #2 on the illumination scenes, trailing DELF. Putting them together, it outperforms its closest competitor, RFP, by 12% relative. <ref type="table">Table 2</ref> This benchmark compiles statistics for large-scale SfM. We select three of the smaller scenes and report results in <ref type="table">Table 2</ref>. Baselines are taken from <ref type="bibr" target="#b5">[6]</ref> and include Root-SIFT <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b1">2]</ref>, SuperPoint <ref type="bibr" target="#b9">[10]</ref>, and Reinforced Feature Points <ref type="bibr" target="#b5">[6]</ref>. We obtain more landmarks than SIFT, with larger tracks and a comparable reprojection error. Note that this benchmark does not standardize the number of input features, so we extract DISK at full resolution and take the top ?12k keypoints in order to remain comparable with SIFT. By comparison, a run on "Fountain" with no cap yields 67k landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on the ETH-COLMAP benchmark [37] -</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation studies and discussion</head><p>Supervision without depth. As outlined in Sec. 3, we use the strongest supervision signal available to us, which are depth maps. Unfortunately, this means we only reward matches on areas with reliable depth estimates, which may cause biases. We also experimented with a variant of R that relies only on epipolar constraints, as in a recent paper <ref type="bibr" target="#b42">[43]</ref>. We evaluate both variants on the validation set of the Image Matching Challenge and report the results in <ref type="table" target="#tab_4">Table 3</ref>. Performance improves for multiview but decreases for stereo. Qualitatively, we observe that new keypoints appear on textureless areas outside object boundaries, probably due to the U-Net's large receptive field (see appendix). Nevertheless, this illustrates that DISK can be learned just as effectively with much weaker supervision.</p><p>Non-maximum suppression and grid size. The softmax-within-grid training time mechanism models the relative importance of features under a constrained budget, in a differentiable way. It can be replaced with an alternative solution, such as NMS, which we use at inference. In <ref type="table" target="#tab_5">Table 4</ref> we compare the training regime, where we sample at most one feature per grid cell, against the inference regime, where we apply NMS on the heatmap. We report results in terms of pose mAA on the validation set of the Image Matching Challenge in <ref type="table" target="#tab_5">Table 4</ref>. For this experiment we removed the budget limit and took all features provided by the model. This shows that this inference strategy is  <ref type="table">Table 2</ref>: Results on ETH-COLMAP <ref type="bibr" target="#b36">[37]</ref>. We compare Root-SIFT <ref type="bibr" target="#b19">[20]</ref>, SuperPoint <ref type="bibr" target="#b9">[10]</ref>, Reinforced Feature Points <ref type="bibr" target="#b5">[6]</ref>, and DISK. We report: (NL) number of landmarks, (TL) track length (average number of observations per landmark), and ( r ) reprojection error.  <ref type="figure">Figure 6</ref>: Rotation invariance vs. rotations in data. We report the ratio of correct matches between a reference images and their copies rotated by ?. Overlaid is a histogram of relative image rotations in IMC2020-val.    clearly beneficial, despite departing from the training pipeline. In <ref type="table" target="#tab_6">Table 5</ref> we show how mAA varies with grid size used for training. A smaller grid is beneficial in terms of performance but increases the number of extracted features, leading to larger distance matrices and higher computational expense.</p><p>Feature duplication at grid edges. Experimentally, we observe that 19.9% of features from grid selection (training) have a neighbour within 2 px, which likely corresponds to double detections. This has three potential downsides. (1) Compute/memory is increased, due to unnecessarily large matching matrices. (2) It rescales ? kp w.r.t. its intuitive meaning. Imagine that some detections are strictly duplicated: both forward and backward probabilities will "split in half", but the total probability of matching the two locations remains constant -this means that learning dynamics are not impacted, other than ? kp acting more strongly (on a larger number of detections). (3) In reality, detections are close by, instead of duplicated, which may make the algorithm less spatially precise: since duplication means a failure of the sparsity mechanism, we learn in a regime where imprecise correspondences are more common than at inference, favoring shift-invariance in the descriptors more than desired. The results DISK attains on HPatches, including at a 1-pixel error threshold, and the very low reprojection error on the ETH-COLMAP benchmark, suggest that these do not pose a significant problem for performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>We introduced a novel probabilistic approach to learn local features end to end with policy gradient. It can easily train from scratch, and yields many more matches than its competitors. We demonstrate state-of-the-art results in pose accuracy for stereo and 3D reconstruction, placing #1 in the 2kkeypoints category of the Image Matching Challenge using off-the-shelf matchers. In future work we intend to replace the match relaxation introduced in Sec. 3, with learned matchers such as <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader impact</head><p>There already are many applications that rely on keypoints, and although our method has the potential to make them more effective, we do not expect new, specific issues arising from our research. As all technology, it can also be used unethically. In this instance, use in visually guided missiles or localizing photographs without user consent, further compromising privacy on the web, could be of concern. More generally, all automation of data processing brings disproportionately larger gains for established players with access to such data and resources, furthering the imbalance in global competitiveness, despite the nominal openness of the research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX -DISK: Learning local features with policy gradient</head><p>Supplementary material for NeurIPS submission 1194.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training data</head><p>In order to avoid image pairs which are not co-visible and ones which are too easy, we use a simple procedure. For every image I we access the set of their 3D keypoints {L} I , as provided in the COLMAP <ref type="bibr" target="#b35">[36]</ref> metadata for the dataset, and for each pair A, B we compute the ratio</p><formula xml:id="formula_2">r = |{L} A ? {L} B | min(|{L} A |, |{L} B |)</formula><p>which we use as a proxy for co-visibility, and pick all pairs A, B for which 0.15 ? r ? 0.8. In order to obtain the image triplets used during training, we randomly sample a "seed" image A and then two more images B, C among those with which A was paired, based on the ratio criterion described above. We do not enforce that B and C are co-visible with respect to the criterion. We perform this sampling until we obtain roughly 10k triplets per scene.</p><p>We manually blacklist scenes which overlap with the test subset of the Image Matching Challenge: '0024' ("British Museum"), '0021' ("Lincoln Memorial Statue"), '0025' ("London Bridge"), '1589' ("Mount Rushmore"), '0019' ("Sagrada Familia"), '0008' ("Piazza San Marco"), '0032' ("Florence Cathedral"), and '0063' ("Milan Cathedral"). We also blacklist scenes which overlap with the validation subset of the Image Matching Challenge: '0015' ("St. Peter's Square") and '0022' ("Sacre Coeur"), which we use for the purposes of validation and hyperparameter selection, as in <ref type="bibr" target="#b15">[16]</ref>. Finally, as per <ref type="bibr" target="#b11">[12]</ref>, we blacklist scenes with low quality depth maps: '0000', '0002', '0011', '0020', '0033', '0050', '0103', '0105', '0143', '0176', '0177', '0265', '0366', '0474', '0860', and '4541', as well as automatically remove scenes which produced less than 10k co-visible triplets.</p><p>In effect, we train on 135 scenes yielding ? 133k co-visible triplets. The dataset is available for download at at https://datasets.epfl.ch/disk-data/index.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous evaluation</head><p>With so many co-visible triplets a single iteration through the dataset (epoch) would take very long. In order to continuously evaluate performance of the model, we pause every 5k optimization steps (10k triplets) and evaluate stereo performance. To do so, we re-implement the mAA(10 o ) metric used by the benchmark <ref type="bibr" target="#b15">[16]</ref>, and apply it to a smaller subset of the validation set. We pick our best model according to this metric and then proceed with hyper-parameter tuning as described in Sec. 4.1. Our highest-performing model was obtained after 300k optimization steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Computational cost</head><p>Our code, implemented in PyTorch <ref type="bibr" target="#b29">[30]</ref>, is run on an NVIDIA V100 GPU, with F32 precision. At inference time we obtain ? 7 frames per second for 1024 ? 1024 input and training with 768 ? 768 input requires ? 1.2 seconds per two triplets. Our code release includes an option to reduce the memory requirements through gradient accumulation, allowing for training with 12 Gb GPUs.</p><p>Qualitative results for epipolar supervision - <ref type="figure">Fig. 7</ref> As outlined in Sec. 4.4, our models may be supervised with pixel-to-pixel correspondences in the form of depth maps, or with simple epipolar constraints. With the latter, points appear around 3D object boundaries, as illustrated in <ref type="figure">Fig. 7</ref>. For simplicity, the main paper focuses on models trained with depth-based supervision.</p><p>Breakdown by scene for the Image Matching Challenge <ref type="bibr" target="#b15">[16]</ref> - <ref type="table">Tables 6 and 7</ref> We break down out results per scene in <ref type="table">Table 6</ref>, for 2k features, and <ref type="table">Table 7</ref>, for 8k features. Values copied from the challenge leaderboards (submissions #708 and #709). Qualitative results: depth vs epipolar supervision. With depth-based supervision, our models learn to (usually) avoid textureless areas such as the sky. With epipolar-based supervision, points appear on the boundaries of 3D objects. They may or may not be matched: see for instance the obelisk on the rightmost images for (c) and (d). Thin structures, such as the lamp-posts on the leftmost images for (a) and (b), create features with epipolar supervision but not with depth supervision, presumably because they are typically absent in the depth maps. To illustrate this point we use the validation set from the Image Matching Challenge, following the same convention as in <ref type="figure" target="#fig_1">Fig. 4</ref>  <ref type="table">Table 6</ref>: Image Matching Challenge: Breakdown by scene (2k features). We report results for each of the 9 scenes, and their average.  <ref type="table">Table 7</ref>: Image Matching Challenge: Breakdown by scene (8k features). We report results for each of the 9 scenes, and their average.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Stereo results on the Image Matching Challenge (2k features). Top: DoG w/ Upright HardNet descriptors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Multiview results on the Image Matching Challenge (8k features). Top: DoG w/ Upright HardNet descriptors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) "Sacre Coeur" w/ depth-based supervision (b) "Sacre Coeur" w/ epipolar-based supervision (c) "Saint Peter's Square" w/ depth-based supervision (d) "Saint Peter's Square" w/ epipolar-based supervision Figure 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Multiview Method NM NI mAA(10 o ) NM NL TL mAA(10 o ) NM NI mAA(10 o ) NM NL TL mAA(10 o )</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Up to 2048 features/image</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Up to 8000 features/image</cell></row><row><cell cols="13">Task 1: stereo Task 2: Upright Root-SIFT Task 2: Multiview Task 1: stereo 194.0 112.3 0.3986 199.3 1341.7 4.09 0.5623 525.4 358.9 0.5075 542.9 4404.6 4.38</cell><cell>0.6792</cell></row><row><cell>Upright L2-Net</cell><cell cols="6">174.1 117.1 0.4192 179.8 1361.3 4.23</cell><cell>0.5968</cell><cell cols="2">657.3 435.7</cell><cell>0.5450</cell><cell cols="2">395.5 3603.8 4.38</cell><cell>0.6849</cell></row><row><cell>Upright HardNet</cell><cell cols="6">274.0 152.7 0.4609 201.3 1467.9 4.31</cell><cell>0.6354</cell><cell cols="2">791.7 527.6</cell><cell>0.5728</cell><cell cols="2">509.1 4250.4 4.55</cell><cell>0.7231</cell></row><row><cell>Upright GeoDesc</cell><cell cols="6">235.8 132.7 0.4136 161.1 1287.3 4.24</cell><cell>0.5837</cell><cell cols="2">598.9 409.9</cell><cell>0.5267</cell><cell cols="2">458.6 4146.8 4.41</cell><cell>0.7044</cell></row><row><cell>Upright SOSNet</cell><cell cols="6">265.6 171.2 0.4505 194.0 1442.3 4.31</cell><cell>0.6359</cell><cell cols="2">752.9 508.4</cell><cell>0.5738</cell><cell cols="2">464.4 3988.6 4.52</cell><cell>0.7129</cell></row><row><cell cols="7">Upright LogPolarDesc 296.8 162.2 0.4567 211.9 1553.4 4.33</cell><cell>0.6370</cell><cell cols="2">821.7 543.2</cell><cell>0.5510</cell><cell cols="2">505.4 4414.1 4.52</cell><cell>0.7109</cell></row><row><cell>SuperPoint</cell><cell cols="6">292.8 126.8 0.2964 169.3 1184.3 4.34</cell><cell>0.5464</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>LF-Net</cell><cell cols="6">191.1 106.5 0.2344 196.7 1385.0 4.14</cell><cell>0.5141</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>D2-Net (SS)</cell><cell cols="6">505.7 188.4 0.1813 513.1 2357.9 3.39</cell><cell cols="3">0.3943 1258.2 482.3</cell><cell cols="3">0.2228 1278.7 5893.8 3.62</cell><cell>0.4598</cell></row><row><cell>D2-Net (MS)</cell><cell cols="6">327.8 134.8 0.1355 337.6 2177.3 3.01</cell><cell cols="3">0.3007 1028.6 470.6</cell><cell cols="3">0.2506 1054.7 6759.3 3.39</cell><cell>0.4751</cell></row><row><cell>R2D2</cell><cell cols="6">273.6 213.9 0.3346 280.8 1228.4 4.29</cell><cell cols="3">0.6149 1408.8 842.2</cell><cell>0.4437</cell><cell cols="2">739.8 4432.9 4.59</cell><cell>0.6832</cell></row><row><cell>Submission #609</cell><cell cols="6">439.7 270.0 0.4690 280.4 1489.6 4.69</cell><cell>0.6812</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Submission #578</cell><cell cols="6">439.5 246.6 0.4542 331.6 1621.7 4.57</cell><cell>0.6741</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Submission #599</cell><cell cols="6">227.4 129.5 0.4507 176.6 1209.6 4.44</cell><cell>0.6609</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Submission #611</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">945.4 622.1</cell><cell>0.5887</cell><cell cols="2">899.1 6086.2 4.65</cell><cell>0.7513</cell></row><row><cell>Submission #613</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">934.9 624.1</cell><cell>0.5873</cell><cell cols="2">964.8 6350.7 4.64</cell><cell>0.7495</cell></row><row><cell>Submission #625</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">945.4 605.1</cell><cell>0.5878</cell><cell cols="2">899.1 6095.8 4.65</cell><cell>0.7485</cell></row><row><cell cols="7">DISK (#708 &amp; #709) 514.2 404.2 0.5132 527.5 2428.0 5.55</cell><cell cols="6">0.7271 1621.9 1238.5 0.5585 1663.8 7484.0 5.92</cell><cell>0.7502</cell></row><row><cell>? (%)</cell><cell cols="2">+1.7 +49.7</cell><cell>+9.4</cell><cell cols="3">+2.8 +3.0 +18.3</cell><cell>+6.7</cell><cell cols="2">+15.1 +47.1</cell><cell>-5.4</cell><cell cols="2">+30.1 +10.7 +27.3</cell><cell>-0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation: match supervision. We compare mAA on the Image Matching Challenge validation set, for DISK models learned with pixelto-pixel supervision or epipolar constraints.</figDesc><table><row><cell>Variant</cell><cell cols="4">Num. features matches mAA(10 0 ) mAA(10 0 ) Num. Stereo Multiview</cell></row><row><cell>1-per-cell</cell><cell>5456.8</cell><cell>796.5</cell><cell>0.74774</cell><cell>0.84685</cell></row><row><cell cols="2">NMS 3?3 8434.6</cell><cell>1699.9</cell><cell>0.77833</cell><cell>0.86864</cell></row><row><cell cols="2">NMS 5?5 7656.0</cell><cell>1547.9</cell><cell>0.77657</cell><cell>0.87622</cell></row><row><cell cols="2">NMS 7?7 6423.4</cell><cell>1271.1</cell><cell>0.77070</cell><cell>0.85642</cell></row><row><cell cols="2">NMS 9?9 4946.2</cell><cell>942.0</cell><cell>0.75558</cell><cell>0.85362</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation: NMS. We compare the feature selection strategy used for training (top) with NMS at inference time. Here we use all detected features, rather than subsample by score.</figDesc><table><row><cell>Grid</cell><cell>NMS</cell><cell>3?3</cell><cell>5?5</cell><cell>7?7</cell><cell>9?9</cell></row><row><cell>8?8</cell><cell></cell><cell cols="4">0.7751 0.7824 0.7778 0.7586</cell></row><row><cell>12?12</cell><cell></cell><cell cols="4">0.7576 0.7580 0.7502 0.7431</cell></row><row><cell>16?16</cell><cell></cell><cell cols="4">0.7213 0.7214 0.7120 0.6999</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation: NMS vs grid size. We show mAA vs. grid &amp; NMS size on IMC2020-val, capping the number of features to 2k.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Acknowledgement. This research was partially funded by Google's Visual Positioning System.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building Rome in One Day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2911" to="2918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hpatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SURF: Speeded Up Robust Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is there anything new to say about SIFT matching?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Bellavia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Colombo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reinforced feature points: Optimizing feature detection and description for a high-level task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brachmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two-View Geometry Estimation Unaffected by a Dominant Plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Matching features without descriptors: Implicitly matched interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titus</forename><surname>Cieslewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10681</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sips: Succinct interest points from unsupervised inlierness probability learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titus</forename><surname>Cieslewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">D2-Net: A Trainable CNN for Joint Detection and Description of Local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-View Optimization of Local Feature Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Dusmanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond Cartesian Representations for Local Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MatchNet: Unifying Feature and Metric Learning for Patch-Based Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reconstructing the World in Six Days</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heinly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schoenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image Matching across Wide Baselines: From Paper to Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhe</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasiia</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">net: Keypoint detection by handcrafted and learned cnn filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Barroso Laguna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Key</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Megadepth: Learning single-view depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2004-11-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contextdesc: Local Descriptor Augmentation with Cross-Modality Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geodesc: Learning Local Descriptors by Integrating Geometry Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale, real-time visual-inertial localization revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lynen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scale and Affine Invariant Interest Point Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Working Hard to Know Your Neighbor&apos;s Margins: Local Descriptor Learning Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Repeatability is Not Enough: Learning Affine Regions via Discriminability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Orb-Slam: A Versatile and Accurate Monocular Slam System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tard?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3456" to="3465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lf-Net: Learning Local Features from Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Enet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv Preprint</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
		<title level="m">R2D2: Repeatable and Reliable Detector and Descriptor. In arXiv Preprint</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Superglue</surname></persName>
		</author>
		<title level="m">Learning Feature Matching with Graph Neural Networks. Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding the limitations of CNN-based absolute camera pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qunjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3302" to="3312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quad-Networks: Unsupervised Learning to Rank for Interest Point Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structure-From-Motion Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Comparative Evaluation of Hand-Crafted and Learned Local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discriminative Learning of Deep Convolutional Feature Point Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">L2-Net: Deep Learning of Discriminative Patch Descriptor in Euclidean Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SOSNet: Second order similarity regularization for local descriptor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huub</forename><surname>Heijnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Glampoints: Greedily learned accurate match points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Apostolopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stucky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ciller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. De</forename><surname>Zanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">TILDE: A Temporally Invariant Learned DEtector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning feature descriptors using camera pose supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">LIFT: Learned Invariant Feature Transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to Find Good Correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to Assign Orientations to Feature Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Verdie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to Compare Image Patches via Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning two-view correspondences and geometry using order-aware network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongen</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5845" to="5854" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
