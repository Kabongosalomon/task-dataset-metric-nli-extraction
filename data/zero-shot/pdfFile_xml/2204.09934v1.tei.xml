<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">W Y</forename><surname>Lam</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Su</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<email>zhouzhao@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hindered their applications to speech synthesis. This paper proposes FastDiff, a fast conditional diffusion model for high-quality speech synthesis. FastDiff employs a stack of time-aware location-variable convolutions of diverse receptive field patterns to efficiently model long-term time dependencies with adaptive conditions. A noise schedule predictor is also adopted to reduce the sampling steps without sacrificing the generation quality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer, FastDiff-TTS, which generates high-fidelity speech waveforms without any intermediate feature (e.g., Mel-spectrogram). Our evaluation of Fast-Diff demonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech samples. Also, FastDiff enables a sampling speed of 58x faster than real-time on a V100 GPU, making diffusion models practically applicable to speech synthesis deployment for the first time. We further show that FastDiff generalized well to the mel-spectrogram inversion of unseen speakers, and FastDiff-TTS outperformed other competing methods in end-toend text-to-speech synthesis. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the recent development of deep generative models, speech synthesis has seen an extraordinary progress. Among the conventional speech synthesis methods, WaveNets <ref type="bibr" target="#b1">[Oord et al., 2016]</ref> were demonstrated to generate high-fidelity audio samples in an autoregressive manner yet suffering from prohibitively expensive computational costs. In contrast, non-autoregressive approaches such as flow-based and GAN-based models <ref type="bibr" target="#b1">[Prenger et al., 2019;</ref><ref type="bibr" target="#b1">Jang et al., 2021</ref>; * Work done during internship at Tencent AI Lab ? Equal contribution ? Corresponding author 1 Audio samples are available at https://FastDiff.github.io/. <ref type="bibr" target="#b1">Kong et al., 2020a;</ref><ref type="bibr" target="#b1">Huang et al., 2021]</ref> were also proposed to generate speech audios with satisfactory speed. However, these models were still criticized for other problems, e.g., the limited sample quality or sample diversity <ref type="bibr" target="#b7">[Xiao et al., 2021]</ref>. In speech synthesis, our goal is mainly two-fold:</p><p>? High-quality: generating high-quality speech is a challenging problem especially when the sampling rate of an audio is high. It is vital to reconstruct details at different timescales for waveforms of highly variable patterns.</p><p>? Fast: high generation speed is essential when considering real-time speech synthesis. This poses a challenge for all high-quality neural synthesizers.</p><p>As a blossoming class of generative models, denoising diffusion probabilistic models (DDPMs) <ref type="bibr" target="#b1">[Ho et al., 2020;</ref><ref type="bibr">Song et al., 2020a;</ref><ref type="bibr" target="#b1">Lam et al., 2022;</ref><ref type="bibr" target="#b1">Liu et al., 2022]</ref> has emerged to prove its capability to achieve leading performances in both image and audio syntheses <ref type="bibr" target="#b1">[Dhariwal and Nichol, 2021;</ref><ref type="bibr" target="#b4">San-Roman et al., 2021;</ref><ref type="bibr" target="#b1">Kong et al., 2020b;</ref><ref type="bibr" target="#b1">Lam et al., 2022]</ref>. However, current development of DDPMs in speech synthesis was hampered by two major challenges:</p><p>? Different from other existing generative models, diffusion models are not trained to directly minimize the difference between the generated audio and the reference audio, but to de-noise a noisy sample given an optimal gradient. This in practice could lead to overly de-noised speech after a large number of sampling steps, in which natural voice characteristics including breathiness and vocal fold closure are removed.</p><p>? While DDPMs inherently are gradient-based models, a guarantee of high sample quality typically comes at a cost of hundreds to thousands of de-noising steps. When reducing the sampling steps, an apparent degradation in quality due to perceivable background noise is observed.</p><p>In this work, we propose FastDiff, a fast conditional diffusion model for high-quality speech synthesis. To improve audio quality, FastDiff adopts a stack of time-aware locationvariable convolutions of diverse receptive field patterns to efficiently model long-term time dependencies with adaptive conditions. To accelerate the inference procedure, FastDiff also includes a noise schedule predictor, which derives a short and effective noise schedule and significantly reduces the de-noising steps. Based on FastDiff, we also introduce an end-toend phoneme-to-waveform synthesizer FastDiff-TTS, which simplifies the text-to-speech generation pipeline and does not require intermediate features or specialized loss functions to enjoy low inference latency.</p><p>Experimental results demonstrated that FastDiff achieved a higher MOS score than the best publicly available models and outperformed the strong WaveNet vocoder <ref type="bibr">(MOS: 4.28 vs. 4.20)</ref>. FastDiff further enjoys an effective sampling process and only needs 4 iterations to synthesize high-fidelity speech, 58x faster than real-time on a V100 GPU without engineered kernels. To the best of our knowledge, FastDiff is the first diffusion model with a sampling speed comparable to previous for the first time applicable to interactive, realworld speech synthesis applications at a low computational cost. FastDiff-TTS successfully simplify the text-to-speech generation pipeline and outperform competing architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Denoising Diffusion Probabilistic Models</head><p>Denoising diffusion probabilistic models (DDPMs) <ref type="bibr" target="#b1">[Ho et al., 2020;</ref><ref type="bibr">Song et al., 2020a;</ref><ref type="bibr" target="#b1">Lam et al., 2022]</ref> are likelihoodbased generative models that have recently succeeded to advance the state-of-the-art results in benchmark generative tasks <ref type="bibr" target="#b1">[Dhariwal and Nichol, 2021]</ref> and have proved its capability to produce high-quality samples. The basic idea of DDPMs is to train a gradient neural network for reversing a diffusion process. Given i</p><formula xml:id="formula_0">.i.d. samples {x 0 ? R D } from an unknown data distribution p data (x 0 ), DDPMs try to ap- proximate p data (x 0 ) by a marginal distribution p ? (x 0 ) = ? ? ? p(x T ) T t=1 p ? (x t?1 |x t )dx 1 . . . dx T .</formula><p>In data distribution as q(x 0 ), the diffusion process is defined by a fixed Markov chain from data x 0 to the latent variable x T . For a small positive constant ? t , a small Gaussian noise is added from x t to the distribution of x t?1 under the function of q(x t |x t?1 ). The whole process gradually converts data x 0 to whitened latents x T according to the fixed noise schedule ? 1 , ? ? ? , ? T . The reverse process is a Markov chain from x T to x 0 parameterized by a shared ?, which aims to recover samples from Gaussian noises though eliminating the Gaussian noise added in the diffusion process in each iteration.</p><p>It has been demonstrated that diffusion probabilistic models <ref type="bibr" target="#b1">[Dhariwal and Nichol, 2021;</ref><ref type="bibr" target="#b7">Xiao et al., 2021]</ref> can learn diverse data distribution in multiple domains, such as images and time series. While the main issue with the proposed neural diffusion process is that it requires up to thousands of iterative steps to reconstruct the target distribution during reverse sampling. In this work, we offer a fast conditional diffusion model to reduce reverse iterations and improve computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FastDiff</head><p>This section presents our proposed FastDiff, a fast conditional diffusion model for high-quality speech synthesis. We first describe the motivation of the design in FastDiff. Secondly, we introduce the iterative refinement model ? for high-quality speech synthesis and the noise predictor ? for accelerated sampling. Furthermore, we describe the training and inference procedures in detail. At last, we extend FastDiff to FastDiff-TTS for fully end-to-end text-to-speech syntheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>While denoising diffusion probabilistic models have shown high potential in synthesizing high-quality speech samples <ref type="bibr" target="#b1">Kong et al., 2020b;</ref>, several challenges remain for industrial deployment: 1) Different from the traditional generative models, diffusion models catch dynamic dependencies from noisy audio instead of clean ones, which introduce more variation information (i.e, noise levels) in addition to the spectrogram fluctuation. 2) With limited receptive field patterns, a distinct degradation could emerge when reducing the reverse iterations, making diffusion models difficult to get accelerated. As a result, hundred or thousand orders of iterations prevent existing diffusion models from real-world deployment.</p><p>In FastDiff, we propose two key components to complement the above issues: 1) FastDiff adopts a time-aware location-variable convolution to catch the details of noisy samples at dynamic dependencies. The convolution operations are conditioned on dynamic variations in speech including diffusion steps and spectrogram fluctuations, equipping the model with diverse receptive field patterns and promoting the robustness of diffusion models during reverse acceleration. 2) To accelerate the inference procedure, FastDiff adopts a noise schedule predictor to reduce the number of reverse iterations, frees diffusion models from hundreds or thousands of refinement iterations. This makes FastDiff for the first time applicable to interactive, real-world applications at a low computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Time-Aware Location-Variable Convolution</head><p>In comparison with traditional convolution networks, location-variable convolution <ref type="bibr" target="#b7">[Zeng et al., 2021]</ref> shows efficiency in modeling the long-term dependency of audio and gets neural network free from a significant number of dilated convolution layers. Inspired by this, we introduce the Time-Aware Location-Variable Convolution, which is sensitive to time steps in diffusion probabilistic models. At time step t, we follow <ref type="bibr" target="#b6">[Vaswani et al., 2017]</ref> to embed the step index into an 128-dimensional positional encoding (PE) vector e t : e t = sin 10 0?4 63 t , ? ? ? , sin 10 63?4 63 t , cos 10 0?4 63 t , ? ? ? , cos 10 63?4 63 t ,</p><p>In time-aware location-variable convolution, FastDiff requires multiple predicted variation-sensitive kernels to perform convolutional operations on the associated intervals of input sequence. These kernels should be time-aware and sensitive to variations of noisy audio including diffusion steps and acoustic features (i.e., Mel-spectrogram). Therefore, we propose a time-aware location-variable convolution (LVC) module, which is coupled with a kernel predictor as shown in <ref type="figure" target="#fig_1">Figure 1</ref>  For the q-th time-aware LVC layer, we split the input</p><formula xml:id="formula_1">x t ? R D using a M -length window with 3 q dilations to produce K segments with each x k t ? R M : {x 1 t , . . . , x K t } = split(x t ; M, q)<label>(1)</label></formula><p>Next, we perform convolutional operations on the associated intervals of input sequence using the kernels generated by a kernel predictor ?:</p><formula xml:id="formula_2">{F t , G t } = ?(t, c) (2) z k t = tanh(F t * x k t ) ?(G t * x k t ) (3) z t = concat({z 1 t , . . . , z K t }),<label>(4)</label></formula><p>where F t , G t denote the filter and the gate kernels for x i t , respectively, * denotes the 1d convolution, denotes the element-wise product and concat(?) denotes the concatenation between vectors. Since the time-aware kernels are adaptive to the noise-level and dependent to the acoustic features, FastDiff is capable of precisely estimating de-noising gradient with a superior speed given a noisy signal input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Accelerated Sampling Noise Predictor</head><p>To avoid sampling with hundreds to thousands steps, FastDiff adopts the noise scheduling algorithm in the bilateral denoising diffusion models (BDDMs) <ref type="bibr" target="#b1">[Lam et al., 2022]</ref> to predict a sampling noise schedule much shorter than the noise schedule used in training. This scheduling method has been revealed to be superior than other sampling acceleration methods, e.g., the grid search algorithm in WaveGrad  and the fast sampling algorithm in DiffWave <ref type="bibr" target="#b1">[Kong et al., 2020b]</ref>. The noise predictor iteratively derives a continuous noise schedule? ? R Tm . We attach the learning objective and corresponding likelihood in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Schedule Alignment</head><p>In FastDiff, similar to DDPMs, during training we use T = 1000 discrete time steps. Therefore, when needed to condition on t during sampling, we also need to approximate T m discrete time indices by aligning the T m -step sampling noise schedule? to the T -step training noise schedule ?, with N &lt;&lt; T . We have attached the detailed algorithms in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training, Noise Scheduling and Sampling</head><p>All illustrated in Algorithm 1, we separately parameterize FastDiff by two modules: 1) a iterative refinement model ? that minimizes a variational bound of the score function, and 2) a noise predictor ? that optimizes the noise schedule for a tighter evidence lower bound. For inference, we first derive the tighter and more efficient noise schedules? via an one-shot noise scheduling procedure, which makes Fast-Diff achieve orders of magnitude faster at sampling. It has been demonstrated <ref type="bibr" target="#b1">[Lam et al., 2022]</ref> that the noise schedule searched for as few as 1 sample could be robust enough to maintain a high-quality generation among all samples in testing set. Secondly, we map the continuous noise schedules to discrete time indexes T m using schedule alignment. Finally, FastDiff iteratively refines gaussian noise to generate high-quality samples with computational efficiency. The detailed information on training, noise scheduling and inference procedures has been presented in Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">FastDiff-TTS</head><p>Existing text-to-speech methods usually adopt a two-stage pipeline: 1) A text-to-spectrogram generation module (a.k.a. acoustic model) aims to generate prosodic attributes according to variance prediction; 2) A conditional waveform generation module (a.k.a. vocoder) adds the phase information and synthesizes a detailed waveform. To further simplify the text-to-speech synthesis pipeline, we propose a fully end-to-end model FastDiff-TTS, which does not require intermediate features or specialized loss functions. FastDiff-TTS is designed to be a fully differentiable and efficient architecture that directly produces waveforms from contexts (e.g. phonemes) without needing to generate acoustic features (e.g., Mel-spectrograms) explicitly. </p><formula xml:id="formula_3">Sample x 0 ? q data , ? N (0, I), and t ? Unif({1, ? ? ? , T }) 4: x t = ? t x 0 + 1 ? ? 2 t 5: Take gradient descent steps on ? ? ? ? (x t |c, t) 2 2 6: until refinement model ? converged Algorithm 2 Training noise predictor ? 1: Input: Pre-defined discrete ?, trained refinement net- work ?, hyperparameter ? . 2: repeat 3: Sample x 0 ? q data , ? N (0, I), and t ? Unif({?, ? ? ? , T ? ? }) 4: x t = ? t x 0 + 1 ? ? 2 t 5:? t = min 1 ? ? 2 t , 1 ? ? 2 t+? ? 2 t ?(x t ) 6: Take gradient descent steps on ? ? ? 2 t 2(? 2 t ??t) ?? t ? 2 t ? (x t |c, t) 2 2 7: until noise predictor ? converged Architecture</formula><p>The architecture design of FastDiff-TTS refers to a convectional non-autoregressive text-to-speech model -FastSpeech 2 <ref type="bibr" target="#b1">[Ren et al., 2020]</ref> as the backbone. The architecture of FastDiff-TTS is illustrated in <ref type="figure" target="#fig_1">Figure 1(d)</ref>. In FastDiff-TTS, the encoder first converts the phoneme embedding sequence into the phoneme hidden sequence. Then, the duration predictor expands the encoder output to match the length of the desired waveform output. Given the aligned sequence, the variance adaptor adds pitch information into the hidden sequence. Note that it is difficult to use the full audio corresponding to the full text sequence for training due to the typically high sampling rate for high-fidelity waveform (i.e., 24,000 samples per second) and the limited GPU memory. Therefore, we sample a small segment to synthesize the waveform before passing to the FastDiff model. Finally, the FastDiff model decodes the adapted hidden sequence into a speech waveform as in the vocoder task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Loss</head><p>FastDiff-TTS does not require specialized loss functions and adversarial training to improve sample quality as suggested by the previous works <ref type="bibr" target="#b1">[Ren et al., 2020;</ref><ref type="bibr" target="#b1">Donahue et al., 2020;</ref><ref type="bibr" target="#b1">Kim et al., 2021]</ref>. This, to a large extend, simplifies the textto-speech generation. The final training loss consists of the following terms: 1) a duration prediction loss L dur : the mean squared error between the predicted and the ground-truth word-level duration in log-scale, 2) a diffusion loss L diff : the mean squared error between the estimated and gaussian noise, and 3) a pitch reconstruction loss L pitch : the mean squared error between the predicted and the ground-truth pitch sequences. We empirically found that the pitch reconstruction loss L pitch is helpful for handling the one-to-many mapping  </p><formula xml:id="formula_4">Sample x t?1 ? p ? (x t?1 |x t ;?) 6: end for 7: return x 0 Diffusion Process ! ( "#$ | " ) ? ? Denoising Process ( " | "#$ ) "#$ " &amp;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Works</head><p>Text-to-speech (TTS) systems aim to synthesize raw speech waveforms from given text. In recent years, Neural network based TTS <ref type="bibr" target="#b1">[Ren et al., 2020;</ref><ref type="bibr" target="#b1">Kim et al., 2020;</ref> has made huge progress and attracted a lot of attention in the machine learning and speech community. Neural vocoder plays the most important role in the recent success of speech synthesis, which require diverse receptive field patterns to catch audio dependencies: 1) autoregressive model WaveNet <ref type="bibr" target="#b1">[Oord et al., 2016]</ref>  Another important line of work covers directly waveform generation from text: FastSpeech 2s <ref type="bibr" target="#b1">[Ren et al., 2020]</ref> and VITS <ref type="bibr" target="#b1">[Kim et al., 2021]</ref> adopt adversarial training process and spectral losses for improving audio quality, while they do not take full advantage of end-to-end training. Recently proposed WaveGrad 2  estimates the gradient of the log conditional density of the waveform given a phoneme sequence, but suffers from a large model footprint and slow inference. Unlike all of the aforementioned methods, as highlighted in section 3.5, FastDiff-TTS is a fully differentiable and efficient architecture that produces waveforms directly without generating middle features (e.g., spectrograms) explicitly. In additional, our diffuion probabilistic model gets free from hundred or thousands of iterations and enjoy computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup Dataset</head><p>For a fair and reproducible comparison against other competing methods, we used the benchmark LJSpeech dataset <ref type="bibr" target="#b1">[Ito, 2017]</ref>. LJSpeech consists of 13,100 audio clips of 22050 Hz from a Female speaker with about 24 hours in total. To evaluate the generalization ability of our model over unseen speakers in multi-speaker scenarios, we also used the VCTK dataset <ref type="bibr" target="#b6">[Yamagishi et al., 2019]</ref>, which was downsampled to 22050 Hz to match the sampling rate with the LJSpeech datset. VCTK consists of approximately 44,200 audio clips uttered by 109 native English speakers with various accents. For both datasets, we used 80-band Mel-spectrograms as the condition for the vocoding task. The FFT size, window size, and hop size were, respectively, set to 1024, 1024, and 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configurations</head><p>FastDiff mainly consists of the refinement model ? and noise schedule predictor ?. The refinement model ? comprises three Diffusion-UBlock and DBlock with the upsample or downsample rate of <ref type="bibr">[8,</ref><ref type="bibr">8,</ref><ref type="bibr">4]</ref>, respectively. We adopt a lightweight GALR network effective in separating the added gaussian noise from audio as the noise schedule predictor ?. For end-to-end text-to-speech generation, FastDiff-TTS follows the basic structure in FastSpeech 2 <ref type="bibr" target="#b1">[Ren et al., 2020]</ref>, which consists of 4 feed-forward transformer blocks in the phoneme encoder. More details have been attached in the Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training and Evaluation</head><p>The complete training pipeline has been illustrated in Algorithm 1: FastDiff was trained with constant learning rate lr = 2 ? 10 ?4 . The refinement model ? and noise predictor ? were trained for 1M and 10K steps until convergence, respectively. FastDiff-TTS was trained until 500k steps using the AdamW optimizer with ? 1 = 0.9, ? 2 = 0.98, = 10 ?9 . Both models were trained on 4 NVIDIA V100 GPUs using random short audio clips of 16,000 samples from each utterance with a batch size of 16 each GPU. More details have been attached in the Appendix C.</p><p>We crowd-sourced 5-scale MOS tests via Amazon Mechanical Turk to evaluate the audio quality. The MOS scores were recorded with 95% confidence intervals (CI). Raters listened to the test samples randomly, where they were allowed to evaluate each audio sample once. We further include additional objective evaluation metrics including STOI and PESQ to test sample quality. To evaluate the sampling speed, we implemented real-time factor (RTF) accessment on a single NVIDIA V100 GPU. In addition, we employed two metrics NDB and JSD to explore the diversity of generated melspectrograms. More information about both objective and subjective evaluation has been attached in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparsion with other models</head><p>We compared our FastDiff in audio quality, diversity, and sampling speed with competing models, including 1) WaveNet <ref type="bibr" target="#b1">[Oord et al., 2016]</ref>, the autoregressive generative model for raw audio. In terms of audio quality, FastDiff achieved the highest MOS with a gap of 0.24 compared to the ground truth audio, and it matched the performance of the autoregressive WaveNet baseline and outperformed the non-autoregressive baselines. For objective evaluation, FastDiff also demonstrated a large improvement in PESQ and STOI. For inference speed, with the efficient noise schedules searched by noise predictor, FastDiff could generate high-quality speech samples within as few as 4 reverse steps, significantly reducing the inference time compared with competing diffusion architectures. To the best of our knowledge, FastDiff makes diffusion models for the first time applicable to interactive, high-quality real-world speech synthesis at a low computational cost. In terms of sample diversity, we can see that Fast-Diff still witnessed a gap from autoregressive WaveNet, but it achieve a higher variety for generated speeches than nonautoregressive baselines. More detailed evaluation on sample diversity has been attached in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation study</head><p>We conducted ablation studies to demonstrate the effectiveness of several designs in FastDiff, including the time-aware location variable convolution and noise predictor in neural vocoding. The results of both subjective and objective evaluations have been presented in <ref type="table" target="#tab_4">Table 3</ref>, and we have the following observations: 1) Replacing time-aware location-variable convolution by traditional convolutional operations causes a distinct degradation in sampling speed and perceptual quality. 2) Using grid search instead of the noise predictor to search schedules had witnessed the decreased audio quality, demonstrating that the noise schedule prediction process provides more efficient reverse sampling without sacrificing quality.</p><p>Further, we compare two variants of FastDiff to test the modality differences of diffusion condition (i.e., continuous noise-level or discrete time-step). Note that the former model    does not require the schedule alignment process mentioned in Section 3.3. We empirically find that the FastDiff model conditioned on discrete time steps could synthesize samples with higher quality, demonstrating that learning proposed FastDiff with discrete diffusion times could be a better choice. More information on the variant of FastDiff extended to continuous noise schedules has been attached in Appendix E</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Generalization to unseen speakers</head><p>We used 50 randomly selected utterances of 5 unseen speakers in the VCTK dataset that were excluded from the training set for the MOS test. <ref type="table" target="#tab_3">Table 2</ref> shows the experimental results for the mel-spectrogram inversion of the unseen speakers: In summary, we noticed that FastDiff achieved state-of-the-art in terms of audio quality for out-of-domain generalization, indicating that FastDiff could universally generate high-fidelity audio from entirely new (unseen) speakers outside the train set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">End-to-End Text-to-Speech</head><p>To demonstrate the robustness of the proposed model in end-to-end text-to-speech synthesis, we compare FastDiff-TTS with other neural TTS systems, including 1) GT, the ground truth audio; 2) GT (voc.), where we first convert the ground truth audio into mel-spectrograms, and then convert the mel-spectrograms back to audio using FastDiff; 3) Por-taSpeech  + FastDiff: vocoder cascaded with mel-spectrogram generation using the most popular nonautoregressive TTS models; 4) FastSpeech 2s <ref type="bibr" target="#b1">[Ren et al., 2020]</ref>: the extension of FastSpeech 2 to fully end-to-end textto-waveform generation with multi-task learning; 5) Wave-Grad 2 : a diffusion probabilistic model to generate waveforms via gradient estimation. The results are shown in <ref type="table" target="#tab_5">Table 4</ref>: FastDiff-TTS could surpass competing end-to-end speech synthesis models and match the voice quality of the state-of-the-art cascaded TTS systems, demonstrating that FastDiff-TTS is efficient in simplifying the overall text-to-speech synthesis pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we proposed FastDiff, a fast conditional diffusion model for high-quality speech synthesis. FastDiff employed a stack of time-aware location-variable convolutions with diverse receptive field patterns to model long-term time dependencies with adaptive conditions. A noise predictor was further adopted to derive tighter schedules for reducing reverse iterations without distinct quality degradation. The extension model FastDiff-TTS discarded intermediate features (e.g., spectrograms) and simplified the end-to-end text-towaveform syntheses pipeline. Experimental results demonstrated that our proposed model outperformed the best publicly available models in terms of synthesis quality, even comparable to the human level. Moreover, FastDiff showed a significant improvement in synthesis speed, which required as few as 4 iterations to generate high-quality samples. To the best of our knowledge, FastDiff made diffusion models for the first time applicable to interactive, real-world speech generation with a low computational cost. In addition, Fast-Diff performed strong robustness and enjoyed high-quality synthesis in out-of-domain generalization to unseen speakers. We will release our code and pre-trained models in the future, and we envisage that our work could serve as a basis for future speech synthesis studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Diffusion Probabilistic models</head><p>Given i.i.d. samples {x 0 ? R D } from an unknown data distribution p data (x 0 ). In this section, we introduce the theory of diffusion probabilistic model <ref type="bibr" target="#b1">[Ho et al., 2020;</ref><ref type="bibr" target="#b1">Lam et al., 2022;</ref><ref type="bibr">Song et al., 2020a;</ref><ref type="bibr" target="#b6">Song et al., 2020b]</ref>. First, we present diffusion and reverse process given by denoising diffusion probabilistic models (DDPMs), which could be used to learn a model distribution p ? (x 0 ) that approximates p data (x 0 ). Secondly, we introduce the recently proposed bilateral denoising diffusion models (BDDMs) and its tighter evidence lower bound (ELBO) for acceleration. Diffusion process Similar as previous work <ref type="bibr" target="#b1">[Ho et al., 2020;</ref><ref type="bibr" target="#b1">Lam et al., 2022;</ref><ref type="bibr">Song et al., 2020a]</ref>, we define the data distribution as q(x 0 ). The diffusion process is defined by a fixed Markov chain from data x 0 to the latent variable x T :</p><formula xml:id="formula_5">q(x 1 , ? ? ? , x T |x 0 ) = T t=1 q(x t |x t?1 ),<label>(5)</label></formula><p>For a small positive constant ? t , a small Gaussian noise is added from x t to the distribution of x t?1 under the function of q(x t |x t?1 ).</p><p>The whole process gradually converts data x 0 to whitened latents x T according to the fixed noise schedule ? 1 , ? ? ? , ? T .</p><formula xml:id="formula_6">q(x t |x t?1 ) := N (x t ; 1 ? ? t x t?1 , ? t I)<label>(6)</label></formula><p>Efficient training is optimizing a random term of t with stochastic gradient descent:</p><formula xml:id="formula_7">L ? = ? ? t x 0 + 1 ? ? 2 t ? 2 2</formula><p>, ? N (0, I) (7)</p><p>Reverse process Unlike the diffusion process, reverse process is to recover samples from Gaussian noises. The reverse process is a Markov chain from x T to x 0 parameterized by shared ?:</p><formula xml:id="formula_8">p ? (x 0 , ? ? ? , x T ?1 |x T ) = T t=1 p ? (x t?1 |x t ),<label>(8)</label></formula><p>where each iteration eliminate the Gaussian noise added in the diffusion process:</p><formula xml:id="formula_9">p(x t |x t?1 ) := N (x t?1 ; ? ? (x t , t), ? ? (x t , t) 2 I) (9)</formula><p>Acceration Recently, Bilateral denoising diffusion models (BDDMs) <ref type="bibr" target="#b1">[Lam et al., 2022]</ref> demonstrates its tighter evidence lower bound (ELBO) for noise schedule prediction. Given a leaned diffusion network ?, a scheduling network ? could be applied in reducing the gap between the proposed surrogate objective. To be more specific, instead of using the fixed one in diffusion process, a much more efficient N-step noise schedule (i.e.,?) could be derived by the well-leaned noise scheduling network ?. The noise schedule could be applied in reverse process, making it possible to explicitly tradeoff between inference computation and output quality in one model.</p><p>For learning the noise schedule predictor ?, we apply the loss function as a KL divergence term between the forward and the reverse distribution:</p><formula xml:id="formula_10">L ? = 1 2(1?? t ?? 2 t ) 1 ? ? 2 t t ? ? t ? 1?? 2 t ? (x t , ? t ) 2 2 + C t (10) where C t = 1 4 log 1?? 2 t ?t + D 2 ( ?t 1?? 2 t ? 1)</formula><p>is a constant that can be ignored during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 FastDiff</head><p>As illustrated in <ref type="table" target="#tab_7">Table 5</ref>, we list the hyper-parameters of Fast-Diff. We further visualize the detailed architectures of the noise predictor and DBlock in the refinement model in <ref type="figure">Figure</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 FastDiff-TTS</head><p>In this section, we list the model hyper-parameters of FastDiff-TTS in <ref type="table" target="#tab_7">Table 5</ref>.  <ref type="table" target="#tab_7">Table 5</ref> Total Number of Parameters 40M  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Noise Scheduling</head><p>Our noise scheduling algorithm mainly follows the bilateral denoising diffusion models <ref type="bibr" target="#b1">[Lam et al., 2022]</ref>:</p><p>Algorithm 4 Noise scheduling process 1: Input: Pre-defined discrete ?, trained refinement network ?, hyperparameter N,? t ,? t . 2: for t = N, ? ? ? , 2 do 3:</p><formula xml:id="formula_11">Sample x t?1 ? p ? (x t?1 |x t ) 4:? t?1 =? t ? 1??t 5:? t?1 = min 1 ?? 2 t?1 ,? t ? (x t?1 ) 6: if? t?1 &lt; ? 1 then 7:</formula><p>return? t , . . . ,? N 8:</p><p>end if 9: end for 10: return? t , . . . ,? N</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Schedule Alignment</head><p>Here we search and interpolate ? s between two training noise constants l t and l t+1 , enforcing ? s to get closed to l t . In the end, we gain the well-mapped diffusion step t m :</p><p>Firstly we compute the corresponding constants respective to diffusion and reverse process:</p><formula xml:id="formula_12">l t = t i=1 1 ? ? i , ? s = s i=1 1 ?? i<label>(11)</label></formula><p>Here we search and interpolate ? s between two training noise constants l t and l t+1 , enforcing ? s to get closed to l t . In the end, we gain the well-mapped diffusion step t m :</p><formula xml:id="formula_13">t m = t + l t ? ? s l t ? l t+1 if ? s ? [ l t+1 , l t ].<label>(12)</label></formula><p>Where integer t represents a single pre-defined diffusion step, and s presents a single step of noise schedule obtained through the scheduling process. Given these two schedules mentioned above, we could conduct schedule alignment and derive the floating-point t m for much more efficient reverse sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Evaluation Matrix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 PESQ and STOI</head><p>Perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b3">[Rix et al., 2001]</ref> and The short-time objective intelligibility (STOI) <ref type="bibr" target="#b6">[Taal et al., 2010]</ref> assesses the denoising quality for speech enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 NDB and JSD</head><p>Number of Statistically-Different Bins (NDB) and Jensen-Shannon divergence (JSD). They measure diversity by 1) clustering the training data into several clusters, and 2) measuring how well the generated samples fit into those clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Details in MOS Evaluation</head><p>All our Mean Opinion Score (MOS) tests are crowd-sourced and conducted by native speakers. The scoring criteria has been included in <ref type="table" target="#tab_12">Table 8</ref> for completeness. The samples are presented and rated one at a time by the testers, each tester is asked to evaluate the subjective naturalness of a sentence on a 1-5 Likert scale. The screenshots of instructions for testers are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. We paid $8 to participants hourly and totally spent about $750 on participant compensation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Extension to Continuous Condition</head><p>Our ablation study extends FastDiff to be conditioned on continuous noise levels and compares it to the basic model with the discrete condition. To be more specific, the Fast-Diff model conditioned on continuous noise levels does not require an additional schedule alignment process, which has a separated training and sampling procedure:</p><p>Algorithm 5 Training refinement network ? (Continuous Condition) 1: Input: Pre-defined noise schedule l 2: repeat 3:</p><p>Sample x 0 ? q data , ? N (0, I), and t ? Unif({1, ? ? ? , T }) 4: ? s ? Uniform (? t?1 , ? t ) 5:</p><p>x s = ? s x 0 + ? s 6:</p><p>Take gradient descent steps on ? ? ? ? (x s , ? s ) 2 2 7: until iterative refinement model ? converged Algorithm 6 Sampling 1: Input: Pre-defined ?, T and? derived in noise scheduling process. 2: for t = T, ? ? ? , 1 do 3: Sample x t?1 ? p ? (x t?1 |x t ) 4: end for 5: return x 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Sample Diversity</head><p>Previous works <ref type="bibr" target="#b1">[Dhariwal and Nichol, 2021;</ref><ref type="bibr" target="#b7">Xiao et al., 2021]</ref> in the image generation task has demonstrated that diffusion probabilistic model outperforms GAN in sample diversity, while the comparison in the speech domain is relatively overlooked. Similarly, we can intuitively infer that diffusion probabilistic models are good at generating highfidelity diverse speech samples. To verify our hypothesis, we employed two metrics NDB and JSD to explore the diversity of generated mel-spectrograms. As shown in <ref type="table" target="#tab_0">Table 1</ref>, we can see that diffusion probabilistic model achieve a higher JSD and matching NDB score for generated speeches compare to GAN-based model, which is expected for the following reasons:</p><p>1) It is well-known that the mode collapse problem <ref type="bibr" target="#b0">[Creswell et al., 2018]</ref> appears in the dominated GANbased generative models, which leads to very similar output samples from a single or few modes of the distribution, especially in the strongly conditional generation task. 2) In contrast, diffusion probabilistic model is meant to reduce mode collapse compared to one-shot generation. It breaks the generation process into several conditional denoising diffusion steps in which each step is relatively simple to model. Thus, we expect our model to exhibit better training stability and mode coverage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) and Figure 1(c). We describe the overall calculations below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>The overall architecture for FastDiff and FastDiff-TTS. The refinement model ? in FastDiff takes noisy audio xt as input and computes ? (xt|c, t) conditioned on diffusion time index t and Mel-spectrogram c. We use LReLU to denote the leaky rectified linear unit, LVC to denote the location-variable convolution, FC to denote the fully-connected layer, and PE to denote the positional encoding operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Conditional Diffusion Model for Speech Synthesis issue in text-to-speech generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>requires causal convolutions layers and large filters to increase the receptive field while suffering from slow inference speed. 2) Flow-based generative models [Prenger et al., 2019] fully utilize modern parallel computing processors to broaden corresponding receptive fields and speed-up sampling, while they usually achieve a limited sample quality. 3) Generative adversarial networks (GANs) [Jang et al., 2021; Kong et al., 2020a] are one of the most dominant deep generative models in audio generation. UnivNet [Jang et al., 2021] has demonstrated its success in using local-variable convolution on different waveform intervals, and HIFI-GAN [Kong et al., 2020a] proposes multi-receptive field fusion (MRF) to model the periodic patterns matters. However, GAN-based models are often difficult to train, collapsing [Creswell et al., 2018] without carefully selected hyperparameters and regularizers, and showing less sample diversity. 4) Recently proposed diffusion models Diffwave [Kong et al., 2020b] and WaveGrad [Chen et al., 2020] could generate high-quality speech samples, while suffering from a distinct degradation when reducing reverse iterations, making diffusion models difficult to get accelerated. Different from vocoders mentioned above, FastDiff improves the robustness of conditional diffusion model by catching the details of noisy samples at dynamic dependencies, and reduces reverse iterations with predicted noise schedule. The proposed conditional diffuion model allows the high-quality speech synthesis with computational efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2) WaveGlow [Prenger et al., 2019], non-autoregressive flow-based model. 3) HIFI-GAN V1 [Kong et al., 2020a] and UnivNet [Jang et al., 2021], the most dominant and popular GAN-based models. 4) Diffwave [Kong et al., 2020b] and WaveGrad [Chen et al., 2020], recently proposed diffusion probabilistic models which achieve state-of-the-art in speech synthesis. For easy comparison, the results are compiled and presented in Table 1, and we have the following observations:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Screenshot of MOS testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>Training refinement network ? 1: Input: Pre-defined noise schedule ? 2: repeat</figDesc><table><row><cell>3:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Input: Searched? in noise scheduling process. 2: Compute discrete steps T m sequences via schedule alignment in Section 3.3. 3: Sample x Tm ? N (0, I) 4: for t = T m , ? ? ? , 1 do</figDesc><table><row><cell>Algorithm 3 Sampling</cell></row><row><cell>1: 5:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: Comparison with other</cell></row><row><cell>neural vocoders of synthesized ut-</cell></row><row><cell>terances for unseen speakers.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study results. Comparison of the effect of each component in terms of quality and synthesis speed.</figDesc><table><row><cell>Model</cell><cell>MOS</cell></row><row><cell>GT</cell><cell>4.52?0.09</cell></row><row><cell>GT(voc.)</cell><cell>4.28?0.07</cell></row><row><cell>Cascaded</cell><cell>4.13?0.07</cell></row><row><cell cols="2">FastSpeech 2s 3.94?0.06</cell></row><row><cell>WaveGrad 2</cell><cell>3.68?0.09</cell></row><row><cell>FastDiff-TTS</cell><cell>4.03?0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison with other text-tospeech models in terms of quality.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Architecture hyperparameters of FastDiff.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>The details of network architectures. Left: The GALR-block based noise predictor ?. Right: DBlock in FastDiff ?</figDesc><table><row><cell></cell><cell>Positional</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Encoding</cell><cell></cell><cell></cell><cell>+</cell></row><row><cell>Layer Norm</cell><cell cols="2">Multi-Head Self-Attention</cell><cell></cell><cell>3 ? 1 Conv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>LReLU</cell></row><row><cell></cell><cell>Attention Layers</cell><cell>Dropout</cell><cell>DownSample</cell><cell>3 ? 1 Conv</cell></row><row><cell>Layer Norm</cell><cell></cell><cell></cell><cell>1 ? 1 Conv</cell><cell>LReLU</cell></row><row><cell></cell><cell>Layer Norm</cell><cell></cell><cell></cell><cell>3 ? 1 Conv</cell></row><row><cell>FC</cell><cell></cell><cell>Layer Norm</cell><cell></cell><cell>LReLU</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DownSample</cell></row><row><cell>Bi-LSTM</cell><cell>Average</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a) Noise Predictor</cell><cell></cell><cell cols="2">(b) DBlock</cell></row><row><cell>Figure 3: Hyperparameter</cell><cell>FastDiff-TTS</cell><cell></cell><cell></cell></row><row><cell>Phoneme Embedding Dimension</cell><cell>256</cell><cell></cell><cell></cell></row><row><cell>Pre-net Layers</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell>Pre-net Hidden</cell><cell>256</cell><cell></cell><cell></cell></row><row><cell>Encoder Layers</cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>Encoder Hidden</cell><cell>256</cell><cell></cell><cell></cell></row><row><cell>Encoder Conv1D Kernel</cell><cell>9</cell><cell></cell><cell></cell></row><row><cell>Encoder Conv1D Filter Size</cell><cell>1024</cell><cell></cell><cell></cell></row><row><cell>Encoder Attention Heads</cell><cell>2</cell><cell></cell><cell></cell></row><row><cell>Encoder/Decoder Dropout</cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell>Variance Predictor Conv1D Kernel</cell><cell>3</cell><cell></cell><cell></cell></row><row><cell cols="2">Variance Predictor Conv1D Filter Size 256</cell><cell></cell><cell></cell></row><row><cell>Variance Predictor Dropout</cell><cell>0.5</cell><cell></cell><cell></cell></row><row><cell>FastDiff wave decoder</cell><cell>Follow</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Architecture hyperparameters of FastDiff-TTS.</figDesc><table><row><cell>C Training, Noise scheduling and Inference</cell></row><row><cell>details</cell></row><row><cell>C.1 Diffusion hyperparameters</cell></row><row><cell>We list the diffusion hyper-parameters of FastDiff and</cell></row><row><cell>FastDiff/FastDiff-TTS in Table 7.</cell></row><row><cell>Diffusion Hyperparameter</cell></row><row><cell>Noise Scheduling</cell></row><row><cell>? = 200,?t = 0.54,?t = 0.70, N = 4</cell></row><row><cell>Training and Sampling</cell></row><row><cell>Pre-defined (T = 1000):</cell></row><row><cell>? = Linear(1 ? 10 ?4 , 0.005, 1000)</cell></row><row><cell>Grid Search (Tm = 4) derived:</cell></row><row><cell>? = [3.6701e ?7 , 1.7032e ?5 , 7.908e ?4 , 7.6146e ?1 ]</cell></row><row><cell>Noise Predictor (Tm = 4) derived:</cell></row><row><cell>? = [3.2176e ?4 , 2.5743e ?3 , 2.5376e ?2 , 7.0414e ?1 ]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Diffusion hyperparameters of FastDiff and FastDiff-TTS.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Ratings that have been used in evaluation of speech naturalness of synthetic and ground truth samples.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wavegrad 2: Iterative refinement for textto-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR<address><addrLine>Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Wavegrad: Estimating gradients for waveform generation. and Anil A Bharath. Generative adversarial networks: An overview</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Univnet: A neural vocoder with multi-resolution spectrogram discriminators for highfidelity waveform generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Donahue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09778</idno>
	</analytic>
	<monogr>
		<title level="m">Pseudo numerical methods for diffusion models on manifolds</title>
		<editor>Liu et al., 2022] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao</editor>
		<meeting><address><addrLine>Sander Dieleman; Alex Graves</addrLine></address></meeting>
		<imprint>
			<publisher>Heiga Zen, Karen Simonyan</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3945" to="3954" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proc. of ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Portaspeech: Portable and high-quality generative text-tospeech</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>San-Roman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>A short-time objective intelligibility measure for time-frequency weighted noisy speech. et al. Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lvcnet: Efficient condition-dependent modeling network for waveform generation</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
