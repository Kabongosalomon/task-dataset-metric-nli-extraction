<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Sequence-Level Training for Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minji</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp;</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungkwan</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Deeping Source Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungseul</forename><surname>Ok</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECE &amp;</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">IPAI</orgName>
								<orgName type="institution" key="instit2">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Sequence-Level Training for Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>https://github.com/byminji/SLTtrack</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>visual tracking</term>
					<term>sequence-level training</term>
					<term>reinforcement learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the extensive adoption of machine learning on the task of visual object tracking, recent learning-based approaches have largely overlooked the fact that visual tracking is a sequence-level task in its nature; they rely heavily on frame-level training, which inevitably induces inconsistency between training and testing in terms of both data distributions and task objectives. This work introduces a sequence-level training strategy for visual tracking based on reinforcement learning and discusses how a sequence-level design of data sampling, learning objectives, and data augmentation can improve the accuracy and robustness of tracking algorithms. Our experiments on standard benchmarks including LaSOT, TrackingNet, and GOT-10k demonstrate that four representative tracking models, SiamRPN++, SiamAttn, TransT, and TrDiMP, consistently improve by incorporating the proposed methods in training without modifying architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual object tracking aims to estimate the spatial extent, e.g., a bounding box, of a target object over a sequence of video frames <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b24">26]</ref>. This task has been drawing significant attention due to its wide range of applications including visual surveillance, robotics, and autonomous driving <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b10">12]</ref>. Unlike standard recognition tasks such as image classification and object detection, the class of the target object is unknown and only its bounding box at the initial frame is given for testing. Despite the long history of its study <ref type="bibr" target="#b32">[34]</ref>, object tracking in the wild still remains challenging due to appearance variation, occlusion, interference, distracting clutter, etc. To tackle the issues, recent methods increasingly rely on robust feature representations that are learned by deep neural networks with convolutions <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b46">48]</ref> and attention <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Although learning to track has been widely adopted in the research community, it is largely overlooked that visual tracking is essentially a sequence-level task; the estimated target state in the current frame is affected by the history of target states in the previous frames and also influences tracking results in the subsequent frames. For example, recent state-of-the-art methods <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b41">43]</ref> rely heavily on frame-level training, which encourages the trackers to better localize target objects in each frame through supervised learning. While it greatly improves the tracker by learning robust features for tracking, disregarding the sequential dependency across frames can lead to unexpected tracking failures. Let us assume a tracker that is trained with a typical frame-level training scheme using a set of annotated training videos; random pairs of a target template and a search frame are sampled from a video and the tracker is trained to best localize the target on the search frame independently for each pair. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, now consider one of the training videos that contains a hard frame, where the tracker fails to localize the target object. Although the tracker is trained to perform almost perfectly on frame-level localization except for the hard frame ( <ref type="figure" target="#fig_0">Fig. 1a)</ref>, its sequence-level performance may turn out to be poor as it loses the target from the hard frame in actual tracking on the sequence <ref type="figure" target="#fig_0">(Fig. 1b</ref>).</p><p>This pitfall of frame-level training mainly stems from inconsistency between training and testing in terms of both data distributions and task objectives. First, the tracker observes data samples, i.e., tracking situations, that significantly deviate from a real data distribution. That is, while in actual tracking the search window at each frame is determined based on the estimation at the previous frame, in the frame-level training it is not; the search window is typically sampled by adding a random transformation to the ground-truth bounding box. Second, the tracker learns with an objective, i.e., a reward system, that is largely different from actual tracking. The tracking performance in testing puts significant importance on retaining localization accuracy over a sequence, whereas it is only immediate localization quality that matters in the frame-level training. The mismatch of the objectives between training and testing often leads to unexpected results as shown in <ref type="figure" target="#fig_0">Fig. 1c</ref>; two trackers, A and B, being trained with the same network architecture and the same frame-level objective, are tested on the GOT-10k validation split where the loss and the performance are measured 1 . While constantly yielding a higher loss, tracker A achieves better performance than tracker B after 10 epochs. Such inconsistency should be rectified for more robust tracking but has hardly been explored so far in the tracking community.</p><p>This work investigates the sequence-level training for visual object tracking and analyzes how the performance of a tracking algorithm improves by resolving the aforementioned inconsistency issues. Without adding any architectural components, we train a tracker end-to-end by simulating sequence-level tracking scenarios with a properly matching reward system in the framework of reinforcement learning (RL). Specifically, our tracker observes a sequence of frames sampled from an actual tracking trajectory and optimizes the objective based on the test-time metric such as the average overlap <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b14">16]</ref>. Our training strategy not only resolves the inconsistency in data distributions but also addresses the discrepancy in task objectives by teaching the tracker how its decision in the current frame affects future ones. Furthermore, this approach enables us to extend data augmentation to a temporal domain; on top of commonly-used data augmentation strategies in the spatial domain <ref type="bibr" target="#b24">[26]</ref>, we can simulate temporally-varying tracking scenarios for training, corresponding to videos with diverse object/camera motions (Sec. 3). Note that this new type of augmentation has not been available under the frame-level training of previous trackers. To sum up, the proposed sequence-level training allows a tracker to learn a robust strategy for realistic tracking scenarios by leveraging the simulated tracking samples, (i.e., sequence-level sampling), the longterm objective (i.e., sequence-level objective), and the data augmentation in the temporal domain (i.e., sequence-level augmentation).</p><p>Our contributions are summarized as follows.</p><p>? We analyze the inherent drawbacks of frame-level training adopted in recent trackers, which motivate sequence-level training (SLT) for robust visual object tracking. ? We introduce an SLT strategy for visual tracking in an RL framework and propose an effective toolset of data sampling, training objectives, and data augmentation. ? We demonstrate the effectiveness of SLT using four recent trackers, SiamRPN++ <ref type="bibr" target="#b21">[23]</ref>, SiamAttn <ref type="bibr" target="#b42">[44]</ref>, TransT <ref type="bibr" target="#b3">[4]</ref>, and TrDiMP <ref type="bibr" target="#b34">[36]</ref>, and achieve competitive performance on the standard benchmarks, LaSOT <ref type="bibr" target="#b9">[10]</ref>, TrackingNet <ref type="bibr" target="#b26">[28]</ref>, and GOT-10k <ref type="bibr" target="#b14">[16]</ref>. ? We provide in-depth analyses of SLT by studying the effects of the sequence-level data sampling and the corresponding objective as well as the sequence-level data augmentation diversifying tracking episodes in a temporal domain.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Object Tracking (VOT)</head><p>There has been a large body of active research on VOT <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b16">18]</ref>, which still remains one of the major topics in computer vision. Recent methods for VOT have greatly improved tracking performance based on deep learning with large-scale datasets <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b9">10]</ref>. Currently, state-of-the-art trackers are represented by two families of trackers: Siamese <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b3">4]</ref> and DiMP <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">36]</ref>. The Siamese trackers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b21">23]</ref> rely heavily on an effective template-matching mechanism that is trained offline using large-scale datasets. While being fast and accurate in a short term, they tend to be vulnerable to long-term tracking due to the lack of online adaptability. Recent variants mitigate this limitation by updating template features during tracking <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b48">50]</ref> or using an attention mechanism to diversify the feature representation <ref type="bibr" target="#b42">[44]</ref>. DiMP trackers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">36</ref>] learn the online model predictor for target center regression and combine it with bounding box regression. Their model predictor, which is an iterative optimization-based neural module, is trained offline with a meta-learning-based objective and is used to build an online target model during tracking. Recently, Transformerbased architectures are adopted in both Siamese <ref type="bibr" target="#b3">[4]</ref> and DiMP <ref type="bibr" target="#b34">[36]</ref> trackers to enhance target feature representations. Note that all of these state-of-the-art trackers are trained with frame-level objectives; it forces the model to take an instantaneous greedy decision at each frame, ignoring that the tracking errors accumulate over the sequence. In contrast, we study how to improve tracking by considering temporal dependency in the sequence of frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reinforcement Learning for VOT</head><p>Our sequence-level training scheme is built on reinforcement learning (RL), which provides a natural framework for sequential decision-making on the problem with interactive temporal dependency. RL is not new in visual tracking and there exist several RL-based trackers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b44">46]</ref>. Most of them <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b15">17,</ref><ref type="bibr" target="#b13">15]</ref> aim to assist tracking by learning an additional RL agent while considering both a given tracker and its input sequence as an environment. HP-Siam <ref type="bibr" target="#b8">[9]</ref> uses RL to optimize hyperparameters of the tracker such as scale step, penalty, and window weight. DRL-IS <ref type="bibr" target="#b29">[31]</ref> and P-Track <ref type="bibr" target="#b15">[17]</ref> learn a policy to decide the state transition of the tracker. EAST <ref type="bibr" target="#b13">[15]</ref> uses RL to speed up tracking by learning to stop feed-forwarding frames through layers.</p><p>Only a few RL-based methods <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b44">46]</ref> learn the tracker itself as an RL agent, which performs actual tracking, e.g., estimating a target bounding box. ADNet <ref type="bibr" target="#b43">[45]</ref> formulates tracking as a discrete box adjustment problem, where at each time step the agent observes a current frame and a previous localization box and then decides discrete actions for adjusting the box. In a similar manner, ACT <ref type="bibr" target="#b2">[3]</ref> learns to predict box transformation parameters in a continuous space of actions while PACNet <ref type="bibr" target="#b44">[46]</ref> jointly learns both box estimation and state transition of the tracker. Their methods, however, require a specific form of output for training trackers in RL, e.g., the output of box transformation parameters <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">46]</ref> and pre-defined actions for box adjustment <ref type="bibr" target="#b43">[45]</ref>, and hardly exploit the advantage of RL training in a generic perspective. In contrast, we introduce a generic training strategy using RL and analyze its advantages over its frame-level counterpart, which is prevalent in recent state-of-the-art trackers. We advocate sequence-level training per se as the integral role of RL in tracking, showing that recent learnable trackers greatly benefit from RL-based training without additional components.</p><p>Regarding our effort to address the training-testing inconsistency, the most related work is <ref type="bibr" target="#b30">[32]</ref>, which tackles a similar problem in image captioning. It proposes selfcritical sequence training (SCST), a form of the well-known REINFORCE <ref type="bibr" target="#b36">[38]</ref> algo-rithm, to train image captioning models directly on NLP metrics. We build our sequencelevel training scheme on SCST and adapt it for visual object tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sequence-Level Training (SLT)</head><p>Given a video v = (v 0 , ..., v T ) of T + 1 frames and the ground-truth bounding box g 0 of the target object in the initial frame v 0 , a tracker sequentially predicts a bounding box l t of the target in each frame v t , t = 1, 2, ..., T . The tracker, parameterized by ?, is modeled as a function ? ? that takes observation o t and predicts l t , i.e.,</p><formula xml:id="formula_0">l t = ? ? (o t ),</formula><p>where o t is the information available at time t including the video frames (v 0 , ..., v t ), the initial target bounding box g 0 , and the previously estimated target states (l 1 , ..., l t?1 ). In online tracking, most trackers estimate the current state based on the previous prediction l t?1 and the observation of the current frame v t , e.g., searching for the optimal local window in v t around l t?1 . The objective of a tracking algorithm is to maximize the sequence-level performance r(l), where l = (l 1 , ..., l T ) is the estimated target states in a video and r is an evaluation metric, e.g., average overlap ratio <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b14">16]</ref>.</p><p>Due to the sequential structure of the tracking process, its current decision l t naturally affects the future ones l t+1 , ..., l T . In the frame-level training, which is the de facto standard for recent methods <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b40">42]</ref>, however, trackers do not simulate sequential target state estimation procedure in training. In other words, given each frame v t , trackers are trained to localize the target object bounding box g t from its random perturbation ?(g t ) instead of l t?1 , where ? is a random perturbation function. Such a frame-level approximation, i.e., l t?1 ? ?(g t ), requires additional hyper-parameters for ? and, more importantly, introduces inconsistency of data distributions between training and testing; the trackers have no opportunity to learn how the previous decision affects the current one in a real tracking scenario.</p><p>To overcome the limitation of frame-level training and capture the temporal dependency between decisions, we build our sequence-level training scheme based on RL. We simulate the tracker (agent) on a sequence of video frames, and directly optimize it with respect to the test-time evaluation metric:</p><formula xml:id="formula_1">L(?) := ?E l?? ? [r(l)] .<label>(1)</label></formula><p>Note that this sequence-level objective directly optimizes the real objective of tracking, and thus is a more natural way to train trackers than frame-level counterparts. This objective relieves the aforementioned issues in frame-level training; the tracker observes the real data distributions via sequence-level sampling, which draws samples from actual tracking trajectories and facilitates learning temporal dependency in tracking. To directly optimize the task objective in (1), we employ the REINFORCE algorithm <ref type="bibr" target="#b36">[38]</ref>. According to the algorithm, the expected gradient is computed as follows:</p><formula xml:id="formula_2">? ? L(?) = ?E l?? ? [r(l)? ? log p ? (l)].<label>(2)</label></formula><p>In practice, the expected gradient is approximated by using a single Monte Carlo sample l = (l 1 , ..., l T ) of sequential decisions from ? ? . For each training episode, the gradient</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Video</head><p>Sample frames with random interval</p><formula xml:id="formula_3">Video (Episode) = ( ! , ? , " ) ! "</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampling Tracker</head><p>Argmax Tracker</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Weights</head><p>Target Trajectory = ( # , ? , " ) <ref type="figure">Fig. 2</ref>: Illustration of our sequence-level training framework. In training time, a video (episode), which is sampled from the original video with random intervals, is tracked twice by the sampling tracker and the argmax tracker. In this example, when a target person is fully occluded for a while, the argmax tracker mistakenly localizes the other person as the target due to its highest score in the occluded scene. In contrast, the sampling tracker stays nearby the previously estimated location (because of the random sampling) and successfully re-tracks the target object. In such a case, the reward becomes positive so that the sampled action is encouraged. In the opposite case, the reward becomes negative so that the sampled action is discouraged.</p><formula xml:id="formula_4">Target Trajectory $ = ( ? # , ? , ? " ) Reward ( ) Reward ( ?) 75.5 ? ? ? ? $ ? log % ( ) 56.7 Sample ! ~ " ( ; ! , !#$ ) ? ! = argmax % " ( ; ! , ? !#$ )</formula><p>is given by</p><formula xml:id="formula_5">? ? L(?) ? ?r(l)? ? log p ? (l).<label>(3)</label></formula><p>To reduce the variance of gradient estimation, we adopt the self-critical sequence training (SCST) <ref type="bibr" target="#b30">[32]</ref>, which exploits the test-mode performance of the current model as a baseline for the reward. To be specific, we adopt two trackers sharing network parameters: a sampling tracker and an argmax tracker. During training, a video (episode) is played twice independently by both trackers. Given the probability distribution of actions, the sampling tracker decides the target bounding box stochastically while the argmax tracker selects the most confident one. If the agent obtains a higher reward from the sampling mode than the argmax mode, the resulting reward becomes positive to encourage the sampled actions. Otherwise, the agent receives a negative reward to suppress the sampled actions. In this way, we employ the SCST algorithm to train the tracker using the following gradient:</p><formula xml:id="formula_6">? ? L(?) ? ?(r(l) ? r(l ? ))? ? log p ? (l),<label>(4)</label></formula><p>where r(l) and r(l ? ) are rewards obtained from the current model by the sampling mode and the argmax mode during training, respectively. Such a REINFORCE-based training scheme is a certain realization of SLT and may be further improved by other RL-based algorithms. We illustrate the proposed sequence-level training pipeline in <ref type="figure">Fig. 2</ref>, and provide the pseudo-code in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integration into Tracking Algorithms</head><p>We now present how to integrate the proposed SLT scheme into existing trackers. To demonstrate the effect of SLT, we adopt four representative trackers as our baselines: while not converged do 3:</p><p>Sample a video v = (v0, ..., vT ) and ground-truth g = (g0, ..., gT ) from ? 4:</p><p>Initialize the tracker by using {v0, g0} 5: l0 = g0 ? Initial target location for the sampling tracker 6:</p><p>l ? 0 = g0 ? Initial target location for the argmax tracker 7:</p><p>for t ? 1, ..., T do 8:</p><formula xml:id="formula_7">lt = sample l from p ? (l; vt, lt?1) 9: l ? t = arg max l p ? (l; vt, l ? t?1 ) 10: end for 11: r = evaluate({l1, ..., lT }, {g1, ..., gT }) 12: r ? = evaluate({l ? 1 , ..., l ? T }, {g1, ..., gT }) 13: L = ?(r ? r ? ) t=T t=1 log p ? (lt; vt, lt?1) ? (4) 14: ? = ? ? ?? ? L ? Update model parameters 15:</formula><p>end while 16: end procedure SiamRPN++ <ref type="bibr" target="#b21">[23]</ref>, SiamAttn <ref type="bibr" target="#b42">[44]</ref>, TransT <ref type="bibr" target="#b3">[4]</ref>, and TrDiMP <ref type="bibr" target="#b34">[36]</ref>. In the following, we briefly describe each tracker and explain how it is trained.</p><p>SLT-SiamRPN++. SiamRPN++ <ref type="bibr" target="#b21">[23]</ref> is a representative Siamese tracker based on the region proposal network (RPN) <ref type="bibr">[11]</ref>. This method tracks the target object by repeatedly matching between feature embeddings of a template image and a search image <ref type="bibr" target="#b0">[1]</ref>. Specifically, the tracker outputs confidence scores and box coordinates of N anchor boxes and performs greedy selection to choose the most confident one, where its box coordinates are selected as the estimation of the target location in the current frame. Since the current prediction for the target state, i.e., position and size, determines the search area in the next frame, this decision not only influences the current frame but also will potentially affect predictions in the future. Thus, we reinforce the box selection procedure of SiamRPN++ with the proposed sequence-level training strategy to teach the tracker temporal dependencies.</p><p>Since our training method assumes that the target localization of the tracker is a stochastic action, we convert the greedy anchor selection of SiamRPN++ to become stochastic. Let x = (x 1 , ..., x N ) ? R N denote the output anchor scores of SiamRPN++, where N = a ? H ? W denotes the number of candidates in a score map with size of H ? W and a anchor types. We define a categorical distribution p(n) as follows:</p><formula xml:id="formula_8">p(n) = exp(? ?1 (x n )) N m=1 exp(? ?1 (x m )) ,<label>(5)</label></formula><p>where ? ?1 indicates the logit (inverse sigmoid) function. Since x n is a normalized score whose value is between 0 and 1, we first apply the logit function before applying the softmax function. In training time, the tracker samples an anchor box from p for the current target localization. In test time, it selects the most confident anchor box deterministically as in the original SiamRPN++. For each training episode, the loss in the classification branch is given by</p><formula xml:id="formula_9">L = ?(r(l) ? r(l ? )) T t=1 log p(n t ),<label>(6)</label></formula><p>where (r(l) ? r(l ? )) is the self-critical reward (Sec. 3.1) and n t is the sampled anchor box at frame t. The overall sequence-level training loss of SiamRPN++ is defined by combining the loss L for the classification branch and the ? 1 loss for the bounding-box regression branch <ref type="bibr" target="#b21">[23]</ref>, which is given by</p><formula xml:id="formula_10">L siamrpn++ = L + L bbox .<label>(7)</label></formula><p>SLT-SiamAttn. SiamAttn <ref type="bibr" target="#b42">[44]</ref> is an extension of SiamRPN++ with an additional bounding box refinement module and a mask prediction module, along with attention modules for enhancing the feature representation. Similar to SiamRPN++, SiamAttn takes greedy anchor selection to choose the best target candidate among N anchor boxes from RPN. Once the box is chosen, SiamAttn additionally refines the bounding box using a deformable RoI pooling operation <ref type="bibr" target="#b5">[6]</ref>. We thus use the same loss L for the classification branch. The overall sequence-level training loss of SiamAttn is to enhance the capability of target classification with SLT and increase the accuracy of localization modules with the help of sequence-level data sampling:</p><formula xml:id="formula_11">L siamattn = L + ? 1 L bbox + ? 2 L refine-bbox + ? 3 L mask ,<label>(8)</label></formula><p>where each loss term except for L follows <ref type="bibr" target="#b42">[44]</ref>. The weight parameters are set as ? 1 = 0.5, ? 2 = 0.5, and ? 3 = 0.2.</p><p>SLT-TransT. TransT <ref type="bibr" target="#b3">[4]</ref> adopts Transformer-like feature fusion networks into Siamese architecture and localizes the target object by computing attention between template vectors and search vectors. Unlike SiamRPN++ and SiamAttn, TransT has neither anchor points nor anchor boxes, and its prediction heads directly make N classification results and N normalized box estimations from fusion vectors corresponding to each position of the feature map, where N = H ? W denotes the size of the feature map. We also take a categorical distribution p(n) of equation <ref type="bibr" target="#b4">(5)</ref> for N candidate vectors, and use the same loss L for the classification branch. The overall sequence-level training loss for TransT is:</p><formula xml:id="formula_12">L transt = L + ? 4 L bbox-L1 + ? 5 L bbox-GIoU ,<label>(9)</label></formula><p>where the box regression losses follow <ref type="bibr" target="#b3">[4]</ref> and ? 4 = 0.33, ? 5 = 0.13 in our implementation.</p><p>SLT-TrDiMP. TrDiMP <ref type="bibr" target="#b34">[36]</ref> is one of the state-of-the-art DiMP tracker using Transformer architectures. Its tracking procedure consists of two steps: target center prediction and bounding box regression. Given template samples generated from the initial frame, the model predictor generates a discriminative CNN kernel to convolve with the feature embedding from a search image for target response generation. The most confident location of the score map becomes the target center prediction, and starting from the randomly drawn candidate boxes around the location, the final bounding box estimation l t is obtained from the IoU-Net-based box optimization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>For sequence-level training of TrDiMP, we convert the target center prediction into stochastic action by simply taking softmax on its score prediction. Let y = (y 1 , ..., y N ) ? R N denotes the center prediction score of TrDiMP, where N = H ?W means a number of candidates in a score map with a spatial size of H ? W . Now we define a categorical distribution p(n) as follows:</p><formula xml:id="formula_13">p(n) = exp(y n ) N m=1 exp(y m ) .<label>(10)</label></formula><p>The loss for the center prediction module is same with L, and the overall sequence-level training loss for TrDiMP is:</p><formula xml:id="formula_14">L trdimp = L + ? 6 L iou-net ,<label>(11)</label></formula><p>where the loss term for IoU-Net follows <ref type="bibr" target="#b34">[36]</ref> and ? 6 is set to 0.0025.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sequence-Level Data Augmentation</head><p>Learning visual tracking in a sequence level naturally motivates sequence-level data augmentation that is conceptually incompatible with frame-level training. To improve the data quality and avoid the over-fitting problem of the networks, data augmentation strategies in a spatial domain such as geometric transformation, color perturbations, and blur, are widely adopted for convolutional trackers <ref type="bibr" target="#b24">[26]</ref>. Conventional frame-level training, however, treats the training sequence merely as a group of independent images that need to be sampled and cropped, hardly considering the relationship between each image, thereby missing the potential effect of data augmentation towards the temporal domain. In contrast, our sequence-level training effectively benefits from exploring diverse changes in the temporal axis that can enrich the tracking scenarios, as well as the conventional data augmentation strategies in the spatial axis. Among many possible ways of sequence-level augmentation, here we focus on a simple frame-interval augmentation, i.e., subsampling the training videos with different frame intervals. In our sequence-level augmentation setting, some episodes are sampled from the original video with random intervals as shown on the left side of <ref type="figure">Fig. 2</ref>. This scheme simulates dynamic visual differences along time steps and teaches the tracker to adapt to situations in which objects and/or cameras move faster. Diversifying the tracking scenarios in terms of frame rates makes the tracker improve or at least maintain the performance in general test videos. Experimental results in Sec. 4 show how our augmentation strategy affects the tracking performance. We believe that more advanced sequence-level augmentation strategies, e.g., temporal motion blur, may help sequencelevel training further in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>This section presents the effectiveness of the proposed sequencel-level training using four baseline trackers, SiamRPN++ <ref type="bibr" target="#b21">[23]</ref>, SiamAttn <ref type="bibr" target="#b42">[44]</ref>, TransT <ref type="bibr" target="#b3">[4]</ref>, and TrDiMP <ref type="bibr" target="#b34">[36]</ref>, on three standard benchmarks, LaSOT <ref type="bibr" target="#b9">[10]</ref>, TrackingNet <ref type="bibr" target="#b26">[28]</ref>, and GOT-10k <ref type="bibr" target="#b14">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>As widely adopted in deep RL <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b43">45]</ref>, we pre-train the trackers with supervised learning, which is done with frame-level training in our case, to stabilize and speed up the subsequent SLT. For each training iteration, k tracking episodes are randomly sampled from training datasets, where k is set to 8 for SiamRPN++, TransT, and TrDiMP and 12 for SiamAttn, respectively. Each episode is composed of T video frames and a single template frame and T is a hyper-parameter for training. For frame-interval augmentation, the interval is randomly chosen every time sampling the video frames and its maximum is set to 7 for SiamRPN++ and SiamAttn, and 10 for TransT and TrDiMP, respectively. We use the average overlap (AO) score for the reward function r in Eq. 6.</p><p>Many recent trackers have post-processing strategies based on geometric priors <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b3">4]</ref>. For example, SiamRPN++ has the cosine-window penalty and the shape penalty, which prevent drastic updates in target bounding box estimation. These penalties are typically not applied during frame-level training, which also brings inconsistency between training and testing. However, our sequence-level training also resolves this inconsistency problem. Note that x n in Eq. 5 is the anchor score after postprocessing.</p><p>The argmax and sampling trackers in our SLT framework share all weights for training, and our tracker behaves like the argmax tracker during inference. Thus, the additional memory cost is marginal in training, and the test-time efficiency of the original tracker is not affected by SLT. Our algorithm is implemented in Python using PyTorch with NVIDIA RTX A6000 2GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Dataset</head><p>For a fair comparison, we aligned the pre-training datasets for the baseline with the fine-tuning datasets for SLT as similar as possible. 1) For SiamRPN++, we adopt La-SOT, TrackingNet, and GOT-10k for both pre-training and fine-tuning. 2) Following the original paper, SiamAttn is trained on LaSOT, TrackingNet, COCO <ref type="bibr" target="#b23">[25]</ref>, and YouTube-VOS <ref type="bibr" target="#b38">[40]</ref>. Since COCO is an image dataset, a data augmentation scheme such as shift, scale, and blur is adopted to extend the image to compose an episode. Note that the data augmentation strategy except for frame-interval augmentation is used only for COCO. 3) Finally, TransT and TrDiMP are both pre-trained using LaSOT, TrackingNet, GOT-10k, and COCO, as same as the original papers, and then fine-tuned on three video datasets, LaSOT, TrackingNet, and GOT-10k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>We compare the performance of SLT with four baseline trackers. Note that for a fair comparison, we strictly maintain the same test-time hyper-parameters for each method for all datasets. Only TrDiMP uses a different hyper-parameter setting for evaluation in LaSOT following the baseline paper <ref type="bibr" target="#b34">[36]</ref>.</p><p>LaSOT <ref type="bibr" target="#b9">[10]</ref> is a recently published dataset that consists of 1,400 videos with more than 3.5M frames in total. This benchmark is widely used to measure the long-term capability of trackers. The average video length of LaSOT is more than 2,500 frames, and each sequence comprises various challenging attributes. The one-pass evaluation (OPE) protocol is used to measure the normalized precision (P Norm ) and the area under curve (AUC) of the success plot. <ref type="table" target="#tab_1">Table 1</ref> shows that the proposed method consistently improves all baseline trackers.</p><p>TrackingNet <ref type="bibr" target="#b26">[28]</ref> is a large-scale dataset that provides 30K videos in training split and 511 videos in test split. We evaluate our trackers on the test split of TrackingNet through the evaluation server. <ref type="table" target="#tab_1">Table 1</ref> shows that our SLT improves the AUC score by 7.6%p, 2.6%p, and 1.7%p for SiamRPN++, SiamAttn, and TransT, respectively. It is noteworthy that the simple convolutional tracker SiamRPN++ shows competitive performance with SiamAttn with the power of SLT.</p><p>GOT-10k <ref type="bibr" target="#b14">[16]</ref> is a large-scale dataset that contains 10k sequences for training and 180 videos for testing. For evaluation metrics, the average overlap (AO) and the success rate (SR) at overlap thresholds 0.5 and 0.75 are adopted. Following the evaluation protocol of GOT-10k, we retrain our models using only the GOT-10k train split and submit the tracking results to the evaluation server. Since the GOT-10k benchmark does not provide mask annotations, SLT-SiamAttn is trained without the mask branch. <ref type="table" target="#tab_1">Table 1</ref> shows that our SLT successfully improves all the baseline trackers in all evaluation metrics. Baseline models are reproduced using only the GOT-10k train split.</p><p>Comparison with SOTA trackers. We compare the performance of the proposed SLT family with the other state-of-the-art trackers on LaSOT and TrackingNet as shown in <ref type="table" target="#tab_2">Table 2</ref> and 3. When compared to the recently proposed RL-based tracker PACNet <ref type="bibr" target="#b44">[46]</ref>, all four SLT trackers are showing superior performance by a large margin. SLT-TransT, which is our best model, achieves state-of-the-art performance in both benchmarks.   Note that STARK-ST101 <ref type="bibr" target="#b40">[42]</ref> uses deeper backbone (ResNet101) than TransT, which use ResNet50 backbone. SLT-TransT thus needs to be compared with STARK-ST50 for fairness. <ref type="table" target="#tab_4">Table 4</ref> also shows that both SLT-TrDiMP and SLT-TransT achieve comparable performance with state-of-the-art trackers on GOT-10k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>We also analyze the effects of SLT using SiamRPN++ as the base tracker. The experimental analyses in this subsection are done on the validation split of GOT-10k and the test splits of LaSOT and TrackingNet.</p><p>Sequence-level training components. The benefit of SLT comes from sequence-level sampling, sequence-level objective, and sequence-level data augmentation. We validate the components of SLT by measuring the accuracy gains on the three benchmarks (Table 5) and performing an attribute-based analysis on LaSOT <ref type="figure" target="#fig_1">(Fig. 3)</ref>. Sequence-level sampling (SS). To measure the net effect of SS, we train a tracker with SS but use the frame-level objective. As shown at '+SS' in <ref type="table" target="#tab_5">Table 5</ref>, by learning more accurate input data distribution, the tracker with SS outperforms the baseline with frame-level sampling by 4.1%p, 5.3%p, and 3.8%p, respectively, on the three benchmarks. SS allows the tracker to observe realistic appearance variations of target objects during tracking, making itself more robust to variations of aspect ratio, scale, rotation, and illumination (ARC, SC, R, IV) as seen in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>Sequence-level objective (SO). As shown at '+SS+SO' in <ref type="table" target="#tab_5">Table 5</ref>, SO additionally improves the performance by 2.2%p, 1.5%p, and 3.6%p on three benchmarks, respectively. SO enables the tracker to reflect accumulated localization errors, preventing it from losing the target in challenging situations such as full occlusion, background clutters, and motion blur (FO, BC, MB) as seen in <ref type="figure" target="#fig_1">Fig. 3</ref>.  Sequence-level augmentation (SA). As shown at '+SS+SO+SA' in <ref type="table" target="#tab_5">Table 5</ref>, SA further improves the performance by 1.1%p, 0.8%p, and 0.5%p, respectively, resulting in the significant gain of SLT in total as 7.4%p, 7.6%p, and 7.9%p. The effectiveness of SA is also evident from the improvement in the overall attributes in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>To show that the frame-interval augmentation strategy of SA also potentially helps in adapting to videos with diverse frame rates, we set up tracking scenarios with lower frame rates (i.e., faster motion). In the evaluation protocol, we track objects every ith frame only, skipping all the other frames; when the interval is 1, the evaluation protocol is the same as the original benchmark. <ref type="table" target="#tab_6">Table 6</ref> shows that the frame-interval augmentation strategy not only improves the performance in normal videos, but also makes the tracker more robust to videos with lower frame rates.</p><p>Length of training episodes. In training time, we randomly sample training episodes of pre-defined sequence length T . Learning temporal dependency may be affected by the length of training sequences. We thus experiment with varying T while fixing the sampled frame interval to 1. The best result is obtained with T = 24, as can be seen in <ref type="table" target="#tab_7">Table 7</ref>. When T = 1, the performance does not improve over the pre-trained tracker.</p><p>Frame-level pre-training. In our experiments, we perform SLT from a model pretrained by frame-level training (FLT). We analyze the effect of warm-up FLT in <ref type="table" target="#tab_8">Table 8</ref>,   showing that it improves tracking performance indeed, and FLT with only a few epochs is sufficient for the warm-up. We also observed that the gain from SLT easily disappears by another few epochs of FLT (i.e., FLT ? SLT ? FLT). In particular, the AO score of SiamRPN++ on the GOT-10k validation split reverted from 74.3% to 66.2% after 5 epochs of FLT. This indicates that SLT grants a unique gain, which FLT cannot provide.</p><p>For additional analysis and qualitative results, see the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a novel sequence-level training strategy for visual object tracking to resolve the training-testing inconsistency problem of existing trackers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Training Details</head><p>We use Adam optimizer for SiamRPN++, SiamAttn, and TrDiMP, and AdamW optimizer for TransT. SiamRPN++ and SiamAttn are trained for 20 epochs with 10000 videos per epoch, while the learning rate starts from 10 ?5 and exponentially decays to 10 ?6 . Following the original papers, we use 0.1 times smaller learning rate for backbone layers for SiamRPN++ and 0.05 times smaller for SiamAttn, respectively. TransT is trained for 120 epochs with 1000 videos per epoch, and the learning rate starts from 0.1 times the original model setup in the paper and decreases by a factor of 10 after 100 epochs. TrDiMP is trained for 40 epochs with 5000 videos per epoch, and the learning rate starts from 0.04 times the initial learning rate from the original model and halves every 8 epochs. After the pre-training stage, the statistics of batch normalization layers are fixed and not updated during the RL fine-tuning stage.  <ref type="table" target="#tab_1">Table 1</ref> summarizes the evaluation results of our method on additional benchmarks, where SLT consistently improves the AUC scores on NFS <ref type="bibr" target="#b18">[20]</ref>, UAV123 <ref type="bibr" target="#b25">[27]</ref>, and TNL2K <ref type="bibr" target="#b35">[37]</ref>, while strengthening the robustness (R) on VOT2020 <ref type="bibr" target="#b20">[22]</ref>. Note that the re-initialization policy of VOT evaluation does not match with the reward system of SLT, which is designed for one-pass evaluation. Therefore, we also compare AO and AUC on VOT2018 <ref type="bibr" target="#b19">[21]</ref>, showing that SLT further benefits when the test-time metric and the reward system are aligned. Because VOT2020 does not provide the bounding box annotation, we cannot report AO and AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation on Additional Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Results</head><p>Section 4.4 of the main paper describes the benefits of sequence-level sampling (SS) and sequence-level objective (SO). To support our argument about the effect of these two components, we present qualitative results in <ref type="figure" target="#fig_0">Fig. 1</ref>, which visualizes the boundingboxes corresponding to ground truth (white) and results from baseline (blue), base-line+SS (yellow), and baseline+SS+SO (magenta). For this analysis, we adopt SiamRPN++ <ref type="bibr" target="#b21">[23]</ref> as the baseline tracker. As discussed in the main paper, SS makes trackers more robust to appearance updates given by scale changes, aspect ratio variations, and rotation. In <ref type="figure" target="#fig_0">Fig. 1a</ref>, there are two videos whose target objects change their appearance significantly. The tracker trained with SS successfully adapts to appearance variations, while the baseline tracker fails to capture the entire bodies of the target objects. Moreover, SO alleviates the drift issue in trackers in some challenging situations such as occlusion and background clutter. To qualitatively validate the properties, we present the target trajectories of the baseline+SS and baseline+SS+SO trackers in <ref type="figure" target="#fig_0">Fig. 1b</ref>, which includes the videos with such challenging attributes. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Pitfall of frame-level training for visual tracking. Training a tracker to better localize a target in each of the individual frames of (a) does not necessarily improve actual tracking in the sequence of (b). Green/red boxes indicate success/failure in localization. Due to the issue, inconsistency between the loss and the performance is often observed during training as shown in (c) where trackers, A and B, being frame-level trained are evaluated while the validation loss (top) and the tracking performance of average overlap (bottom) are measured. After 10 epochs, A outperforms B in spite of higher losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Benefits of sequence-level training components to individual attributes on the LaSOT dataset. The baseline tracker is SiamRPN++, and the y-axis is performance (AUC) gain compared with the baseline tracker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( a )Fig. 1 :</head><label>a1</label><figDesc>Baseline vs. Baseline+SS (b) Baseline+SS vs. Baseline+SS+SO Qualitative results. white: ground truth, blue: baseline, yellow: tracker trained with SS, magenta: tracker trained with SS and SO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Sequence-Level Training 1: procedure SEQUENCE-LEVEL TRAINING Input: A tracker parametrized by ?, training dataset ? 2:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance of sequence-level training on LaSOT, TrackingNet, and GOT-10k.</figDesc><table><row><cell>Method</cell><cell>LaSOT AUC (?)</cell><cell>PNorm</cell><cell cols="2">TrackingNet AUC (?) PNorm</cell><cell>P</cell><cell>AO (?)</cell><cell cols="3">GOT-10k SR0.5 SR0.75</cell></row><row><cell>SiamRPN++</cell><cell>Base +SLT 58.4 (+7.4) 51.0</cell><cell>60.3 66.6</cell><cell>68.2 75.8 (+7.6)</cell><cell>78.3 81.0</cell><cell cols="3">68.9 71.3 62.1 (+12.6) 49.5</cell><cell>58.0 74.9</cell><cell>30.5 49.0</cell></row><row><cell>SiamAttn</cell><cell>Base +SLT 57.4 (+2.6) 54.8</cell><cell>63.5 66.2</cell><cell>74.3 76.9 (+2.6)</cell><cell>80.9 82.3</cell><cell>70.6 72.6</cell><cell cols="2">53.4 62.5 (+9.1)</cell><cell>61.8 75.4</cell><cell>36.4 50.2</cell></row><row><cell>TrDiMP</cell><cell>Base +SLT 64.4 (+1.1) 63.3</cell><cell>72.3 73.5</cell><cell>78.1 78.1 (+0.0)</cell><cell>83.3 83.1</cell><cell>73.1 73.1</cell><cell cols="2">67.1 67.5 (+0.4)</cell><cell>77.4 78.8</cell><cell>58.5 58.7</cell></row><row><cell>TransT</cell><cell>Base +SLT 66.8 (+2.6) 64.2</cell><cell>73.7 75.5</cell><cell>81.1 82.8 (+1.7)</cell><cell>86.8 87.5</cell><cell>80.1 81.4</cell><cell cols="2">66.2 67.5 (+1.3)</cell><cell>75.5 76.5</cell><cell>58.7 60.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with the state-of-the-art trackers on LaSOT.</figDesc><table><row><cell cols="7">PACNet Ocean DiMP50 PrDiMP50 TransT STARK-STARK-</cell><cell>SLT-</cell><cell>SLT-</cell><cell>SLT-</cell><cell>SLT-</cell></row><row><cell>[46]</cell><cell>[48]</cell><cell>[2]</cell><cell>[8]</cell><cell cols="7">[4] ST50 [42] ST101 [42] SiamRPN++ SiamAttn TrDiMP TransT</cell></row><row><cell>AUC (%) 55.3</cell><cell>56.0</cell><cell>56.9</cell><cell>59.8</cell><cell>64.2</cell><cell>66.4</cell><cell>67.1</cell><cell>58.4</cell><cell>57.4</cell><cell>64.4</cell><cell>66.8</cell></row><row><cell>PNorm (%) 62.8</cell><cell>65.1</cell><cell>64.3</cell><cell>68.0</cell><cell>73.7</cell><cell>76.3</cell><cell>77.0</cell><cell>66.6</cell><cell>66.2</cell><cell>73.5</cell><cell>75.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the state-of-the-art trackers on TrackingNet.</figDesc><table><row><cell cols="7">DiMP50 SiamFC++ MAML PrDiMP50 TransT STARK-STARK-</cell><cell>SLT-</cell><cell>SLT-</cell><cell>SLT-</cell><cell>SLT-</cell></row><row><cell>[2]</cell><cell>[41]</cell><cell>[35]</cell><cell>[8]</cell><cell cols="7">[4] ST50 [42] ST101 [42] SiamRPN++ SiamAttn TrDiMP TransT</cell></row><row><cell>AUC (%) 74.0</cell><cell>75.4</cell><cell>75.7</cell><cell>75.8</cell><cell>81.1</cell><cell>81.3</cell><cell>82.0</cell><cell>75.8</cell><cell>76.9</cell><cell>78.1</cell><cell>82.8</cell></row><row><cell>PNorm (%) 80.1</cell><cell>80.0</cell><cell>82.2</cell><cell>81.6</cell><cell>86.8</cell><cell>86.1</cell><cell>86.9</cell><cell>81.0</cell><cell>82.3</cell><cell>83.1</cell><cell>87.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with the state-of-the-art trackers on GOT-10k. 'Add. data' denotes that trackers are trained using additional training datasets other than GOT-10k.</figDesc><table><row><cell></cell><cell cols="8">Add. SiamFC++ DiMP50 Ocean PrDiMP50 TransT TrDiMP STARK-</cell><cell>SLT-</cell><cell>SLT-</cell><cell>SLT-</cell><cell>SLT-</cell></row><row><cell></cell><cell>data</cell><cell>[41]</cell><cell>[2]</cell><cell>[48]</cell><cell>[8]</cell><cell>[4]</cell><cell cols="6">[36] ST50 [42] SiamRPN++ SiamAttn TrDiMP TransT</cell></row><row><cell>AO (%)</cell><cell></cell><cell>59.5</cell><cell>61.1</cell><cell>61.1</cell><cell>63.4</cell><cell>66.2</cell><cell>67.1</cell><cell>68.0</cell><cell>62.1</cell><cell>62.5</cell><cell>67.5</cell><cell>67.5</cell></row><row><cell>SR0.5 (%)</cell><cell>-</cell><cell>69.5</cell><cell>71.7</cell><cell>72.1</cell><cell>73.8</cell><cell>75.5</cell><cell>77.4</cell><cell>77.7</cell><cell>74.9</cell><cell>75.4</cell><cell>78.8</cell><cell>76.5</cell></row><row><cell>SR0.75 (%)</cell><cell></cell><cell>47.9</cell><cell>49.2</cell><cell>47.3</cell><cell>54.3</cell><cell>58.7</cell><cell>58.5</cell><cell>62.3</cell><cell>49.0</cell><cell>50.2</cell><cell>58.7</cell><cell>60.3</cell></row><row><cell>AO (%)</cell><cell>?</cell><cell>-</cell><cell>60.4</cell><cell>-</cell><cell>65.2</cell><cell>71.9</cell><cell>68.6</cell><cell>71.5</cell><cell>56.9</cell><cell>62.8</cell><cell>69.0</cell><cell>72.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Effect of sequence-level training components.</figDesc><table><row><cell></cell><cell></cell><cell>Benchmark</cell><cell>Baseline</cell><cell>+SS (?)</cell><cell>SiamRPN++ +SS+SO (?) +SS+SO+SA (?)</cell></row><row><cell></cell><cell></cell><cell>LaSOT (AUC)</cell><cell>51.0</cell><cell>55.1 (+4.1)</cell><cell>57.3 (+6.3)</cell><cell>58.4 (+7.4)</cell></row><row><cell></cell><cell></cell><cell>TrackingNet (AUC)</cell><cell>68.2</cell><cell>73.5 (+5.3)</cell><cell>75.0 (+6.8)</cell><cell>75.8 (+7.6)</cell></row><row><cell></cell><cell></cell><cell>GOT-10k (AO)</cell><cell>66.4</cell><cell>70.2 (+3.8)</cell><cell>73.8 (+7.4)</cell><cell>74.3 (+7.9)</cell></row><row><cell></cell><cell>15.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>+SS</cell><cell></cell><cell></cell><cell>ARC: Aspect Ratio Change</cell></row><row><cell></cell><cell></cell><cell>+SS +SO</cell><cell></cell><cell></cell><cell>LR: Low Resolution</cell></row><row><cell>Performance gain</cell><cell>5.0 10.0</cell><cell>+SS +SO +SA</cell><cell></cell><cell></cell><cell>FO: Full Occlusion SV: Scale Variation VC: Viewpoint Change BC: Background Clutter R: Rotation CM: Camera Motion MB: Motion Blur OV: Out-of-View FM: Fast Motion</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>D: Deformation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PO: Partial Occlusion</cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell>IV: Illumination Variation</cell></row><row><cell></cell><cell></cell><cell cols="4">ARC LR OV FM FO SV VC BC R CM MB D PO IV</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Attribute</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Effect of sequence-level augmentation (SA) in terms of video frame interval with the low frame rate protocol. The frame interval is denoted by i.</figDesc><table><row><cell>Method</cell><cell>SA</cell><cell cols="7">GOT-10k (AO) i = 1 i = 2 i = 3 i = 1 i = 2 i = 3 i = 4 LaSOT (AUC)</cell></row><row><cell>SiamRPN++</cell><cell>-</cell><cell>66.4</cell><cell>63.1</cell><cell>60.8</cell><cell>51.0</cell><cell>50.0</cell><cell>50.2</cell><cell>48.8</cell></row><row><cell>SLT-SiamRPN++</cell><cell>-</cell><cell>73.8</cell><cell>67.9</cell><cell>65.5</cell><cell>57.3</cell><cell>55.1</cell><cell>54.1</cell><cell>52.6</cell></row><row><cell>SLT-SiamRPN++</cell><cell>?</cell><cell>74.3</cell><cell>70.8</cell><cell>67.8</cell><cell>58.4</cell><cell>56.9</cell><cell>56.2</cell><cell>54.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Effect of training sequence length.</figDesc><table><row><cell>Benchmark</cell><cell>1</cell><cell cols="4">Training sequence length (T ) 4 8 16 24</cell><cell>32</cell></row><row><cell>GOT-10k (AO)</cell><cell>65.8</cell><cell>69.6</cell><cell>70.1</cell><cell>73.0</cell><cell>73.8</cell><cell>73.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Effect of frame-level pre-training. The zero (0) epoch stands for random initialization.</figDesc><table><row><cell>Method</cell><cell>0</cell><cell cols="5">Frame-level pre-training (epoch) 1 5 10</cell><cell>20</cell></row><row><cell>SiamRPN++</cell><cell>-</cell><cell></cell><cell>62.0</cell><cell>64.2</cell><cell>64.9</cell><cell>66.4</cell></row><row><cell>SLT-SiamRPN++</cell><cell cols="2">60.3</cell><cell>68.3</cell><cell>70.6</cell><cell>72.1</cell><cell>74.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Unlike existing methods, it trains a tracker by actually tracking on a video and directly optimizing a tracking performance metric, boosting the generalization performance without modifying the model architecture. Experiments on SiamRPN++, SiamAttn, TransT, and TrDiMP trackers show that sequence-level sampling, objective, and augmentation are all effective in learning visual tracking. Level Training for Visual Tracking Minji Kim 1* Seungkwan Lee 3,4* Jungseul Ok 3 Bohyung Han 1,2 Minsu Cho 3</figDesc><table><row><cell>Supplementary Material for</cell></row><row><cell>Towards Sequence-1 ECE &amp; 2 IPAI, Seoul National University</cell></row><row><cell>3 Pohang University of Science and Technology (POSTECH)</cell></row><row><cell>4 Deeping Source Inc.</cell></row><row><cell>https://github.com/byminji/SLTtrack</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 1 :</head><label>1</label><figDesc>Experimental results on NFS, UAV123, TNL2K, VOT2018, and VOT2020 baseline analysis. Test-time hyper-parameters are tuned only in VOT.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">NFS UAV123 TNL2K AUC AUC AUC</cell><cell cols="4">VOT2018 EAO AO* AUC* EAO</cell><cell>VOT2020 A</cell><cell>R</cell></row><row><cell>SiamRPN++</cell><cell>Base +SLT</cell><cell>50.5 56.5</cell><cell>59.3 61.2</cell><cell>38.8 44.1</cell><cell>39.7 34.3</cell><cell>45.2 48.8</cell><cell>44.9 48.6</cell><cell cols="2">24.3 45.5 65.5 25.4 45.3 70.8</cell></row><row><cell>TrDiMP</cell><cell>Base +SLT</cell><cell>65.0 65.6</cell><cell>64.8 66.3</cell><cell>49.8 50.7</cell><cell>44.8 45.0</cell><cell>53.6 54.1</cell><cell>53.2 53.6</cell><cell cols="2">28.2 46.8 74.7 28.9 47.0 76.2</cell></row><row><cell>TransT</cell><cell>Base +SLT</cell><cell>65.3 66.2</cell><cell>66.6 68.6</cell><cell>53.5 55.0</cell><cell>30.0 30.6</cell><cell>51.0 52.6</cell><cell>50.6 52.3</cell><cell cols="2">29.3 47.7 75.3 29.3 46.7 76.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Both trackers A and B adopt SiamRPN++ as their network architectures, but the backbone of tracker A is frozen in the early training stages.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* These authors contributed equally to this work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (2016) 1, 4</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2019) 1, 4, 5</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Real-time &apos;actor-critic&apos; tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transformer tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>In: CVPR (2021) 1, 3, 4, 5, 7, 8</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Siamese box adaptive network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020) 1, 4</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Hyperparameter optimization for tracking with continuous deep q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2019) 3, 4</title>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiple people tracking using body and joint detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep q-learning from demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Joint monocular 3d vehicle detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning policies for adaptive tracking with deep feature cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tracking as online decision-making: Learning a policy from streaming videos with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S S</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02838</idno>
		<title level="m">Visual object tracking with discriminative filters and siamese networks: A survey and outlook</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real-time MDNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiani</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking VOT2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV Workshops</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The eighth visual object tracking vot2020 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>K?m?r?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">?</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luke?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Drbohlav</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Junliang Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning for visual tracking: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Marvasti-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ghanei-Yakhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">TrackingNet: a large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ECCV</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning with iterative shift for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tracking by instance detection: A metalearning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformer meets tracker: Exploiting temporal context for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2021) 1, 3, 4, 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Towards more flexible and accurate object tracking with natural language: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal transformer for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Alpha-refine: Boosting tracking performance by precise bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformable siamese attention networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2020) 1, 3, 4, 5</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Action-decision networks for visual tracking with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Young Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visual tracking via hierarchical deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI (2021)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Online decision based visual tracking via reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning the model update for siamese trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
