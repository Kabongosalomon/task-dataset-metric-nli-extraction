<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepFaceLab: Integrated, flexible and extensible face-swapping framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Petrov Freelancer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Daiheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freelancer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunlin</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugasa</forename><forename type="middle">Marangonda</forename><surname>Freelancer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">Um?</forename><surname>Vfx</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ume</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiang</surname></persName>
							<email>jiangjian@008tech.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tech</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Rp Freelancer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang Freelancer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingyu</forename><forename type="middle">Wu</forename><surname>Freelancer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
							<email>zhangwm@ustc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">USTC</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">USTC</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepFaceLab: Integrated, flexible and extensible face-swapping framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Nikolay Chervoniy Freelancer</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deepfake defense not only requires the research of detection but also requires the efforts of generation methods. However, current deepfake methods suffer the effects of obscure workflow and poor performance. To solve this problem, we present DeepFaceLab, the current dominant deepfake framework for face-swapping. It provides the necessary tools as well as an easy-to-use way to conduct highquality face-swapping. It also offers a flexible and loose coupling structure for people who need to strengthen their pipeline with other features without writing complicated boilerplate code. We detail the principles that drive the implementation of DeepFaceLab and introduce its pipeline, through which every aspect of the pipeline can be modified painlessly by users to achieve their customization purpose. It is noteworthy that DeepFaceLab could achieve cinemaquality results with high fidelity. We demonstrate the advantage of our system by comparing our approach with other face-swapping methods. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since deep learning has empowered the realm of computer vision in recent years, manipulating digital images, especially the manipulation of human portrait images, has improved rapidly and achieved photorealistic results in most cases. Face swapping is an eye-catching task in generating fake content by transferring a source face to the destination while maintaining the destination's facial movements and expression deformations.</p><p>The fundamental motivation behind face manipulation techniques is Generative Adversarial Networks (GANs) <ref type="bibr" target="#b6">[7]</ref>. More and more faces synthesized by StyleGAN <ref type="bibr" target="#b9">[10]</ref>, Style-GAN2 <ref type="bibr" target="#b10">[11]</ref> are becoming more and more realistic and en- <ref type="bibr" target="#b0">1</ref> For more information, please visit: https://github.com/iperov/DeepFaceLab/.</p><p>(Kunlin Liu is the corresponding author.) <ref type="figure">Figure 1</ref>. Face swapping results generated by DeepFaceLab. Left: Source face. Middle: Destination face for replacement. Our results appear on the right, demonstrating that DeepFaceLab could handle occlusion, bad illumination , and side face with high fidelity.</p><p>tirely indistinguishable for the human vision system. Numerous spoof videos synthesized by GAN-based face-swapping methods are published on YouTube and other video websites. Commercial mobile applications such as ZAO <ref type="bibr" target="#b1">2</ref> and FaceApp 3 which allow general netizens to create fake images and videos effortlessly significantly boost the spreading of these swapping techniques, called deepfake.</p><p>These content generation and modification technologies may affect public discourse quality and infringe upon the citizens' right of portrait, especially given that deepfake may be used maliciously as a source of misinformation, manipulation, harassment, and persuasion. Identifying manipulated media is a technically demanding and rapidly evolving challenge that requires collaborations across the entire tech industry and beyond.</p><p>Research on media anti-forgery detection is being invigorated and dedicating growing efforts to forgery face detection. DFDC 4 is a typical example, a million-dollar competition launched by Facebook and Microsoft. Training robust forgery detection models requires high-quality fake data. Data generated by our methods are involved in the DFDC dataset <ref type="bibr" target="#b4">[5]</ref>.</p><p>However, detection after being attacked is not the unique manner for reducing the malicious influence of deepfake. It is always too late to detect spreading spoofing content. In our perspective, for both academia and the general public, helping netizens know what deepfake is and how a cinemaquality swapped video is generated is much better. As the old saying goes:"The best defense is a good offense". Making general netizens realize the existence of deepfake and strengthening their identification ability for spoof media published in social networks is much more critical than agonizing the fact whether spoof media is true or not. 2 https://apps.apple.com/cn/app/id1465199127 3 https://apps.apple.com/gb/app/faceapp-ai-face-editor/id1180884341 4 https://deepfakedetectionchallenge.ai/ In 2018, DeepFakes <ref type="bibr" target="#b2">[3]</ref> introduced a complete production pipeline in replacing a source person's face with the target person's along with the same facial expression such as eye movement, facial muscle movement. However, the results produced by DeepFakes are poor somehow, so are the results with contemporary Nirkin et al.'s automatic face swapping <ref type="bibr" target="#b14">[15]</ref>. In order to further awaken people's awareness of facial-manipulation videos and provide convenience for forgery detection researchers, we established an open-source deepfake project, DeepfaceLab (DFL for short), which is used to build enormous high-quality faceswapping videos for entertainment and greatly help the development of forgery detection by providing high-quality forgery data. This paper introduces DeepFaceLab, an integrated opensource system with a clean-state design of the pipeline, achieving photorealistic face-swapping results without painful tuning. DFL has turned out to be very popular with the public. For instance, many artists create DFL-based videos and publish them on their YouTube channels. These videos made by DFL have more than 100 million hits.</p><p>The contributions of DeepFaceLab can be summarized as three-folds:</p><p>? A state-of-the-art framework consists of a maturity pipeline is proposed, aiming to achieve photorealistic face-swapping results.</p><p>? DeepFaceLab open-sourced the code in 2018 and always kept up to the progress in the computer vision area, making a positive contribution for defending deepfake, which has drawn broad attention in the open-source community and VFX areas.</p><p>? A series of high-efficiency components and tools are introduced in DeepFaceLab to build better faceswapping videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Characteristics of DeepFaceLab</head><p>DeepFaceLab's success stems from weaving previous ideas into a design that balances speed and ease of use and the booming of computer vision in face recognition, alignment, reconstruction, segmentation, etc. There are Four main characteristics behind our implementation:</p><p>Convenience DFL strives to make the usage of its pipeline, including data loader and processing, model training, and post-processing, as easy and productive as possible. Unlike other face swapping systems, DFL provides a complete command-line tool with every aspect of the pipeline that could be implemented in the way that users choose. Notably, the complexity inherent and many hand-picked features for fine-grained control such as the canonical face landmark for face alignment should be handled internally and hidden behind DFL. People could achieve the smooth and photorealistic face-swapping results without the need for hand-picked features if they follow the settings of the workflow, but only with the need of two videos: the source video (src) and the destination video (dst) without the requirement to pair the same facial expression between src and dst. To some extent, DFL could function as a pointand-shoot camera.</p><p>Wide engineering support Some practical measures were added to improve the performance: multi-GPU support, half-precision training, usage of pinned CUDA memory to improve throughput, use of multiple threads to accelerate graphics operations and data processing. Even a machine with 2GB VRAM can also conduct a successful face-swapping project.</p><p>Extensibility To strengthen the flexibility of DFL's workflow and attract the interests of the research community, users are free to replace any component of DFL that does not meet their requirements. Most of DFL's modules are designed to be interchangeable. For instance, people could provide a newer face detector to achieve higher performance in detecting faces with extreme angles or outlying areas.</p><p>Scalability Having good datasets is essential for the faceswapping task. Generally, the larger the datasets, the better the final results. However, results that are directly extracted from src and dst are always with noises, which significantly harm the final quality. In consideration of the complex situations of input videos, DFL provides a series of measures to clean up datasets. With these measures, DFL has robust scalability and can even support massive scale datasets and conduct cinema-quality face-swapping basing on large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pipeline</head><p>DeepFaceLab provides a set of workflow which form the flexible pipeline. In DeepFaceLab (DFL for short), we can abstract the pipeline into three phases: extraction, training, and conversion. These three parts are presented sequentially. Besides, it is noteworthy that DFL falls in a typical one-to-one face-swapping paradigm, which means there are only two kinds of data: src and dst, the abbreviation for source and destination, are used in the following narrative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Extraction</head><p>The extraction phase is the first phase in DFL, aiming to extract a face from src and dst data. This phase consists of many algorithms and processing parts, i.e., face detection, face alignment, and face segmentation. DFL provides many extraction modes (i.e, half-face, full-face, wholeface), which represents the face coverage area of the extraction phase. Generally, we take full-face mode by default.</p><p>Face Detection The first step in extraction phase is to find the target face in the given data: src and dst. DFL regards S3FD <ref type="bibr" target="#b23">[24]</ref> as its default face detector. S3FD can be replaced with other face detection algorithms painlessly, i.e RetinaFace <ref type="bibr" target="#b3">[4]</ref>.</p><p>Face Alignment The second step is face alignment. After numerous experiments and failures, we realized that facial landmarks are the key to maintaining stability over time. We need to find an effective facial landmarks algorithm essential in producing an excellent successive footage shot and film.</p><p>DFL provides two canonical types of facial landmark extraction algorithms to solve this: (a) heatmap-based facial landmark algorithm 2DFAN <ref type="bibr" target="#b1">[2]</ref> (for faces with standard pose) and (b) PRNet <ref type="bibr" target="#b5">[6]</ref> with 3D face prior information (for faces with large Euler angle (yaw, pitch, roll), e.g., A face with a large yaw angle, means one side of the face is out of sight). After facial landmarks are retrieved, we also provide an optional function with a configurable time step to smooth facial landmarks of consecutive frames in a single shot to ensure stability further.</p><p>Then we adopt a classical point pattern mapping and transformation method proposed by Umeyama <ref type="bibr" target="#b20">[21]</ref> to calculate a similarity transformation matrix used for face alignment. As the method proposed by Umeyama et al. <ref type="bibr" target="#b20">[21]</ref> needs standard facial landmark templates in calculating similarity transformation matrix, DFL provides a canonical aligned facial landmark template. It is noteworthy that DFL could automatically predict the Euler angle by using the obtained facial landmarks. Face Segmentation After face alignment, a data folder with face of standard front/side-view (aligned src or aligned dst) is obtained. We employ a fine-grained Face Segmentation network (TernausNet <ref type="bibr" target="#b7">[8]</ref>) on top of (aligned src or aligned dst), through which a face with either hair, fingers, or glasses could be segmented exactly. It is optional but useful to remove irregular occlusions to keep the network in the training process robust to hands, glasses, and any other objects which may cover the face somehow.</p><p>However, since some state-of-the-art face segmentation models fail to generate fine-grained masks in some particular shots, the XSeg was introduced in DFL. XSeg allows everyone to train their model for the segmentation of a specific face set (aligned src or aligned dst) through a few-shot learning paradigm ( <ref type="figure" target="#fig_7">Figure 8</ref> is the schematic of XSeg).</p><p>As the above workflow executed sequentially, everything DFL needs in the training phase is already prepared: cropped faces with their corresponding coordinates in its original images, facial landmarks, aligned faces, and pixelwise segmentation masks from src (Since the extraction procedure of dst is the same with src, there is no need to elaborate that in detail).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>The training phase plays the most vital role in achieving photorealistic face-swapping results of DFL.</p><p>No need for facial expressions of aligned src and aligned dst being strictly matched, DFL aims to provide an efficient algorithm paradigm to solve this unpaired problem along with maintaining high fidelity and perceptual quality of the generated face. Motivated by the previous methods, we propose two structures, DF structure, and LIAE structure, to address this issue.</p><p>As shown in <ref type="figure">Figure 3</ref>(a), DF structure consists of an Encoder as well as Inter with shared weights between src and dst, two Decoders which belong to src and dst separately. The generalization of src and dst is achieved through the shared Encoder and Inter, which solves the aforementioned unpaired problem easily. DF structure can finish the face-swapping task but cannot inherit enough information from dst, such as lighting.</p><p>To further enhance the problem of light consistency, we propose LIAE. As depicted in <ref type="figure">Figure 3(b)</ref>, LIAE structure is a more complex structure with a shared-weight Encoder, Decoder and two independent Inters. The main difference compared to the DF is that InterAB is used to generate both latent code of src and dst while InterB only output the latent code of dst. Here, F AB src denotes the latent code of src produced by InterAB and we generalize this representation to F AB dst , F B dst . After getting all the latent codes from InterAB and InterB, LIAE then concatenate these feature maps through channel: F AB src ||F AB src is obtained for a new latent code representation of src and F AB dst ||F B dst for dst as the same way.</p><p>Then F AB src ||F AB src and F AB dst ||F B dst are put into the Decoder and we get the predicted src (dst) alongside with their masks. The motivation of concatenating F B dst with F AB dst is to shift the direction of latent code in direction of the class (src or dst) we need, through which InterAB obtained a compact and well-aligned representation of src and dst in the latent space.</p><p>Except for the structure of the model, some useful tricks are effective for improving the quality of the generated face: A weighted sum mask loss in general SSIM <ref type="bibr" target="#b21">[22]</ref> can be added to make each part of the face carry different weights under the AE training architecture, for example, we add more weights to the eye area than the cheek, which aims to make the network concentrate on generating a face with vivid eyes.</p><p>As for losses, DFL uses a mixed loss (DSSIM (structural dissimilarity) <ref type="bibr" target="#b13">[14]</ref> + MSE) by default. The motivation for this combination is to get benefits from both: DSSIM generalizes human faces faster while MSE provides better clarity. This combination of losses serves to find a compromise between generalization and clarity.</p><p>Besides, we adopt a fancy true face mode TrueFace, which serves for the generated face of better likeness to the dst in the conversion phase. For LIAE structure, we enforce F AB src approaches F AB dst . And for DF structure , counterparts turn out to be F src and F dst . Two rarely used methods have been validated by DFL: Convolutional Aware Initialization <ref type="bibr" target="#b0">[1]</ref> along with Learning Rate Dropout <ref type="bibr" target="#b12">[13]</ref>, which greatly enhanced the final quality of the fake face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Conversion</head><p>The conversion phase is the last but not least phase. Previous methods often ignore the importance of this phase. As depicted in <ref type="figure" target="#fig_2">Figure 4</ref>, users can swap faces of src to dst and vice versa.</p><p>In the case of src2dst, the first step of the proposed faceswapping scheme in the conversion phase is to transform the generated face alongside with its mask from dst Decoder to the original position of the target image in src due to the reversibility of Umeyama <ref type="bibr" target="#b20">[21]</ref>.</p><p>The following piece is about blending, with the ambition for the realigned reenacted face to seamlessly fit with the target image along its outer contour. For the sake of remaining consistent complexion, DFL provides five more color transfer algorithms (i.e., Reinhard color transfer: RCT <ref type="bibr" target="#b17">[18]</ref>, iterative distribution transfer: IDT <ref type="bibr" target="#b16">[17]</ref> and etc.) to approximate the color of the reenacted face to the target. Any blending must account for different skin tones, face shapes, and illumination conditions, especially at the junctions between reenacted face with the delimited region and target face. DFL implemented this by Poisson blending <ref type="bibr" target="#b15">[16]</ref>.</p><p>Finally, sharpening is indispensable. A pre-trained face super-resolution neural network was added to sharpen the blended face since it is noted that the generated faces in almost current state-of-the-art face-swapping works, more or less, are smoothed and lack minor details (i.e., moles, wrinkles).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>In this section, we compare the DeepFaceLab with several other commonly-used face-swapping frameworks and two state-of-the-art works. We find that DFL has competitive performance among them under identical experimental conditions.  <ref type="bibr" target="#b14">[15]</ref> and Face2Face <ref type="bibr" target="#b19">[20]</ref>) taken from FaceForensics++ dataset <ref type="bibr" target="#b18">[19]</ref>. Examples of different expressions, face shapes, and illuminations are selected in our experiment. It's clear from observing the video clips from FaceForensics++ that they are not only trained inadequately but chosen from models with low-resolution. To be fair in our comparison, Quick96 mode is taken: a lightweight model that DF structure underneath, which outputs the I output of 96 ? 96 resolutions (without GAN and TrueFace). The average training time is restricted to 3 hours. We use Adam optimizer (lr=0.00005, ? 1 = 0.5, ? 2 = 0.999) to optimize our model. All of our networks were trained on a single NVIDIA GeForce GTX 1080Ti GPU and an Intel Core i7-8700 CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Qualitative results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative results</head><p>We compare our results with videos of FaceForensics++ in quantitative experiments. In practice, the naturalness and realness of the results of the face-swapping method are hard to describe with some specified quantitative indexes. However, pose and expression indeed embodies valuable insights of the face-swapping results. Besides, SSIM is used to compare the structure similarity as well as perceptual loss <ref type="bibr" target="#b8">[9]</ref> is adopted to compare high-level differences, like content and style discrepancies, between the target subject and the swapped subject.</p><p>To measure the accuracy of the pose, we calculate the Euclidean distance between the Euler angles (extracted through FSA-Net <ref type="bibr" target="#b22">[23]</ref>) of I t and I output . Besides, the accuracy of the facial expression is measured through the Euclidean distance between the 2D landmarks (2DFAN <ref type="bibr" target="#b1">[2]</ref>). We use the default face verification method of DLIB <ref type="bibr" target="#b11">[12]</ref> for the comparison of identities.</p><p>To be statistically significant, we compute the mean and variance of those measurements on the 100 frames (uniform sampling over time) of the first 500 videos in FaceForen-sics++, averaging them across the videos. Here, Deep-Fakes <ref type="bibr" target="#b2">[3]</ref> and Nirkin et al. <ref type="bibr" target="#b14">[15]</ref> are chosen as the baselines to compare. It should be noted that all the videos produced by DFL were followed by the same settings with 4.1.</p><p>From the indicators listed in <ref type="table" target="#tab_0">Table 1</ref>, DFL is more adept at retaining pose and expression than baselines. Besides, with the empowerment of super-resolution in the conversion phase, DFL often produces I output with vivid eyes and sharp teeth, but this phenomenon couldn't be reflected clearly in the SSIM-like score for they only take a small part of the whole face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation study</head><p>To compare the visual effects of different model choices, GAN settings and etc., we perform several ablation tests. The ablation study is conducted on top of three essential parts: network structure, training paradigm, and latent space constraint.</p><p>Aside from DF structure and LIAE structure, we also enhance them to DFHD and LIAEHD by adding more feature extraction layers and residual blocks than the original version, which enriches the model structures for comparison. Related details are demonstrated in the supplemental material. The qualitative results of different model structures can be seen in <ref type="figure" target="#fig_3">Figure 6</ref>, and the qualitative results of different training paradigms are depicted in <ref type="figure" target="#fig_6">Figure 7</ref>. As shown in <ref type="figure" target="#fig_3">Figure 6</ref>, we can see that LIAE can inherit a wellshaped face shape from dst and generate more advanced results than DF, which solves the unmatched face shape problem gracefully. Moreover, to probe whether the introduction of GAN works in DFL, we compare GAN-based with non-GAN-based in <ref type="figure" target="#fig_6">Figure 7</ref>, it is apparent that the details of the face are more realistic and lumpy than non-GAN generated.</p><p>Quantitative ablation results are reported in <ref type="table">Table 2</ref>. The experiment settings of the training are almost the same with 4.1 except for the structure of the model. <ref type="table">Table 2</ref> show that source identities are preserved across networks with the same structure. With more shortcut connections added to the model (i.e., DF to DFHD, LIAE to LIAEHD), scores of landmarks and pose decrease without GAN. Meanwhile, the generated results could have a better chance to get rid of the influence of the source face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verification results from</head><p>Also, we found that TrueFace is effectively relieved the instability of GAN, through which a more photo-realistic result without much degradation is then achieved. Besides, SSIM progressively increases with more shortcut connections, TrueFace and GAN also do good to it in varying degrees.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Integrity</head><p>Previous methods often lack integrity. As mentioned in Sec.3, DFL consists of three main phases, extraction, train-ing, and conversion. Each phase plays a different role and has various kinds of alternative techniques. Thanks to the long development progress, DFL has become the most mature face-swapping system in the world. For example, we provide several kinds of face segmentation methods. It is  noteworthy that DFL is not a simple combination of current state-of-the-art methods. Instead, most efficient tools are developed by ourselves according to users' requirements.</p><p>Considering the complex requirements for face segmentation, DFL provides an automatic algorithm, TernausNet <ref type="bibr" target="#b7">[8]</ref> as default. As mentioned in Section 3.1, TernausNet can remove irregular occlusions efficiently. However, this model may fail to generate fine-grained masks in some particular shots. So we develop a highefficiency face segmentation tool, XSeg, which allows everyone to customize to suit specific needs by few-shot learning. As shown in <ref type="figure" target="#fig_7">Figure 8</ref>, users can create the mask label and train a customized segmentation model. With the help of XSeg, users can define the swapping mask by themselves and solve almost all problems in the extraction and conversion phase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Potential</head><p>Previous methods usually focus on synthesizing highquality results by feeding two videos or hundreds of images. However, making good face-swapping results in this manner is not reasonable at all. DFL supports huge-scale datasets, up to ? 100k images. With the help of enormous data, final swapped results can achieve significant quality improvements. Besides, previous methods always lack potentials. Unlike previous methods, DFL can provide face-swapping and support lip manipulation, head replacement, do-age and etc. These new functions can be realized by simply finetuning our framework. We also encourage users to use video editing tools, such as DaVinci Resolve and After Effect, further enhance the visual quality of the final video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Broader Impact</head><p>Since the attention deepfake-related productions received grew exponentially, DFL, the most commonly used deepfake generation tool for VFX artists, has played an irreplaceable role. The emergence of DFL certainly adds to the entertainment to the world meanwhile of high economic value in the post-production industry when it comes to replacing the stunt actor with pop stars.</p><p>Because DFL could produce the cinema-quality faceswapping result, researchers who engage in forgery detection areas may be motivated to design robust classifiers for high-quality forgery video clips or images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>The rapidly evolving DeepFaceLab has become a popular face-swapping tool in the deep learning practitioner community by freeing people from laborious, complicated data processing, trivial detailed work in training, and conversion. As more and more people participate in the development of DeepFaceLab, deepfake entertainment has been trending in social media. In the future, we want to dig deeper into the entertainment-related AI framework while pushing the forgery detection field forward.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Overview of extraction phase in DeepFaceLab (DFL for short). [DF Structure] [LIAE Structure] Overview of training phase in DeepFaceLab (DFL). DF structure and LIAE structure are both provided here for illustration, ? ? represents the concatenation of latent vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 5 (</head><label>5</label><figDesc>a) offers face-swapping results of representative open-source projects (DeepFakes [3], Nirkin et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Overview of conversion phase in DeepFaceLab(DFL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>6 (</head><label>6</label><figDesc>a) The comparison of DFL and representative open-source face-swapping projects. (b) The comparison of DFL and the latest state-of-the-art face-swapping works.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative face swapping results on FaceForensics++<ref type="bibr" target="#b18">[19]</ref> face images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Ablation experiments of different model structures (with GAN and TrueFace). (Here, we provide training previews instead of the converted faces, which aims to make a fair comparison in model architectures of DFL meanwhile avoid the impact of postprocessing from the conversion phase.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Ablation experiments of different training paradigms: non-GAN-based and GAN-based (The image on the left is the original face, a reconstruction image produced by a model that trained without GAN listed to its right, far right is produced by a model trained with GAN). Obviously, GAN enforces the model to become more sensible in capturing the sharp details, i.e., wrinkles and moles. Meanwhile, significantly reduce the vagueness compared to the model without the empower of GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>The preview of XSeg. Users can label the masks they want by XSegEditor. With the help of XSeg, users can use it to eliminate the occlusion of hands, glasses, and any other objects which may cover the face somehow and control specific areas for swapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative face swapping results on FaceForensics++ [19] face images. Method SSIM ? perceptual loss ? verification ? landmarks ? pose ? DeepFakes 0.71 ? 0.07 0.41 ? 0.05 0.69 ? 0.04 1.15 ? 1.10 4.75 ? 1.73 Nirkin et al. 0.65 ? 0.08 0.50 ? 0.08 0.66 ? 0.05 0.35 ? 0.18 6.01 ? 3.21 DFL(ours) 0.73 ? 0.07 0.39 ? 0.04 0.61 ? 0.04 0.73 ? 0.36 1.12 ? 1.07Table 2. Quantitative ablation results on FaceForensics++ [19] face images. TrueFace) 0.80 ? 0.04 0.58 ? 0.03 0.65 ? 0.33 0.83 ? 0.81</figDesc><table><row><cell>Method</cell><cell>SSIM ?</cell><cell>verification ? landmarks ? pose ?</cell></row><row><cell>DF</cell><cell cols="2">0.73 ? 0.07 0.61 ? 0.04 0.73 ? 0.36 1.12 ? 1.07</cell></row><row><cell>DFHD</cell><cell cols="2">0.75 ? 0.09 0.61 ? 0.04 0.71 ? 0.37 1.06 ? 0.97</cell></row><row><cell>DFHD (GAN)</cell><cell cols="2">0.72 ? 0.11 0.61 ? 0.04 0.79 ? 0.40 1.33 ? 1.21</cell></row><row><cell>DFHD (GAN + TrueFace)</cell><cell cols="2">0.77 ? 0.06 0.61 ? 0.04 0.70 ? 0.35 0.99 ? 1.02</cell></row><row><cell>LIAE</cell><cell cols="2">0.76 ? 0.06 0.58 ? 0.03 0.66 ? 0.32 0.91 ? 0.86</cell></row><row><cell>LIAEHD</cell><cell cols="2">0.78 ? 0.06 0.58 ? 0.03 0.65 ? 0.32 0.90 ? 0.88</cell></row><row><cell>LIAEHD (GAN)</cell><cell cols="2">0.79 ? 0.05 0.58 ? 0.03 0.69 ? 0.34 1.00 ? 0.97</cell></row><row><cell>LIAEHD (GAN +</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06295</idno>
		<title level="m">Convolution aware initialization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepfakes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepfakes</surname></persName>
		</author>
		<ptr target="https://github.com/deepfakes/faceswap,2017.2" />
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Retinaface</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00641</idno>
		<title level="m">Singlestage dense face localisation in the wild</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Menglin Wang, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Dolhansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pflaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jikuo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Howes</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="534" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Shvets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05746</idno>
		<title level="m">Ternausnet: U-net with vgg11 encoder pre-trained on imagenet for image segmentation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04958</idno>
		<title level="m">Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangxing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Paisley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00144</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Learning rate dropout. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structural similarity-based object tracking in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur</forename><surname>Loza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyudmila</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishan</forename><surname>Canagarajah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Information Fusion</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On face segmentation, face swapping, and face perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2003 Papers</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Automated colour grading using colour distribution transfer. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Piti?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rozenn</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahyot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="123" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Adhikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer graphics and applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faceforen-sics++: Learning to detect manipulated facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Rossler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face2face: Real-time face capture and reenactment of rgb videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2387" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fsa-net: Learning fine-grained structure aggregation for head pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1087" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">S3fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
