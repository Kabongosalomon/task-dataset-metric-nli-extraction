<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Score-based Generative Modeling in Latent Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
							<email>avahdat@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
							<email>kkreis@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<email>jkautz@nvidia.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Score-based Generative Modeling in Latent Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The long-standing goal of likelihood-based generative learning is to faithfully learn a data distribution, while also generating high-quality samples. Achieving these two goals simultaneously is a tremendous challenge, which has led to the development of a plethora of different generative models. Recently, score-based generative models (SGMs) demonstrated astonishing results in terms of both high sample quality and likelihood <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. These models define a forward diffusion process that maps data to noise by gradually perturbing the input data. Generation corresponds to a reverse process that synthesizes novel data via iterative denoising, starting from random noise. The problem then reduces to learning the score function-the gradient of the log-density-of the perturbed data <ref type="bibr" target="#b2">[3]</ref>. In a seminal work, Song et al. <ref type="bibr" target="#b1">[2]</ref> show how this modeling approach is described with a stochastic differential equation (SDE) framework which can be converted to maximum likelihood training <ref type="bibr" target="#b3">[4]</ref>. Variants of SGMs have been applied to images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, audio <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>, graphs <ref type="bibr" target="#b10">[11]</ref> and point clouds <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Albeit high quality, sampling from SGMs is computationally expensive. This is because generation amounts to solving a complex SDE, or equivalently ordinary differential equation (ODE) (denoted as the probability flow ODE in <ref type="bibr" target="#b1">[2]</ref>), that maps a simple base distribution to the complex data distribution. The resulting differential equations are typically complex and solving them accurately requires numerical integration with very small step sizes, which results in thousands of neural network evaluations <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref>. Furthermore, generation complexity is uniquely defined by the underlying data distribution and the forward SDE for data perturbation, implying that synthesis speed cannot be <ref type="figure">Figure 1</ref>: In our latent score-based generative model (LSGM), data is mapped to latent space via an encoder q(z0|x) and a diffusion process is applied in the latent space (z0 ? z1). Synthesis starts from the base distribution p(z1) and generates samples in latent space via denoising (z0 ? z1). Then, the samples are mapped from latent to data space using a decoder p(x|z0). The model is trained end-to-end. increased easily without sacrifices. Moreover, SDE-based generative models are currently defined for continuous data and cannot be applied effortlessly to binary, categorical, or graph-structured data.</p><p>Here, we propose the Latent Score-based Generative Model (LSGM), a new approach for learning SGMs in latent space, leveraging a variational autoencoder (VAE) framework <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. We map the input data to latent space and apply the score-based generative model there. The score-based model is then tasked with modeling the distribution over the embeddings of the data set. Novel data synthesis is achieved by first generating embeddings via drawing from a simple base distribution followed by iterative denoising, and then transforming this embedding via a decoder to data space (see <ref type="figure">Fig. 1</ref>). We can consider this model a VAE with an SGM prior. Our approach has several key advantages: Synthesis Speed: By pretraining the VAE with a Normal prior first, we can bring the marginal distribution over encodings (the aggregate posterior) close to the Normal prior, which is also the SGM's base distribution. Consequently, the SGM only needs to model the remaining mismatch, resulting in a less complex model from which sampling becomes easier. Furthermore, we can tailor the latent space according to our needs. For example, we can use hierarchical latent variables and apply the diffusion model only over a subset of them, further improving synthesis speed. Expressivity: Training a regular SGM can be considered as training a neural ODE directly on the data <ref type="bibr" target="#b1">[2]</ref>. However, previous works found that augmenting neural ODEs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and more generally generative models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> with latent variables improves their expressivity. Consequently, we expect similar performance gains from combining SGMs with a latent variable framework. Tailored Encoders and Decoders: Since we use the SGM in latent space, we can utilize carefully designed encoders and decoders mapping between latent and data space, further improving expressivity. Additionally, the LSGM method can therefore be naturally applied to non-continuous data.</p><p>LSGMs can be trained end-to-end by maximizing the variational lower bound on the data likelihood. Compared to regular score matching, our approach comes with additional challenges, since both the score-based denoising model and its target distribution, formed by the latent space encodings, are learnt simultaneously. To this end, we make the following technical contributions: (i) We derive a new denoising score matching objective that allows us to efficiently learn the VAE model and the latent SGM prior at the same time. (ii) We introduce a new parameterization of the latent space score function, which mixes a Normal distribution with a learnable SGM, allowing the SGM to model only the mismatch between the distribution of latent variables and the Normal prior. (iii) We propose techniques for variance reduction of the training objective by designing a new SDE and by analytically deriving importance sampling schemes, allowing us to stably train deep LSGMs. Experimentally, we achieve state-of-the-art 2.10 FID on CIFAR-10 and 7.22 FID on CelebA-HQ-256, and significantly improve upon likelihoods of previous SGMs. On CelebA-HQ-256, we outperform previous SGMs in synthesis speed by two orders of magnitude. We also model binarized images, MNIST and OMNIGLOT, achieving state-of-the-art likelihood on the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Here, we review continuous-time score-based generative models (see <ref type="bibr" target="#b1">[2]</ref> for an in-depth discussion). Consider a forward diffusion process {z t } t=1 t=0 for continuous time variable t ? [0, 1], where z 0 is the starting variable and z t its perturbation at time t. The diffusion process is defined by an It? SDE: dz = f (t)z dt + g(t) dw (1) where f : R ? R and g : R ? R are scalar drift and diffusion coefficients, respectively, and w is the standard Wiener process. f (t) and g(t) can be designed such that z 1 ? N (z 1 ; 0, I) follows a Normal distribution at the end of the diffusion process. <ref type="bibr" target="#b1">2</ref> Song et al. <ref type="bibr" target="#b1">[2]</ref> show that the SDE in Eq. 1 can be converted to a generative model by first sampling from z 1 ? N (z 1 ; 0, I) and then running the reverse-time SDE dz = [f (t)z?g(t) 2 ? z log q t (z)] dt+g(t) dw, wherew is a reverse-time standard Wiener process and dt is an infinitesimal negative time step. The reverse SDE requires knowledge of ? zt log q t (z t ), the score function of the marginal distribution under the forward diffusion at time t. One approach for estimating it is via the score matching objective <ref type="bibr" target="#b2">3</ref> :</p><formula xml:id="formula_0">min ? E t?U [0,1] ?(t)E q(z0) E q(zt|z0) [||? zt log q(z t ) ? ? zt log p ? (z t )|| 2 2 ]<label>(2)</label></formula><p>that trains the parameteric score function ? zt log p ? (z t ) at time t ? U[0, 1] for a given weighting coefficient ?(t). q(z 0 ) is the z 0 -generating distribution and q(z t |z 0 ) is the diffusion kernel, which is available in closed form for certain f (t) and g(t). Since ? zt log q(z t ) is not analytically available, Song et al. <ref type="bibr" target="#b1">[2]</ref> rely on denoising score matching <ref type="bibr" target="#b21">[22]</ref> that converts the objective in Eq. 2 to:</p><formula xml:id="formula_1">min ? E t?U [0,1] ?(t)E q(z0) E q(zt|z0) [||? zt log q(z t |z 0 ) ? ? zt log p ? (z t )|| 2 2 ] + C<label>(3)</label></formula><p>Vincent <ref type="bibr" target="#b21">[22]</ref> shows C = E t?U [0,1] [?(t)E q(z0) E q(zt|z0) [||? zt log q(z t )|| 2 2 ? ||? zt log q(z t |z 0 )|| <ref type="bibr">2 2</ref> ]] is independent of ?, making the minimizations in Eq. 3 and Eq. 2 equivalent. Song et al. <ref type="bibr" target="#b3">[4]</ref> show that for ?(t) = g(t) 2 /2, the minimizations correspond to approximate maximum likelihood training based on an upper on the Kullback-Leibler (KL) divergence between the target distribution and the distribution defined by the reverse-time generative SDE with the learnt score function. In particular, the objective of Eq. 2 can then be written: <ref type="bibr" target="#b0">1]</ref> g(t) <ref type="bibr" target="#b1">2</ref> 2 E q(z 0 ) E q(z t |z 0 ) ||?z t log q(zt) ? ?z t log p ? (zt)|| 2 2 <ref type="bibr" target="#b3">(4)</ref> which can again be transformed into denoising score matching (Eq. 3) following Vincent <ref type="bibr" target="#b21">[22]</ref>.</p><formula xml:id="formula_2">KL q(z0)||p ? (z0) ? E t?U [0,</formula><p>3 Score-based Generative Modeling in Latent Space</p><p>The LSGM framework in <ref type="figure">Fig. 1</ref> consists of the encoder q ? (z 0 |x), SGM prior p ? (z 0 ), and decoder p ? (x|z 0 ). The SGM prior leverages a diffusion process as defined in Eq. 1 and diffuses z 0 ? q ? (z 0 |x) samples in latent space to the standard Normal distribution p(z 1 ) = N (z 1 ; 0, I). Generation uses the reverse SDE to sample from p ? (z 0 ) with time-dependent score function ? zt log p ? (z t ), and the decoder p ? (x|z 0 ) to map the synthesized encodings z 0 to data space. Formally, the generative process is written as p(z 0 , x) = p ? (z 0 )p ? (x|z 0 ). The goal of training is to learn {?, ?, ?}, the parameters of the encoder q ? (z 0 |x), score function ? zt log p ? (z t ), and decoder p ? (x|z 0 ), respectively.</p><p>We train LSGM by minimizing the variational upper bound on negative data log-likelihood log p(x):</p><formula xml:id="formula_3">L(x, ?, ?, ?) = E q ? (z 0 |x) ? log p ? (x|z0) +KL q ? (z0|x)||p ? (z0) (5) = E q ? (z 0 |x) ? log p ? (x|z0) reconstruction term +E q ? (z 0 |x) log q ? (z0|x) negative encoder entropy +E q ? (z 0 |x) ? log p ? (z0)</formula><p>cross entropy <ref type="bibr" target="#b5">(6)</ref> following a VAE approach <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, where q ? (z 0 |x) approximates the true posterior p(z 0 |x).</p><p>In this paper, we use Eq. 6 with decomposed KL divergence into its entropy and cross entropy terms. The reconstruction and entropy terms are estimated easily for any explicit encoder as long as the reparameterization trick is available <ref type="bibr" target="#b13">[14]</ref>. The challenging part in training LSGM is to train the cross entropy term that involves the SGM prior. We motivate and present our expression for the cross-entropy term in Sec. 3.1, the parameterization of the SGM prior in Sec. 3.2, different weighting mechanisms for the training objective in Sec. 3.3, and variance reduction techniques in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Cross Entropy Term</head><p>One may ask, why not train LSGM with Eq. 5 and rely on the KL in Eq. 4. Directly using the KL expression in Eq. 4 is not possible, as it involves the marginal score ? zt log q(z t ), which is unavailable analytically for common non-Normal distributions q(z 0 ) such as Normalizing flows.</p><p>Transforming into denoising score matching does not help either, since in that case the problematic ? zt log q(z t ) term appears in the C term (see Eq. 3). In contrast to previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>, we cannot simply drop C, since it is, in fact, not constant but depends on q(z t ), which is trainable in our setup.</p><p>To circumvent this problem, we instead decompose the KL in Eq. 5 and rather work directly with the cross entropy between the encoder distribution q(z 0 |x) and the SGM prior p(z 0 ). We show: Theorem 1. Given two distributions q(z 0 |x) and p(z 0 ), defined in the continuous space R D , denote the marginal distributions of diffused samples under the SDE in Eq. 1 at time t with q(z t |x) and p(z t ). Assuming mild smoothness conditions on log q(z t |x) and log p(z t ), the cross entropy is:</p><formula xml:id="formula_4">CE(q(z0|x)||p(z0)) = E t?U [0,1] g(t) 2 2 E q(z t ,z 0 |x) ||?z t log q(zt|z0)??z t log p(zt)|| 2 2 + D 2 log 2?e? 2 0 , with q(z t , z 0 |x) = q(z t |z 0 )q(z 0 |x) and a Normal transition kernel q(z t |z 0 ) = N (z t ; ? t (z 0 ), ? 2 t I),</formula><p>where ? t and ? 2 t are obtained from f (t) and g(t) for a fixed initial variance ? 2 0 at t = 0. A proof with generic expressions for ? t and ? 2 t as well as an intuitive interpretation are in App. A. Importantly, unlike for the KL objective of Eq. 4, no problematic terms depending on the marginal score ? zt log q(z t |x) arise. This allows us to use this denoising score matching objective for the cross entropy term in Theorem 1 not only for optimizing p(z 0 ) (which is commonly done in the score matching literature), but also for the q(z 0 |x) encoding distribution. It can be used even with complex q(z 0 |x) distributions, defined, for example, in a hierarchical fashion <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> or via Normalizing flows <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Our novel analysis shows that, for diffusion SDEs following Eq. 1, only the cross entropy can be expressed purely with ? zt log q(z t |z 0 ). Neither KL nor entropy in <ref type="bibr" target="#b3">[4]</ref> can be expressed without the problematic term ? zt log q(z t |x) (details in the Appendix).</p><p>Note that in Theorem 1, the term ? zt log p(z t ) in the score matching expression corresponds to the score that originates from diffusing an initial p(z 0 ) distribution. In practice, we use the expression to learn an SGM prior p ? (z 0 ), which models ? zt log p(z t ) by a neural network. With the learnt score ? zt log p ? (z t ) (here we explicitly indicate the parameters ? to clarify that this is the learnt model), the actual SGM prior is defined via the generative reverse-time SDE (or, alternatively, a closely-connected ODE, see Sec. 2 and App. D), which generally defines its own, separate marginal distribution p ? (z 0 ) at t = 0. Importantly, the learnt, approximate score ? zt log p ? (z t ) is not necessarily the same as one would obtain when diffusing p ? (z 0 ). Hence, when considering the learnt score ? zt log p ? (z t ), the score matching expression in our Theorem only corresponds to an upper bound on the cross entropy between q(z 0 |x) and p ? (z 0 ) defined by the generative reverse-time SDE. This is discussed in detail in concurrent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref>. Hence, from the perspective of the learnt SGM prior, we are training with an upper bound on the cross entropy (similar to the bound on the KL in Eq. 4), which can also be considered as the continuous version of the discretized variational objective derived by Ho et al. <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mixing Normal and Neural Score Functions</head><p>In VAEs <ref type="bibr" target="#b13">[14]</ref>, p(z 0 ) is often chosen as a standard Normal N (z 0 ; 0, I). For recent hierarchical VAEs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>, using the reparameterization trick, the prior can be converted to N (z 0 ; 0, I) (App. E).</p><p>Considering a single dimensional latent space, we can assume that the prior at time t is in the form of a geometric mixture p(z t ) ? N (z t ; 0, 1) 1?? p ? (z t ) ? where p ? (z t ) is a trainable SGM prior and ? ? [0, 1] is a learnable scalar mixing coefficient. Formulating the prior this way has crucial advantages: (i) We can pretrain LSGM's autoencoder networks assuming ?=0, which corresponds to training the VAE with a standard Normal prior. This pretraining step will bring the distribution of latent variable close to N (z 0 ; 0, 1), allowing the SGM prior to learn a much simpler distribution in the following end-to-end training stage. (ii) The score function for this mixture is of the form ? zt log p(z t ) = ?(1 ? ?)z t + ?? zt log p ? (z t ). When the score function is dominated by the linear term, we expect that the reverse SDE can be solved faster, as its drift is dominated by this linear term.</p><p>For our multivariate latent space, we obtain diffused samples at time t by sampling z t ? q(z t |z 0 ) with z t = ? t (z 0 ) + ? t , where ? N ( ; 0, I). Since we have ? zt log q(z t |z 0 ) = ? /? t , similar to <ref type="bibr" target="#b0">[1]</ref>, we parameterize the score function by ? zt log p(z t ) :</p><formula xml:id="formula_5">= ? ? (z t , t)/? t , where ? (z t , t) := ? t (1 ? ?) z t + ? ? (z t , t)</formula><p>is defined by our mixed score parameterization that is applied elementwise to the components of the score. With this, we simplify the cross entropy expression to:</p><formula xml:id="formula_6">CE(q ? (z0|x)||p ? (z0)) = E t?U [0,1] w(t) 2 E q ? (z t ,z 0 |x), || ? ? (zt, t)|| 2 2 + D 2 log 2?e? 2 0 ,<label>(7)</label></formula><p>where w(t) = g(t) 2 /? 2 t is a time-dependent weighting scalar. Weighted</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training with Different Weighting Mechanisms</head><formula xml:id="formula_7">wll(t) = g(t) 2 /? 2 t Unweighted wun(t) = 1 Reweighted wre(t) = g(t) 2</formula><p>The weighting term w(t) in Eq. 7 trains the prior with maximum likelihood. Similar to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, we observe that when w(t) is dropped while training the SGM prior (i.e., w(t) = 1), LSGM often yields higher quality samples at a small cost in likelihood. However, in our case, we can only drop the weighting when training the prior. When updating the encoder parameters, we still need to use the maximum likelihood weighting to ensure that the encoder q(z 0 |x) is brought closer to the true posterior p(z 0 |x) <ref type="bibr" target="#b3">4</ref> . Tab. 1 summarizes three weighting mechanisms we consider in this paper: w ll (t) corresponds to maximum likelihood, w un (t) is the unweighted objective used by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, and w re (t) is a variant obtained by dropping only 1/? 2 t . This weighting mechanism has a similar affect on the sample quality as w un (t) = 1; however, in Sec. 3.4, we show that it is easier to define a variance reduction scheme for this weighting mechanism.</p><p>The following summarizes our training objectives (with t ? U[0, 1] and ? N ( ; 0, I)):</p><formula xml:id="formula_8">min ?,? E q ? (z 0 |x) ?log p ? (x|z0) +E q ? (z 0 |x) log q ? (z0|x) +E t, ,q(z t |z 0 ),q ? (z 0 |x) wll(t) 2 || ? ? (zt, t)|| 2 2 (8) min ? E t, ,q(z t |z 0 ),q ? (z 0 |x) w ll/un/re (t) 2 || ? ? (zt, t)|| 2 2 with q(zt|z0) = N (zt; ? t (z0), ? 2 t I),<label>(9)</label></formula><p>where Eq. 8 trains the VAE encoder and decoder parameters {?, ?} using the variational bound L(x, ?, ?, ?) from Eq. 6. Eq. 9 trains the prior with one of the three weighting mechanisms. Since the SGM prior participates in the objective only in the cross entropy term, we only consider this term when training the prior. Efficient algorithms for training with the objectives are presented in App. G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Variance Reduction</head><p>The objectives in Eqs. 8 and 9 involve sampling of the time variable t, which has high variance <ref type="bibr" target="#b25">[26]</ref>. We introduce several techniques for reducing this variance for all three objective weightings. We focus on the "variance preserving" SDEs (VPSDEs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27]</ref>, defined by dz = ? 1 2 ?(t)z dt + ?(t) dw where ?(t) = ? 0 + (? 1 ? ? 0 )t linearly interpolates in [? 0 , ? 1 ] (other SDEs discussed in App. B).</p><p>We denote the marginal distribution of latent variables by q(z 0 ) := E pdata(x) [q(z 0 |x)]. Here, we derive variance reduction techniques for CE(q(z 0 )||p(z 0 )), assuming that both q(z 0 ) = p(z 0 ) = N (z 0 ; 0, I). This is a reasonable simplification for our analysis because pretraining our LSGM model with a N (z 0 ; 0, I) prior brings q(z 0 ) close to N (z 0 ; 0, I) and our SGM prior is often dominated by the fixed Normal mixture component. We empirically observe that the variance reduction techniques developed with this assumption still work well when q(z 0 ) and p(z 0 ) are not exactly N (z 0 ; 0, I).</p><p>Variance reduction for likelihood weighting: In App. B, for q(z 0 ) = p(z 0 ) = N (z 0 ; 0, I), we show CE(q(z 0 )||p(z 0 )) is given by D 2 E t?U [0,1] [d log ? 2 t /dt] + const. We consider two approaches: (1) Geometric VPSDE: To reduce the variance sampling uniformly from t, we can design the SDE such that d log ? 2 t /dt is constant for t ? [0, 1]. We show in App. B that a ?(t) = log(? 2 max /? 2 min )</p><formula xml:id="formula_9">? 2 t (1?? 2 t )</formula><p>with geometric variance ? 2 t = ? 2 min (? 2 max /? 2 min ) t satisfies this condition. We call a VPSDE with this ?(t) a geometric VPSDE. ? 2 min and ? 2 max are the hyperparameters of the SDE, with 0 &lt; ? 2 min &lt; ? 2 max &lt; 1. Although our geometric VPSDE has a geometric variance progression similar to the "variance exploding" SDE (VESDE) <ref type="bibr" target="#b1">[2]</ref>, it still enjoys the "variance preserving" property of the VPSDE. In App. B, we show that the VESDE does not come with a reduced variance for t-sampling by default.</p><p>(2) Importance sampling (IS): We can keep ?(t) and ? 2 t unchanged for the original linear VPSDE, and instead use IS to minimize variance. The theory of IS shows that the proposal r(t) ? d log ? 2 t /dt has minimum variance <ref type="bibr" target="#b27">[28]</ref>. In App. B, we show that we can sample from r(t) using inverse transform sampling t = var ?1 ((? 2 1 ) ? (? 2 0 ) 1?? ) where var ?1 is the inverse of ? 2 t and ? ? U[0, 1]. This variance reduction technique is available for any VPSDE with arbitrary ?(t).</p><p>In <ref type="figure" target="#fig_0">Fig. 2</ref>, we train a small LSGM on CIFAR-10 with w ll weighting using (i) the original VPSDE with uniform t sampling, (ii) the same SDE but with our IS from t, and (iii) the proposed geometric VPSDE. Note how both (ii) and (iii) significantly reduce the variance and allow us to monitor the progress of the training objective. In this case, (i) has difficulty minimizing the objective due to the high variance. In App. B, we show how IS proposals can be formed for other SDEs, including the VESDE and Sub-VPSDE from <ref type="bibr" target="#b1">[2]</ref>.   Variance reduction for unweighted and reweighted objectives:</p><p>When training with w un , analytically deriving IS proposal distributions for arbitrary ?(t) is challenging. For linear VPSDEs, we provide a derivation in App. B to obtain the optimal IS distribution. In contrast, defining IS proposal distributions is easier when training with w re . In App. B, we show that the optimal distribution is in the form r(t) ? d? 2 t /dt which is sampled by <ref type="figure" target="#fig_1">Fig. 3</ref>, we visualize the IS distributions for the three weighting mechanisms for the linear VPSDE with the original [? 0 , ? 1 ] parameters from <ref type="bibr" target="#b1">[2]</ref>. r(t) for the likelihood weighting is more tilted towards t = 0 due to the 1/? 2 t term in w ll . When using differently weighted objectives for training, we can either sample separate t with different IS distributions for each objective, or use IS for the SGM objective (Eq. 9) and reweight the samples according to the likelihood objective for encoder training (Eq. 8). See App. G for details.</p><formula xml:id="formula_10">t=var ?1 ((1??)? 2 0 +?? 2 1 ) with ? ? U[0, 1]. In</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our work builds on score-matching <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, specifically denoising score matching <ref type="bibr" target="#b21">[22]</ref>, which makes our work related to recent generative models using denoising score matching-and denoising diffusion-based objectives <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref>. Among those, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref> use a discretized diffusion process with many noise scales, building on <ref type="bibr" target="#b26">[27]</ref>, while Song et al. <ref type="bibr" target="#b1">[2]</ref> introduce the continuous time framework using SDEs. Experimentally, these works focus on image modeling and, contrary to us, work directly in pixel space. Various works recently tried to address the slow sampling of these types of models and further improve output quality. <ref type="bibr" target="#b38">[39]</ref> add an adversarial objective, <ref type="bibr" target="#b4">[5]</ref> introduce non-Markovian diffusion processes that allow to trade off synthesis speed, quality, and sample diversity, <ref type="bibr" target="#b39">[40]</ref> learn a sequence of conditional energy-based models for denoising, <ref type="bibr" target="#b40">[41]</ref> distill the iterative sampling process into single shot synthesis, and <ref type="bibr" target="#b41">[42]</ref> learn an adaptive noise schedule, which is adjusted during synthesis to accelerate sampling. Further, <ref type="bibr" target="#b25">[26]</ref> propose empirical variance reduction techniques for discretized diffusions and introduce a new, heuristically motivated, noise schedule. In contrast, our proposed noise schedule and our variance reduction techniques are analytically derived and directly tailored to our learning setting in the continuous time setup.</p><p>Recently, <ref type="bibr" target="#b10">[11]</ref> presented a method to generate graphs using score-based models, relaxing the entries of adjacency matrices to continuous values. LSGM would allow to model graph data more naturally using encoders and decoders tailored to graphs <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>Since our model can be considered a VAE <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> with score-based prior, it is related to approaches that improve VAE priors. For example, Normalizing flows and hierarchical distributions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, as well as energy-based models <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> have been proposed as VAE priors. Furthermore, classifiers <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, adversarial methods <ref type="bibr" target="#b56">[57]</ref>, and other techniques <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref> have been used to define prior distributions implicitly. In two-stage training, a separate generative model is trained in latent space as a new prior after training the VAE itself <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b9">10]</ref>. Our work also bears a resemblance to recent methods on improving the sampling quality in generative adversarial networks using gradient flows in the latent space <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68]</ref>, with the main difference that these prior works use a discriminator to update the latent variables, whereas we train an SGM.</p><p>Concurrent works: <ref type="bibr" target="#b9">[10]</ref> proposed to learn a denoising diffusion model in the latent space of a VAE for symbolic music generation. This work does not introduce an end-to-end training framework of the combined VAE and denoising diffusion model and instead trains them in two separate stages. In contrast, concurrently with us <ref type="bibr" target="#b68">[69]</ref> proposed an end-to-end training approach, and <ref type="bibr" target="#b69">[70]</ref> combines contrastive learning with diffusion models in the latent space of VAEs for controllable generation. However, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref> consider the discretized diffusion objective <ref type="bibr" target="#b0">[1]</ref>, while we build on the continuous time framework. Also, these models are not equipped with the mixed score parameterization and variance reduction techniques, which we found crucial for the successful training of SGM priors.</p><p>Additionally, <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25]</ref> concurrently with us proposed likelihood-based training of SGMs in data space <ref type="bibr" target="#b4">5</ref> . <ref type="bibr" target="#b3">[4]</ref> developed a bound for the data likelihood in their Theorem 3 of their second version, using a denoising score matching objective, closely related to our cross entropy expression. However, our cross entropy expression is much simpler as we show how several terms can be marginalized out analytically for the diffusion SDEs employed by us (see our proof in App. A). The same marginalization can be applied to Theorem 3 in <ref type="bibr" target="#b3">[4]</ref> when the drift coefficient takes a special affine form (i.e., f (z, t) = f (t)z). Moreover, <ref type="bibr" target="#b24">[25]</ref> discusses the likelihood-based training of SGMs from a fundamental perspective and shows how several score matching objectives become a variational bound on the data likelihood. <ref type="bibr" target="#b70">[71]</ref> introduced a notion of signal-to-noise ratio (SNR) that results in a noise-invariant parameterization of time that depends only on the initial and final noise. Interestingly, our importance sampling distribution in Sec. 3.4 has a similar noise-invariant parameterization of time via t = var ?1 ((? 2 1 ) ? (? 2 0 ) 1?? ), which also depends only on the initial and final diffusion process variances. We additionally show that this time parameterization results in the optimal minimumvariance objective, if the distribution of latent variables follows a standard Normal distribution. Finally, <ref type="bibr" target="#b71">[72]</ref> proposed a modified time parameterization that allows modeling unbounded data scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Here, we examine the efficacy of LSGM in learning generative models for images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details:</head><p>We implement LSGM using the NVAE <ref type="bibr" target="#b19">[20]</ref> architecture as VAE backbone and NCSN++ <ref type="bibr" target="#b1">[2]</ref> as SGM backbone. NVAE has a hierarchical latent structure. The diffusion process input z 0 is constructed by concatenating the latent variables from all groups in the channel dimension. For NVAEs with multiple spatial resolutions in latent groups, we only feed the smallest resolution groups to the SGM prior and assume that the remaining groups have a standard Normal distribution.</p><p>Sampling: To generate samples from LSGM at test time, we use a black-box ODE solver <ref type="bibr" target="#b72">[73]</ref> to sample from the prior. Prior samples are then passed to the decoder to generate samples in data space.</p><p>Evaluation: We measure NELBO, an upper bound on negative log-likelihood (NLL), using Eq. 6. For estimating log p(z 0 ), we rely on the probability flow ODE <ref type="bibr" target="#b1">[2]</ref>, which provides an unbiased but stochastic estimation of log p(z 0 ). This stochasticity prevents us from performing an importance weighted estimation of NLL <ref type="bibr" target="#b73">[74]</ref> (see App. F for details). For measuring sample quality, Fr?chet inception distance (FID) <ref type="bibr" target="#b74">[75]</ref> is evaluated with 50K samples. Implementation details in App. G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>Unconditional color image generation: Here, we present our main results for unconditional image generation on CIFAR-10 [89] (Tab. 2) and CelebA-HQ-256 (5-bit quantized) <ref type="bibr" target="#b87">[88]</ref> (Tab. 3). For CIFAR-10, we train 3 different models: LSGM (FID) and LSGM (balanced) both use the VPSDE with linear ?(t) and w un -weighting for the SGM prior in Eq. 9, while performing IS as derived in Sec. 3.4. They only differ in how the backbone VAE is trained. LSGM (NLL) is a model that is trained with our novel geometric VPSDE, using w ll -weighting in the prior objective (further details in App. G). When set up for high image quality, LSGM achieves a new state-of-the-art FID of 2.10. When tuned towards NLL, we achieve a NELBO of 2.87, which is significantly better than previous score-based models. Only autoregressive models, which come with very slow synthesis, and VDVAE <ref type="bibr" target="#b20">[21]</ref> reach similar or higher likelihoods, but they usually have much poorer image quality.</p><p>For CelebA-HQ-256, we observe that when LSGM is trained with different SDE types and weighting mechanisms, it often obtains similar NELBO potentially due to applying the SGM prior only to small latent variable groups and using Normal priors at the larger groups. With w re -weighting and linear VPSDE, LSGM obtains the state-of-the-art FID score of 7.22 on a par with the original SGM <ref type="bibr" target="#b1">[2]</ref>.</p><p>For both datasets, we also report results for the VAE backbone used in our LSGM. Although this baseline achieves competitive NLL, its sample quality is behind our LSGM and the original SGM.</p><p>Modeling binarized images: Next, we examine LSGM on dynamically binarized MNIST <ref type="bibr" target="#b92">[93]</ref> and OMNIGLOT <ref type="bibr" target="#b73">[74]</ref>. We apply LSGM to binary images using a decoder with pixel-wise independent Bernoulli distributions. For these datasets, we report both NELBO and NLL in nats in Tab. 4 and Tab. 5. On OMNIGLOT, LSGM achieves state-of-the-art likelihood of ?87.79 nat, outperforming previous models including VAEs with autoregressive decoders, and even when comparing its NELBO     against importance weighted estimation of NLL for other methods. On MNIST, LSGM outperforms previous VAEs in NELBO, reaching a NELBO 1.09 nat lower than the state-of-the-art NVAE.</p><p>Qualitative results: We visualize qualitative results for all datasets in <ref type="figure" target="#fig_3">Fig. 5</ref>. On the complex multimodal CIFAR-10 dataset, LSGM generates sharp and high-quality images. On CelebA-HQ-256, LSGM generates diverse samples from different ethnicity and age groups with varying head poses and facial expressions. On MNIST and OMNIGLOT, the generated characters are sharp and high-contrast.   </p><formula xml:id="formula_11">SGM-obj.-weighting wll wun wre t-sampling (SGM-obj.) U[0, 1] rll(t) U[0, 1] run(t) U[0, 1] rre(t) t-sampling (q-obj.) rew. rew. rew. rll(t) rew. rll(t) rew. rll(t) rew. rll(t)</formula><p>Geom sampling, respectively. In <ref type="figure">Fig. 4</ref>, we visualize FID scores and NFEs for different ODE solver error tolerances. Our LSGM achieves low FID scores for relatively large error tolerances.</p><p>We identify three main reasons for this significantly faster sampling from LSGM: (i) The SGM prior in our LSGM models latent variables with 32?32 spatial dim., whereas the original SGM <ref type="bibr" target="#b1">[2]</ref> directly models 256?256 images. The larger spatial dimensions require a deeper network to achieve a large receptive field. (ii) Inspecting the SGM prior in our model suggests that the score function is heavily dominated by the linear term at the end of training, as the mixing coefficients ? are all &lt; 0.02. This makes our SGM prior smooth and numerically faster to solve. (iii) Since SGM is formed in the latent space in our model, errors from solving the ODE can be corrected to some degree using the VAE decoder, while in the original SGM <ref type="bibr" target="#b1">[2]</ref> errors directly translate to artifacts in pixel space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies</head><p>SDEs, objective weighting mechanisms and variance reduction. In Tab. 6, we analyze the different weighting mechanisms and variance reduction techniques and compare the geometric VPSDE with the regular VPSDE with linear ?(t) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In the table, SGM-obj.-weighting denotes the weighting mechanism used when training the SGM prior (via Eq. 9). t-sampling (SGM-obj.) indicates the sampling approach for t, where r ll (t), r un (t) and r re (t) denote the IS distributions for the weighted (likelihood), the unweighted, and the reweighted objective, respectively. For training the VAE encoder q ? (z 0 |x) (last term in Eq. 8), we either sample a separate batch t with importance sampling following r ll (t) (only necessary when the SGM prior is not trained with w ll itself), or we reweight the samples drawn for training the prior according to the likelihood objective (denoted by rew.). n/a indicates fields that do not apply: The geometric VPSDE has optimal variance for the weighted (likelihood) objective already with uniform sampling; there is no additional IS distribution. Also, we did not derive IS distributions for the geometric VPSDE for w un . NaN indicates experiments that failed due to training instabilities. Previous work <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> have reported instability in training large VAEs. We find that our method inherits similar instabilities from VAEs; however, importance sampling often stabilizes training our LSGM. As expected, we obtain the best NELBOs (red) when training with the weighted, maximum likelihood objective (w ll ). Importantly, our new geometric VPSDE achieves the best NELBO. Furthermore, the best FIDs (blue) are obtained either by unweighted (w un ) or reweighted (w re ) SGM prior training, with only slightly worse NELBOs. These experiments were run on the CIFAR10 dataset, using a smaller model than for our main results above (details in App. G).</p><p>End-to-end training. We proposed to train LSGM end-to-end, in contrast to <ref type="bibr" target="#b9">[10]</ref>. Using a similar setup as above we compare end-to-end training of LSGM during the second stage with freezing the VAE encoder and decoder and only training the SGM prior in latent space during the second stage. When training the model end-to-end, we achieve an FID of 5.19 and NELBO of 2.98; when freezing the VAE networks during the second stage, we only get an FID of 9.00 and NELBO of 3.03. These results clearly motivate our end-to-end training strategy.</p><p>Mixing Normal and neural score functions. We generally found training LSGM without our proposed "mixed score" formulation (Sec. 3.2) to be unstable during end-to-end training, highlighting its importance. To quantify the contribution of the mixed score parametrization for a stable model, we train a small LSGM with only one latent variable group. In this case, without the mixed score, we reached an FID of 34.71 and NELBO of 3.39; with it, we got an FID of 7.60 and NELBO of 3.29. Without the inductive bias provided by the mixed score, learning that the marginal distribution is close to a Normal one for large t purely from samples can be very hard in the high-dimensional latent space, where our diffusion is run. Furthermore, due to our importance sampling schemes, we tend to oversample small, rather than large t. However, synthesizing high-quality images requires an accurate score function estimate for all t. On the other hand, the log-likelihood of samples is highly sensitive to local image statistics and primarily determined at small t. It is plausible that we are still able to learn a reasonable estimate of the score function for these small t even without the mixed score formulation. That may explain why log-likelihood suffers much less than sample quality, as estimated by FID, when we remove the mixed score parameterization.</p><p>Additional experiments and model samples are presented in App. H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed the Latent Score-based Generative Model, a novel framework for end-to-end training of score-based generative models in the latent space of a variational autoencoder. Moving from data to latent space allows us to form more expressive generative models, model non-continuous data, and reduce sampling time using smoother SGMs. To enable training latent SGMs, we made three core contributions: (i) we derived a simple expression for the cross entropy term in the variational objective, (ii) we parameterized the SGM prior by mixing Normal and neural score functions, and (iii) we proposed several techniques for variance reduction in the estimation of the training objective.</p><p>Experimental results show that latent SGMs outperform recent pixel-space SGMs in terms of both data likelihood and sample quality, and they can also be applied to binary datasets. In large image generation, LSGM generates data several orders of magnitude faster than recent SGMs. Nevertheless, LSGM's synthesis speed does not yet permit sampling at interactive rates, and our implementation of LSGM is currently limited to image generation. Therefore, future work includes further accelerating sampling, applying LSGMs to other data types, and designing efficient networks for LSGMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Broader Impact</head><p>Generating high-quality samples while fully covering the data distribution has been a long-standing challenge in generative learning. A solution to this problem will likely help reduce biases in generative models and lead to improving overall representation of minorities in the data distribution. SGMs are perhaps one of the first deep models that excel at both sample quality and distribution coverage. However, the high computational cost of sampling limits their widespread use. Our proposed LSGM reduces the sampling complexity of SGMs by a large margin and improves their expressivity further. Thus, in the long term, it can enable the usage of SGMs in practical applications.</p><p>Here, LSGM is examined on the image generation task which has potential benefits and risks discussed in <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b94">95]</ref>. However, LSGM can be considered a generic framework that extends SGMs to non-continuous data types. In principle LSGM could be used to model, for example, language <ref type="bibr" target="#b95">[96,</ref><ref type="bibr" target="#b96">97]</ref>, music <ref type="bibr" target="#b97">[98,</ref><ref type="bibr" target="#b9">10]</ref>, or molecules <ref type="bibr" target="#b98">[99,</ref><ref type="bibr" target="#b99">100]</ref>. Furthermore, like other deep generative models, it can potentially be used also for non-generative tasks such as semi-supervised and representation learning <ref type="bibr" target="#b100">[101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b102">103]</ref>. This makes the long-term social impacts of LSGM dependent on the downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding Statement</head><p>All authors were funded by NVIDIA through full-time employment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof for Theorem 1</head><p>Without loss of generality, we state the theorem in general form without conditioning on x.</p><p>Theorem 1. Given two distributions q(z 0 ) and p(z 0 ) defined in the continuous space R D , denote the marginal distributions of diffused samples under the SDE dz = f (t)z dt + g(t) dw at time t ? [0, 1] with q(z t ) and p(z t ). Assuming that log q(z t ) and log p(z t ) are smooth with at most polynomial growth at z t ? ??, and also assuming that f (t) and g(t) are chosen such that q(z 1 ) = p(z 1 ) at t = 1, the cross entropy is given by:</p><formula xml:id="formula_12">CE(q(z0)||p(z0)) = E t?U [0,1] g(t) 2 2 E q(z t ,z 0 |x) ||?z t log q(zt|z0)??z t log p(zt)|| 2 2 + D 2 log 2?e? 2 0 , with q(z t , z 0 ) = q(z t |z 0 )q(z 0 ) and a Normal transition kernel q(z t |z 0 ) = N (z t ; ? t (z 0 ), ? 2 t I) where ? t and ? 2</formula><p>t are obtained from f (t) and g(t) for a fixed initial variance ? 2 0 at t = 0.</p><p>Theorem 1 amounts to estimating the cross entropy between q(z 0 ) and p(z 0 ) with denoising score matching and can be understood intuitively in the context of LSGM: We are drawing samples from a potentially complex encoding distribution q(z 0 ), add Gaussian noise with small initial variance ? 2 0 to obtain a well-defined initial distribution, and then smoothly perturb the sampled encodings using a diffusion process, while learning a denoising model, the SGM prior. Note that from the perspective of the learnt SGM prior, which is defined by the separate reverse-time generative SDE with the learnt score function model (see Sec. 2), the expression in our theorem becomes an upper bound (see discussion in Sec. 3.1).</p><p>Proof. The first part of our proof follows a similar proof strategy as was used by Song et al. <ref type="bibr" target="#b3">[4]</ref>. We start the proof with a more generic diffusion process in the form:</p><formula xml:id="formula_13">dz = f (z, t)dt + g(t)dw</formula><p>The time-evolution of probability densities q(z t ) and p(z t ) under this SDE is described by the Fokker-Planck equation <ref type="bibr" target="#b103">[104]</ref> (note that we follow the same notation as in the main paper: We omit the t-subscript of the diffused distributions q t , indicating the time dependence at the variable, i.e. q(z t ) ? q t (z t )):</p><formula xml:id="formula_14">?q(z t ) ?t = ? zt 1 2 g 2 (t)q(z t )? zt log q(z t ) ? f (z, t)q(z t ) = ? zt h q (z t , t)q(z t )<label>(10)</label></formula><p>with</p><formula xml:id="formula_15">h q (z t , t) := 1 2 g 2 (t)? zt log q(z t ) ? f (z, t)<label>(11)</label></formula><p>and analogously for p(z t ).</p><p>The cross entropy can be written as</p><formula xml:id="formula_16">CE(q(z 0 )||p(z 0 )) = CE(q(z 1 )||p(z 1 )) + 0 1 ? ?t CE(q(z t )||p(z t ))dt = H q(z 1 ) ? 1 0 ? ?t CE(q(z t )||p(z t ))dt</formula><p>since q(z 1 ) = p(z 1 ), as assumed in the Theorem (in practice, the used SDEs are designed such that q(z 1 ) = p(z 1 )).</p><p>Furthermore, we have</p><formula xml:id="formula_17">? ?t CE(q(z t )||p(z t )) = ? ?q(z t ) ?t log p(z t ) + q(z t ) p(z t ) ?p(z t ) ?t dz (i) = ? ? zt (h q (z t , t)q(z t )) log p(z t ) + q(z t ) p(z t ) ? zt (h p (z t , t)p(z t )) dz (ii) = h q (z t , t) q(z t )? zt log p(z t ) + h p (z t , t) p(z t )? zt q(z t ) p(z t ) dz (iii) = q(z t ) h q (z t , t) ? zt log p(z t ) + h p (z t , t) ? zt log q(z t ) ? h p (z t , t) ? zt log p(z t ) dz (iv) = q(z t ) ? 1 2 g 2 (t)||? zt log p(z t )|| 2 ? f (z t , t) ? zt log q(z t ) + g 2 (t)? zt log q(z t ) ? zt log p(z t ) dz</formula><p>where (i) inserts the Fokker Planck equations for q(z t ) and p(z t ), respectively. Furthermore, (ii) is integration by parts assuming similar limiting behavior of q(z t ) and p(z t ) at z t ? ?? as Song et al. <ref type="bibr" target="#b3">[4]</ref>. Specifically, we know that q(z t ) and p(z t ) must decay towards zero at z t ? ?? to be normalized. Furthermore, we assumed log q(z t ) and log p(z t ) to have at most polynomial growth (or decay, when looking at it from the other direction) at z t ? ??, which implies faster exponential growth/decay of q(z t ) and p(z t ). Also, ? zt log q(z t ) and ? zt log p(z t ) grow/decay at most polynomially, too, since the gradient of a polynomial is still a polynomial. Hence, one can work out that all terms to be evaluated at z t ? ?? after integration by parts vanish. Finally, (iii) uses the log derivative trick and some rearrangements, and (iv) is obtained by inserting h q and h p .</p><p>Hence, we obtain</p><formula xml:id="formula_18">CE(q(z 0 )||p(z 0 )) = H q(z 1 ) + 1 0 E q(zt) 1 2 g 2 (t)||? zt log p(z t )|| 2 2 + f (z t , t)? zt log q(z t ) ? g 2 (t)? zt log q(z t ) ? zt log p(z t ) dt,</formula><p>which we can interpret as a general score matching-based expression for calculating the cross entropy, analogous to the expressions for the Kullback-Leibler divergence and entropy derived by Song et al. <ref type="bibr" target="#b3">[4]</ref>.</p><p>However, as discussed in the main paper, dealing with the marginal score ? zt log q(z t ) is problematic for complex "input" distributions q(z 0 ). Hence, we further transform the cross entropy expression into a denoising score matching-based expression:</p><formula xml:id="formula_19">CE(q(z0)||p(z0)) = H q(z1) + 1 0 E q(z t ) 1 2 g 2 (t)||?z t log p(zt)|| 2 2 + f (zt, t)?z t log q(zt) ? g 2 (t)?z t log q(zt) ?z t log p(zt) dt (i) = 1 2 1 0 g(t) 2 E q(z 0 ,z t ) ?2?z log q(zt|z0) ?z t log p(zt) + ||?z t log p(zt)|| 2 2 dt + 1 2 1 0 E q(z 0 ,z t ) 2f (z, t) ?z t log q(zt|z0) dt + H q(z1) (ii) = 1 2 1 0 g(t) 2 E q(z 0 ,z t ) ||?z t log q(zt|z0)|| 2 2 ? 2?z t log q(zt|z0) ?z t log p(zt) + ||?z t log p(zt)|| 2 2 dt + 1 2 1 0 E q(z t ) 2f (z, t) ?z t log q(zt|z0) ? g(t) 2 ||?z t log q(zt|z0)|| 2 2 dt + H q(z1) (iii) = 1 2 1 0 g(t) 2 E q(z 0 ,z t ) ||?z t log q(zt|z0) ? ?z t log p(zt)|| 2 2 dt + 1 2 1 0 E q(z 0 ,z t ) 2f (z, t) ? g(t) 2 ?z t log q(zt|z0) ?z t log q(zt|z0) dt (I): Model-independent term +H q(z1)</formula><p>with q(z 0 , z t ) = q(z t |z 0 )q(z 0 ) and where in (i) we have used the following identity from Vincent <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_20">E q(zt) ? zt log q(z t ) = E q(zt) E q(z0|zt) ? zt log q(z t |z 0 ) = E q(z0)q(zt|z0) ? zt log q(z t |z 0 ) .</formula><p>In (ii), we have added and subtracted g(t) 2 ||? zt log q(z t |z 0 )|| 2 2 and in (iii) we rearrange the terms into denoising score matching. In the following, we show that the term marked by (I) depends only on the diffusion parameters and does not depend on q(z 0 ) when f (z, t) takes a special affine (linear) form f (z, t) := f (t)z, which is often used for training SGMs and which we assume in our Theorem.</p><p>Note that for linear f (z, t) := f (t)z, we can derive the mean and variance (there are no "offdiagonal" co-variance terms here, since all dimensions undergo diffusion independently) of the distribution q(z t |z 0 ) at any time t in closed form, essentially solving the Fokker-Planck equation for this special case analytically. In that case, if the initial distribution at t = 0 is Normal then the distribution stays Normal and the mean and variance completely describe the distribution, i.e. q(z t |z 0 ) = N (z t ; ? t (z 0 ), ? 2 t I). The mean and variance are given by the differential equations and their solutions <ref type="bibr" target="#b103">[104]</ref>:</p><formula xml:id="formula_21">d? dt = f (t)? ? ? t = z 0 e t 0 f (s)ds<label>(12)</label></formula><formula xml:id="formula_22">d? 2 dt = 2f (t)? 2 + g 2 (t) ? ? 2 t = 1 F (t) t 0F (s)g 2 (s)ds + ? 2 0 ,F (t) := e ?2 t 0 f (s)ds (13)</formula><p>Here, z 0 denotes the mean of the distribution at t = 0 and ? 2 0 the component-wise variance at t = 0. After transforming into the denoising score matching expression above, what we are doing is essentially drawing samples z 0 from the potentially complex q(z 0 ), then placing simple Normal distributions with variance ? 2 0 at those samples, and then letting those distributions evolve according to the SDE. ? 2 0 acts as a hyperparameter of the model. In this case, i.e. when the distribution q(z t |z 0 ) is Normal at all t, we can represent samples z t from the intermediate distributions in reparameterized from z t = ? t (z 0 ) + ? t where ? N ( ; 0, I). We also know that ? z log q(z t |z 0 ) = ? ?t With this we can write down (i) as:</p><formula xml:id="formula_23">(I) = 1 2 1 0 E q(z0), 2f (t)(? t (z 0 ) + ? t ) + g(t) 2 ? t T ? ? t dt (14) = 1 0 ? f (t) ? t E q(z0), ? t (z 0 ) T =0 ? 2f (t)? 2 t + g(t) 2 2? 2 t E [ T ] =D dt (15) = ? D 2 1 0 2f (t)? 2 t + g(t) 2 ? 2 t dt (16) = ? D 2 ? 2 1 ? 2 0 1 ? 2 t d? 2 t = D 2 (log ? 2 0 ? log ? 2 1 ),<label>(17)</label></formula><p>where we have used Eq. 13.</p><p>Furthermore, since q(z T ) ? N (z T , 0, ? 2 1 I) at t = 1, its entropy is H q(z T ) = D 2 log(2?e? 2 1 ). With this, we get the following simple expression for the cross-entropy:</p><formula xml:id="formula_24">CE(q(z 0 )||p(z 0 )) = 1 2 1 0 g(t) 2 E q(z0,zt) ||? z log q(z t |z 0 ) ? ? z log p(z t )|| 2 2 dt + D log( 2?e? 2 0 )</formula><p>Expressing the integral as an expectation completes the proof:</p><formula xml:id="formula_25">CE(q(z 0 )||p(z 0 )) = E t?U [0,1] g(t) 2 2 E q(zt,z0) ||? zt log q(z t |z 0 )?? zt log p(z t )|| 2 2 + D 2 log 2?e? 2 0</formula><p>The expression in Theorem 1 measures the cross entropy between q and p at t = 0. However, one should consider practical implications of the choice of initial variance ? 2 0 when estimating the cross entropy between two distributions using our expression, as we discuss below.</p><p>Consider two arbitrary distributions q (z) and p (z). If the forward diffusion process has a nonzero initial variance (i.e., ? 2 0 &gt; 0), the actual distributions q and p at t = 0 in the score matching expression are defined by q(z 0 ) := q (z)N (z 0 , z, ? 2 0 I)dz and p(z 0 ) := p (z)N (z 0 , z, ? 2 0 I)dz, which correspond to convolving q (z) and p (z) each with a Normal distribution with variance ? 2 0 I. In this case, q (z) and p (z) are not identical to q(z 0 ) and p(z 0 ), respectively, in general. However, we can approximate q (z) and p (z) using p(z 0 ) and q(z 0 ), respectively, when ? 2 0 is small. That is why our expression in Theorem 1 that measures CE(q(z 0 )||p(z 0 )), can be considered as an approximation of CE(q (z)||p (z)) when ? 2 0 takes a positive small value. Note that in practice, our ? 2 0 is indeed generally very small (see Tab. 7).</p><p>On the other hand, when ? 2 0 = 0 (e.g., when using the VPSDE from Song et al. <ref type="bibr" target="#b1">[2]</ref>), we know that q (z) and p (z) are identical to q(z 0 ) and p(z 0 ). However, in this case, the initial distribution at t = 0 is essentially an infinitely sharp Normal and we cannot evaluate the integral over the full interval t ? [0, 1]. Hence, we limit its range to t ? [ , 1], where is another hyperparameter. In this case, we can approximate the cross entropy CE(q (z)||p (z)) using:</p><formula xml:id="formula_26">CE(q(z 0 )||p(z 0 )) ? 1 2 1 g(t) 2 E q(z0,zt) ||? z log q(z t |z 0 ) ? ? z log p(z t )|| 2 2 dt + D log( 2?e? 2 ) = E t?U [ ,1] g(t) 2 2 E q(zt,z0) ||? zt log q(z t |z 0 )?? zt log p(z t )|| 2 2 + D 2 log 2?e? 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Variance Reduction</head><p>The variance of the cross entropy in a mini-batch update depends on the variance of CE(q(z 0 )||p(z 0 )) where q(z 0 ) := E pdata(x) [q(z 0 |x)] is the aggregate posterior (i.e., the distribution of latent variables) and p data is the data distribution. This is because, for training, we use a mini-batch</p><formula xml:id="formula_27">estimation of E pdata(x) [L(x, ?, ?, ?)].</formula><p>For the cross entropy term in L(x, ?, ?, ?), we have E pdata(x) [CE(q(z 0 |x)||p(z 0 ))] = CE(q(z 0 )||p(z 0 )).</p><p>In order to study the variance of the training objective, we derive CE(q(z 0 )||p(z 0 )) analytically, assuming that both q(z 0 ) = p(z 0 ) = N (z 0 ; 0, I). This is a reasonable simplification for our analysis because pretraining our LSGM model with a N (z 0 ; 0, I) prior brings q(z 0 ) close to N (z 0 ; 0, I) and our SGM prior is often dominated by the fixed Normal mixture component. Nevertheless, we empirically observe that the variance reduction techniques developed with this simplification still work well when q(z 0 ) and p(z 0 ) are not exactly N (z 0 ; 0, I).</p><p>In this section, we start with presenting the mixed score parameterization for generic SDEs in App. B.1. Then, we discuss variance reduction with importance sampling for these generic SDEs in App. B.2. Finally, in App. B.3 and App. B.4, we focus on variance reduction of the VPSDEs and VESDEs, respectively, and we briefly discuss the Sub-VPSDE <ref type="bibr" target="#b1">[2]</ref> in App. B.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Generic Mixed Score Parameterization for Non-Variance Preserving SDEs</head><p>The mixed score parameterization uses the score that is obtained when dealing with Normal input data and just predicts an additional residual score. In the main text, we assume that the variance of the standard Normal data stays the same throughout the diffusion process, which is the case for VPSDEs. But the way Normal data diffuses depends generally on the underlying SDE and generic SDEs behave differently than the regular VPSDE in that regard.</p><p>Consider the generic forward SDEs in the form:</p><formula xml:id="formula_28">dz = f (t)z dt + g(t) dw<label>(18)</label></formula><p>If our data distribution is standard Normal, i.e. z 0 ? N (z 0 ; 0, I), using Eq. 13, we have</p><formula xml:id="formula_29">? 2 t := 1 F (t) t 0F (s)g 2 (s)ds + 1 = 1 F (t) ? 2 t + 1<label>(19)</label></formula><p>with the definition? 2 t := t 0F (s)g 2 (s)ds. Hence, the score function at time t is ? zt log p(z t ) = ? zt ? 2 t . Using the geometric mixture p(z t ) ? N (z t ; 0,? 2 t ) 1?? p ? (z t ) ? , we can generally define our mixed score parameterization as</p><formula xml:id="formula_30">? (z t , t) := ? t ? 2 t (1 ? ?) z t + ? ? (z t , t).<label>(20)</label></formula><p>In the case of VPSDEs, we have? 2 t = 1 which corresponds to the mixed score introduced in the main text.</p><p>Remark: It is worth noting that both? 2 t and ? 2 t are solutions to the same differential equation in Eq. 13 with different initial conditions. It is easy to see that? 2 t ? ? 2 t = (1 ? ? 2 0 )F (t) ?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Variance Reduction of Cross Entropy with Importance Sampling for Generic SDEs</head><p>Let's consider the cross entropy expression for p(z 0 ) = N (z 0 , 0, I) and q(z 0 ) = N (z 0 , 0, (1 ? ? 2 0 )I) where we have scaled down the variance of q(z 0 ) to (1 ? ? 2 0 ) to accommodate the fact that the diffusion process with initial variance ? 2 0 applies a perturbation with variance ? 2 0 in its initial step (hence, the marginal distribution at t = 0 is N (z 0 , 0, I) and we know that the optimal score is ? (z t , t) = ?t ? 2 t z t , i.e., the Normal component).</p><p>The cross entropy CE(q(z 0 )||p(z 0 )) with the optimal score ? (z t , t) = ?t ? 2 t z t is:</p><formula xml:id="formula_31">CE ? const. = 1 2 1 g 2 (t) ? 2 t E z0, || ? ? (z t , t)|| 2 2 dt (21) = 1 2 1 g 2 (t) ? 2 t E z0, || ? ? t ? 2 t z t || 2 2 dt (22) = 1 2 1 g 2 (t) ? 2 t E z0, || ? ? t ? 2 t (F (t) ? 1 2 z 0 + ? t )|| 2 2 dt (23) = 1 2 1 g 2 (t) ? 2 t E z0, ||? 2 t ? ? 2 t ? 2 t ? ? t ? 2 tF (t) ? 1 2 z 0 || 2 2 dt (24) = 1 2 1 g 2 (t) ? 2 t (? 2 t ? ? 2 t ) 2 (? 2 t ) 2 E || || 2 2 + ? 2 t (? 2 t ) 2F (t) ?1 E z0 ||z 0 || 2 2 dt (25) = D 2 1 g 2 (t) ? 2 t (? 2 t ? ? 2 t ) 2 (? 2 t ) 2 + ? 2 t (? 2 t ) 2F (t) ?1 (1 ? ? 2 0 ) dt (26) = D 2 1 g 2 (t) ? 2 t (? 2 t ? ? 2 t ) 2 (? 2 t ) 2 + ? 2 t (? 2 t ? ? 2 t ) (? 2 t ) 2 dt (27) = D 2 1 g 2 (t) ? 2 t dt ? D 2 1 g 2 (t) ? 2 t dt (28) = D 2 1 d dt ? 2 t + 2f (t)? 2 t ? 2 t dt ? D 2 1 d dt? 2 t + 2f (t)? 2 t ? 2 t dt (29) = D 2 1 d dt ? 2 t ? 2 t dt ? D 2 1 d dt? 2 t ? 2 t dt (30) = D 1 ? 2 E t?U [ ,1] ? ? d dt log ? 2 t ? 2 t ? ? (31) = D 1 ? 2 E t?U [ ,1] ? ? d dt log ? 2 t + ? 2 0 ? 2 t + 1 ? ? ,<label>(32)</label></formula><p>where in Eq. 23, we have used z t =F (t) ? 1 2 z 0 + ? t . In Eq. 25, we have used the fact that z 0 and are independent. In Eq. 27, we have used the identity? 2 t ? ? 2 t = (1 ? ? 2 0 )F (t) ?1 . In Eq. 29, we have used g 2 (t) = d dt ? 2 t + 2f (t)? 2 t from Eq. 13. Therefore, the IW distribution with minimum variance for CE(q(z 0 )||p(z 0 )) is</p><formula xml:id="formula_32">r(t) ? d dt log ? 2 t + ? 2 0 ? 2 t + 1<label>(33)</label></formula><p>with normalization constantR = log</p><formula xml:id="formula_33">? ? ? 2 1 + ? 2 0 ? 2 1 + 1 ? 2 + 1 ? 2 + ? 2 0 ? ?<label>(34)</label></formula><p>and CDF</p><formula xml:id="formula_34">R(t) = 1 R log ? ? ? 2 t + ? 2 0 ? 2 t + 1 ? 2 + 1 ? 2 + ? 2 0 ? ?<label>(35)</label></formula><p>Hence, the inverse CDF is</p><formula xml:id="formula_35">t = ? 2 t inv ? ? ? ? ? 2 0 ? ? 2 +? 2 0 ? 2 +1 1?? ? 2 1 +? 2 0 ? 2 1 +1 ? ? 2 +? 2 0 ? 2 +1 1?? ? 2 1 +? 2 0 ? 2 1 +1 ? ? 1 ? ? ? ?<label>(36)</label></formula><p>Finally, the cross entropy objective with importance weighting becomes</p><formula xml:id="formula_36">1 2 1 g 2 (t) ? 2 t E z0, || ? ? (z t , t)|| 2 2 dt =R 2 E t?r(t) 1 +? 2 t 1 ? ? 2 0 E z0, || ? ? (z t , t)|| 2 2 (37) = 1 2 log ? ? ? 2 1 + ? 2 0 ? 2 1 + 1 ? 2 + 1 ? 2 + ? 2 0 ? ? E t?r(t) 1 +? 2 t 1 ? ? 2 0 E z0, || ? ? (z t , t)|| 2 2<label>(38)</label></formula><p>The idea here is to write everything as a function of? 2 t = t 0F (s)g 2 (s)ds. We see that? 2 t is monotonically increasing for any g(t) and f (t); hence, it always has an inverse and inverse transform sampling is, in principle, always possible. However, we should pick g(t) and f (t) such that? 2 t and its inverse are also analytically tractable to avoid dealing with numerical methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 VPSDE</head><p>Consider the simple forward diffision process in the form:</p><formula xml:id="formula_37">dz = ? 1 2 ?(t)zdt + ?(t)dw<label>(39)</label></formula><p>which corresponds to the VPSDE from Song et al. <ref type="bibr" target="#b1">[2]</ref>. The appealing characteristic of this diffusion model is that if z 0 ? N (z 0 ; 0, I), intermediate z(t) will also have a standard Normal distribution and its variance is constant (i.e., d dt? 2 t = 0). In the original VPSDE, ?(t) is defined by a linear function</p><formula xml:id="formula_38">?(t) = ? 0 + (? 1 ? ? 0 )t that interpolates between [? 0 , ? 1 ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.1 Variance Reduction for Likelihood Weighting (Geometric VPSDE)</head><p>Our analysis in App. B.2, Eq. 30 shows that the cross entropy can be expressed as:</p><formula xml:id="formula_39">CE(q(z 0 )||p(z 0 )) ? const = D 2 1 d dt ? 2 t ? 2 t dt ? D 2 1 d dt? 2 t ? 2 t dt (40) = D 2 1 d dt ? 2 t ? 2 t dt (41) = D 1 ? 2 E t?U [ ,1] d dt ? 2 t ? 2 t<label>(42)</label></formula><p>where for the VPSDE we have used d dt? 2 t = 0.</p><p>A sample-based estimation of this expectation has a low variance if 1 Using Eq. 13, we can find an expression for ?(t) that generates such noise schedule:</p><formula xml:id="formula_40">?(t) = 1 1 ? ? 2 t d? 2 t dt = ? 2 t 1 ? ? 2 t log( ? 2 max ? 2 min ) = ? 2 min ( ? 2 max ? 2 min ) t 1 ? ? 2 min ( ? 2 max ? 2 min ) t log( ? 2 max ? 2 min )<label>(43)</label></formula><p>We call a VPSDE with ?(t) defined as above a geometric VPSDE. For small ? 2 min and ? 2 max close to 1, all inputs diffuse closely towards the standard Normal prior at t = 1. In that regard, notice that our geometric VPSDE is well-behaved with positive ?(t) only within the relevant interval t ? [0, 1] and for 0 &lt; ? 2 min &lt; ? 2 max &lt; 1. These conditions also imply ? 2 t &lt; 1 for all t ? [0, 1]. This is expected for any VPSDE. We can approach unit variance arbitrarily closely but not reach it exactly. Importantly, our geometric VPSDE is different from the "variance-exploding" SDE (VESDE), proposed by Song et al. <ref type="bibr" target="#b4">[5]</ref> (also see App. C). The VESDE leverages a SDE in which the variance grows in an almost unbounded way, while the mean of the input distribution stays constant. Because of this, the hyperparameters of the VESDE must be chosen carefully in a data-dependent manner <ref type="bibr" target="#b37">[38]</ref>, which can be problematic in our case (see discussion in App. B.4). Furthermore, Song et al. also found that the VESDE does not perform well when used with probability flow-based sampling <ref type="bibr" target="#b1">[2]</ref>. In contrast, our geometric VPSDE combines the variance preserving behavior (i.e. standard Normal input data remains standard Normal throughout the diffusion process; all individual inputs diffuse towards standard Normal prior) of the VPSDE with the geometric growth of the variance in the diffusion process, which was first used in the VESDE.</p><p>Finally, for the geometric VPSDE we also have that ? ?t CE(q(z t )||p(z t )) = const. for Normal input data. Hence, data is encoded "as continuously as possible" throughout the diffusion process. This is in line with the arguments made by Song et al. in <ref type="bibr" target="#b37">[38]</ref>. We hypothesize that this is particularly beneficial towards learning models with strong likelihood or NELBO performance. Indeed, in our experiments we observe the geometric VPSDE to perform best on this metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.2 Variance Reduction for Likelihood Weighting (Importance Sampling)</head><p>Above, we have assumed that we sample from a uniform distribution for t and we have defined ?(t) and ? 2 t such that the variance of a Monte-Carlo estimation of the expectation is minimum. Another approach for improving the sample-based estimate of the expectation is to keep ?(t) and ? 2 t unchanged and to use importance sampling such that the variance of the estimate is minimum.</p><p>Using importance sampling, we can rewrite the expectation in Eq. 42 as:</p><formula xml:id="formula_41">E t?U [ ,1] 1 ? 2 t d? 2 t dt = E t?r(t) 1 r(t) 1 ? 2 t d? 2 t dt (44)</formula><p>where r(t) is a proposal distribution. The theory of importance sampling <ref type="bibr" target="#b27">[28]</ref> shows that r(t) ?</p><formula xml:id="formula_42">1 ? 2 t d? 2 t dt = d log ? 2 t dt</formula><p>will have the smallest variance. In order to use this proposal distribution, we require (i) sampling from r(t) and (ii) evaluating the objective using this importance sampling technique.</p><p>Sampling from r(t) by inverse transform sampling: It's easy to see that the normalization constant for r(t) is 1 d log ? 2 t dt dt = log ? 2 1 ? log ? 2 . Thus, the PDF r(t) is:</p><formula xml:id="formula_43">r(t) = 1 log ? 2 1 ? log ? 2 1 ? 2 t d? 2 t dt = ?(t)(1 ? ? 2 t ) (log ? 2 1 ? log ? 2 )? 2 t<label>(45)</label></formula><p>We can derive inverse transform sampling by deriving the inverse CDF:</p><formula xml:id="formula_44">R(t) = log ? 2 t ? 2 log ? 2 1 ? 2 = ? ? ? 2 t ? 2 = ? 2 1 ? 2 ? ? t = var ?1 ? 2 1 ? ? 2 1??<label>(46)</label></formula><p>where var ?1 is the inverse of ? 2 t . Importance Weighted Objective: The cross entropy is then written as (ignoring the constants here):</p><formula xml:id="formula_45">1 2 1 ?(t) ? 2 t E z0, || ? ? (z t , t)|| 2 2 dt = 1 2 E t?r(t) (log ? 2 1 ? log ? 2 ) (1 ? ? 2 t ) E z0, || ? ? (z t , t)|| 2 2<label>(47)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.3 Variance Reduction for Unweighted Objective</head><p>Using a similar derivation as in App. B.2, we can show that for the unweighted objective for p(z 0 ) = N (z 0 , 0, I) and q(z 0 ) = N (z 0 , 0, (1 ? ? 2 0 )I), we have</p><formula xml:id="formula_46">1 E z0, || ? ? (z t , t)|| 2 2 dt = D 2 1 (? 2 t ? ? 2 t ) 2 (? 2 t ) 2 + ? 2 t (? 2 t ? ? 2 t ) (? 2 t ) 2 dt (48) = D 1 ? 2 E t?U [ ,1] 1 ? ? 2 t (49) = D 1 ? 2 E t?r(t) 1 ? ? 2 t r(t)<label>(50)</label></formula><p>with proposal distribution r(t) ? 1?? 2 t . Recall that in the VPSDE with linear ?(t)</p><formula xml:id="formula_47">= ? 0 +(? 1 ?? 0 )t, we have 1 ? ? 2 t = (1 ? ? 2 0 )e ? t 0 ?(s)ds = (1 ? ? 2 0 )e ??0t?(?1??0) t 2 2 (51)</formula><p>Hence, the normalization constant of r(t) is</p><formula xml:id="formula_48">R = 1 (1 ? ? 2 0 )e ??0t?(?1??0) t 2 2 dt (52) = (1 ? ? 2 0 )e 1 2 ? 0 ? 1 ?? 0 ? 2(? 1 ? ? 0 ) :=AR ? ? erf ? 1 ? ? 0 2 1 + ? 0 ? 1 ? ? 0 ? erf ? 1 ? ? 0 2 + ? 0 ? 1 ? ? 0 ? ? (53)</formula><p>Similarly, we can write the CDF of r(t) as</p><formula xml:id="formula_49">R(t) = AR R ? ? erf ? 1 ? ? 0 2 t + ? 0 ? 1 ? ? 0 ? erf ? 1 ? ? 0 2 + ? 0 ? 1 ? ? 0 ? ?<label>(54)</label></formula><p>solving ? = R(t) for t then results in</p><formula xml:id="formula_50">t = 2 ? 1 ? ? 0 erfinv ? ? ?R AR + erf ? 1 ? ? 0 2 + ? 0 ? 1 ? ? 0 ? ? ? ? 0 ? 1 ? ? 0 (55) Importance Weighted Objective: 1 E z0, || ? ? (z t , t)|| 2 2 dt = E t?r(t) R (1 ? ? 2 t ) E z0, || ? ? (z t , t)|| 2 2 (56)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3.4 Variance Reduction for Reweighted Objective</head><p>For the reweighted mechanism, we drop only ? 2 t from the cross entropy objective but we keep g 2 (t) = ?(t). Using a similar derivation in App. B.2, we can show that unweighted objective for p(z 0 ) = N (z 0 , 0, I) and q(z 0 ) = N (z 0 , 0, (1 ? ? 2 0 )I), we have</p><formula xml:id="formula_51">1 ?(t)E z0, || ? ? (z t , t)|| 2 2 dt = D 1 ? 2 E t?U [ ,1] d? 2 t dt = D 1 ? 2 E t?r(t) ? ? d? 2 t dt r(t) ? ? (57) with proposal distribution r(t) ? d? 2 t dt = ?(t)(1 ? ? 2 t )</formula><p>. In this case, we have the following proposal r(t), its CDF R(t) and inverse CDF R ?1 (?):</p><formula xml:id="formula_52">r(t) = ?(t)(1 ? ? 2 t ) ? 2 1 ? ? 2 , R(t) = ? 2 t ? ? 2 ? 2 1 ? ? 2 , t = R ?1 (?) = var ?1 ((1 ? ?)? 2 + ?? 2 1 )<label>(58)</label></formula><p>Note that usually ? 2 0 and ? 2 1 1. In that case, the inverse CDF can be thought of as R ?1 (?) ? var ?1 (?). Importance Weighted Objective:</p><formula xml:id="formula_53">1 2 1 ?(t)E z0, || ? ? (z t , t)|| 2 2 dt = 1 2 E t?r(t) (? 2 1 ? ? 2 ) (1 ? ? 2 t ) E z0, || ? ? (z t , t)|| 2 2<label>(59)</label></formula><p>Remark: It is worth noting that the derivation of the importance sampling distribution for the reweighted objective does not make any assumption on the form of ?(t). Thus, the IS distribution can be formed for any VPSDE when training with the reweighted objective, including the original VPSDE with linear ?(t) and also our new geometric VPSDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 VESDE</head><p>The VESDE <ref type="bibr" target="#b1">[2]</ref> is defined by:</p><formula xml:id="formula_54">dz = d dt ?(t) 2 dw (60) = ? 2 min log ? 2 max ? 2 min ? 2 max ? 2 min t dw (61) with ?(t) 2 = ? 2 min ? 2 max ? 2 min t .</formula><p>Solving the Fokker-Planck equation for input distribution N (? 0 , ? 2 0 ) results in</p><formula xml:id="formula_55">? t = ? 0 ; ? 2 t = ? 2 0 ? ? 2 min + ? 2 min ? 2 max ? 2 min t<label>(62)</label></formula><p>Typical values for ? 2 min and ? 2 max are ? 2 min = 0.01 2 and ? 2 max = 50 2 (CIFAR10). Usually, we use ? 2 min = ? 2 0 . Note that when the input data is distributed as z 0 ? N (z 0 ; 0, I), the variance at time t in VESDE is given by:</p><formula xml:id="formula_56">? 2 t = 1 ? ? 2 min + ? 2 min ? 2 max ? 2 min t<label>(63)</label></formula><p>Note that ? 2 max is typically very large and chosen empirically based on the scale of the data <ref type="bibr" target="#b37">[38]</ref>. However, this is tricky in our case, as the role of the data is played by the latent space encodings, which themselves are changing during training. We did briefly experiment with the VESDE and calculated ? 2 max as suggested in <ref type="bibr" target="#b37">[38]</ref> using the encodings after the VAE pre-training stage. However, these experiments were not successful and we suffered from significant training instabilities, even with variance reduction techniques. Therefore, we did not further explore this direction.</p><p>Nevertheless, our proposed variance reduction techniques via importance sampling can be derived also for the VESDE. Hence, for completeness, they are shown below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.1 Variance Reduction for Likelihood Weighting</head><p>Let's have a closer look at the likelihood objective when using the VESDE for modeling the standard Normal data. Following similar arguments as in previous sections, we have z 0 ? N (z 0 ; 0, (1 ? ? 2 min )I). With the optimal score ? (z t , t) = ?t ? 2 t z t (i.e., the Normal component), we have the following expression for CE(q(z 0 )||p(z 0 )) from Eq. 30:</p><formula xml:id="formula_57">1 2 1 g 2 (t) ? 2 t E ? 0 , || ? ? (z t , t)|| 2 2 dt = D 2 1 d dt ? 2 t ? 2 t dt ? D 2 1 d dt? 2 t ? 2 t dt = (64) D 2 1 d dt ? 2 t ? 2 t ? d dt? 2 t ? 2 t dt = D 1 ? 2 E t?U [ ,1] d dt ? 2 t ? 2 t ? d dt? 2 t ? 2 t<label>(65)</label></formula><p>Since the term inside the expectation is not constant in t, the VESDE does not result in an objective with naturally minimal variance, opposed to our proposed geometric VPSDE.</p><p>We derive an importance sampling scheme with a proposal distribution</p><formula xml:id="formula_58">r(t) ? 1 ? 2 t d? 2 t dt ? 1 ? 2 t d? 2 t dt = log ? 2 max ? 2 min ? ? ? ? 1 ? ? 2 min ? 2 max ? 2 min t 1 ? ? 2 min + ? 2 min ? 2 max ? 2 min t ? ? ? ?<label>(66)</label></formula><p>Note that the quantity above is always positive as</p><formula xml:id="formula_59">? 2 min ? 2 max ? 2 min t 1?? 2 min +? 2 min ? 2 max ? 2 min t ? 1 with ? 2 min &lt; 1.</formula><p>In this case the normalization constant of r(t) isR = log</p><formula xml:id="formula_60">? 2 ? 2 ? 2 max ? 2 1</formula><p>and the CDF is:</p><formula xml:id="formula_61">R(t) = 1 R log ? 2 t ? log ? 2 + log? 2 ? log? 2 t = 1 R log ? 2 ? 2 t ? 2 t ? 2<label>(67)</label></formula><p>And the inverse CDF is:</p><formula xml:id="formula_62">t =var ?1 ? ? ? ? 1 ? ? 2 min 1 ? ? 2 ? 2 1?? ? 2 max ? 2 1 ? ? ? ? ?<label>(68)</label></formula><p>wherevar ?1 is the inverse of? 2 t . So, the objective with importance sampling is then:</p><formula xml:id="formula_63">1 2 1 g 2 (t) ? 2 t E z0, || ? ? (z t , t)|| 2 2 dt = 1 2 E t?r(t) ? ? log ? 2 ? 2 ? 2 max ? 2 1 ? 2 t 1 ? ? 2 min E z0, || ? ? (z t , t)|| 2 2 ? ?</formula><p>In contrast to the VESDE, the geometric VPSDE combines the geometric progression in diffusion variance directly with minimal variance in the objective by design. Furthermore, it is simpler to set up, because we can always choose ? 2 max ? 1 for the geometric VPSDE and do not have to use a data-specific ? 2 max as proposed by <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.2 Variance Reduction for Unweighted Objective</head><p>When we drop all "prefactors" in the objective, the importance sampling distribution stays the same as above, since g 2 (t)</p><formula xml:id="formula_64">? 2 t</formula><p>is constant in t. The objective becomes:</p><formula xml:id="formula_65">1 E z0, || ? ? (z t , t)|| 2 2 dt = E t?r(t) ? ? ? log ? 2 ? 2 ? 2 max ? 2 1 log ? 2 max ? 2 min ? 2 t 1 ? ? 2 min E z0, || ? ? (z t , t)|| 2 2 ? ? ? (69)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4.3 Variance Reduction for Reweighted Objective</head><p>To define the importance sampling for the reweighted objective by ? 2 t , we use the fact that</p><formula xml:id="formula_66">d? 2 t dt = d? 2 t dt</formula><p>in VESDEs. Using a similar derivation as in App. B.2, we show:</p><formula xml:id="formula_67">1 2 1 g 2 (t)E z0, || ? ? (z t , t)|| 2 2 dt = D 2 1 d? 2 t dt dt ? D 2 1 d? 2 t dt ? 2 t ? 2 t dt (70) = D 2 1 d? 2 t dt ? 2 t ? ? 2 t ? 2 t dt (71) = D(1 ? ? 2 0 ) 2 1 1 ? 2 t d? 2 t dt dt<label>(72)</label></formula><p>Thus, the optimal proposal for reweighted objective and the inverse CDF are:</p><formula xml:id="formula_68">r(t) ? 1 ? 2 t d? 2 t dt ? r(t) = 1 log(? 2 1 ? 2 ) 1 ? 2 t d? 2 t dt ? R(t) = log(? 2 t ? 2 ) log(? 2 1 ? 2 ) ? t =var ?1 (? 2 ) 1?? (? 2 1 ) ?<label>(73)</label></formula><p>So, the reweighted objective with importance sampling is:</p><formula xml:id="formula_69">1 2 1 g 2 (t)E ? 0 , || ? ? (z t , t)|| 2 2 dt = 1 2 E t?r(t) ? ? log ? 2 1 ? 2 ? 2 t E ? 0 , || ? ? (z t , t)|| 2 2 ? ? (74)</formula><p>Note that in practice, we can safely set = 0 as initial ? 2 0 is non-zero in the VESDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Sub-VPSDE</head><p>Song et al. also proposed the Sub-VPSDE <ref type="bibr" target="#b1">[2]</ref>. It is defined as:</p><formula xml:id="formula_70">dz = ? 1 2 ?(t)zdt + ?(t) 1 ? e ?2 t 0 ?(s)ds dw<label>(75)</label></formula><p>with the same linear ?(t) as for the regular VPSDE.</p><p>Solving the Fokker-Planck equation for input distribution N (? 0 , ? 2 0 ) at t = 0 results in Deriving importance sampling distributions for variance reduction for the Sub-VPSDE can be more complicated than for the VPSDE, Geometric VPSDE, and VESDE and we did not investigate this in detail. However, for the same linear ?(t) the Sub-VPSDE is close to the VPSDE, only slightly reducing the the variance ? 2 t of the diffusion process distribution for small t. This suggests that the IS distribution derived using the regular VPSDE will likely also significantly reduce the variance of the objective due to tsampling of the Sub-VPSDE, just not as optimally as theoretically possible. In <ref type="figure" target="#fig_5">Fig. 6</ref>, we show the training NELBO of an LSGM trained on CIFAR-10 with w ll -weighting using the Sub-VPSDE. We show the NELBO both for uniform t sampling as well as for t sampling from the IS distribution that was originally derived for the regular VPSDE with the same ?(t) (the experiment and model setup is otherwise the same as the one for the ablation study on SDEs, weighting mechanisms and variance reduction). We indeed observe a significantly reduced training objective variance. We were consequently able to train large LSGM models in a stable manner using the Sub-VPSDE with VPSDE-based IS. However, the strongest generative performance in either NLL or FID was not achieved using the Sub-VPSDE, but with the Geometric VPSDE or regular VPSDE. For that reason, we did not focus on the Sub-VPSDE in our main experiments. However, a generative modeling performance comparison of the VPSDE vs. Sub-VPSDE in a smaller LSGM model is presented in App. H.4.</p><formula xml:id="formula_71">? t = e ? 1 2 t 0 ?(s)ds ? 0 ; ? 2 t = 1.0 ? e ? t 0 ?(s)ds 2 + ? 2 0 e ? t 0 ?(s)ds<label>(76)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Expressions for the Normal Transition Kernel</head><p>In our derivations of the Normal transition kernel q(z t |z 0 ), we only considered the general case in Eq. 12 and Eq. 13. However, the expression for q(z t |z 0 ) can be further simplified for different SDEs that are considered in this paper. For completeness, we provide the expressions for q(z t |z 0 ) below:</p><formula xml:id="formula_72">q(z t |z 0 ) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? N z t ; e ? 1 2 t 0 ?(s)ds z 0 , 1 ? (1 ? ? 2 0 )e ? t 0 ?(s)ds I VPSDE (linear ?(t)) N z t ; 1?? 2 min ( ? 2 max ? 2 min ) t 1?? 2 min z 0 , ? 2 min ( ? 2 max ? 2 min ) t I Geometric VPSDE N z t ; z 0 , ? 2 min ( ? 2 max ? 2 min ) t I VESDE<label>(77)</label></formula><p>In both VESDE and Geometric VPSDE, the initial variance ? 2 0 is denoted by ? 2 min &gt; 0. These diffusion processes start from a slightly perturbed version of the data at t = 0. In VESDE, ? 2 max by definition is large (as the name variance exploding SDE suggests) and it is set based on the scale of the data <ref type="bibr" target="#b37">[38]</ref>. In contrast, ? 2 max in the Geometric VPSDE does not depend on the scale of the data and it is set to ? 2 max ? 1. In the VPSDE, the initial variance is denoted by the hyperparameter ? 2 0 . In contrast to VESDE and Geometric VPSDE, we often set the initial variance to zero in VPSDE, meaning that the diffusion process models the data distribution exactly at t = 0. However, using the VPSDE with ? 2 0 = 0 comes at the cost of not being able to sample t in the full interval [0, 1] during training and also prevents us from solving the probability flow ODE all the way to zero during sampling <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Probability Flow ODE</head><p>In LSGM, to sample from our SGM prior in latent space and to estimate NELBOs, we follow Song et al. <ref type="bibr" target="#b1">[2]</ref> and build on the connection between SDEs and ODEs. We use black-box ODE solvers to solve the probability flow ODE. Here, we briefly recap this approach.</p><p>All SDEs used in this paper can be written in the general form</p><formula xml:id="formula_73">dz = f (z, t)dt + g(t)dw</formula><p>The reverse of this diffusion process is also a diffusion process running backwards in time <ref type="bibr" target="#b104">[105,</ref><ref type="bibr" target="#b1">2]</ref>, defined by</p><formula xml:id="formula_74">dz = f (z, t) ? g 2 (t)? zt log q(z t ) dt + g(t)dw,</formula><p>where dw denotes a standard Wiener process going backwards in time, dt now represents a negative infinitesimal time increment, and ? zt log q(z t ) is the score function of the diffusion process distribution at time t. Interestingly, Song et al. have shown that there is a corresponding ODE that generates the same marginal probability distributions q(z t ) when acting upon the same prior distribution q(z 1 ). It is given by</p><formula xml:id="formula_75">dz = f (z, t) ? g 2 (t) 2 ? zt log q(z t ) dt</formula><p>and usually called the probability flow ODE. This connects score-based generative models using diffusion processes to continuous Normalizing flows, which are based on ODEs <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b105">106]</ref>. Note that in practice ? zt log q(z t ) is approximated by a learnt model. Therefore, the generative distributions defined by the ODE and SDE above are formally not exactly equivalent when inserting this learnt model for the score function expression. Nevertheless, they often achieve quite similar performance in practice <ref type="bibr" target="#b1">[2]</ref>. This aspect is discussed in detail in concurrent work by Song et al. <ref type="bibr" target="#b3">[4]</ref>.</p><p>We can use the above ODE for efficient sampling of the model via black-box ODE solvers. Specifically, we can draw samples from the standard Normal prior distribution at t = 1 and then solve this ODE towards t = 0. In fact, this is how we perform sampling from the latent SGM prior in our paper. Similarly, we can also use this ODE to calculate the probability of samples under this generative process using the instantaneous change of variables formula (see <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b105">106]</ref> for details). We rely on this for calculating the probability of latent space samples under the score-based prior in LSGM. Note that this involves calculating the trace of the Jacobian of the ODE function. This is usually approximated via Hutchinson's trace estimator, which is unbiased but has a certain variance (also see discussion in Sec. F).</p><p>This approach is applicable similarly for all diffusion processes and SDEs considered in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Converting VAE with Hierarchical Normal Prior to Standard Normal Prior</head><p>Converting a VAE with hierarchical prior to a standard Normal prior can be done using a simple change of variables. Consider a VAE with hierarchical encoder q(z|x) = l q(z l |z &lt;l , x) and</p><formula xml:id="formula_76">hierarchical prior p(z) = l p(z l |z &lt;l ) where z = {z l } L</formula><p>where H is the Hessian matrix for the log exp function at w. Note that the gradient ? w log exp(w) = e w i j e w j is the softmax function and trace(H) = i e w i j e w j 1 ? e w i j e w j ? 1.</p><p>Thus, we have:</p><formula xml:id="formula_77">E [log exp(w + ? )] log exp(w) + ? 2<label>(88)</label></formula><p>So, when the importance weights w = {w (k) } K k=1 are estimated with sufficiently small variance ? 2 , the bias is proportional to the variance of this estimate.</p><p>In our experiments, we observe that the variance of the log p(z 0 ) estimate is not small enough to obtain a reliable estimate of test likelihood using the importance weighted bound. One way to reduce the variance is to use many randomly sampled noise vectors in Hutchinson's trick. However, this makes NLL estimation computationally too expensive. Fortunately, when evaluating NELBO (which corresponds to K = 1 here), the NELBO estimate is unbiased and its variance is small because of averaging across big test datasets (with often 10k samples). For example, on MNIST the standard deviation of our log p(z 0 ) estimate is 0.36 nat, while the standard deviation of NELBO is 0.07 nat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional Implementation Details</head><p>All hyperparameters for our main models are provided in Tab. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 VAE Backbone</head><p>The VAE backbone for all LSGM models is NVAE <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b6">7</ref> , one of the best-performing VAEs in the literature. It has a hierarchical latent space with group-wise autoregressive latent variable dependencies and it leverages residual neural networks (for architecture details see <ref type="bibr" target="#b19">[20]</ref>). It uses depth-wise separable convolutions in the decoder. Although both the approximate posterior and the prior are hierarchical in its original version, we can reparametrize the prior and write it as a product of independent Normal distributions (see Sec. E).</p><p>The VAE's most important hyperparameters include the number of latent variable groups and their spatial resolution, the channel depth of the latent variables, the number of residual cells per group, and the number of channels in the convolutions in the residual cells. Furthermore, when training the VAE during the first stage we are using KL annealing and KL balancing, as described in <ref type="bibr" target="#b19">[20]</ref>. For some models, we complete KL annealing during the pre-training stage, while for other models we found it beneficial to anneal only up to a KL-weight ? KL &lt; 1.0 in the ELBO during the first stage and complete KL annealing during the main end-to-end LSGM training stage. This provides additional flexibility in learning an expressive distribution in latent space during the second training stage, as it prevents more latent variables from becoming inactive while the prior is being trained gradually. However, when using a very large backbone VAE together with an SGM objective that does not correspond to maximum likelihood training, i.e. w un -or w re -weighting, we empirically observe that this approach can also hurt NLL, while slightly improving FID (see CIFAR10 (best FID) model).</p><p>Note that the VAE Backbone performance for CIFAR10 reported in Tab. 2 in the main paper corresponds to the 20-group backbone VAE (trained to full KL-weight ? KL = 1.0) from the CIFAR10 (balanced) LSGM model (see hyperparameter Tab. 7).</p><p>Image Decoders: Since SGMs <ref type="bibr" target="#b1">[2]</ref> assume that the data is continuous, they rely on uniform dequantization when measuring data likelihood. However, in LSGM, we rely on decoders designed specifically for images with discrete intensity values. On color images, we use mixtures of discretized logistics <ref type="bibr" target="#b81">[82]</ref>, and on binary images, we use Bernoulli distributions. These decoder distributions are both available from the NVAE implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Latent SGM Prior</head><p>Our denoising networks for the latent SGM prior are based on the NCSN++ architecture from Song et al. <ref type="bibr" target="#b1">[2]</ref>, adapted such that the model ingests and predicts tensors according to the VAE's latent variable dimensions. We vary hyperparameters such as the number of residual cells per spatial resolution level and the number of channels in convolutions. Note that all our models use 0.2 dropout in the SGM prior. Some of our models use upsampling and downsampling operations with anti-aliasing based on Finite Impulse Response (FIR) <ref type="bibr" target="#b106">[107]</ref>, following Song et al. <ref type="bibr" target="#b1">[2]</ref>. NVAE has a hierarchical latent structure. For small image datasets including CIFAR-10, MNIST and OMNIGLOT all the latent variables have the same spatial dimensions. Thus, the diffusion process input z 0 is constructed by concatenating the latent variables from all groups in the channel dimension. Our NVAE backbone on the CelebA-HQ-256 dataset comes with multiple spatial resolutions in latent groups. In this case, we only feed the smallest resolution groups to the SGM prior and assume that the remaining groups have a standard Normal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Training Details</head><p>To optimize our models, we are mostly following the previous literature. The VAE's encoder and decoder networks are trained using an Adamax optimizer <ref type="bibr" target="#b107">[108]</ref>, following NVAE <ref type="bibr" target="#b19">[20]</ref>. In the second stage, the whole model is trained with an Adam optimizer <ref type="bibr" target="#b107">[108]</ref> and we perform learning rate annealing for the VAE network optimization, while we keep the learning rate constant when optimizing the SGM prior parameters. At test time, we use an exponential moving average (EMA) of the parameters of the SGM prior with 0.9999 EMA decay rate, following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Note that, when using the VPSDE with linear ?(t), we are also generally following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and use ? 0 = 0.1 and ? 1 = 20.0. We did not observe any benefits in using the EMA parameters for the VAE networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.4 Evaluation Details</head><p>For evaluation, we are drawing samples and calculating log-likelihoods using the probability flow ODE, leveraging black-box ODE solvers, following <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b1">2]</ref>. Similar to <ref type="bibr" target="#b1">[2]</ref>, we are using an RK45 ODE solver <ref type="bibr" target="#b108">[109]</ref>, based on scipy, using the torchdiffeq interface 8 . Integration cutoffs close to zero and ODE solver error tolerances used for evaluation are indicated in Tab. 7 (for example, for the VPSDE with linear ?(t) we usually use ? 2 0 = 0 and therefore have that ? 2 t goes to 0 at t = 0, hence preventing us from integrating the probability flow ODE all the way to exactly 0. This was handled similarly by Song et al. <ref type="bibr" target="#b1">[2]</ref>).</p><p>Following the conventions established by previous work <ref type="bibr" target="#b87">[88,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5]</ref>, when evaluating our main models we compute FID at frequent intervals during training and report FID and NLL at the minimum observed FID.</p><p>Vahdat and Kautz in NVAE <ref type="bibr" target="#b19">[20]</ref> observe that setting the batch normalization (BN) layers to train mode during sampling (i.e., using batch statistics for normalization instead of moving average statistics) improves sample quality. We similarly observe that setting BN layers to train mode improves sample quality by about 1 FID score on the CelebA-HQ-256 dataset, but it does not affect performance on the CIFAR-10 dataset. In contrast to NVAE, we do not change the temperature of the prior during sampling, as we observe that it hurts generation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5 Ablation Experiments</head><p>Here we provide additional details and discussions about the ablation experiments performed in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5.1 Ablation: SDEs, Objective Weighting Mechanisms and Variance Reduction</head><p>The models that were used for the ablation experiment on SDEs, objective weighting mechanisms and variance reduction and produced the results in Tab. 6 in the main paper use an overall similar setup as the CIFAR10 (best NLL) one, with a few exceptions: They are trained only for 1000 epochs and evaluation always happens using the checkpoint at the end of training. Furthermore, the total batchsize over all GPUs is reduced from 256 to 128. Additionally, only 2 instead of 8 cells per residual are used in the latent SGM prior networks. Finally, the VAE's KL term is annealed all the way to ? KL = 1.0 during the first training stage for these experiments. All other hyperparameters correspond to the CIFAR10 (best NLL) setup, except those that are explicitly varied as part of the ablation study and mentioned in Tab. 6 in the paper. </p><formula xml:id="formula_78">w un w ll w re w re w ll w ll t-sampling approach (SGM-obj.) r un (t) r un (t) U[0, 1] r re (t) r re (t) r ll (t) r ll (t) t-sampling approach (q-obj.) rew. rew. rew. r ll (t) - rew. rew.</formula><p>Evaluation ODE solver integration cutoff 10 ?6 10 ?6 10 ?6 10 ?5 10 ?5 10 ?5 10 ?5 ODE solver error tolerance 10 ?5 10 ?5 10 ?5 10 ?5 10 ?5 10 ?5 10 ?5</p><p>As discussed in the main paper, the results of this ablation study overall validate that importance sampling is important to stabilize training, that the w ll -weighting mechanism as well as our novel geometric VPSDE are well suited for training towards strong likelihood, and that the w un -and w re -weighting mechanisms tend to produce better FIDs. Although these trends generally hold, it is noteworthy that not all results translate perfectly to our large models that we used to produce our main results. For instance, the setting with w re -weighting and no importance sampling for the SGM objective, which produced the best FID in Tab. 6 (main paper), is generally unstable for our bigger models, in line with our observation that IS is usually necessary to stabilize training. The stable training run for this setting in Tab. 6 can be considered an outlier.</p><p>Furthermore, for CIFAR10 we obtained our very best FID results using the VPSDE, w un -weighting, IS, and sample reweighting for the q-objective, while for the slightly smaller models used for the results in Tab. 6, there is no difference between using sample reweighting and drawing a separate batch t with r ll (t) for training q for this case (see Tab. 6 main paper, VPSDE, w un , r un (t) fields). Also, CelebA-HQ-256 behaves slightly different for the large models in that the VPSDE with w re -weighting and sampling a separate batch t with r ll (t) for q-training performed best by a small margin (see hyperparameter Tab. 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5.2 Ablation: End-to-End Training</head><p>The model used for the results on the ablation study regarding end-to-end training vs. fully separate VAE and SGM prior training is the same one as used for the ablation study on SDEs, objective weighting mechanisms and variance reduction above, evaluated in a similar way. For this experiment, we used the VPSDE, w un -objective weighting, IS for t with r un (t) when training the SGM prior, and we did draw a second batch t with r ll (t) for training q (only relevant for the end-to-end training setup).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.5.3 Ablation: Mixing Normal and Neural Score Functions</head><p>The model used for the ablation study on mixing Normal and neural score functions is again similar to the one used for the other ablations with the exception that the underlying VAE has only a single latent variable group, which makes it much smaller and removes all hierarchical dependencies between latent variables. We tried training multiple models with larger backbone VAEs, but they were generally unstable when trained without our mixed score parametrization, which only hightlights its importance. As for the previous ablation, for this experiment we used the VPSDE, w un -objective weighting, IS for t with r un (t) when training the SGM prior, and we did draw a second batch t with r ll (t) for training q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.6 Training Algorithms</head><p>To unambiguously clarify how we train our LSGMs, we summarized the training procedures in three different algorithms for different situations:</p><p>1. Likelihood training with IS. In this case, the SGM prior and the encoder share the same weighted likelihood objective and do not need to be updated separately.</p><p>2. Un/Reweighted training with separate IS of t for SGM-objective and q-objective. Here, the SGM prior and the encoder need to be updated with different weightings, because the encoder always needs to be trained using the weighted (maximum likelihood) objective. We draw separate batches t using separate IS distribution for the two differently weighted objectives (i.e. last term in Eq. 8 from main paper vs. Eq. 9).</p><p>3. Un/Reweighted training with IS of t for the SGM-objective and reweighting for the qobjective. What this means is that when training the encoder with the score-based cross entropy term (last term in Eq. 8 from main paper), we are using an importance sampling distribution that was actually tailored to un-or reweighted training for the SGM objective (Eq. 9 from main paper) and therefore isn't optimal for the weighted (maximum likelihood) objective necessary for encoder training. However, if we nevertheless use the same importance sampling distribution, we do not need to draw a second batch of t for encoder training. In practice, this boils down to different (re-)weighting factors in the cross entropy term (see <ref type="bibr">Algorithm 3)</ref>.</p><p>For efficiency comparison between approaches <ref type="formula" target="#formula_0">(2)</ref> and <ref type="formula" target="#formula_1">(3)</ref>, we observe that (3) consumes more memory than (2) in general but it can be faster due to the shared computation for the denoising step. Due to the memory limitations, we use (2) on large image datasets. Note that the choice between <ref type="formula" target="#formula_0">(2)</ref> and <ref type="formula" target="#formula_1">(3)</ref> may affect generative performance as we empirically observed in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Likelihood training with IS</head><p>Input: data x, parameters {?, ?, ?} Draw z 0 ? q ? (z 0 |x) using encoder. Draw t ? r ll (t) with IS distribution of likelihood weighting (Sec. B). Calculate ? t (z 0 ) and ? 2 t according to SDE. Draw z t ? q(z t |z 0 ) using z t = ? t (z 0 ) + ? 2 t where ? N ( , 0, I). Calculate score ? (z t , t) = ? t (1 ? ?) z t + ? ? (z t , t). Calculate cross entropy CE(q ? (z 0 |x)||p ? (z 0 )) ? 1 rll(t) wll(t) 2 || ? ? (z t , t)|| 2 2 . Calculate objective L(x, ?, ?, ?) = ? log p ? (x|z 0 ) + log q ? (z 0 |x) + CE(q ? (z 0 |x)||p ? (z 0 )). Update all parameters {?, ?, ?} by minimizing L(x, ?, ?, ?).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 MNIST: Small VAE Experiment</head><p>Here, we examine our LSGM on a small VAE architecture. We specifically follow <ref type="bibr" target="#b23">[24]</ref> and build a small VAE in the NVAE codebase. In particular, the model does not have hierarchical latent variables, but only a single latent variable group with a total of 64 latent variables. Encoder and decoder consist of small ResNets with 6 residual cells in total (every two cells there is a down-or up-sampling operation, so we have 3 blocks with 2 residual cells per block). The experiments are done on dynamically binarized MNIST. As we can see in <ref type="table" target="#tab_11">Table 8</ref>, our implementation of the VAE obtains a similar test NELBO as <ref type="bibr" target="#b23">[24]</ref>. However, our LSGM improves the NELBO by almost 4.6 nats. This simple experiment shows that we can even obtain good generative performance with our LSGM using small VAE architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 CIFAR-10: Neural Network Evaluations during Sampling</head><p>In Tab. 9, we report the number of neural network evaluations performed by the ODE solver during sampling from our CIFAR-10 models. ODE solver error tolerance is 10 ?5 and time integration cutoff is 10 ?6 . CIFAR-10 is a highly diverse and more multimodal dataset, compared to CelebA-HQ-256. Because of that, the latent SGM prior that is learnt is more complex, requiring more function evaluations.</p><p>H.4 CIFAR-10: Sub-VPSDE vs. VPSDE</p><p>In App. B.5 we discussed how variance reduction techniques derived based on the VPSDE can also help reducing the variance of the sample-based estimate of the training objective when using the Sub-VPSDE in the latent space SGM. Here, we perform a quantitative comparison between the VPSDE and the Sub-VPSDE, following the same experimental setup and using the same models as for the ablation study on SDEs, objective weighting mechanisms, and variance reduction (experiment details in App. G.5.1). The results are reported in Tab. <ref type="bibr" target="#b9">10</ref>. We find that the VPSDE generally performs slightly better in FID, while we observed little difference in NELBO in these experiments. Importantly, the Sub-VPSDE also did not outperform our novel geometric VPSDE in NELBO. We also see that the combination of Sub-VPSDE with w re -weighting performs poorly. Consequently, we did not explore the Sub-VPSDE further in our main experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.5 CelebA-HQ-256: Different ODE Solver Error Tolerances</head><p>In <ref type="figure">Fig. 9</ref>, we visualize CelebA-HQ-256 samples from our LSGM model for varying ODE solver error tolerances.   For our experiments in this paper, we use the probability flow ODE to sample from the model. However, on CelebA-HQ-256, we observe that ancestral sampling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27]</ref> from the prior instead of solving the probability flow ODE often generates much higher quality samples. However, the FID score is slightly worse for this approach. In <ref type="figure" target="#fig_9">Fig. 10, Fig. 11</ref>, and <ref type="figure" target="#fig_0">Fig. 12</ref>, we visualize samples generated with different numbers of steps in ancestral sampling.</p><p>H.7 CelebA-HQ-256: Sampling from VAE Backbone vs. LSGM</p><p>For the quantitative results on the CelebA-HQ-256 dataset in the main text, we use an LSGM with spatial dimension of 32?32 for the latent variables in the SGM prior. However, for the qualitative results we used an LSGM with the prior spatial dimension of 64?64. The 32?32 dimensional model achieves a better FID score compared to the 64?64 dimensional model (FID 7.22 vs. 8.53) and sampling from it is much faster (2.7 sec. vs. 39.9 sec.). However, the visual quality of the samples is slightly worse. In this section, we visualize samples generated by the 32?32 dimensional model as well as the VAE backbone for this model. In this experiment, the VAE backbone is fully trained. Samples from our VAE backbone are visualized in <ref type="figure" target="#fig_1">Fig. 13</ref> and for our 32?32 dimensional LSGM in <ref type="figure">Fig.</ref> 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.8 Evolution Samples on the ODE and SDE Reverse Generative Process</head><p>In <ref type="figure" target="#fig_3">Fig. 15</ref>, we visualize the evolution of the latent variables under both the reverse generative SDE and also the probability flow ODE. We are decoding the intermediate latent samples along the reverse-time generative process via the decoder to pixel space. where high error tolerance results in pixelated images (see <ref type="figure" target="#fig_1">Fig. 3</ref> in <ref type="bibr" target="#b1">[2]</ref>), in our case high error tolerances create low-frequency artifacts. Reducing the error tolerance improves subtle details slightly.      <ref type="figure" target="#fig_3">Figure 15</ref>: We visualize the evolution of the latent variables under both the reverse generative SDE (a-b) and also the probability flow ODE (c-d). Specifically, we feed latent variables from different stages along the generative denoising diffusion process to the decoder to map them back to image space. The 13 different images in each row correspond to the times t = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.01, 10 ?5 ] along the reverse denoising diffusion process. The evolution of the images is noticeably different from diffusion models that are run directly in pixel space (see, for example, <ref type="figure" target="#fig_0">Fig. 1 in [2]</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>Figure 2: Variance reduction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>IS distributions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>12 FIDFigure 4 :</head><label>124</label><figDesc>FID and number of function evaluations (NFEs) for different ODE solver error tolerances on CelebA-HQ-256. LSGM takes 4.15 sec. for sampling while the original SGM [2] takes 45 min. with PC and 3.9 min. with ODE-based sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Generated samples for different datasets. For binary datasets, we visualize the decoder mean. LSGM successfully generates sharp, high-quality, and diverse samples (additional samples in appendix).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>? 2 t d? 2 t 2 t d? 2 t 2 t = ? 2 min ( ? 2 max ? 2 min)</head><label>22222</label><figDesc>dt is constant for all t ? [0, 1]. By solving the ODE 1 ? dt = const., we can see that a log-linear noise schedule of the form ? t satisfies this condition, with t ? [0, 1], 0 &lt; ? 2 min &lt; ? 2 max &lt; 1, and ? 2 min = ? 2 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Variance reduction of the sample-based estimate of the training objective for the Sub-VPSDE, using an IS distribution derived from the regular VPSDE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Additional uncurated samples generated by LSGM on the CIFAR-10 dataset (best FID model). Sampling in the latent space is done using the probability flow ODE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Additional uncurated samples generated by LSGM on the CelebA-HQ-256 dataset. Sampling in the latent space is done using the probability flow ODE. H.6 CelebA-HQ-256: Ancestral Sampling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(a) ODE solver error tolerance 10 ? 2 ( 5 Figure 9 :</head><label>259</label><figDesc>b) ODE solver error tolerance 10 ?3 (c) ODE solver error tolerance 10 ?4 (d) ODE solver error tolerance 10 ?The effect of ODE solver error tolerance on the quality of samples. In contrast to the original SGM [2]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Uncurated samples generated by LSGM on the CelebA-HQ-256 dataset using 200-step ancestral sampling for the prior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Uncurated samples generated by LSGM on the CelebA-HQ-256 dataset using 1000-step ancestral sampling for the prior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Additional uncurated samples generated by LSGM on the CelebA-HQ-256 dataset using 1000-step ancestral sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Uncurated samples generated by our VAE backbone without changing the temperature of the prior. The poor quality of the samples from the VAE backbone is partially due to the large spatial dimensions of the latent space in which long-range correlations are not encoded well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Uncurated samples generated by LSGM with the SGM prior applied to the latent variables of 32?32 spatial dimensions, on the CelebA-HQ-256 dataset. Sampling in the latent space is done using the probability flow ODE.(a) Evolution of latent variables under the SDE (b) Evolution of latent variables under the SDE (c) Evolution of latent variables under the ODE (d) Evolution of latent variables under the ODE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Weighting mechanismsMechanism Weights</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Generative performance on CIFAR-10.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">NLL? FID?</cell></row><row><cell></cell><cell>LSGM (FID)</cell><cell cols="2">?3.43 2.10</cell></row><row><cell>Ours</cell><cell>LSGM (NLL)</cell><cell cols="2">?2.87 6.89</cell></row><row><cell></cell><cell>LSGM (balanced)</cell><cell cols="2">?2.95 2.17</cell></row><row><cell></cell><cell>VAE Backbone</cell><cell cols="2">2.96 43.18</cell></row><row><cell></cell><cell>VDVAE [21]</cell><cell>2.87</cell><cell>-</cell></row><row><cell></cell><cell>NVAE [20]</cell><cell cols="2">2.91 23.49</cell></row><row><cell>VAEs</cell><cell>VAEBM [76] NCP-VAE [56]</cell><cell>--</cell><cell>12.19 24.08</cell></row><row><cell></cell><cell>BIVA [48]</cell><cell>3.08</cell><cell>-</cell></row><row><cell></cell><cell>DC-VAE [77]</cell><cell>-</cell><cell>17.90</cell></row><row><cell></cell><cell>NCSN [3]</cell><cell>-</cell><cell>25.32</cell></row><row><cell></cell><cell>Rec. Likelihood [40]</cell><cell cols="2">3.18 9.36</cell></row><row><cell>Score</cell><cell>DSM-ALS [39] DDPM [1]</cell><cell cols="2">3.65 3.75 3.17 -</cell></row><row><cell></cell><cell cols="3">Improved DDPM [26] 2.94 11.47</cell></row><row><cell></cell><cell>SDE (DDPM++) [2]</cell><cell cols="2">2.99 2.92</cell></row><row><cell></cell><cell>SDE (NCSN++) [2]</cell><cell>-</cell><cell>2.20</cell></row><row><cell>Flows</cell><cell>VFlow [19] ANF [18]</cell><cell>2.98 3.05</cell><cell>--</cell></row><row><cell></cell><cell>DistAug aug [78]</cell><cell cols="2">2.53 42.90</cell></row><row><cell></cell><cell cols="2">Sp. Transformers [79] 2.80</cell><cell>-</cell></row><row><cell>Aut. Reg.</cell><cell>?-VAE [80]</cell><cell>2.83</cell><cell>-</cell></row><row><cell></cell><cell>PixelSNAIL [81]</cell><cell>2.85</cell><cell>-</cell></row><row><cell></cell><cell>PixelCNN++ [82]</cell><cell>2.92</cell><cell>-</cell></row><row><cell>GANs</cell><cell>AutoGAN [83] StyleGAN2-ADA [84]</cell><cell>--</cell><cell>12.42 2.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Generative results on CelebA-HQ-256.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">NLL? FID?</cell></row><row><cell>Ours</cell><cell>LSGM VAE Backbone</cell><cell>?0.70 0.70</cell><cell>7.22 30.87</cell></row><row><cell></cell><cell>NVAE [20]</cell><cell>0.70</cell><cell>29.76</cell></row><row><cell>VAEs</cell><cell>VAEBM [76] NCP-VAE [56]</cell><cell>--</cell><cell>20.38 24.79</cell></row><row><cell></cell><cell>DC-VAE [77]</cell><cell>-</cell><cell>15.80</cell></row><row><cell>Score</cell><cell>SDE [2]</cell><cell>-</cell><cell>7.23</cell></row><row><cell>Flows</cell><cell>GLOW [85]</cell><cell>1.03</cell><cell>68.93</cell></row><row><cell cols="2">Aut. Reg. SPN [86]</cell><cell>0.61</cell><cell>-</cell></row><row><cell></cell><cell>Adv. LAE [87]</cell><cell>-</cell><cell>19.21</cell></row><row><cell>GANs</cell><cell>VQ-GAN [64]</cell><cell>-</cell><cell>10.70</cell></row><row><cell></cell><cell>PGGAN [88]</cell><cell>-</cell><cell>8.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Dyn. binarized OMNIGLOT results.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">NELBO? NLL?</cell></row><row><cell>Ours</cell><cell>LSGM</cell><cell cols="2">87.79 ?87.79</cell></row><row><cell></cell><cell>NVAE [20]</cell><cell>93.92</cell><cell>90.75</cell></row><row><cell>VAEs</cell><cell>BIVA [48] DVAE++ [51]</cell><cell>93.54 -</cell><cell>91.34 92.38</cell></row><row><cell></cell><cell>Ladder VAE [90]</cell><cell>-</cell><cell>102.11</cell></row><row><cell></cell><cell>VLVAE [47]</cell><cell>-</cell><cell>89.83</cell></row><row><cell>Aut. Reg.</cell><cell>VampPrior [59]</cell><cell>-</cell><cell>89.76</cell></row><row><cell></cell><cell>PixelVAE++ [91]</cell><cell>-</cell><cell>88.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Dynamically binarized MNIST results.</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">NELBO? NLL?</cell></row><row><cell>Ours</cell><cell>LSGM</cell><cell cols="2">78.47 ?78.47</cell></row><row><cell></cell><cell>NVAE [20]</cell><cell>79.56</cell><cell>78.01</cell></row><row><cell>VAEs</cell><cell>BIVA [48] IAF-VAE [24]</cell><cell>80.06 80.80</cell><cell>78.41 79.10</cell></row><row><cell></cell><cell>DVAE++ [51]</cell><cell>-</cell><cell>78.49</cell></row><row><cell></cell><cell>PixelVAE++ [91]</cell><cell>-</cell><cell>78.00</cell></row><row><cell>Aut. Reg.</cell><cell>VampPrior [59]</cell><cell>-</cell><cell>78.45</cell></row><row><cell></cell><cell>MAE [92]</cell><cell>-</cell><cell>77.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Sampling time: We compare LSGM against the original SGM [2] trained on the CelebA-HQ-256 dataset in terms of sampling time and number of function evaluations (NFEs) of the ODE solver. Song et al. [2] propose two main sampling techniques including predictor-corrector (PC) and probability flow ODE. PC sampling involves 4000 NFEs and takes 44.6 min. on a Titan V for a batch of 16 images. It yields 7.23 FID score (see Tab. 3). ODE-based sampling from SGM takes 3.91 min. with 335 NFEs, but it obtains a poor FID score of 128.13 with 10 ?5 as ODE solver error tolerance 6 .</figDesc><table /><note>In a stark contrast, ODE-based sampling from our LSGM takes 0.07 min. with average of 23 NFEs, yielding 7.22 FID score. LSGM is 637? and 56? faster than original SGM's [2] PC and ODE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Ablations on SDEs, objectives, weighting mechanisms, and variance reduction. Details in App. G.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>The Cross Entropy Term . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3.2 Mixing Normal and Neural Score Functions . . . . . . . . . . . . . . . . . . . . . 4 3.3 Training with Different Weighting Mechanisms . . . . . . . . . . . . . . . . . . . 5 3.4 Variance Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 5.2 Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Generic Mixed Score Parameterization for Non-Variance Preserving SDEs . . . . . 22 B.2 Variance Reduction of Cross Entropy with Importance Sampling for Generic SDEs 22 B.3 VPSDE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.3.1 Variance Reduction for Likelihood Weighting (Geometric VPSDE) . . . . 24 B.3.2 Variance Reduction for Likelihood Weighting (Importance Sampling) . . . 25 B.3.3 Variance Reduction for Unweighted Objective . . . . . . . . . . . . . . . 25 B.3.4 Variance Reduction for Reweighted Objective . . . . . . . . . . . . . . . . 26 B.4 VESDE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B.4.1 Variance Reduction for Likelihood Weighting . . . . . . . . . . . . . . . . 27 B.4.2 Variance Reduction for Unweighted Objective . . . . . . . . . . . . . . . 28 B.4.3 Variance Reduction for Reweighted Objective . . . . . . . . . . . . . . . . 28 B.5 Sub-VPSDE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Converting NVAE Prior to Standard Normal Prior . . . . . . . . . . . . . . . . . . 31 F Bias in Importance Weighted Estimation of Log-Likelihood 31 G Additional Implementation Details 32 G.1 VAE Backbone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 G.2 Latent SGM Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 G.3 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 G.4 Evaluation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 G.5 Ablation Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 G.5.1 Ablation: SDEs, Objective Weighting Mechanisms and Variance Reduction 33 G.5.2 Ablation: End-to-End Training . . . . . . . . . . . . . . . . . . . . . . . . 34 G.5.3 Ablation: Mixing Normal and Neural Score Functions . . . . . . . . . . . 35 G.6 Training Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 G.7 Computational Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36</figDesc><table><row><cell>1 Introduction</cell><cell>1</cell></row><row><cell>2 Background</cell><cell>2</cell></row><row><cell>3 Score-based Generative Modeling in Latent Space</cell><cell>3</cell></row><row><cell>3.1 4 Related Work</cell><cell>6</cell></row><row><cell>5 Experiments</cell><cell>7</cell></row><row><cell>5.1 6 Conclusions 7 Broader Impact A Proof for Theorem 1 B Variance Reduction B.1 C Expressions for the Normal Transition Kernel D Probability Flow ODE E Converting VAE with Hierarchical Normal Prior to Standard Normal Prior E.1 H Additional Experiments</cell><cell>10 10 18 21 29 30 30 36</cell></row></table><note>H.1 Additional Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 H.2 MNIST: Small VAE Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 H.3 CIFAR-10: Neural Network Evaluations during Sampling . . . . . . . . . . . . . . 37 H.4 CIFAR-10: Sub-VPSDE vs. VPSDE . . . . . . . . . . . . . . . . . . . . . . . . . 37 H.5 CelebA-HQ-256: Different ODE Solver Error Tolerances . . . . . . . . . . . . . . 37 H.6 CelebA-HQ-256: Ancestral Sampling . . . . . . . . . . . . . . . . . . . . . . . . 40 H.7 CelebA-HQ-256: Sampling from VAE Backbone vs. LSGM . . . . . . . . . . . . 40 H.8 Evolution Samples on the ODE and SDE Reverse Generative Process . . . . . . . 40</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters for our main models. We use the same notations and abbreviations as in Tab. 6 in main paper. HQ-256 CelebA-HQ-256 OMNIGLOT MNIST (best FID) (balanced) (best NLL) (best quantitative) (best qualitative)</figDesc><table><row><cell cols="5">Hyperparameter CelebA-VAE Backbone CIFAR10 CIFAR10 CIFAR10</cell><cell></cell><cell></cell><cell></cell></row><row><cell># normalizing flows</cell><cell>0</cell><cell>0</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>0</cell><cell>0</cell></row><row><cell># latent variable scales</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>1</cell></row><row><cell># groups in each scale</cell><cell>20</cell><cell>20</cell><cell>4</cell><cell>8</cell><cell>10</cell><cell>3</cell><cell>2</cell></row><row><cell>spatial dims. of z in each scale</cell><cell>16 2</cell><cell>16 2</cell><cell>16 2</cell><cell>128 2 , 64 2 , 32 2</cell><cell>128 2 , 64 2</cell><cell>16 2</cell><cell>8 2</cell></row><row><cell># channel in z</cell><cell>9</cell><cell>9</cell><cell>45</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell></row><row><cell># initial channels in enc.</cell><cell>128</cell><cell>128</cell><cell>256</cell><cell>64</cell><cell>64</cell><cell>64</cell><cell>64</cell></row><row><cell># residual cells per group</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>2</cell><cell>3</cell><cell>1</cell></row><row><cell>NVAE's spectral reg. ?</cell><cell>10 ?2</cell><cell>10 ?2</cell><cell>10 ?2</cell><cell>3 ? 10 ?2</cell><cell>3 ? 10 ?2</cell><cell>10 ?2</cell><cell>10 ?2</cell></row><row><cell>Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(VAE pre-training)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># epochs</cell><cell>400</cell><cell>600</cell><cell>400</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell>learning rate VAE</cell><cell>10 ?2</cell><cell>10 ?2</cell><cell>10 ?2</cell><cell>10 ?2</cell><cell>10 ?2</cell><cell>10 ?2</cell><cell>10 ?2</cell></row><row><cell>batch size per GPU</cell><cell>32</cell><cell>32</cell><cell>64</cell><cell>4</cell><cell>4</cell><cell>64</cell><cell>100</cell></row><row><cell># GPUs</cell><cell>8</cell><cell>8</cell><cell>4</cell><cell>16</cell><cell>16</cell><cell>2</cell><cell>2</cell></row><row><cell>KL annealing to</cell><cell cols="2">? KL =0.7 ? KL =1.0</cell><cell>? KL =0.7</cell><cell>? KL =1.0</cell><cell>? KL =1.0</cell><cell>? KL =1.0</cell><cell>? KL =0.7</cell></row><row><cell>Latent SGM Prior</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># number of scales</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>3</cell><cell>2</cell></row><row><cell># residual cells per scale</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>8</cell></row><row><cell># conv. channels at each scale</cell><cell>[512]?3</cell><cell>[512]?3</cell><cell>[512]?3</cell><cell>256, [512]?3</cell><cell>[320]?2, [640]?3</cell><cell>[256]?3</cell><cell>[256]?2</cell></row><row><cell>use FIR [107]</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>yes</cell><cell>no</cell><cell>no</cell></row><row><cell>Training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(Main LSGM training)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># epochs</cell><cell>1875</cell><cell>1875</cell><cell>1875</cell><cell>1000</cell><cell>2000</cell><cell>1500</cell><cell>800</cell></row><row><cell>learning rate VAE</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>-</cell><cell>10 ?4</cell><cell>10 ?4</cell></row><row><cell>learning rate SGM prior</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>10 ?4</cell><cell>3 ? 10 ?4</cell><cell>3 ? 10 ?4</cell></row><row><cell>batch size per GPU</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>4</cell><cell>8</cell><cell>32</cell><cell>32</cell></row><row><cell># GPUs</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>16</cell><cell>4</cell><cell>4</cell></row><row><cell>KL annealing</cell><cell>continued</cell><cell>no</cell><cell>continued</cell><cell>no</cell><cell>no</cell><cell>continued</cell><cell>continued</cell></row><row><cell>SDE</cell><cell>VPSDE</cell><cell cols="2">VPSDE Geo. VPSDE</cell><cell>VPSDE</cell><cell>VPSDE</cell><cell>VPSDE</cell><cell>VPSDE</cell></row><row><cell>? 2 0 (= ? 2 min for Geo. VPSDE) ? 2 max (only for Geo. VPSDE) t-sampling cutoff during training</cell><cell>0.0 -0.01</cell><cell>0.0 -0.01</cell><cell>3 ? 10 ?5 0.999 0.0</cell><cell>0.0 -0.01</cell><cell>0.0 -0.01</cell><cell>0.0 -0.01</cell><cell>0.0 -0.01</cell></row><row><cell>SGM prior weighting mechanism</cell><cell>w un</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Experiment with a small VAE architecture on dynamically binarized MNIST.</figDesc><table><row><cell>Method</cell><cell>NELBO ? (nats)</cell></row><row><cell>Small VAE [24]</cell><cell>84.08?0.10</cell></row><row><cell>Small VAE + inverse autoregressive flow [24]</cell><cell>80.80?0.07</cell></row><row><cell>Our small VAE</cell><cell>83.85</cell></row><row><cell>Our LSGM w/ small VAE</cell><cell>79.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Number of function evaluations (NFE) of ODE solver during probability flow-based latent SGM prior sampling and corresponding sampling time for our main CIFAR-10 models. Sampling was done in batches of size 16 using a single Titan V GPU. Results are averaged over 20 sampling runs. See Tab. 2 in main text for generative performance metrics.</figDesc><table><row><cell>Method</cell><cell>NFE ?</cell><cell>Sampling Time ?</cell></row><row><cell>LSGM (FID)</cell><cell>138</cell><cell>11.07 sec.</cell></row><row><cell>LSGM (NLL)</cell><cell>120</cell><cell>9.58 sec.</cell></row><row><cell>LSGM (balanced)</cell><cell>128</cell><cell>10.26 sec.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Comparing the VPSDE and Sub-VPSDE in LSGM. For detailed explanations of abbreviations in the table, see Tab. 6 in main paper. Note that importance sampling distributions are generally based on derivations with the VPSDE, even when using the Sub-VPSDE, as discussed in App. B.5.</figDesc><table><row><cell cols="2">SGM-obj.-weighting</cell><cell>w ll</cell><cell>w un</cell><cell>w re</cell></row><row><cell cols="2">t-sampling (SGM-obj.)</cell><cell>r ll (t)</cell><cell>r un (t)</cell><cell>r re (t)</cell></row><row><cell cols="2">t-sampling (q-obj.)</cell><cell>rew.</cell><cell>r ll (t)</cell><cell>r ll (t)</cell></row><row><cell>VPSDE</cell><cell>FID? NELBO?</cell><cell>8.00 2.97</cell><cell>5.39 2.98</cell><cell>6.19 2.99</cell></row><row><cell>Sub-VPSDE</cell><cell>FID? NELBO?</cell><cell>8.46 2.97</cell><cell>5.73 2.97</cell><cell>19.10 3.04</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Other distributions at t = 1 are possible; for instance, see the "variance-exploding" SDE in<ref type="bibr" target="#b1">[2]</ref>. In this paper, however, we use only SDEs converging towards N (z1; 0, I) at t = 1.<ref type="bibr" target="#b2">3</ref> We omit the t-subscript of the diffused distributions qt in all score functions of the form ?z t log qt(zt).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Minimizing L(x, ?, ?, ?) w.r.t ? is equivalent to minimizing KL q(z0|x)||p(z0|x) w.r.t q(z0|x).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We build on the V1 version of<ref type="bibr" target="#b3">[4]</ref>, which was substantially updated after the NeurIPS submission deadline.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We use the VESDE checkpoint at https://github.com/yang-song/score_sde_pytorch. Song et al.<ref type="bibr" target="#b1">[2]</ref> report that ODE-based sampling yields worse FID scores for their models (see D.4 in<ref type="bibr" target="#b1">[2]</ref>). The problem is more severe for VESDEs. Unfortunately, at submission time only a VESDE model was released.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/NVlabs/NVAE (NVIDIA Source Code License)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/rtqichen/torchdiffeq (MIT License)</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix l=1 represent all latent variables and: p(z l |z &lt;l ) = N (z l ; ? l (z &lt;l ), ? 2 l (z &lt;l )I) (78) q(z l |z &lt;l , x) = N (z l ; ? l (z &lt;l , x), ? 2 l (z &lt;l , x)I) <ref type="bibr" target="#b78">(79)</ref> where for simplicity we have assumed that the variance is shared for all the components. We can reparameterize the latent variables by introducing l = z l ?? l (z &lt;l ) ? l (z &lt;l ) . With this reparameterization, the equivalent VAE is:</p><p>where z l = ? l (z &lt;l ) + ? l (z &lt;l ) l . In this equivalent parameterization, we can consider l as latent variables with a standard Normal prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Converting NVAE Prior to Standard Normal Prior</head><p>In NVAE <ref type="bibr" target="#b19">[20]</ref>, the prior has the same hierarchical form as in Eq. 78. However, the authors observe that the residual parameterization of the encoder often improves the generative performance. In this parameterization, with a small modification, the encoder is defined by:</p><p>where the encoder is tasked to predict the residual parameters ?? l (z &lt;l , x) and ?? 2 l (z &lt;l , x). Using the same reparameterization as above ( l = z l ?? l (z &lt;l ) ? l (z &lt;l ) ), we have the equivalent VAE in the form:</p><p>where z l = ? l (z &lt;l )+? l (z &lt;l ) l . In other words, the residual parameterization of encoder, introduced in NVAE, predicts the mean and variance for the l distributions directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Bias in Importance Weighted Estimation of Log-Likelihood</head><p>A common approach for estimating test log-likelihood in VAEs is to use the importance weighted bound on log-likelihood <ref type="bibr" target="#b73">[74]</ref>. In LSGM, we have access to an unbiased but stochastic estimation of the prior likelihood log p(z 0 ) which we obtain using the probability flow ODE <ref type="bibr" target="#b1">[2]</ref>. The stochasticity in the estimation comes from Hutchinson's trick <ref type="bibr" target="#b105">[106]</ref>. In VAEs, the test log-likelihood is estimated using importance weighted (IW) estimation <ref type="bibr" target="#b73">[74]</ref>:</p><p>which is a statistical lower bound on log p(x).</p><p>In this section, we provide an informal analysis that shows that IW estimation with K &gt; 1 can overestimate the log-likelihood when log p(z) is measured with an unbiased estimator with variance ? 2 . In our analysis we assume that ? 2 is small and we use Taylor expansion to study how the IW bound varies. Under our analysis, we observe that the bias has O(? 2 ) and it can be minimized by ensuring that ? 2 is sufficiently small.</p><p>Consider the Taylor expansion around w up to second order of the function log exp(w) = log k e wi where w = {w (k) } K k=1 (log exp : R K ? R). With ? N ( , 0, I) and assuming that ? 2 is sufficiently small so that all terms beyond second order contribute negligibly, we have: Update SGM prior Draw t ? r un/re (t) with IS distribution for un/reweighted objective (Sec. B). Calculate ? t (z 0 ) and ? 2 t according to SDE.</p><p>|| ? ? (z t , t)|| 2 2 . Update SGM prior parameters ? by minimizing L(?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Update VAE Encoder and Decoder with new t sample</head><p>Draw t ? r ll (t) with IS distribution for likelihood weighting (Sec. B). Calculate ? t (z 0 ) and ? 2 t according to SDE.</p><p>Update VAE parameters {?, ?} by minimizing L(x, ?, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Un/Reweighted training with IS of t for the SGM objective</head><p>Input: data x, parameters {?, ?, ?} Draw z 0 ? q ? (z 0 |x) using encoder. Draw t ? r un/re (t) with IS distribution for un/reweighted objective (Sec. B). Calculate ? t (z 0 ) and ? 2 t according to SDE.</p><p>VAE Encoder and Decoder loss computed with the same t sample Calculate cross entropy CE(q ? (z 0 |x)||p ? (z 0 )) ? 1 r un/re (t)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Update all parameters</head><p>Update SGM prior parameters ? by minimizing L(?). Update VAE parameters {?, ?} by minimizing L(x, ?, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.7 Computational Resources</head><p>In total, the research project consumed ? 350, 000 GPU hours, which translates to an electricity consumption of about ? 50 MWh. We used an in-house GPU cluster of V100 NVIDIA GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 Additional Samples</head><p>In this section, we provide additional samples generated by our models for CIFAR-10 in <ref type="figure">Fig. 7</ref>, and CelebA-256-HQ in <ref type="figure">Fig. 8</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Denoising Diffusion Probabilistic Models</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative Modeling by Estimating Gradients of the Data Distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Maximum likelihood training of score-based diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Durkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05233</idno>
		<title level="m">Diffusion models beat gans on image synthesis</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavegrad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<title level="m">Estimating Gradients for Waveform Generation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<title level="m">DiffWave: A Versatile Diffusion Model for Audio Synthesis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Diff-tts: A denoising diffusion model for text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeonghun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Jun</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Byoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Soo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01409</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Symbolic music generation with diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16091</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Permutation invariant graph generation via score-based generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 23rd International Conference on Artificial Intelligence and Statistics</title>
		<meeting><address><addrLine>Palermo, Sicily, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08-28" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning gradient fields for shape generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruojin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12348</biblScope>
			<biblScope unit="page" from="364" to="381" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Diffusion probabilistic models for 3d point cloud generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shitong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01458</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Augmented neural ODEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilien</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3140" to="3150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The expressive power of a class of normalizing flow models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00392</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07101</idno>
		<title level="m">Augmented normalizing flows: Bridging the gap between generative flows and latent variable models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vflow: More expressive generative flows with variational data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqi</forename><surname>Chenli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09741</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03898</idno>
		<title level="m">NVAE: A Deep Hierarchical Variational Autoencoder</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very deep VAEs generalize autoregressive models and can outperform them on images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Connection between Score Matching and Denoising Autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A variational perspective on diffusion-based generative models and score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02808</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Learning Using Nonequilibrium Thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Monte Carlo theory, methods and examples</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimation of Non-Normalized Statistical Models by Score Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interpretation and Generalization of Score Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI &apos;09<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularized estimation of image statistics by Score Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">L</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generalized Denoising Auto-Encoders as Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scheduled denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krzysztof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">A</forename><surname>Geras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08306</idno>
	</analytic>
	<monogr>
		<title level="j">Deep Energy Estimator Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sliced Score Matching: A Scalable Approach to Density and Score Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahaj</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019</title>
		<meeting>the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019<address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><forename type="middle">T</forename><surname>Sommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07762</idno>
		<title level="m">Learning Energy-Based Models in High-Dimensional Spaces with Multi-scale Denoising Score Matching</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03317</idno>
		<title level="m">Efficient Learning of Generative Models via Finite-Difference Score Matching</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improved Techniques for Training Score-Based Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adversarial score matching and improved sampling for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Ioannis Mitliagkas, and Remi Tachet des Combes</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning energy-based models by diffusion recovery likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Knowledge distillation in iterative generative models for improved sampling speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Luhman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Troy</forename><surname>Luhman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02388</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Noise estimation for generative diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San-Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02600</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03480</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graphite: Iterative generative modeling of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient graph generation with graph recurrent attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02731</idno>
		<title level="m">Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">BIVA: A very deep hierarchy of latent variables for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Li?vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6548" to="6558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolfe</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Discrete variational autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">DVAE#: Discrete variational autoencoders with relaxed Boltzmann priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DVAE++: Discrete variational autoencoders with overlapping transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Khoshaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Undirected graphical models as approximate posteriors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning latent space energybased prior model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21994" to="22008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Latent constraints: Learning to generate conditionally from unconditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Resampled priors for variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics</title>
		<editor>Kamalika Chaudhuri and Masashi Sugiyama</editor>
		<meeting>the Twenty-Second International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2019-04" />
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="16" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyoti</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ncp-Vae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02917</idno>
		<title level="m">Variational autoencoders with noise contrastive priors</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial autoencoders</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Variational autoencoder with implicit optimal priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoharu</forename><surname>Iwata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Yamanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Yagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5066" to="5073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Vae with a vampprior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Diagnosing and enhancing VAE models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14837" to="14847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">From variational to deterministic autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09841</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Refining deep generative models via discriminator gradient flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Abdul Fatir Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Liang Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Your gan is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Ruixiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Discriminator optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Controllable and compositional generation with latent-space energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Diffusion priors in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Wehenkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Louppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">D2c: Diffusion-denoising models for few-shot conditional generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06819</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00630</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Variational diffusion models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Score matching model for unbounded data score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjae</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05527</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Neural ordinary differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Importance weighted autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">VAEBM: A symbiosis between variational autoencoders and energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10063</idno>
		<title level="m">Dual contradistinctive generative autoencoder</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Distribution augmentation for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5006" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Preventing posterior collapse with delta-vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">PixelSNAIL: An improved autoregressive generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Xi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">PixelCNN++: Improving the pixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Autogan: Neural architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Adversarial latent autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Casper Kaae S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>S?ren Kaae S?nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3738" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Andriyash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Vinci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad H</forename><surname>Amin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09948</idno>
		<title level="m">Pixel-vae++: Improved pixelvae with discrete prior</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">MAE: Mutual posterior-divergence regularization for variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">The tools of generative art, from flash to neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Art in America</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Deepfakes and disinformation: Exploring the impact of synthetic political video on deception, uncertainty, and trust in news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Vaccari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chadwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Media+ Society</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2056305120903408</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Optimus: Organizing sentences via pre-trained modeling of a latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Inverse molecular design using machine learning: Generative models for matter engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Lengeling</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<biblScope unit="issue">6400</biblScope>
			<biblScope unit="page" from="360" to="365" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Zaccary Alperstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Tyler</forename><surname>Cherkasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rolfe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13343</idno>
		<title level="m">All smiles variational autoencoder</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01583</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Semantic segmentation with generative models: Semi-supervised learning and strong out-of-domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Applied Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>S?rkk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Solin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Institute of Mathematical Statistics Textbooks</title>
		<meeting><address><addrLine>United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">O</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic Process. Appl</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="326" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">A family of embedded Runge-Kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
