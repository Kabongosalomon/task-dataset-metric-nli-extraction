<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Action Quality Assessment Using Weighted Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafkat</forename><surname>Farabi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Islamic University of Technology</orgName>
								<address>
									<settlement>Gazipur</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data61-CSIRO</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Action Quality Assessment Using Weighted Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Action Quality Assessment ? Aggregation ? MTL-AQA</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1[0000?0003?4712?1208] , Hasibul Himel 1[0000?0002?9246?2477] , Fakhruddin Gazzali 1[0000?0001?5609?5852] , Md. Bakhtiar Hasan 1[0000?0001?8093?5006] , Md. Hasanul Kabir 1[0000?0002?6853?8785] , and Moshiur Farazi 2[0000?0003?1494?5921]</p><p>Abstract. Action quality assessment (AQA) aims at automatically judging human action based on a video of the said action and assigning a performance score to it. The majority of works in the existing literature on AQA divide RGB videos into short clips, transform these clips to higherlevel representations using Convolutional 3D (C3D) networks, and aggregate them through averaging. These higher-level representations are used to perform AQA. We find that the current clip level feature aggregation technique of averaging is insufficient to capture the relative importance of clip level features. In this work, we propose a learning-based weightedaveraging technique. Using this technique, better performance can be obtained without sacrificing too much computational resources. We call this technique Weight-Decider(WD). We also experiment with ResNets for learning better representations for action quality assessment. We assess the effects of the depth and input clip size of the convolutional neural network on the quality of action score predictions. We achieve a new state-of-the-art Spearman's rank correlation of 0.9315 (an increase of 0.45%) on the MTL-AQA dataset using a 34 layer (2+1)D ResNet with the capability of processing 32 frame clips, with WD aggregation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Action quality assessment (AQA) addresses the problem of developing a system that can automatically judge the quality of an action performed by a human. This is done by processing a video of the performance and assigning a score to it. The motivation to develop such a system stems from its potential use in applications such as health care <ref type="bibr" target="#b15">[13]</ref>, sports video analysis <ref type="bibr" target="#b18">[16]</ref>, skill discrimination for a specific task <ref type="bibr" target="#b16">[14]</ref>, assessing the skill of trainees in professions such as surgery <ref type="bibr" target="#b3">[3]</ref>  <ref type="figure">Fig. 1</ref>: Overview of a general AQA pipeline and our improvement over it. Generally, the input video is divided into clips. A feature extractor extracts features from these clips. These features are then aggregated into a video level feature vector. A linear-regressor predicts action quality scores based on this feature vector. We introduce a Weight-Decider module to this architecture, which proposes weights based on the clip level features for better aggregation. Additionally, we use a ResNet instead of the commonly used C3D.</p><p>Almost all existing works have treated AQA as a regression problem <ref type="bibr" target="#b16">[14,</ref><ref type="bibr" target="#b17">15,</ref><ref type="bibr" target="#b20">18]</ref>. As shown in <ref type="figure">Figure 1</ref>, most approaches boil down to dividing an RGB video of the action in multiple clips, extracting higher-level features from each clip, aggregating them, and then training a linear-regressor to predict a score based on these aggregated features. Most of these works <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b26">22]</ref> utilize a convolutional neural network <ref type="bibr" target="#b12">[11]</ref> to extract complex higher-level features and simple averaging to aggregate the features. The best performing models <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b20">18</ref>] make use of the Convolutional-3D (C3D) network <ref type="bibr" target="#b23">[20]</ref> with average aggregation.</p><p>The majority of works in AQA <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b20">18,</ref><ref type="bibr" target="#b26">22]</ref> aggregate clip level features into a video level feature vector by simply averaging them. We think this fails to preserve temporal information present in the data. We hypothesize that a more sophisticated method would improve the overall performance of the pipeline. We propose one such technique by introducing a module called Weight-Decider(WD). This module inspects the feature vectors extracted from individual clips and proposes a corresponding weight vector. Finally, these weight vectors can be used to calculate a weighted average of the clip-level feature vectors. In this way, the final video level feature vector contains more contributions from the important features of each clip. This is similar to how real-world judges base their final scoring on key mistakes/skills of the performer and not on an average of all the moments in the action. We design the WD as a shallow neural network that can be trained along with the rest of the AQA pipeline. In our experiments, we show the performance of the AQA pipeline to improve when using WD as aggregation.</p><p>Spatio-temporal versions of ResNets capable of processing videos have been proposed by <ref type="bibr" target="#b6">[5]</ref> and <ref type="bibr" target="#b25">[21]</ref>. These approaches have achieved state-of-the-art results in the task of action recognition and outperform the C3D network <ref type="bibr" target="#b6">[5]</ref>. Hence, we decide to use spatio-temporal ResNets as feature extractors for performing AQA. We experiment with various 3D and (2+1)D ResNet feature extractors on the MTL-AQA dataset <ref type="bibr" target="#b18">[16]</ref>. We find that 3D and (2+1)D ResNets of depth 34 and 50 with pretraining on large-scale action recognition datasets have performance comparable to the state-of-the-art. We see that (2+1)D and 3D convolutions perform fairly similarly. For 34 layer (2+1)D ResNets, we experiment with 3 different versions that can process 8, 16, or 32 frame clips at once and find the 32 frame clip version to clearly outperform the rest. Our results suggest that processing longer clips is more beneficial than going deeper with convolutions. The 34 layers (2+1)D ResNet with WD processing 32 frame clips achieves a Spearman's rank correlation of 0.9315 on the MTL-AQA dataset, achieving a new state-of-the-art.</p><p>Contributions:</p><p>-We propose a novel learning-based light-weight aggregation technique called Weight-Decider and demonstrate that it can improve the AQA pipeline's performance.</p><p>-To the best of our knowledge, this is the first work to do a comparative analysis of the effect of the depth, convolution type, and input clip size of the ResNet feature extractor on the final quality of the scores predicted in AQA. -One of our approaches outperforms all the previous works in AQA on the MTL-AQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Pirsiavash et al. <ref type="bibr" target="#b20">[18]</ref> proposed a novel dataset containing videos of Diving and <ref type="figure">Figure-</ref>skating annotated with action quality scores by expert human judges. They provided Discrete Cosine Transform (DCT) and extracted human pose as inputs to a Support Vector Regressor, which predicted the score. More recent works have utilized the Convolutional 3D (C3D) network <ref type="bibr" target="#b23">[20]</ref> as a feature extractor. Parmar and Morris <ref type="bibr" target="#b17">[15]</ref> proposed three architectures, C3D-SVR, C3D-LSTM, C3D-LSTM-SVR, all of which used features extracted from short video clips using C3D network, and later aggregated them and predicted an action score using Support Vector Regressor (SVR) and Long Short-Term Memory (LSTM).In a later work, Parmar and Morris <ref type="bibr" target="#b18">[16]</ref> took a multitask approach towards action quality assessment. They released a novel AQA dataset called MTL-AQA and proposed a multi-task learning-based C3D-AVG-MTL framework that extracted features using the C3D network and aggregated these through averaging. They trained these features to do well in score prediction, action classification, and generating captions. Tang et al. <ref type="bibr" target="#b22">[19]</ref> took a probabilistic approach (MUSDL). They divided 103 frame videos into 10 overlapping 16 frame clips, used I3D <ref type="bibr" target="#b1">[1]</ref> architecture to extract clip level features, averaged as aggregation, and finally predicted parameters of a probabilistic distribution from which the final score prediction was sampled. The authors calculated 7 different scores corresponding to 7 judges for Olympic scoring and summed up the 5 scores in the middle. With this advantage over simple regression-based methods which directly predict the score, this approach achieved a SOTA spearman's correlation of 0.9273. Diba et al. <ref type="bibr" target="#b2">[2]</ref> used a method called "STC Block" for action recognition. This is similar to our proposed aggregation method. However, they utilize this on spatial and temporal features separately after each convolution layer for action recognition, whereas our method is applied to the output of the CNN to aggregate clip level spatiotemporal features for performing AQA.</p><p>Our proposed approach differs from these works in that we use 3D and (2+1)D ResNets as the feature extractor and we aggregate these features using the WD network, which is a lightweight and learning-based feature aggregation scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Pipeline Overview</head><p>Let, V = {F t } L t=1 be the input video containing L frames, where F t denotes the t th frame. It is divided into N non-overlapping clips, each of size n = L N . Thus we define the i th clip as</p><formula xml:id="formula_0">C i = {F j } i?(n+1)?1 j=i?n</formula><p>. The feature extractor takes in a clip C i and outputs a feature vector f i . For the feature extractor, we experiment with 3D ResNets <ref type="bibr" target="#b7">[6]</ref> and (2+1)D ResNets <ref type="bibr" target="#b25">[21]</ref> with varying depth and input clip size. Next, we aggregate these clip level features to obtain a global video level representation. Finally, a linear-regressor is trained to predict the score from the video level feature representation. Following the majority of previous works [14 <ref type="bibr">-16, 18]</ref>, we model the problem as linear regression. This makes sense as the action quality score is a real number. To experiment with the relation of the ResNet feature extractor's depth with the AQA pipeline's ability to learn, we experiment with 3 different depths:</p><p>-34 layer: We experiment with both 34 layer 3D and (2+1)D ResNets. The only difference being 3D ResNets use 3 ? 3 ? 3 convolution kernels, on the other hand (2+1)D ResNets use a 1 ? 3 ? 3 convolution followed by 3 ? 1 ? 1 convolution <ref type="bibr" target="#b25">[21]</ref>. We take the final average-pool layer output (512 dimensional) and pass it through 2 fully-connected layers having 256 and 128 units. The 3D ResNet takes input 16 frame clips. On the other hand, we experiment with 3 different variations of (2+1)D 34-layer ResNet, processing 8, 16, and 32 frame clips using available pre-trained weights, -50 layer: We experiment with 50 layer 3D and (2+1)D ResNets. The final average-pool layer outputs a feature vector of size 2048. We take this feature vector and input it into 3 fully-connected layers having 512, 256, and 128 units. Only 16 frame clips are processed. -101 layer: We experiment with 101 layer 3D ResNet. The remaining details about the input clip size and output feature vector processing are identical to the 50 layer ResNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Aggregation</head><p>Most of the previous works dealing with AQA process the entire input video by first dividing it into multiple smaller clips of equal size, due to memory and computational budget. Most CNNs are designed to process <ref type="bibr">8, 16,</ref>   level feature description. Next, a linear-regressor predicts the final score based on this feature description. The best performing works aggregated the clips by simply averaging them <ref type="bibr" target="#b17">[15,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b22">19]</ref>. Some other works <ref type="bibr" target="#b16">[14,</ref><ref type="bibr" target="#b17">15]</ref> aggregated using LSTMs <ref type="bibr" target="#b8">[7]</ref>. However, LSTM networks, although make sense in theory because of their ability to handle time sequences, perform worse due to the lack of bigenough datasets dedicated to AQA. We propose that simply averaging the clip-wise features is an ineffective measure. It should not be able to preserve the temporal information available in the data. This follows from the fact changing the order of the clip level features will generate the same average and hence the same score prediction. Furthermore, expert human judges focus more on mistakes and deviations of the performers and these have a bigger impact on the score. Hence we think, a weighted averaging technique might be more suitable, as the linear-regressor will be able to base its decision on features more important from each clip.</p><p>More concretely, if the feature vector extracted from clip C i is f i , we propose the video level feature vector as</p><formula xml:id="formula_1">f video = N i=1 (f i w i )<label>(1)</label></formula><p>where w i is a weight vector corresponding to the feature vector f i and represents Hadamard Product or elementwise multiplication. w i is of the same dimensions as f i and learned using a small neural network of 4 layers. This smaller neural network takes as input 128-dimensional feature vector f i and runs it through fully connected layers containing 64, 32, 64, and 128 neurons. All but the final layers employ a ReLU activation function. The architecture is explained in <ref type="figure" target="#fig_0">Figure 2</ref>. Finally, to ensure the weights corresponding to the same element of different weight vectors sum up to one, a softmax is applied along with the corresponding elements of all the weight vectors. We call this shallow neural network Weight-Decider(WD).</p><formula xml:id="formula_2">w i = W D(f i )<label>(2)</label></formula><formula xml:id="formula_3">[w 0 w 1 . . . w N ] = sof tmax([w 0 w 1 . . . w N ])<label>(3)</label></formula><p>Finally, the linear-regressor can predict the score using the feature vector f video as proposed by equation 1.</p><p>The proposed WD module can be used with any of the feature extractors we planned to use in our experiments. WD replaces averaging as aggregation within the typical AQA pipeline. It can be trained with the rest of the AQA model in an end-to-end manner.  Adding WD to the AQA pipeline to replace averaging as aggregation does not cost much in terms of computational resources. To see this, recall that the WD module has 3 hidden layers and an output layer, consisting of 64, 32, 64, and 128 neurons in that order. Each of these layers contains a weight matrix and a bias. The first hidden layer takes 128 inputs and has 64 neurons. Thus the weight matrix is of dimensions 64 ? 128. Hence, number of trainable parameters in this layer (including the bias term) is (64 ? 128 + 1) = 8193. Similarly, the second hidden layer, third hidden layer, and the output layers contain 2049, 2049, and 8193 trainable parameters correspondingly. By summing up all the trainable parameters in each layer, we can see that WD only contains 20484 trainable parameters. Spatio-temporal ResNet feature extractors have millions of trainable parameters. Hence, additional resources required to train the WD module used in an AQA pipeline are not much. As an example, the 3D and (2+1)D ResNet-34 feature extractors we are using contain 63.6 million trainable parameters <ref type="bibr" target="#b14">[12]</ref>. Thus, using the WD module on top of ResNet-34 would increase the number of trainable parameters by approximately 0.03%. The ratio would be smaller for a deeper ResNet. Hence, our proposed WD does not require many computational resources to be incorporated into the pipeline. Our experiments support this. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MTL-AQA Dataset [16]</head><p>The biggest dataset dedicated to AQA. It contains 1412 video samples split into 1059 training and 353 test samples. The samples are collected from 16 Olympic dive events. Each sample is 103 frames long and accompanied by the final action quality scores from expert Olympic judges. It was published by Parmer and Morris <ref type="bibr" target="#b18">[16]</ref>. We used the exact same train/test split made public in their work <ref type="bibr" target="#b18">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metric:</head><p>In line with previously published literature, we use Spearman's rank correlation as the evaluation metric. This metric measures the correlation between two sequences containing ordinal or numerical data. It is calculated using the equation:</p><formula xml:id="formula_4">? = 1 ? 6 d 2 i n(n 2 ? 1)<label>(4)</label></formula><p>? = Spearman's rank correlation d i = The difference between the ranks of corresponding variables n = Number of observations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details:</head><p>We implemented our proposed methods using PyTorch <ref type="bibr" target="#b19">[17]</ref>. All the 3D ResNets and (2+1)D ResNets processing 16 frame clips were pre-trained on Kinetics-700 [9] dataset 3 . The (2+1)D Resnets processing 8 and 32 frame clips were pre-trained on IG-65M dataset <ref type="bibr" target="#b5">[4]</ref> and fine tuned on Kinetics-400 <ref type="bibr" target="#b10">[9]</ref> dataset 4 . For each ResNet, we separately experimented using both averaging and WD as feature aggregation. We temporally augmented by randomly picking an ending frame from the last 6 and chose the preceding 96 frames for processing. The frames were resized to 171 ? 128 and center cropped to 112 ? 112. Random horizontal flipping was applied. Batch-normalization was used for regularization. We defined the loss function as a sum of L2 and L1 loss between the predicted score and ground-truth score as Parmar and Morris <ref type="bibr" target="#b18">[16]</ref> suggested. We trained the network using the ADAM optimizer <ref type="bibr" target="#b11">[10]</ref> for 50 epochs. We used a learning rate of 0.0001 for modules with randomly initialized weights and 0.00001 for modules with pretrained weights. Train and Test batch sizes were 2 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on MTL-AQA Dataset:</head><p>In <ref type="table" target="#tab_3">Table 1a</ref>, we present the experiment results of varying the depth of the ResNet feature extractor and the aggregation scheme. We can see that 34 layer (2+1)D ResNet with WD as aggregation performs the best with a Spearman's correlation of 0.8990. Increasing the depth to 50 layers somewhat decreases Spearman's correlation. However, the results are still competitive. At 101 layer depth, even when initialized with pretrained weights from Kinetics <ref type="bibr" target="#b10">[9]</ref>, overfitting occurs fairly quickly. The overfitting is also evident from the train/test curves presented in <ref type="figure" target="#fig_4">Figure 4</ref>. The likely reason behind this is the increased number of parameters in the feature extractor. This leads us to establish that the current biggest AQA dataset has enough data to train a 34-layer and 50-layer ResNet feature extractor with generalization, however it overfits the 101-layer ResNet feature extractor.   Because of how (2+1)D ResNets are designed, they have a similar parameter count to their 3D counterparts <ref type="bibr" target="#b25">[21]</ref>. Because the overfitting is occurring due to the high parameter count, we do not repeat the experiment with a (2+1)D ResNet-101 feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of clip length:</head><p>We check the effect of clip length on the performance. We take the best performing model from <ref type="table" target="#tab_3">Table 1a</ref> (ResNet(2+1)D-34-WD) and input various clip sizes. We experiment with clip sizes of 8 frames, 16 frames, and 32 frames. We vary the aggregation method as well.</p><p>From <ref type="table" target="#tab_3">Table 1b</ref> we can see that the performance of the pipeline increases with the number of frames in each clip. We hypothesize that longer clips allow the ResNet to look for bigger patterns in the temporal dimension, which in turn enables the feature descriptors to be more informative. This enables the linear-regressor to better discriminate between similar-looking examples with fine-grained action quality differences. From <ref type="table" target="#tab_3">Table 1b</ref>, notice that using WD over simple averaging as aggregation gives a boost in performance. However, this performance boost is quite significant in case of 8 frame clips. We believe the reason behind this improved performance is that increasing clip size reduces the number of clips being averaged. Whatever detrimental effect the averaging might have, it will be more prominent when the number of objects being averaged is larger, and less when this number is smaller. Furthermore, CNNs with longer input clips can look at more frames, this in effect increases their temporal horizon. It follows that the feature vectors extracted would have a better encoding of action patterns across time, to begin with. Thus they perform well enough even with averaging as aggregation. But using WD increases performance nevertheless.</p><p>For qualitative results, refer to <ref type="table" target="#tab_5">Table 2</ref>.   <ref type="bibr" target="#b20">[18]</ref> 0.2682 C3D-SVR <ref type="bibr" target="#b17">[15]</ref> 0.7716 C3D-LSTM <ref type="bibr" target="#b17">[15]</ref> 0.8489 MSCADC-STL <ref type="bibr" target="#b18">[16]</ref> 0.8472 MSCADC-MTL <ref type="bibr" target="#b18">[16]</ref> 0.8612 USDL-Regression <ref type="bibr" target="#b22">[19]</ref> 0.8905 C3D-AVG-STL <ref type="bibr" target="#b18">[16]</ref> 0.8960 C3D-AVG-MTL <ref type="bibr" target="#b18">[16]</ref> 0.9044 MUSDL <ref type="bibr" target="#b22">[19]</ref> 0.9273 Ours C3D-WD 0.9223 Ours ResNet34-(2+1)D-WD (32 frame) 0.9315 Ours ResNet50-3D-WD (16 frame) 0.8935 Ours ResNet101-3D-AVG (16 frame) 0.6633</p><p>The ground truth scores provided in the MTL-AQA <ref type="bibr" target="#b18">[16]</ref> are taken from expert Olympic judges during a live broadcast of events. 7 judges independently score the athlete's performance on a scale of 0 (completely failed) to 10 (excellent). The 3 median scores are then added together and multiplied with a pre-determined difficulty degree to obtain the final score. This final score is the one provided in the "True Score" column of <ref type="table" target="#tab_5">Table 2</ref>. Our various pipelines attempt to predict this final score from the input performance video.</p><p>Comparison with the state of the art: In <ref type="table" target="#tab_6">Table 3</ref>, we compare our best performing models of each depth with previous works on the MTL-AQA dataset. For comparison, we combined C3D architecture with WD to test the result. For the C3D-WD model, 16 frame clips were used. The C3D portion of the model was initialized with Sports-1M <ref type="bibr" target="#b9">[8]</ref> pretrained weights. We include this result in <ref type="table" target="#tab_6">Table  3</ref> as well. We can see that our (2+1)D ResNet-34 (32 frame)-WD outperforms all previous works in the literature. This shows the effectiveness of our approach. We can further see that 3D ResNet-50 (16 frame) obtains results comparable to the SOTA. However, the ResNet-101 based approach overfits the dataset and hence performs poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed a ResNet-based regression-oriented pipeline for action quality assessment. We demonstrated experimentally that the MTL-AQA dataset has enough data to train 34 and 50 layer ResNet-based pipelines when initialized with pretrained weights from a related task (like action recognition). Our experiments suggest processing longer clips is more effective than using deeper ResNets. We also propose a lightweight learning-based aggregation technique called WD to replace simple averaging. Experiments show our methods to be more effective than previous works. In the future, we want to investigate with CNNs that can process longer clips (64 or higher) to see if this translates to better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The architecture of the WD network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The video is divided into clips and each clip is processed by a ResNet feature extractor. The ResNet can have depths 34, 50, or 101. Elements with a solid border indicate trainable modules. The solid bordered white modules are initialized with pretrained weights. The solid bordered green modules are trained from scratch. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>We found the training time for 32-frame ResNet(2+1)D-34 to be 4557 seconds per epoch, and the inference time to be 604 seconds per epoch. The introduction of WD increases the training time per epoch by 24 seconds (0.52%) and the inference time by 21 seconds (3.47%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Train and Test curves obtained from training the pipeline using 3D-ResNet 34, 50, and 101 as feature extractors. Notice that ResNet-101 based architectures show signs of significant overfitting compared to ResNet-34 and ResNet-50 based architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>:2102.10555v2 [cs.CV] 11 Mar 2022</figDesc><table><row><cell></cell><cell>Weight</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Decider</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Video</cell><cell>Feature Extractor</cell><cell>Aggregator</cell><cell>Regressor</cell><cell>Predicted Score</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>,</cell></row><row><cell>etc.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>arXiv</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>or 32 frames at once. Then the features extracted by the CNN are aggregated to form a video</figDesc><table><row><cell>Feature-vector (128 Dimensions)</cell><cell>Fully connected 64 neurons</cell><cell>ReLU</cell><cell>Fully connected 32 neurons</cell><cell>ReLU</cell><cell>Fully connected 64 neurons</cell><cell>ReLU</cell><cell>Fully connected 128 neurons</cell><cell>Weight-vector (128 Dimensions)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of the various types of ResNets as feature extractors and varying clip length in our pipeline</figDesc><table><row><cell cols="3">(a) Effect of various types of ResNets</cell><cell>(b) Effect of varying input clip size in</cell></row><row><cell>Depth ResNet-34 ResNet-50 ResNet-101</cell><cell>Convolution 3D (2+1)D 3D (2+1)D 3D</cell><cell>Aggregation Average WD 0.8982 0.8951 0.8932 0.8990 0.8880 0.8935 0.8818 0.8814 0.6663 0.6033</cell><cell>ResNet(2+1)D-34 Clip length (Input Frames) Average WD Aggregation 8 0.8570 0.8853 16 0.8932 0.8990 32 0.9289 0.9315</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Qualitative results. Every 16 th frame processed starting from frame 0 is shown. Italic scores correspond to WD aggregation, plain text scores correspond to average aggregation.<ref type="bibr" target="#b9">The 8,</ref><ref type="bibr" target="#b18">16</ref>, and 32 correspond to input clip sizes.</figDesc><table><row><cell></cell><cell></cell><cell>ResNet-34</cell><cell cols="2">ResNet-50 ResNet-101</cell></row><row><cell>Input Frames</cell><cell>8</cell><cell>Prediction (2+1)D 16 32</cell><cell cols="2">Prediction Prediction True 3D (2+1)D 3D Score 3D</cell></row><row><cell></cell><cell cols="3">54.84 30.46 8.39 7.29 33.23 34.10 38.76 18.11 16.41 22.93 38.29 29.93</cell><cell>45.22 52.21</cell><cell>25.64</cell></row><row><cell></cell><cell cols="3">66.94 59.69 47.92 67.92 43.57 58.30 63.85 40.88 53.21 63.62 52.80 52.31</cell><cell>122.20 76.64</cell><cell>52.79</cell></row><row><cell></cell><cell cols="3">71.46 71.34 69.39 83.34 67.38 80.41 69.85 64.90 70.40 81.31 67.53 74.25</cell><cell>167.60 132.50</cell><cell>69.59</cell></row><row><cell></cell><cell cols="3">67.13 46.16 27.73 34.25 44.06 46.61 64.54 32.29 42.87 39.62 49.03 47.13</cell><cell>54.28 51.51</cell><cell>46.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the state of the art on the MTL-AQA dataset</figDesc><table><row><cell>Method</cell><cell>Sp. Corr.</cell></row><row><cell>Pose+DCT</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Weights available at: https://github.com/kenshohara/3D-ResNets-PyTorch 4 Weights available at: https://github.com/moabitcoin/ig65m-pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">ResNet-34-Avg(test) ResNet-34-WD(test) ResNet-50-Avg(test) ResNet-50-WD(test) ResNet-101-Avg(test) ResNet-101-WD</title>
		<imprint/>
	</monogr>
	<note>test</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.502</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.502" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatio-temporal channel correlation networks for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Arzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yousefzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01225-0_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01225-018" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<editor>Weiss, Y.</editor>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11208</biblScope>
			<biblScope unit="page" from="299" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video-based surgical skill assessment using 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Assisted Radiology and Surgery</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1217" to="1225" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/s11548-019-01995-1</idno>
		<ptr target="https://doi.org/10.1007/s11548-019-01995-1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pretraining for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01232</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.01232" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12046" to="12055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCVW.2017.373</idno>
		<ptr target="https://doi.org/10.1109/ICCVW.2017.373" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops, ICCV Workshops</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Largescale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.223</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.223" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950abs/1705.06950</idno>
		<ptr target="http://arxiv.org/abs/1705.06950" />
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Convolutional Networks for Images, Speech, and Time Series</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="255" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<idno type="DOI">10.5555/303568.303704</idno>
		<ptr target="https://doi.org/10.5555/303568.303704" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-cnn architecture for effective spatio-temporal learning in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.3390/app10020557</idno>
		<ptr target="https://doi.org/10.3390/app10020557" />
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">557</biblScope>
			<date type="published" when="2020-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring the quality of exercises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMBC.2016.7591175</idno>
		<ptr target="https://doi.org/10.1109/EMBC.2016.7591175" />
	</analytic>
	<monogr>
		<title level="m">38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2241" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action quality assessment across multiple actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV.2019.00161</idno>
		<ptr target="https://doi.org/10.1109/WACV.2019.00161" />
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision, WACV 2019</title>
		<meeting><address><addrLine>Waikoloa Village, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1468" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to score olympic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2017.16</idno>
		<ptr target="https://doi.org/10.1109/CVPRW.2017.16" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="76" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What and how well you performed? A multitask learning approach to action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00039</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00039" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Assessing the quality of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<editor>Tuytelaars, T.</editor>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8694</biblScope>
			<biblScope unit="page" from="556" to="571" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10599-4_36</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10599-436" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Uncertaintyaware score distribution learning for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00986</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00986" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9836" to="9845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2015.510</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.510" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00675</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00675" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">S3D: stacking segmental P3D for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2018.8451364</idno>
		<ptr target="https://doi.org/10.1109/ICIP.2018.8451364" />
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-10-07" />
			<biblScope unit="page" from="928" to="932" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
