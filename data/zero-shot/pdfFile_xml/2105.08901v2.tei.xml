<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Sequence-to-Set Network for Nested Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Tan</surname></persName>
							<email>zqtan@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<email>yzhuang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Sequence-to-Set Network for Nested Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Named entity recognition (NER) is a widely studied task in natural language processing. Recently, a growing number of studies have focused on the nested NER. The span-based methods, considering the entity recognition as a span classification task, can deal with nested entities naturally. But they suffer from the huge search space and the lack of interactions between entities. To address these issues, we propose a novel sequence-to-set neural network for nested NER. Instead of specifying candidate spans in advance, we provide a fixed set of learnable vectors to learn the patterns of the valuable spans. We utilize a non-autoregressive decoder to predict the final set of entities in one pass, in which we are able to capture dependencies between entities. Compared with the sequence-tosequence method, our model is more suitable for such unordered recognition task as it is insensitive to the label order. In addition, we utilize the loss function based on bipartite matching to compute the overall training loss. Experimental results show that our proposed model achieves state-ofthe-art on three nested NER corpora: ACE 2004,  ACE 2005 and KBP 2017. The code is available at https://github.com/zqtan1024/sequence-to-set. * Equal contribution. ? Corresponding author. Some 2,500 Republican Guard forces surrendered between Kut and Baghdad .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ORG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPE GPE</head><p>He 's a professor of psychiatry at nyu , chairman of the forensic panel .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is a fundamental task in natural language processing because named entities often contain the key information <ref type="bibr" target="#b6">[Lample et al., 2016]</ref>. Nested named entities refer to that internal named entities are contained in external named entities as in <ref type="figure" target="#fig_0">Figure 1</ref>, in which "nyu" and "a professor of psychiatry at nyu" are both named entities. Such nested entities are common in practice. For example, about 38% of the entities in the ACE 2005 training dataset are nested. Traditional work <ref type="bibr" target="#b3">[Huang et al., 2015;</ref><ref type="bibr" target="#b6">Lample et al., 2016;</ref><ref type="bibr" target="#b0">Chiu and Nichols, 2016</ref>] models named entity recognition as a sequence labeling task, thus enabling to leverage RNN-based methods to recognize flat entities. However, since tokens in nested entities may belong to multiple labels, the traditional sequence labeling approaches cannot meet the demand.</p><p>Various methods have been proposed to deal with the nested NER task, including the sequence-to-sequence methods <ref type="bibr" target="#b3">[Ju et al., 2018;</ref><ref type="bibr" target="#b10">Strakov? et al., 2019;</ref><ref type="bibr" target="#b4">Jue et al., 2020]</ref> and the span-based methods <ref type="bibr" target="#b10">[Sohrab and Miwa, 2018;</ref><ref type="bibr" target="#b11">Zheng et al., 2019;</ref><ref type="bibr" target="#b11">Yu et al., 2020]</ref>. The sequence-to-sequence method treats nested NER as a sequence generation task in which labels are decoded one by one in order. However, in the named entity recognition task, the output labels are essentially an unordered set. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, there is apparently no ordinal relationship between the two GPE entities ("Kut" and "Baghdad"). <ref type="bibr" target="#b10">Strakov? et al. [2019]</ref> generates the output labels sequentially which is sensitive to the label order. In this manner, even if the model predicts all correct labels, it may cause an unreasonable training loss as a result of inconsistent order. The span-based models classify the candidate spans which are extracted from a text sequence through various approaches. <ref type="bibr" target="#b10">Sohrab and Miwa [2018]</ref> enumerates all possible spans within a limit length and then predicts their classes. Unlike the exhaustive method, Zheng et al. <ref type="bibr">[2019]</ref> first identifies the left and right boundaries separately and then matches them to form candidate spans. Similarly, <ref type="bibr">ARN [Lin et al., 2019]</ref> recognizes the spans of interest based on previously identified anchor words. <ref type="bibr">Recently, Yu et al. [2020]</ref> scores candidate spans via the biaffine model <ref type="bibr">[Dozat and Manning, 2016]</ref> and reaches the state-of-the-art. Although the above span-based methods achieve good performances, they also face some difficulties. The span-selecting methods face the problem of error propagation, because the boundaries or anchors are tend to be identified incorrectly, while the spanenumerating methods need to search for all possible regions. Besides, the candidate spans in these methods do not interact directly with each other and thus inappropriately ignore the dependencies between named entities.</p><p>To address the above issues, we propose a novel model that treats the named entity recognition as a sequence-to-set task. Considering that the entities in a sequence are inherently unordered, we expect to predict the entire set of entities in one pass. Therefore our model is no longer as sensitive to the label order as the sequence-to-sequence model. Compared to the span-based methods, instead of specifying candidate spans in advance, we provide a fixed set of learnable vectors that go through the entire train data to learn the patterns of the valuable spans. Hence we do not need to search for all possible spans anymore. Specifically, our model consists of three components: a sequence encoder, an entity set decoder and a loss function based on bipartite matching. We utilize <ref type="bibr">BERT [Devlin et al., 2019]</ref> and BiLSTM <ref type="bibr" target="#b3">[Huang et al., 2015]</ref> to construct the sequence encoder. In order to predict all entities of the sequence in one pass, we design a non-autoregressive entity set decoder. It receives the sequence encoding and a set of learnable vectors, which are called entity queries, to decode the final entity set. Furthermore, our decoder is able to capture the dependencies between entities quite naturally through a self-attention mechanism between entity queries. At the end, we utilize the loss function based on bipartite matching to compute the overall training loss.</p><p>Our main contributions are as follow:</p><p>? We propose a novel sequence-to-set network for the prediction of the entity set. To the best of our knowledge, we are the first to consider named entity recognition as a sequence-to-set task. In addition, we predict the final entity set in one pass, while the sequence-to-sequence model predicts entities one by one. Since entities are inherently unordered, our model which is insensitive to the label order achieves a better performance.</p><p>? We provide a fixed set of entity queries to replace the explicit candidate spans, thus eliminating the need to search for all possible spans. Besides, we are able to capture the dependencies between entities by using a self-attention mechanism which performs direct interactions between entity queries.</p><p>? Experimental results show that our model achieves stateof-the-art on three widely used datasets, and outperforms several competing baseline models on F1 score by 0.56% on ACE 2004, 1.65% on ACE 2005 and 2.99% on KBP 2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>In order to transform the input sequence to the entity set, our model has three necessary components: a sequence encoder, an entity set decoder and a loss function based on bipartite matching. Firstly, the sequence encoder obtains rich contextual information from the input sentence. The entity set decoder then receives the sequence representation from the encoder and a set of entity queries to decode the left and right boundaries and categories of the predicted entity set. The bipartite matching is utilized to assign a unique prediction to each target entity so that the prediction loss of the entire model can be computed. The overview of our sequence-toset model is shown in <ref type="figure">Figure 2</ref>. In the following subsections, we will introduce them in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence Encoder</head><p>We use <ref type="bibr">BERT [Devlin et al., 2019]</ref> and BiLSTM <ref type="bibr" target="#b3">[Huang et al., 2015]</ref> to construct our sequence encoder. Given a input sequence, we initially represent its i-th token by concatenating the BERT contextualized embeddings [2020] to obtain the contextualized embeddings x bert i by encoding a target token with one surrounding tokens each side. The character-level embeddings are produced by a BiLSTM module which is the same as Lample et al. <ref type="bibr">[2016]</ref>. Then, the token representations {x i } are feed into another BiLSTM to get the final sequence representation H ? R l?d as:</p><formula xml:id="formula_0">x i = x bert i ? x glove i ? x pos i ? x char i ,<label>(1)</label></formula><formula xml:id="formula_1">? ? H i = LSTM f x i , ? ? H i?1 ; ? f ,<label>(2)</label></formula><formula xml:id="formula_2">? ? H i = LSTM b x i , ? ? H i+1 ; ? b ,<label>(3)</label></formula><formula xml:id="formula_3">H i = ? ? H i ? ? ? H i ,<label>(4)</label></formula><p>where l is the sequence length and d is twice the hidden size of LSTM, ? denotes the concatenation operation and ? f and ? b denote the parameters of the forward and backward LSTM. The ? ? H i and ? ? H i are the hidden states at the position i of the forward and backward LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Entity Set Decoder</head><p>The entity set decoder follows the classical transformer framework and uses self-attention as well as cross-attention mechanisms to transform N entity queries. Different from the sequence-to-sequence method <ref type="bibr" target="#b10">[Strakov? et al., 2019</ref>] to predict entities one by one, we aim to decode all entities of a sequence in parallel due to the inherent disorder of entities. Similar to <ref type="bibr" target="#b3">[Gu et al., 2018]</ref>, we use a non-autoregressive decoder thereby allowing us to obtain N predictions in one pass. Through the self-attention mechanism between entity queries, the decoder is able to capture the dependencies between entities. The decoder can also effectively acquire contextual information via the cross-attention mechanism. Because the decoding manner is non-autoregressive, we do not need to adopt the masking mechanism to prevent information leakage and thus can obtain the complete contextual semantic information. In addition, since the set of entity queries is used as the input for each sequence during the training process, we have an access to the global view of the entire dataset.</p><p>The N entity queries denoted by Q span are transformed into output embeddings by the entity set decoder of M layers. This process mainly involves the multi-head attention</p><formula xml:id="formula_4">(0, 3, PER) (3, 3, ?) (1, 3, ORG) (5, 6, ?) (0, 0, ?) (0, 4, PER) (0, 0, ?) (2, 3, ORG)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bipartite Matching</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preds Golds</head><p>Sequence Encoder </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left Right Class</head><p>Entity queries <ref type="figure">Figure 2</ref>: The architecture of our Sequence-to-Set Network. The representation of each token in sentence "Some 2,500 Republican Guard forces surrendered." is fed into the entity set decoder along with the entity queries. Then the decoder transforms the queries to predicted entities. Finally, we score them by the loss function based on bipartite matching.</p><p>mechanism <ref type="bibr" target="#b10">[Vaswani et al., 2017]</ref>. For simplicity, we denote the attention as the following equation:</p><formula xml:id="formula_5">Attention (Q, K, V) = softmax QK T ? d k V,<label>(5)</label></formula><p>where Q, K, V are the query matrix, key matrix and value matrix respectively, and the 1/ ? d k is the scaling factor. In the self-attention, Q = K = V = Q span . And in the crossattention, Q = Q span while K = V = H. The multi-head attention can be formulated as follows:</p><formula xml:id="formula_6">head i = Attention QW Q i , KW K i , VW V i ,<label>(6)</label></formula><formula xml:id="formula_7">R = Concat (head 1 , . . . , head h ) W O ,<label>(7)</label></formula><p>where W Q i , W K i , W V i and W O are trainable projection parameters, and h is the number of head. We take R computed by the cross-attention module as the input of FFN.</p><p>The FFN module is composed of a 3-layer perceptron with ReLU activation function and a linear projection layer. Through FFN, we denote the final output embeddings as U ? R N ?d . We add an additional label ? to indicate that no entity is recognized because we predict a fixed-size set of N entities, where N is set larger than the actual number of entities in a sequence. Given an entity query u ? R d in U, the classification process can be defined by:</p><formula xml:id="formula_8">p c = MLP c (u),<label>(8)</label></formula><formula xml:id="formula_9">H f use = dup(u, l) ? H,<label>(9)</label></formula><formula xml:id="formula_10">p l = MLP l (H f use ),<label>(10)</label></formula><formula xml:id="formula_11">p r = MLP r (H f use ),<label>(11)</label></formula><p>where p c , p l and p r denote the probability of classification for classes, left boundaries and right boundaries respectively. ? denotes the concatenation operation and MLP denotes the multilayer perceptron with softmax function in the last layer.</p><p>The function dup will duplicate u for l times into shape R l?d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bipartite Matching</head><p>After decoding the N predictions, the main difficulty in training is to score the predicted entities (left, right, class) according to the golden entities. To cope with the problem, we design a loss function based on bipartite matching. Before calculating the training loss, we first need to find a optimal matching between the predicted entity set and the golden entity set. We denote the golden set of entities by y, and the set of N predictions by? = {? i } N i=1 . We also pad y to the size of N with ?. To find the optimal matching we search for a permutation of N elements ? ? O N with the lowest cost:</p><formula xml:id="formula_12">? = arg min ??O N N i L match y i ,? ?(i) ,<label>(12)</label></formula><p>where L match y i ,? ?(i) is a pair matching cost between the golden entity y i and a prediction with index ?(i). We compute this optimal assignment efficiently by using the Hungarian algorithm <ref type="bibr" target="#b6">[Kuhn, 1955]</ref>. Considering the left boundaries, the right boundaries and the classes of entities, each element i of the golden entity set can be seen as a y i = (l i , r i , c i ). We define L match y i ,? ?(i) as:  <ref type="table" target="#tab_1">Train  Dev  Test  Train  Dev  Test  Train  Dev  Test  Train  Test   # sentences  6200  745  812  7194  969 1047 10546  545  4267 16692 1854  # with nested entities  2712  294  388  2691  338  320  2809  182  1223  3522  446  avg sentence</ref>  </p><formula xml:id="formula_13">L match y i ,? ?(i) = ?1 {ci =?} p c ?(i) (c i ) + p l ?(i) (l i ) +p r ?(i) (r i ) .<label>(13)</label></formula><p>After we get the optimal matching?(i), we define the final loss L(y,?) as:</p><formula xml:id="formula_14">L(y,?) = N i=1 ? log p ? ?(i) (c i ) + 1 {ci =?} ? log p l? (i) (l i ) ? log p r ?(i) (r i ) .<label>(14)</label></formula><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>In the experimental setup, we utilize the widely used ACE 2004, ACE 2005, KBP 2017 and GENIA datasets. For ACE 2004 and ACE 2005, we follow the previous work <ref type="bibr" target="#b5">[Katiyar and Cardie, 2018;</ref><ref type="bibr" target="#b7">Lin et al., 2019]</ref> to keep files from bn, nw and wl and divide these files into train, dev and test sets in the ratio of 8:1:1, respectively. For GENIA, we use genia-corpus3.02 as in <ref type="bibr" target="#b5">Katiyar and Cardie [2018]</ref>. For KBP 2017, we use the 2017 English evaluation dataset and the same split strategy in Lin et al. <ref type="bibr">[2019]</ref>. <ref type="table" target="#tab_1">Table 1</ref> shows the statistical results in detail for all the above datasets. Standard precision, recall and F1-measure are employed as evaluation metrics. An entity is considered correct only if both the entity boundary and the entity label are correct. The checkpoint that has the best F1 score in the development set is chosen to evaluate the test set.</p><p>For comparison with <ref type="bibr" target="#b4">Jue et al. [2020]</ref> and Yu et al.</p><p>[2020], we use the cased large version of BERT in most experiments. We also adopt the GloVe [Pennington et al., 2014] 100dimension pre-trained word vectors. The character-level BiL-STM layer is set to 1 and the token-level BiLSTM layer is set to 3. The hidden size of the former is 50, and the hidden size of the latter is half of the BERT hidden size. Specifically, on the GENIA dataset, we replace BERT with <ref type="bibr">BioBERT [Lee et al., 2019]</ref>, GloVE with BioWordvec . The number of entity queries N is set to 60 and the vectors are randomly initialized with the normal distribution N (0.0, 0.02).  The number of our decoder layer M is set to 3. The number of the attention heads is set to 8. The number of the MLP layer is set to 1. We use the AdamW <ref type="bibr" target="#b7">[Loshchilov and Hutter, 2017]</ref> optimizer with a linear warmup-decay learning rate schedule (with peak learning rate of 2e-5 and epoch of 100), a dropout with the rate of 0.1 and a batch size of 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We compare our model with several start-of-the-art methods on <ref type="bibr">ACE 2004</ref><ref type="bibr">, ACE 2005</ref><ref type="bibr">, KBP 2017</ref> and GENIA datasets:</p><p>? Biaffine: Yu et al.</p><p>[2020] utilizes the biaffine model to score pairs of start and end tokens in a sequence.</p><p>? Pyramid: <ref type="bibr" target="#b4">Jue et al. [2020]</ref> designs the normal and inverse pyramidal structures to identify entities through bidirectional interactions.</p><p>? BiFlaG: <ref type="bibr" target="#b9">Luo and Zhao [2020]</ref> proposes a bipartite flatgraph network with two interacting subgraph modules.</p><p>? HIT: Wang et al.</p><p>[2020] designs head-tail detector and token-interaction tagger to express the nested structure.</p><p>? Seq2seq: Strakov? et al.</p><p>[2019] considers the nested NER as a sequence-to-sequence problem. The encoded labels are predicted one by one by the decoder.</p><p>? Second-best: Shibuya and Hovy [2019] searches a span of each extracted entity for nested entities with secondbest sequence decoding.</p><p>? ARN: Lin et al.</p><p>[2019] leverages the head-driven phrase structures to predict nested named entities.</p><p>? Hyper-Graph: <ref type="bibr" target="#b5">Katiyar and Cardie [2018]</ref> makes use of the BILOU tagging scheme to learn the hypergraph representation.</p><p>? KBP17-Best: Ji et al.</p><p>[2017] reports the best previous results for nested NER tasks.</p><p>We don't compare our model with BERT-MRC , because it uses additional external resources to construct the questions, which essentially introduces descriptive information about the categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overall Performances</head><p>The overall performances of the proposed model against baselines for nested NER tasks on <ref type="bibr">ACE 2004</ref><ref type="bibr">, ACE 2005</ref><ref type="bibr">, KBP 2017</ref> and GENIA datasets are shown in The main improvement of the proposed model comes from the fact that we treat the nested NER as a sequence-to-set problem, which is consistent with the inherent disorder of entities. Compared with the sequence-to-sequence model, our model is insensitive to the label order and achieves a better performance.   In addition, in order to choose an appropriate number of entity queries, we design a comparison experiment on ACE 2005 with a maximum number of entities of 27. The evaluation results are shown in <ref type="table" target="#tab_6">Table 3</ref>. We observe that when the number is obviously larger than the ground truth, the performance of the model does decrease. Eventually, the query number N in our experiments is set to 60.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Studies</head><p>To demonstrate the effectiveness of the several modules in our method, we conduct a series of ablation studies on ACE 2005. First, according to our intuition, the more decoder layers we stack, the more expressive the model is. <ref type="table" target="#tab_7">Table 4</ref> verifies this point: when we reduce the number of decoder layer from 3 to 2 and from 2 to 1, the model performance decreases by 0.84% and 0.85%, respectively. We also conduct experiments with 6 layers, but the model performance does not improve obviously. Furthermore, from <ref type="table" target="#tab_7">Table 4</ref> we also observe that regardless of the number of layers, the performance of the model consistently decreases with the absence of the interaction (self attention) between entity queries. When the number of decoder layer is 3, 2 and 1, the results decrease by 1.81%, 2.70% and 2.87% respectively. This is because the decoder can capture the dependencies between entities by using a self-attention mechanism which performs direct interactions between entity queries.</p><p>Likewise, we explore the learning ability of entity queries and the effectiveness of bipartite matching loss. In <ref type="table" target="#tab_9">Table  5</ref>, Freeze Queries means we keep the parameters of the entity queries unchanged, and CE Loss refers to replace bipartite matching loss with cross-entropy loss. We can see that freezing the entity queries results in the performance drops of 0.40% and 0.81% on <ref type="bibr">ACE 2004 and</ref><ref type="bibr">ACE 2005, respectively.</ref> This indicates that the entity queries do learn the patterns   <ref type="bibr" target="#b4">[Jue et al., 2020]</ref> were designed with normal and inverse pyramidal structures to identify entities through bidirectional interactions. <ref type="bibr" target="#b8">Lu and Roth [2015]</ref> were the first to propose a hypergraph-based approach to cope with the nested entity mention detection problem. <ref type="bibr" target="#b11">Wang and Lu [2018]</ref>    formulated the named entity recognition problem as a machine reading comprehension task so that the flat and nested cases can be handled uniformly. Other works <ref type="bibr" target="#b7">[Lin et al., 2019;</ref><ref type="bibr">Fisher and Vlachos, 2019;</ref><ref type="bibr" target="#b10">Strakov? et al., 2019]</ref> also presented various approaches to solve the nested NER problem. We consider the nested named entity recognition as a sequence-to-set task, thus avoiding the error propagation problem of the sequence-based methods <ref type="bibr" target="#b3">[Ju et al., 2018]</ref>. Compared to the span-based methods, we provide a fixed set of entity queries to learn the patterns of the valuable spans. Thus we do not need to search for all possible regions anymore. Besides, we predict the final set of entities in one pass, while the sequence-to-sequence method <ref type="bibr" target="#b10">[Strakov? et al., 2019]</ref> predicts entities one by one. Since the entities are inherently unordered, our model which is insensitive to the label order achieves a better performance. Similarly, DETR <ref type="bibr" target="#b0">[Carion et al., 2020]</ref> treats the object detection as a set prediction task. This end-to-end approach effectively eliminates the need for many manually designed components in the twostage methods such as Faster R-CNN <ref type="bibr" target="#b10">[Ren et al., 2016]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel sequence-to-set network for nested NER. Due to the fact that the entities in a sequence are inherently unordered, we design a non-autoregressive decoder to generate the set of entities in one pass. To score the predicted entities respect to the golden entities, we design a bipartite matching loss which is more suitable for the unordered entity recognition task. Compared with the sequenceto-sequence model, our model is insensitive to the label order and achieves a better performance. Besides, we can capture the dependencies between entities by using a self-attention mechanism which performs direct interactions between entity queries. Experiments show that our model achieves state-ofthe-art on ACE 2004, ACE 2005 and KBP 2017 datasets. In the future, how to represent the dependencies between entities better will be considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples for nested entities from ACE05 corpora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>et al., 2014] embeddings x glove i , part-ofspeech (POS) embeddings x pos i and character-level embeddings x char i together. We follow Yu et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>length 22.50 23.02 23.05 19.21 18.93 17.2 19.62 20.61 19.26 25.35 25.99 ) 45.71 46.69 45.61 38.41 34.75 37.35 28.09 32.20 29.42 17.95 21.78 Statistics of the datasets used in the experiments.</figDesc><table><row><cell># total entities</cell><cell cols="6">22204 2514 3035 24441 3200 2993 31236 1879 12601 50509 5506</cell></row><row><cell># nested entities</cell><cell>10149 1092 1417</cell><cell>9389 1112 1118</cell><cell>8773</cell><cell>605</cell><cell>3707</cell><cell>9064 1199</cell></row><row><cell>nested percentage (%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The overall performances for nested NER tasks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Our proposed model outperforms the state-of-the-art model on ACE 2004, ACE 2005 and KBP 2017 datasets. To be specific, the F1 scores of our model improve by +0.56%, +1.65% and +2.99% over previous SOTA performances on ACE 2004, ACE 2005 and KBP 2017, respectively. Meanwhile, we achieve the comparable result with the SOTA model on GE-NIA dataset. The experimental results demonstrate the ability of our model to recognize named entities in the sequence.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>The performances with different number of entity queries.</figDesc><table><row><cell># Layer Interaction</cell><cell></cell><cell>ACE 2005</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>1</cell><cell cols="3">82.14 82.85 82.49</cell></row><row><cell>1</cell><cell cols="3">86.64 84.12 85.36</cell></row><row><cell>2</cell><cell cols="3">83.57 83.46 83.51</cell></row><row><cell>2</cell><cell cols="3">85.80 86.63 86.21</cell></row><row><cell>3</cell><cell cols="3">85.26 85.23 85.24</cell></row><row><cell>3</cell><cell cols="3">87.48 86.63 87.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies for decoder layer number and interaction between entity queries.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies for the learnable entity queries and bipartite matching loss.Recent nested named entity recognition methods can be categorized as sequence-based<ref type="bibr" target="#b3">[Ju et al., 2018;</ref><ref type="bibr" target="#b4">Jue et al., 2020]</ref>, hypergraph-based<ref type="bibr" target="#b8">[Lu and Roth, 2015;</ref><ref type="bibr" target="#b11">Wang and Lu, 2018;</ref><ref type="bibr" target="#b5">Katiyar and Cardie, 2018]</ref>, and span-based<ref type="bibr" target="#b10">Sohrab and Miwa, 2018;</ref><ref type="bibr" target="#b11">Zheng et al., 2019;</ref><ref type="bibr" target="#b11">Yu et al., 2020]</ref> approaches.<ref type="bibr" target="#b3">Ju et al. [2018]</ref> proposed a layered model to recognize entities from inside to outside sequentially. Pyramid</figDesc><table><row><cell>of the valuable regions. Besides, we can gain improvement</cell></row><row><cell>of 2.67% and 1.11% on ACE 2004 and ACE 2005 respec-</cell></row><row><cell>tively by replacing cross-entropy loss with bipartite matching</cell></row><row><cell>loss, which shows the power of bipartite matching loss on the</cell></row><row><cell>unordered prediction task. Compared with the sequence-to-</cell></row><row><cell>sequence model, we can not only predict all named entities</cell></row><row><cell>in one pass, but are able to utilize the more suitable bipartite</cell></row><row><cell>matching loss for the unordered entity recognition task.</cell></row><row><cell>4 Related Work</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [carion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
	</analytic>
	<monogr>
		<title level="j">Named entity recognition with bidirectional lstm-cnns. TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Workshop on Biomedical Natural Language Processing</title>
		<editor>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova</editor>
		<meeting>the 15th Workshop on Biomedical Natural Language Processing<address><addrLine>Berlin, Germany; Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL 2019. Dozat and Manning, 2016] Timothy Dozat and Christopher D. Manning. Deep biaffine attention for neural dependency parsing. CoRR, abs/1611.01734</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joseph Fisher and Andreas Vlachos. Merge and label: A novel neural network architecture for nested ner</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2019</title>
		<meeting>ACL 2019</meeting>
		<imprint>
			<publisher>Fisher and Vlachos</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5840" to="5850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meizhi Ju, Makoto Miwa, and Sophia Ananiadou. A neural layered model for nested named entity recognition</title>
		<idno type="arXiv">arXiv:1508.01991</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TAC, Gaithersburg</title>
		<meeting>TAC, Gaithersburg<address><addrLine>Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1459" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of NAACL 2018</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pyramid: A layered model for nested named entity recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5918" to="5928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nested named entity recognition revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cardie ; Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018</title>
		<meeting>ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="861" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn ; Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<editor>Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim</editor>
		<imprint>
			<publisher>Chan Ho So, and Jaewoo Kang</publisher>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
	<note>Neural architectures for named entity recognition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequence-to-nuggets: Nested entity mention detection via anchor-region networks</title>
		<idno type="arXiv">arXiv:1906.03783</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>A unified MRC framework for named entity recognition. Fixing weight decay regularization in adam. CoRR, abs/1711.05101</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth ; Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
		<meeting>EMNLP 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL 2019</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bipartite flatgraph network for nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020<address><addrLine>Online; Jeffrey Pennington, Richard</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6408" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning ; Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02250</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<meeting><address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A local detection approach for named entity recognition and mention detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu ; Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07150</idno>
	</analytic>
	<monogr>
		<title level="m">Bernd Bohnet, and Massimo Poesio. Named entity recognition as dependency parsing</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of EMNLP-IJCNLP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
