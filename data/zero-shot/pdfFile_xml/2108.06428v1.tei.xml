<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook Reality Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FrankMocap: A Monocular 3D Whole-Body Pose Estimation System via Regression and Integration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Work done during Yu Rong&apos;s internship at Facebook AI Research</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: We present FrankMocap, a system that estimates 3D poses of face, hands, and body from monocular single images. Our method is fast and capable of performing a live demo with a single RGB webcam, as shown on the left. On the right, example from in-the-wild videos are shown. We show input images (left), 3D hand pose estimation (middle), and the whole-body pose estimation (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Most existing monocular 3D pose estimation approaches only focus on a single body part, neglecting the fact that the essential nuance of human motion is conveyed through a concert of subtle movements of face, hands, and body. In this paper, we present FrankMocap, a fast and accurate whole-body 3D pose estimation system that can produce 3D face, hands, and body simultaneously from in-the-wild monocular images. The idea of FrankMocap is its modular design: We first run 3D pose regression methods for face, hands, and body independently, followed by composing the regression outputs via an integration module. The separate regression modules allow us to take full advantage of their state-of-the-art performances without compromising the original accuracy and reliability in practice. We develop three different integration modules that trade off between latency and accuracy. All of them are capable of providing simple yet effective solutions to unify the separate outputs into seamless whole-body pose estimation results. We quantitatively and qualitatively demonstrate that our modularized system outperforms both the optimization-based and end-to-end methods of estimating whole-body pose. Code and models and demo videos are available at https:// github.com/facebookresearch/frankmocap. * Work done during Yu Rong's internship at Facebook AI Research 1 Note that body indicates torso and limbs excluding finger joints and whole-body indicates all of face, hands and body.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating 3D human pose from single RGB images is one of the core technologies to build a computational model to understand human behavioral cues. It can faciliate numerous applications including assistive technology <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53]</ref>, sign language understanding <ref type="bibr" target="#b9">[10]</ref>, AR/VR <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b68">68]</ref>, and social signal understanding <ref type="bibr" target="#b50">[51]</ref>. Importantly, the essential nuance of human behaviors is conveyed through a concert of subtlest movements of face, hands, and body. Thus, it is necessary to estimate whole-body motions to capture the authentic signals.</p><p>Estimating whole-body 3D poses including a face and hands, however, remains challenging. One of the major difficulties comes from the fact that the scale of faces and hands is much smaller than that of torsos and limbs. Furthermore, hands in motion are susceptible to artifacts caused by abrupt viewpoint changes, self-occlusions, and motion blur. These factors largely raise the difficulties of creating large-scale datasets with whole-body 3D poses, even in a controlled environment <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56]</ref>, not to mention capture such data in the wild. The lack of whole-body 3D data is a major obstacle preventing the construction of a unified system to estimate whole-body 3D poses simultaneously. Therefore, most existing methods focus on one of the major body parts individually: single image-based estimation of 3D body pose (i.e., torso and limbs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b69">69,</ref><ref type="bibr" target="#b70">70]</ref>, 3D hand pose <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b78">78]</ref> or 3D face <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b65">65]</ref>.</p><p>In this paper, we present FrankMocap, a modular system for estimating whole-body 3D poses in a unified output in SMPL-X form <ref type="bibr" target="#b55">[56]</ref>, as visualized in <ref type="figure">Fig. 1</ref>. Our system is built upon the insight that training a single model to jointly estimate whole parts is intrinsically limited by the lack of accurate and diverse whole-body motion data. Instead, we design a modularized system to first run 3D pose regression methods for face, hands, and body independently. The three regression outputs are then composed via an integration module. The separate regression modules allow us to take full advantage of their state-of-the-art performances without compromising the original accuracy and reliability in practice. Our integration module provides a simple yet effective solution to unify them into seamless whole-body pose estimation outputs. Unlike previous whole-body pose estimation methods that require computationally heavy optimization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b70">70]</ref>, our method also allows us to run at an interactive frame rate. We quantitatively and qualitatively demonstrate that our modular framework outperforms existing optimization-based methods <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b70">70]</ref> and end-to-end models trained to produce whole-body outputs jointly <ref type="bibr" target="#b17">[18]</ref>.</p><p>In conclusion, we make three major contributions. First, towards a practical system for whole-body pose estimation, we present the idea of regression-and-integration to take full advantage of existing single-part pose datasets and circumvent the hurdle of lacking whole-body pose datasets. Especially, our hand module is comparable with SOTA hand-only methods on in-the-wild scenarios. Second, we present three effective strategies to integrate outputs from separate single-part pose estimation modules, namely the fastest "copy-paste" approach, a full-optimization approach, and an intermediate approach via a simple integration network. Third, we quantitatively and qualitatively demonstrate that our modular system outperforms the alternative approaches based on optimization-based methods or end-to-end methods training all parts jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Parametric Human Body Models. 3D parametric human models are widely used as a strong prior to estimate 3D pose and shape of humans. A main idea is to model the deformation of 3D human (including face, hands and body) via low dimension parameters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b57">[57]</ref><ref type="bibr" target="#b58">[58]</ref>. SCAPE <ref type="bibr" target="#b0">[1]</ref> is a pioneering work that accounts for shape variations and pose deformations of human body. Loper et al. introduce SMPL <ref type="bibr" target="#b45">[46]</ref> that learns local pose-dependent blendshape on top of linear blend skinning for holistic mesh deformation as well as shape variations. Similarly, there exists a hand deformation model called MANO <ref type="bibr" target="#b58">[58]</ref> and several face models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43]</ref>. Recent approaches also introduce whole-body models such as Adam <ref type="bibr" target="#b31">[32]</ref>, SMPL-X <ref type="bibr" target="#b55">[56]</ref>, and GHUM &amp; GHUML <ref type="bibr" target="#b71">[71]</ref> that are capable of expressing face, body, and hands in a unified space. Single Image 3D Body, Hand and Face Estimation. Recent monocular 3D body motion capture approaches adopt parametric model such as the SMPL <ref type="bibr" target="#b45">[46]</ref> or Adam model <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b70">70]</ref> for 3D body representation 1 . A pioneering work <ref type="bibr" target="#b4">[5]</ref> uses an optimization framework to fit the 3D body model to 2D observations. More recent methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b72">72]</ref> leverage a deep learning framework to regress parameters of the body models from RGB images. Non-parametric methods have been also introduced by directly regressing the model vertices <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref> or corresponding UV maps <ref type="bibr" target="#b73">[73]</ref>. There are also approaches that use a hybrid framework by using a deep learning framework to produce intermediate representations such as 2D heatmaps and then fitting the skeletal models on these outputs to reconstruct joint angles <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b70">70]</ref>. Due to the lack of in-the-wild training data with 3D annotations, most of these models are trained with mixed datasets, including indoor datasets such as Hu-man3.6M <ref type="bibr" target="#b26">[27]</ref> and in-the-wild datasets such as COCO <ref type="bibr" target="#b44">[45]</ref>. While most methods are based on single images as input, recent approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref> predict 3D motions from a video.</p><p>Monocular 3D hand pose estimation approaches share a similar pipeline as the body counterparts, based on the parametric 3D hand models, such as MANO <ref type="bibr" target="#b58">[58]</ref>. Deep neural networks are leveraged to either predict model parameters <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b77">77]</ref> or directly regress hand mesh vertices <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref>. Similarly, single image 3D face prediction methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b65">65]</ref> also share similar ideas by leveraging deep neural networks to regress the face landmarks or parameters of face models, such as 3DMM <ref type="bibr" target="#b3">[4]</ref> or FLAME <ref type="bibr" target="#b42">[43]</ref>. We refer the readers to recent surveys on 3D face reconstruction <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b80">80]</ref> for more details. Joint 3D Estimation of Body, Hands and Face. There are a few methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b76">76]</ref> aiming to capture wholebody 3D motions. SMPLify-X optimizes the parameter of the SMPL-X model <ref type="bibr" target="#b55">[56]</ref> to fit it to 2D keypoints with additional constraints, including body pose priors and collision penalizer. Monocular Total Capture (MTC) <ref type="bibr" target="#b70">[70]</ref> is based on the Adam model <ref type="bibr" target="#b31">[32]</ref>, and adopts deep neural networks to get 2.5D predictions first. Then the parameters of Adam are obtained through optimization. Both of these methods rely on optimization procedures with relatively slow computational time (from 10 seconds to a few minutes). Zhou et al. <ref type="bibr" target="#b76">[76]</ref> use SMPL-H <ref type="bibr" target="#b58">[58]</ref> to represent body and hands and 3DMM face model <ref type="bibr" target="#b64">[64]</ref> to represent 3D faces. They first predict 3D body and hand poses and then separately predict the parameters of 3DMM model, including shape, expression, albedo, and illumination. The predicted full-body motion is not represented in a unified format and this paper does not present generalization ability to in-the-wild scenar-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Body Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integration Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Whole Body Pose</head><p>Hand Module</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head><p>Part Poses <ref type="figure">Figure 2</ref>: Overview of FrankMocap, the proposed whole-body 3D pose estimation system. Given a single RGB image input, we first apply our part modules separately to estimate 3D hands, body, and face. Our integration module is then adopted to combine the outputs from each module into a unified whole-body output.</p><p>ios. ExPose <ref type="bibr" target="#b17">[18]</ref>, avoids using the optimization procedure by presenting a neural network to simultaneously predict the body, hand, and face parameters of SMPL-X. In particular, ExPose curates a pseudo-ground truth dataset by fitting SMPL-X model on in-the-wild images, followed by manual quality checking by annotators, and use it to train their model to jointly produces output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given a single image as input, FrankMocap firstly estimates the 3D poses of face, hands (both left and right), and body (the torso and limb parts), in the form of SMPL-X model <ref type="bibr" target="#b55">[56]</ref>. Each part's output is produced by a separate 3D pose regressor trained on public datasets in each sub-field. Their outputs are combined through our integration module, to produce a seamless and unified whole-body pose estimation output. <ref type="figure">Fig. 2</ref> illustrates an overview of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Whole-Body Parameterization</head><p>SMPL-X Model. We formulate the SMPL-X model as:</p><formula xml:id="formula_0">V w = W (? w , ? w , ? w , ? f )<label>(1)</label></formula><p>where W is parameterized by global orientation of the whole-body ? w ? R 3 , facial expressions parameters ? f ? R 10 , whole-body pose parameters ? w ? R (21+15+15+1)?3 accounting for pose-dependent deformation, and shape parameters ? w ? R 10 accounting for cross-identity shape variations of the face, hands, and body. We divide ? w for each of the body, hands, and faces, namely body pose parameters ? b w ? R 21?3 , left hand pose parameters ? lh w ? R 15?3 , right hand pose parameters ? rh w ? R 15?3 , and face pose parameters 2 ? f w ? R 1?3 . In this way,</p><formula xml:id="formula_1">? w = {? b w , ? lh w , ? rh w , ? f w }.</formula><p>All pose parameters are defined in the axis-angle representation, which stores the relative rotation to the parent joints defined in the kinematics map. As output, the SMPL-X model produces a mesh structure with 10,745 vertices, V w ? R 10475?3 . The 3D joint locations of the whole-body can be obtained by applying a joint regression function R from the posed vertices as</p><formula xml:id="formula_2">J 3D w = R w (V w ), where J 3D w ? R (22+20+20+3)?3</formula><p>. Stand-Alone Hand Model. Our hand model is defined by taking the hand parts of SMPL-X:</p><formula xml:id="formula_3">V h = H(? h , ? h , ? h ),<label>(2)</label></formula><p>where ? h ? R 3?15 is hand pose parameters and ? h is the shape parameters for the hand model. Since our hand model is taken from SMPL-X, ? h shares the same shape space as ? w . For brevity, we use ? h , instead of ? rh or ? lh , to denote the hand pose parameters of either part in our description. ? h ? R 3 represents the global orientation of the hand meshes. Our hand model H produces the hand mesh structure with 778 vertices, V h ? R 778?3 , where the hand vertices are selected from the hand area of the original SMPL-X vertices. Given hand vertices V h , 3D hand joints can be regressed as <ref type="bibr" target="#b14">15</ref> finger joints (3 joints per finger), and 5 finger tips. Hand visualization and skeleton hierarchy are listed in Appendix A1.1. The major advantage of our hand representation is that the components of this 3D hand model, including pose parameters, vertices, and 3D joints, are directly compatible with the whole-body parameterization. This enables us to efficiently integrate outputs from the body module and the hand module. </p><formula xml:id="formula_4">J 3D h = R h (V h ), where J 3D h ? R 21?3 contains a wrist,</formula><formula xml:id="formula_5">model, [? h , ? h , ? h , c h ].</formula><p>The estimated parameters are then used to calculate the mesh of SMPL-X hand part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Hand Estimation Module</head><p>We present a monocular 3D hand pose estimation module, denoted by M H , to the parameters of the hand model H. In particular, our hand module is inspired by the recently proposed monocular body pose estimation approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. We follow similar model architecture, parameterization, and training strategies as the pioneering body methods. In particular, we introduce blur augmentation to make the hand module more robust to blurry hands that are frequently observed in full body motion capture scenarios. Network Architecture. Our hand module M H is built upon an end-to-end deep neural network architecture that can predict hand model parameters defined in Eq. (2) from given input images. This process is defined as:</p><formula xml:id="formula_6">[? h , ? h , ? h , c h ] = M H (I H ),<label>(3)</label></formula><p>where I H is an input RGB image cropped around the hand region. c h = (t h , s h ) is a set of weak-perspective camera parameters to project a posed 3D hand model to the input image. t h ? R 2 is for 2D translation on the image plane, and s h ? R is a scale factor. Given camera parameters, the i-th 3D hand joint, J 3D h,i can be projected as:</p><formula xml:id="formula_7">J 2D h,i = s h ?(J 3D h,i ) + t h ,<label>(4)</label></formula><p>where ? is an orthographic projection. Following the practice of HMR <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref>, our hand module M H adopts an encoder-decoder structure, where the encoder (ResNet-50 <ref type="bibr" target="#b25">[26]</ref>) outputs features from an input image, and the decoder regresses the hand model parameters from the features. See <ref type="figure" target="#fig_0">Fig. 3</ref> for the overview of our hand module. In particular, our hand module is trained with the assumption that the inputs are right hands. The images and annotations for the left hand are used after vertical flipping. During inference, the left-hand images are first flipped and processed as if they were a right hand. Then the outputs are flipped back to the original left-hand space. Note that the shape parameter ? h is originally defined for whole-body model ? w , but we only consider the deformation for the hand vertices defined in Eq. <ref type="formula" target="#formula_3">(2)</ref>, ignoring the body part. Training Method. We consider three different types of annotations: (1) 3D pose annotations (in axis-angle representation), (2) 3D keypoint (joint) annotations, and (3) 2D keypoint annotations. The losses for each of the annotation, namely L ? , L 3D and L 2D , are defined as follows:</p><formula xml:id="formula_8">L ? = ? h ?? h 2 2 , L 3D = J 3D h ?J 3D h 2 2 , and L 2D = J 2D h ?J 2D h 1 , where? h ,J 3D h andJ 2D h</formula><p>are the ground-truth axis-angle pose parameters, 3D keypoints, and 2D keypoints, respectively. In particular, the 2D keypoint loss are leveraged to estimate the camera parameters and facilitate the hand module generalize to in-the-wild images with only 2D keypoints annotation. We do not use the shape parameters provided by the 3D hand datasets such as FreiHAND <ref type="bibr" target="#b79">[79]</ref>, since these are defined for the MANO model <ref type="bibr" target="#b58">[58]</ref> and not compatible with our hand model from SMPL-X. Instead, an additional shape parameter regularization loss L reg = ? h 2 2 is applied. The overall loss L used to train our hand module is defined as follows:</p><formula xml:id="formula_9">L = ? 1 L ? + ? 2 L 3D + ? 3 L 2D + ? 4 L reg .<label>(5)</label></formula><p>In experiments, the balanced weights are set as ? 1 = 10, ? 2 = 100, ? 3 = 10 and ? 4 = 0.1. Other hand module training details, including data preprocessing, dataset information, and implementation details, are included in the Appendix A1.1. Motion Blur Augmentation. Performing data augmentations during training is a common practice to enable a model with better generalizability. We firstly apply common data augmentation strategies, including random scale, random translation, color jittering, and random rotation. Importantly, we recognize that in-the-wild videos are often accompanied by severe motion blur. To achieve robustness to motion blur, we additionally apply motion blur augmentation to the images. We use the methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> to generate blur kernels and then use 2D filtering to add blurriness to images. Examples of applying motion blur augmentation are shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. Our experiments clearly show that our motion blur augmentation can effectively insist our hand module generalize to in-the-wild scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D Body Estimation Module</head><p>Our body module M B produces the torso and limb parameters defined in Eq. (1) from a single image:</p><formula xml:id="formula_10">[? b , ? b , ? b , c b ] = M B (I b ),<label>(6)</label></formula><p>where I b is an input image cropped around a target single person's whole-body. Similar to Eq. (1), ? b ? R 3 is the global body orientation, ? b ? R 21?3 is the body pose parameters (without any finger joints), and ? b ? R 10 is the shape parameter. ? b shares the same parameterization space as ? w , as defined in Eq. (1). Similar to Eq. (3), we use weak perspective camera parameters</p><formula xml:id="formula_11">c b = (t b , s b ).</formula><p>We leverage the publicly available body pose estimation model (SPIN <ref type="bibr" target="#b36">[37]</ref>) trained by the EFT dataset <ref type="bibr" target="#b29">[30]</ref>.</p><p>Since the original SPIN model is built with SMPL, we finetune the network by replacing SMPL with SMPL-X. For the fine-tuning process, we use the existing indoor 3D pose dataset Human3.6M <ref type="bibr" target="#b26">[27]</ref> and outdoor pseudo-GT dataset by EFT <ref type="bibr" target="#b29">[30]</ref> that provides SMPL annotations. Since the annotation (SMPL) and our target model (SMPL-X) are not exactly compatible, we only use the pose parameters and 2D keypoint from the annotations during fine-tuning process, ignoring the shape parameters 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">3D Face Estimation Module</head><p>We use the off-the-shelf public 3D face estimation model RingNet <ref type="bibr" target="#b61">[61]</ref> to estimate facial expressions ? f and face poses ? f as follows:</p><formula xml:id="formula_12">[? f , ? f ] = M F (I f ).<label>(7)</label></formula><p>Since RingNet is based on FLAME <ref type="bibr" target="#b42">[43]</ref>, its predictions are compatible with the face part of SMPL-X. As an adjustment, we only keep the first 10 expression parameters from the original 50 dimensional expression parameters? f predicted by RingNet, to make it compatible with the expression space defined in SMPL-X. It is worth noting that the FLAME and SMPL-X share the same PCA-based expression space, while SMPL-X only uses the 10 foremost principal components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Whole-Body Integration Module</head><p>Our integration module combines the outputs from the face, hands, and body modules into a unified representation of the SMPL-X <ref type="bibr" target="#b55">[56]</ref>. While the output of the face module can be easily applied to the face part of the SMPL-X model by copying the expression parameters and jaw poses, additional procedures are required to integrate the outputs of the hand and body modules because different wrist poses are estimated from these two modules. For this objective, we present three strategies: (1) by simple copy-paste composition for fastest processing, (2) by an optimization framework, (3) by an integration network to approximate the optimization via a simple neural network. By Copy-Paste. Our hand and body modules' outputs can be efficiently combined, since they are both defined in SMPL-X parameterization. As the simplest strategy, we transfer the corresponding joint angle parameters from the outputs of each module into the whole-body model. The wrist pose parameters require additional processing because we obtain two different outputs from the body module and the hand module (the wrist pose of hand module is represented by the global hand orientation ? h ). Let us denote the pose parameters for the wrist joint as ? wrist , then</p><formula xml:id="formula_13">? b = ? b ? {? rwrist b , ? lwrist b</formula><p>}, where? b represents all body pose parameters except wrists. We use the similar notations for the whole-body pose parameters,</p><formula xml:id="formula_14">? b w =? w ? {? rwrist w , ? lwrist w }.</formula><p>Then, whole-body integration by copy-paste can be performed as:</p><formula xml:id="formula_15">? w , ? w , c w ,? b w , ? lh w , ? rh w = ? b , ? b , c b ,? b , ? lh , ? rh , ? lwrist w , ? rwrist w = (? l (? b , ? lh ) , ? r (? b , ? rh )) ,<label>(8)</label></formula><p>where ? {l,r} are the functions to convert the global wrist orientation ? h obtained from the hand module to the local wrist pose parameters w.r.t. its parent joint in the SMPL-X skeleton hierarchy. This can be implemented by comparing ? h with the global orientation of the current wrist pose from ? b that can be computed by following the forward kinematics of the body skeleton hierarchy. This strategy requires almost no extra computation, making our separate modules contribute a common whole-body model simultaneously. We found this simple integration produces convincing results, especially for the scenarios with computational bottlenecks as in our live demo. By Optimization. As an alternative integration method for better accuracy, we build an optimization framework to fit the whole-body model parameters given the outputs from body and hand modules. This strategy is particularly helpful to reduce the artifact around the wrist parts over the copy-paste strategy. It can also take advantage from the 2D keypoint estimation output <ref type="bibr" target="#b12">[13]</ref> for better 2D localization quality. Our optimization framework finds the wholebody model parameters that minimize the following objective cost function:</p><formula xml:id="formula_16">F([? w , ? w , ? w , c w ]) = F 2d + F mesh + F pri ,<label>(9)</label></formula><p>where F 2d is the 2D re-projection cost term between the 2D keypoint estimation <ref type="bibr" target="#b12">[13]</ref> and the projection of 3D joints (hand, body and faces). F mesh is the 3D distance loss between the hand mesh of whole-body model and the mesh vertices of the hand module's output localized on the body's wrist joints, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. The prior term F pri is needed to keep the 3D pose and shape parameters in plausible space. We use the EFT introduced in <ref type="bibr" target="#b29">[30]</ref> for the similar goal, by applying neural network finetuning to replace explicit 3D pose prior terms used by previous works <ref type="bibr" target="#b55">[56]</ref>. Therefore, the prior term only accounts for the shape regularization F pri = ? w  We use a multi-stage approach in which optimization is firstly performed without F mesh . Then we optimize all terms in the second stage. During the second stage, we add an additional constraint F 3D to maintain the 3D joints within the final locations of the first stage. Our experiments show that this term can produce reliable results in practice. See <ref type="figure" target="#fig_3">Fig. 5</ref> for the example of our optimization. By Wrist Integration Network. The result of the hand module shows precise 2D localization quality as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. But we lose its precision by applying copy-paste integration strategy since the wrist joint location is determined by the body module, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref> (second last column). Although the optimization method can achieve better precision and resolve this problem, it suffers from relatively slow computation speed due to the requirement of iterative gradient computations. As another alternative strategy to achieve both precision and fast-runtime performance, we introduce an integration network to adjust the arm pose of the whole-body model toward the given 2D location without gradient computation. Specifically, we develop a small neural network to achieve this:</p><formula xml:id="formula_17">? arm w = H(? arm w , d),<label>(10)</label></formula><p>where d ? R 2 is a 2D directional vector that the arm needs to follow in the image space. It is obtained from the wrist locations calculated from the body module and those from the hand module, followed by a normalization with the length of the arms. ? arm w is the pose parameters of the elbow and shoulder joints after the copy-paste integration. Conceptually, H predicts an approximation of optimization gradient to adjust the arm parameters, similar to the recent work of Song et al. <ref type="bibr" target="#b63">[63]</ref>. We use a MLP with six layers to implement H. To train H, we generate a synthetic dataset by capturing our own a range of motion videos to cover diverse arm pose variations and by simulating arbitrary d directions. More details of wrist integration are described in Appendix A2.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first demonstrate that our hand module achieves high performance competing to the previous state-of-the-art 3D hand methods, along with ablation studies to examine the designs of our method. Then, we demonstrate that our whole-body pose estimation method outperforms previous approaches on a public benchmark. See Appendix A1 for the details of implementation and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hand Module Evaluation</head><p>Comparison with State-of-the-art Methods. We compare our hand module with the previous state-of-the-art hand approaches on three public hand benchmarks, namely STB <ref type="bibr" target="#b74">[74]</ref>, RHD <ref type="bibr" target="#b78">[78]</ref> and MPII+NZSL <ref type="bibr" target="#b62">[62]</ref>. The results are listed in Tab. 1 and <ref type="figure" target="#fig_4">Fig. 6</ref>. For each validation dataset, we  <ref type="bibr" target="#b74">[74]</ref> and RHD <ref type="bibr" target="#b78">[78]</ref>, we use 3D AUC and the threshold ranges from 20 mm to 50 mm. For MPII+NZSL <ref type="bibr" target="#b62">[62]</ref>, we use 2D AUC and the threshold ranges from 0 px to 30 px. For fair comparison, all the methods take a single RGB image as input. As shown in the results, our hand module is comparable with the previous state-of-the-art hand-only methods based on MANO <ref type="bibr" target="#b58">[58]</ref>, though our hand model, taken from the whole-body model, has restricted shape variations <ref type="bibr" target="#b3">4</ref> . Ablation Study. We examine the data augmentation strategies used in training hand module. The results are listed in Tab. 2 and <ref type="figure" target="#fig_5">Fig. 7</ref>. The results in Tab. 2 demonstrate that applying data augmentation leads to better results. The qualitative results in <ref type="figure" target="#fig_5">Fig. 7</ref> further show that adopting data augmentation helps our model generalize better on challenging scenarios, including blur, pose variations, and occlusion. In <ref type="figure" target="#fig_5">Fig. 7</ref>, "No Augment" refers to the model trained without any data augmentation, and "No Blur" refers to the model trained with all data augmentation strategies except the motion blur augmentation. "Full" refers to the model trained with all data augmentation strategies. We also do an ablation study on mixture usage of datasets, demonstrating that using diverse datasets improves the performances. See Appendix A1.3 for details on this part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Body Module Evaluation</head><p>We compare our body module with the previous SOTA body-only methods on the test set of 3DPW <ref type="bibr" target="#b67">[67]</ref>. Following the previous approaches, we use Mean Per Joint Position Errors (MPJPE) and PA-MPJPE (after Procrustes analysis <ref type="bibr" target="#b21">[22]</ref>). The results are listed in Tab. 3. Our body module is comparable to the previous state-of-the-arts, including ExPose <ref type="bibr" target="#b17">[18]</ref>. Our body model is a fine-tuned version of SPIN <ref type="bibr" target="#b36">[37]</ref> and EFT <ref type="bibr" target="#b29">[30]</ref>. The slightly degraded performance might be due to the reason that we only use fewer datasets, COCO and Human3.6M datasets. We only compare with HMR [33]-based methods. We believe that the experimental results shown in Tab. 3 can demonsrate the feasibility of extending previous SMPL-based methods to SMPL-X based body module. Thanks to our modularized design, we can also easily leverage the most recent 3D body  poses methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> to build our body module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Integration Module Evaluation</head><p>We quantitatively compare our methods with previous SMPL-X based whole-body methods (SMPLify-X <ref type="bibr" target="#b55">[56]</ref> and ExPose <ref type="bibr" target="#b17">[18]</ref>) on EHF dataset <ref type="bibr" target="#b55">[56]</ref>. The evaluation metrics are vertex-to-vertex distance (V2V) and Procrustes Analysis vertex-to-vertex distance (PA-V2V). The results are listed in Tab. 4. We run the officially released codes of SMPLify-X and ExPose to obtain the evaluation results. FM-CP, FM-WI, and FM-OPT indicate FrankMocap using the copy-paste, wrist integration network, and optimizationbased as the integration module, respectively. The results demonstrate that all our methods significantly outperform the previous methods in terms of whole-body estimation, body estimation, and hand estimation. Our face estimation is comparable with previous methods, where we only use an off-the-shelf 3D face model <ref type="bibr" target="#b61">[61]</ref>. Comparing across different versions of FrankMocap reveal that the model with optimization achieves the best performance in general. Applying our wrist integration network can also lead to better performance than copy-paste version. We also provide qualitative comparisons between different integration modules in <ref type="figure" target="#fig_6">Fig. 8</ref>. It is revealed that using the wrist integration network can bring better hand location while adopting optimization can further improve hand localization while updating body shape and other poses.</p><p>We also qualitatively compare our method with previ- <ref type="figure">Figure 9</ref>: Qualitative comparison over the previous whole-body pose estimation methods. We compare our method adopting the wrist integration module (FM-WI) with MTC <ref type="bibr" target="#b70">[70]</ref>, SMPLify-X <ref type="bibr" target="#b55">[56]</ref> and ExPose <ref type="bibr" target="#b17">[18]</ref>.</p><p>ous whole-body motion capture approaches, MTC <ref type="bibr" target="#b70">[70]</ref>, SMPLify-X <ref type="bibr" target="#b55">[56]</ref>, and ExPose <ref type="bibr" target="#b17">[18]</ref>, with our output with wrist integration network. Results shown in <ref type="figure">Fig. 9</ref> and the demo video demonstrate that our method outperforms previous approaches, with fast and accurate performance in challenging in-the-wild scenes. Qualitative comparison with Zhou et al. <ref type="bibr" target="#b76">[76]</ref> is included in the Appendix A5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present FrankMocap, a monocular 3D whole-body motion capture system built upon the motivation to take full advantages of state-of-the-art part estimation modules, followed by effective integration algorithms. We develop three integration modules, namely copy-paste, wrist integration network, and an optimization-based method. The copypaste and wrist integration modules are suitable for timesensitive applications, while the optimization-based module is suitable for offline applications requiring better precision. Our system surpasses previous state-of-the-art 3D wholebody estimation methods on both public benchmarks and in-the-wild scenarios. Notably, our approach can directly leverage future algorithms by replacing each module, which is a main advantage of our modular design.</p><p>In this section, we provide more details of our standalone hand module described in Sec. 3.2. We first describe the hand mesh and hierarchy of the hand skeleton. Then we discuss how to preprocess data from different datasets. Then we discuss the implementation details of our hand module. In the end, we discuss the hand datasets used in our paper and provide experiment results to demonstrate the effectiveness of adopting different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.1. Hand Module Details</head><p>Hand Mesh. The visualization of cropped hand mesh and hierarchy of the hand skeleton is depicted in <ref type="figure">Fig. 10</ref>. Data preprocessing. Following the practice of 3D body pose estimation methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b59">59]</ref>, we include as many publicly available datasets as possible towards inthe-wild generalization ability. To handle the discrepancy of annotations across different datasets, we perform several pre-processing steps to make them consistent and compatible with our hand model. The pre-processing includes:</p><p>(1) Rescaling all 3D keypoint annotations to be compatible with our hand model, by using the middle finger's knuckle length as a reference. (2) Reordering the 3D keypoints to be the same as our hand model's skeleton hierarchy. Hand Module Implementation Details. Input images of the hand module are center-cropped surrounding the hands, where the bounding boxes for cropping are given by 2D hand keypoints. We use ground-truth 2D keypoints during training and predicted keypoints from OpenPose <ref type="bibr" target="#b12">[13]</ref> during testing. The cropped images are further padded and resized to 224 ? 224. During training, we apply data augmentations to each of the training images via random scaling, translation, rotation, color jittering, and synthetic motion blur. The hand module architecture is based on ResNet-50 <ref type="bibr" target="#b25">[26]</ref> with two additional fully connected layers to map the output features of ResNet to vectors with 61 dimension, which is composed of camera parameters c h (3 dimensions), hand global rotation ? h (3 dimensions), hand pose parameters ? h (45 dimensions) and shape parameters ? h (10 dimensions). The hand module is implemented with PyTorch <ref type="bibr" target="#b54">[55]</ref>. The Adam optimizer <ref type="bibr" target="#b34">[35]</ref> with learning rate 1e?4 is used to train the model. The hand module is trained until converge, which takes about 100 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2. Hand Datasets</head><p>To increase our hand module's generalization ability, we adopt as many public datasets as possible. The summary of different datasets is listed in Tab. 5. The details of these datasets are listed below. FreiHAND. FreiHAND <ref type="bibr" target="#b79">[79]</ref> is a dataset with ground truth 3D hand joints and MANO parameters for real human hand   <ref type="bibr" target="#b79">[79]</ref>, HO-3D <ref type="bibr" target="#b24">[25]</ref>, MTC <ref type="bibr" target="#b70">[70]</ref>, STB <ref type="bibr" target="#b74">[74]</ref> and RHD <ref type="bibr" target="#b78">[78]</ref>. We use MPII+NZSL <ref type="bibr" target="#b62">[62]</ref> for validation only.</p><p>Information ? # of Samples Pose Angles 3D Joints 2D Joints Dataset ? FreiHAND <ref type="bibr" target="#b79">[79]</ref> 60K HO-3D <ref type="bibr" target="#b24">[25]</ref> 60K MTC <ref type="bibr" target="#b70">[70]</ref> 200K STB <ref type="bibr" target="#b74">[74]</ref> 20K RHD <ref type="bibr" target="#b78">[78]</ref> 50K MPII+NZSL <ref type="bibr" target="#b62">[62]</ref> 10K</p><p>images. The 3D annotations are obtained by a multi-camera system and a semi-automated approach. The obtained data is further augmented with synthetic backgrounds. In our experiments, we randomly select 80% of samples from the original training set as training data and use the remaining 20% of samples for validation. HO-3D. HO-3D dataset <ref type="bibr" target="#b24">[25]</ref> is a dataset aiming to study the interaction between hands and objects. The dataset has 3D joints and MANO pose parameters for hands. It also has 3D bounding boxes for objects the hands interact with. In this paper, we only use 3D annotations of hands. The training set is composed of different sequences, each of which records one type of hand-object interaction. Following the similar practice in processing FreiHAND, we randomly choose 80% of sequences from the original training set as training data and use the remaining 20% of sequences for validation. MTC. Monocular Total Capture <ref type="bibr" target="#b70">[70]</ref> is a dataset captured by Panoptic Studio <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31]</ref> in a multi-view setup with 30 HD cameras. It has 3D hand joints annotations for both body and hands. The sequences are mainly the range of motion data of multiple subjects. To polish the dataset, we filter out erroneous samples where hands are not visible or too small. STB. Stereo Hand Pose Tracking Benchmark <ref type="bibr" target="#b74">[74]</ref> is com- <ref type="figure">Figure 11</ref>: Ablation study on training datasets. We show qualitative ablation study on using different datasets in training our hand model. "Subset-01" means using the combination of datasets FreiHADN <ref type="bibr" target="#b79">[79]</ref> and HO-3D <ref type="bibr" target="#b24">[25]</ref>. "Subset-02" means using the combination of datasets: STB <ref type="bibr" target="#b74">[74]</ref>, RHD <ref type="bibr" target="#b78">[78]</ref> and MTC <ref type="bibr" target="#b70">[70]</ref>. "Full set" means using all the above datasets. The images are selected from COCO dataset <ref type="bibr" target="#b44">[45]</ref>.  posed of 15,000 training samples and 3,000 testing samples.</p><p>During our experiments, we only use the RGB images and the paired 3D joint annotations. We use the training set of STB to train our model and compare our models with other state-of-the-art methods on the validation set. To unify the definition of joints, following the practice of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref>, we move the root joint from the palm center to the wrist. RHD. Rendered Hand Dataset <ref type="bibr" target="#b78">[78]</ref> is a synthetic dataset that has 2D and 3D hand joint annotations. It is composed of 41,258 training samples and 2,728 testing samples. We train our models on the training set and compare our models with other state-of-the-art methods on the testing set. MPII+NZSL. MPII+NZSL dataset <ref type="bibr" target="#b62">[62]</ref> is composed of inthe-wild images with manually annotated 2D hand joints. It includes challenging images with occlusion, blur, and low resolution. To show our models' generalization ability, we do not incorporate MPII+NZSL into training set. We only use it for validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.3. Ablation Study on Hand Datasets.</head><p>We examine the influence of using different datasets in this subsection. The results are listed in Tab. 6 and <ref type="figure">Fig. 11</ref>. As expected, the results in Tab. 6 show that using more datasets will lead to better performance. We also show examples of qualitative comparison in <ref type="figure">Fig. 11</ref>. Similar to the conclusion from the quantitative study, the qualitative results show that incorporating more datasets can increase the models' generalization ability and generate more precise results for in-the-wild images. In the figure, "Subset-01" means using the combination of datasets FreiHAND <ref type="bibr" target="#b79">[79]</ref> and HO-3D <ref type="bibr" target="#b24">[25]</ref>. "Subset-02" means using the combination of datasets: STB <ref type="bibr" target="#b74">[74]</ref>, RHD <ref type="bibr" target="#b78">[78]</ref> and MTC <ref type="bibr" target="#b70">[70]</ref>. "Full set" means using all the datasets.</p><p>A2. Wrist Integration Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1. Framework Details.</head><p>The framework of wrist integration network is depicted in <ref type="figure">Fig. 12</ref>. Given hand prediction from hand module and whole-body prediction from copy-paste integration module, the role of wrist integration network is to adjust the arm poses to move hands to the locations determined by the hand module. The input of the wrist integration network is composed of two parts. The first part is the 2D directional vector d that the arm needs to follow in the image space. It is obtained from the wrist locations calculated from the body module and those from the hand module, followed by a normalization with the length of arm. The second part ? arm w is the pose parameters of the elbow and shoulder joints the copy-paste integration. The original shoulder poses ? s w and elbow poses ? e w are all local rotations relative to their parent defined in the kinematics of body skeleton. To ease the training of the wrist integration network, we transform the shoulder poses to the global orientation ? s w , so that the wrist integration network does not need to consider other body parts' poses. The elbow poses are kept as the local rotation. In this way, the input arm poses ? arm w is composed of global orientation of shoulder ? s w and local rotations of elbow ? e w .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Origin Prediction</head><p>Wrist Integrate WI Input WI Output <ref type="figure">Figure 12</ref>: Overview of the proposed wrist integration network.</p><formula xml:id="formula_18">? arm w = [? s w , ? e w ] = [? s (? s w , ? w ), ? e w ],<label>(11)</label></formula><p>where ? s is the function to convert the local shoulder rotation ? s w to global shoulder orientation ? s w . ? s can be implemented by forwarding the kinematics of the SMPL-X body skeleton from the root to the shoulder.</p><p>Taking the arm poses ? arm w and 2D directional vector as input, the wrist integration network predicts the updated arm poses? arm w . The whole process is formulated as Eq. <ref type="bibr" target="#b11">(12)</ref>.?</p><formula xml:id="formula_19">arm w = H(? arm w , d)<label>(12)</label></formula><p>It is worth nothing that? </p><p>where ? s is the function to convert the global shoulder orientation? s w to local shoulder rotation? s w . Its definition is similar to ? l and ? r defined in Eq. (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2. Implementation Details.</head><p>The wrist integration network is a MLP with 6 layers. The input dimension is 8, which is composed of ? arm w ? R 2?3 and d ? R 2 . The output dimension is 6 which is ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.3. Training Data.</head><p>To train H, we generate a synthetic dataset by capturing our own a range of motion of right arms. We first capture videos with varying arm poses. Then we use the optimization method introduced in Section 3.5 to obtain the pseudo ground-truth 3D pose estimations for those video frames. Suppose for one data sample, we obtain ? w , ? w , ? w , c w , which are global rotation, whole-body pose, whole-body shapes and camera parameters. To increase the model's generalization ability to varying body shapes, we add random noise to the obtain body shape ? w . Then we use the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Prediction</head><p>Input Prediction updated body shapes and other obtained parameters to get projected right 2D hand joints J 2D rh by projecting the predicted 3D right hand joints J 3D rh using Eq. 4. After that, we add random translation? = [? x ,? y ] to the original predicted 2D hand joints to obtain permutated 2D hand joint? J 2D rh = J 2D rh +?. We run the optimization method again to obtain the new poses using the permutated 2D hand joint? J 2D rh in calculating F 2d defined in Eq. <ref type="bibr" target="#b8">(9)</ref>. In this process, we fix other parameters and only update the right shoulder and right elbow poses, obtaining? arm w . In the end, we obtain training data with hand directional vector?, original poses ? arm w and updated arm poses? arm w . They form the training data to train wrist integration network defined in <ref type="figure">Fig. 12</ref>.</p><p>We visualize several generated data samples in <ref type="figure" target="#fig_0">Fig. 13</ref>. The right arrows are the random translation? to be added. The green bodies are rendered from the original optimization prediction. The blue arms are the obtained arm poses ? arm w after applying randomly translating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3. Implementation Details</head><p>Bounding Bboxes. We use a lightweight Pytorch Open-Pose implementation <ref type="bibr" target="#b53">[54]</ref>, which only predicts 2D body keypoints, to obtain body bounding bboxes. Face and hand bounding bboxes can also be obtained from OpenPose <ref type="bibr" target="#b12">[13]</ref>. For live demo requiring higher speed, the bounding boxes are obtained by projecting the face and hand part of the estimated 3D body to image space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4. Failure Cases</head><p>We show several typical failure cases in <ref type="figure" target="#fig_1">Fig. 14.</ref> It is revealed that failure cases are typically caused by severe occlusion and blurry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5. More Qualitative Results</head><p>Comparison with Zhou et al. Firstly, we show qualitative comparisons with Zhou et al. <ref type="bibr" target="#b76">[76]</ref> in <ref type="figure" target="#fig_3">Fig. 15</ref>. It is worth</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Zhou et al Ours <ref type="figure" target="#fig_3">Figure 15</ref>: Quanlitative comparison with Zhou et al. <ref type="bibr" target="#b76">[76]</ref>. Hand regions are marked with red circles. It is revealed that our method can generate more precise 3D hand poses than Zhou et al. <ref type="bibr" target="#b76">[76]</ref> do.</p><p>noting that Zhou et al. <ref type="bibr" target="#b76">[76]</ref> have not released their official code yet. Therefore, we compare our methods with them using images shown in their paper. The results demonstrate that our methods can generate more accurate hand and body poses than Zhou et al. <ref type="bibr" target="#b76">[76]</ref> does. Demo. We show more qualitative results of FrankMocap in <ref type="figure" target="#fig_4">Fig.. 16</ref>. The integration module is the Wrist Integration Network. More qualitative results are shown in the demo videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>[ h , ?h, h , ch] &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + u D h b 9 f Q B Q 1 V m H K d i V G M g F u M d R o = " &gt; A A A C P X i c b Z B L S 8 N A E M c 3 P m t 9 R T 1 6 W S y C B y m J C H o s e P F Y o S 9 p Q t h s t s 3 S z Y P d i V B C v 5 g X v 4 M 3 b 1 4 8 K O L V q 5 s 2 B / s Y W P b P b 2 a Y m b + f C q 7 A s t 6 M t f W N z a 3 t y k 5 1 d 2 / / 4 N A 8 O u 6 o J J O U t W k i E t n z i W K C x 6 w N H A T r p Z K R y B e s 6 4 / u i n z 3 i U n F k 7 g F 4 5 S 5 E R n G f M A p A Y 0 8 s 9 V 3 / E Q E a h z p L 3 f S k E + 8 8 B L P Q Q g Z k G X s r 6 J U E 9 c z a 1 b d m g Z e F n Y p a q i M p m e + O k F C s 4 j F Q A V R q m 9 b K b g 5 k c C p Y J O q k y m W E j o i Q 9 b X M i Y R U 2 4 + v X 6 C z z U J 8 C C R + s W A p / R / R 0 4 i V W y n K y M C o V r M F X B V r p / B 4 N b N e Z x m w G I 6 G z T I B I Y E F 1 b i g E t G Q Y y 1 I F R y v S u m I Z G E g j a 8 q k 2 w F 0 9 e F p 2 r u m 3 V 7 Y f r W u O x t K O C T t E Z u k A 2 u k E N d I + a q I 0 o e k b v 6 B N 9 G S / G h / F t / M x K 1 4 y y 5 w T N h f H 7 B 6 z S s V g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + u D h b 9 f Q B Q 1 V m H K d i V G M g F u M d R o = " &gt; A A A C P X i c b Z B L S 8 N A E M c 3 P m t 9 R T 1 6 W S y C B y m J C H o s e P F Y o S 9 p Q t h s t s 3 S z Y P d i V B C v 5 g X v 4 M 3 b 1 4 8 K O L V q 5 s 2 B / s Y W P b P b 2 a Y m b + f C q 7 A s t 6 M t f W N z a 3 t y k 5 1 d 2 / / 4 N A 8 O u 6 o J J O U t W k i E t n z i W K C x 6 w N H A T r p Z K R y B e s 6 4 / u i n z 3 i U n F k 7 g F 4 5 S 5 E R n G f M A p A Y 0 8 s 9 V 3 / E Q E a h z p L 3 f S k E + 8 8 B L P Q Q g Z k G X s r 6 J U E 9 c z a 1 b d m g Z e F n Y p a q i M p m e + O k F C s 4 j F Q A V R q m 9 b K b g 5 k c C p Y J O q k y m W E j o i Q 9 b X M i Y R U 2 4 + v X 6 C z z U J 8 C C R + s W A p / R / R 0 4 i V W y n K y M C o V r M F X B V r p / B 4 N b N e Z x m w G I 6 G z T I B I Y E F 1 b i g E t G Q Y y 1 I F R y v S u m I Z G E g j a 8 q k 2 w F 0 9 e F p 2 r u m 3 V 7 Y f r W u O x t K O C T t E Z u k A 2 u k E N d I + a q I 0 o e k b v 6 B N 9 G S / G h / F t / M x K 1 4 y y 5 w T N h f H 7 B 6 z S s V g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + u D h b 9f Q B Q 1 V m H K d i V G M g F u M d R o = " &gt; A A A C P X i c b Z B L S 8 N A E M c 3 P m t 9 R T 1 6 W S y C B y m J C H o s e P F Y o S 9 p Q t h s t s 3 S z Y P d i V B C v 5 g X v 4 M 3 b 1 4 8 K O L V q 5 s 2 B / s Y W P b P b 2 a Y m b + f C q 7 A s t 6 M t f W N z a 3 t y k 5 1 d 2 / / 4 N A 8 O u 6 o J J O U t W k i E t n z i W K C x 6 w N H A T r p Z K R y B e s 6 4 / u i n z 3 i U n F k 7 g F 4 5 S 5 E R n G f M A pA Y 0 8 s 9 V 3 / E Q E a h z p L 3 f S k E + 8 8 B L P Q Q g Z k G X s r 6 J U E 9 c z a 1 b d m g Z e F n Y p a q i M p m e + O k F C s 4 j F Q A V R q m 9 b K b g 5 k c C p Y J O q k y m W E j o i Q 9 b X M i Y R U 2 4 + v X 6 C z z U J 8 C C R + s W A p / R / R 0 4 i V W y n K y M C o V r M F X B V r p / B 4 N b N e Z x m w G I 6 G z T I B I Y E F 1 b i g E t G Q Y y 1 I F R y v S u m I Z G E g j a 8 q k 2 w F 0 9 e F p 2 r u m 3 V 7 Y f r W u O x t K O C T t E Z u k A 2 u k E N d I + a q I 0 o e k b v 6 B N 9 G S / G h / F t / M x K 1 4 y y 5 w T N h f H 7 B 6 z S s V g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + u D h b 9 f Q B Q 1 V m H K d i V G M g F u M d R o = " &gt; A A A C P X i c b Z B L S 8 N A E M c 3 P m t 9 R T 1 6 W S y C B y m J C H o s e P F Y o S 9 p Q t h s t s 3 S z Y P d i V B C v 5 g X v 4 M 3 b 1 4 8 K O L V q 5 s 2 B / s Y W P b P b 2 a Y m b + f C q 7 A s t 6 M t f W N z a 3 t y k 5 1 d 2 / / 4 N A 8 O u 6 o J J O U t W k i E t n z i W K C x 6 w N H A T r p Z K R y B e s 6 4 / u i n z 3 i U n F k 7 g F 4 5 S 5 E R n G f M A p A Y 0 8 s 9 V 3 / E Q E a h z p L 3 f S k E + 8 8 B L P Q Q g Z k G X s r 6 J U E 9 c z a 1 b d m g Z e F n Y p a q i M p m e + O k F C s 4 j F Q A V R q m 9 b K b g 5 k c C p Y J O q k y m W E j o i Q 9 b X M i Y R U 2 4 + v X 6 C z z U J 8 C C R + s W A p / R / R 0 4 i V W y n K y M C o V r M F X B V r p / B 4 N b N e Z x m w G I 6 G z T I B I Y E F 1 b i g E t G Q Y y 1 I F R y v S u m I Z G E g j a 8 q k 2 w F 0 9 e F p 2 r u m 3 V 7 Y f r W u O x t K O C T t E Z u k A 2 u k E N d I + a q I 0 o e k b v 6 B N 9 G S / G h / F t / M x K 1 4 y y 5 w T N h f H 7 B 6 z S s V g = &lt; / l a t e x i t &gt; Overview of hand module. Our hand module takes a cropped hand image IH as input, and produces the parameters of hand</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Motion Blur Augmentation. We show example images of motion blur augmentation. From left to right: original images, augmented images after applying different motion blur kernels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Optimizing the whole-body model (SMPL-X) with 3D hand prediction and 2D keypoint estimation. (a) Input images and estimated 2D keypoints by OpenPose [13]; (b) 3D body pose estimation from body module; (c) The output of 3D hand module aligned to the wrist joints of SMPL-X; (d) Integration output by copy-paste ; (e) Integration output by optimization framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparison with state-of-the-art 3D hand methods. Images are selected from COCO<ref type="bibr" target="#b44">[45]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Ablation study on data augmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative comparison of FrankMocap model with different integration modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>12 Figure 10 :</head><label>1210</label><figDesc>We take the hand part of SMPL-X as a stand-alone hand model for hand pose estimation. The example mesh is shown in (a) and the skeleton hierarchy is shown in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>composed of updated global shoulder orientation? s w and updated local elbow rotation? e w . To make the obtained shoulder poses compatible with the whole-body poses ? w defined in Eq. (8), we convert the global shoulder orientation? s w back to the local shoulder rotation? s w . This process is similar to converting global hand orientation ? h to local hand rotation ? h defined in Eq. (8). Since the updated local elbow rotation? e w is already the local rotation, there is no need to further process it. The final updated arm poses? arm w with pose shoulder and elbow arms in the local rotation, as defined in Eq. 10, can be formulated as: [? s (? s , ? w ),? e ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>?Figure 13 :</head><label>13</label><figDesc>R 2?3 . The dimension of the intermediate layers are 128, 256, 512, 256, 128, separately. For wrist integration network, we assume the input arm poses and output arm poses are only right arm poses. During training, we only use right arm poses and right hand directional movement. During inference, the input initial left arm poses and 2D Samples of synthesized data for training wrist integration network. The right arrows are the random translation? to be added.The green bodies are rendered from the original optimization output. The blue arms are the obtained arm poses? arm w after applying random translation. directional vector are vertically flipped as there were right arms and hands. The output arm poses are then flipped back to the left arm pose space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Typical failure cases from FrankMocap. It is shown that typical failure cases are caused by severe occlusion and blurry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our hand module with the state-of-the-art 3D hand methods using 2D/3D AUC of PCK.</figDesc><table><row><cell cols="8">Method ? Bouk [8] Baek [2] Ge [20] Zhang [75] Kulon [39] CMR [15] Ours Dataset ?</cell></row><row><cell>STB</cell><cell>0.994</cell><cell>-</cell><cell>0.995</cell><cell>0.995</cell><cell>-</cell><cell>-</cell><cell>0.992</cell></row><row><cell>RHD</cell><cell>-</cell><cell>0.926</cell><cell>0.92</cell><cell>0.901</cell><cell>0.95</cell><cell cols="2">0.944 0.934</cell></row><row><cell cols="2">MPII+NZSL 0.501</cell><cell>-</cell><cell>0.15</cell><cell>-</cell><cell>0.701</cell><cell>-</cell><cell>0.655</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on hand module data augmentation.</figDesc><table><row><cell cols="2">Models are evaluated on MPII+NZSL [62] with 2D AUC.</cell></row><row><cell>Translation Rescale Color Rotation Blur</cell><cell>2D AUC</cell></row><row><cell></cell><cell>0.608</cell></row><row><cell></cell><cell>0.610</cell></row><row><cell></cell><cell>0.618</cell></row><row><cell></cell><cell>0.622</cell></row><row><cell></cell><cell>0.655</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of our body module with the state-of-the-art body-only methods on 3DPW<ref type="bibr" target="#b67">[67]</ref>.Method ? HMR<ref type="bibr" target="#b32">[33]</ref> CMR<ref type="bibr" target="#b37">[38]</ref> SPIN<ref type="bibr" target="#b36">[37]</ref> EFT<ref type="bibr" target="#b29">[30]</ref> ExPose<ref type="bibr" target="#b17">[18]</ref> Ours Metric ?</figDesc><table><row><cell>MPJPE</cell><cell>130</cell><cell>127.2</cell><cell>96.9</cell><cell>92.3</cell><cell>93.4</cell><cell>94.3</cell></row><row><cell>PA-MPJPE</cell><cell>81.3</cell><cell>70.2</cell><cell>59.2</cell><cell>54.2</cell><cell>60.7</cell><cell>60.0</cell></row><row><cell cols="7">calculate the percentage of correct keypoints (PCK) under</cell></row><row><cell cols="7">different thresholds and calculate the corresponding Area</cell></row><row><cell cols="5">Under Curve (AUC) for PCK. For STB</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison with previous SMPL-X based whole-body model on EHF dataset<ref type="bibr" target="#b55">[56]</ref>.</figDesc><table><row><cell cols="2">Methods ? Part &amp; Metrics ?</cell><cell cols="5">SMPLify-X [56] ExPose [18] FM-CP FM-WI FM-OPT</cell></row><row><cell>All</cell><cell>V2V PA-V2V</cell><cell>146.2 68.0</cell><cell>81.9 57.4</cell><cell>69.9 58.2</cell><cell>69.3 57.1</cell><cell>63.5 54.7</cell></row><row><cell>Body</cell><cell>V2V PA-V2V</cell><cell>231.3 75.4</cell><cell>116.4 55.1</cell><cell>83.7 52.7</cell><cell>83.9 53.2</cell><cell>93.1 53.3</cell></row><row><cell>L Hand</cell><cell>V2V PA-V2V</cell><cell>34.6 11.4</cell><cell>39.3 13.1</cell><cell>27.1 10.9</cell><cell>27.0 10.9</cell><cell>28.6 10.8</cell></row><row><cell>R Hand</cell><cell>V2V PA-V2V</cell><cell>34.3 13.1</cell><cell>39.3 12.9</cell><cell>25.4 11.2</cell><cell>25.5 11.2</cell><cell>25.6 11.2</cell></row><row><cell>Face</cell><cell>V2V PA-V2V</cell><cell>23.6 8.4</cell><cell>19.5 5.6</cell><cell>30.7 5.7</cell><cell>30.7 5.7</cell><cell>20.4 5.4</cell></row><row><cell cols="2">Runtime (sec/frame)</cell><cell>60</cell><cell>0.15</cell><cell>0.15</cell><cell>0.15</cell><cell>2</cell></row><row><cell>Input</cell><cell></cell><cell>FM-CP</cell><cell>FM-WI</cell><cell></cell><cell cols="2">FM-OPT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>In training our hand module, we use five datasets in-</figDesc><table /><note>cluding: FreiHAND</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on dataset. We show the results of our hand module trained with different datasets. These models are evaluated on MPII+NZSL<ref type="bibr" target="#b62">[62]</ref> using 2D AUC as metric. For data augmentation, we use all the available datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">While the original SMPL-X model has jaw, left and right eye poses for the face part, we do not estimate eye poses and only consider jaw pose.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Even if the 3D joint locations of SMPL and SMPL-X are not exactly identical, we found that this distinction is not very significant in practice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that the code of Kulton et al.<ref type="bibr" target="#b38">[39]</ref> is not publicly available, and, thus, we are unable to make more comparisons in challenging in-the-wild scenes, our target applications. CMR<ref type="bibr" target="#b14">[15]</ref> requires camera intrinsic as known, which prevents applying them to in-the-wild scenarios.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We acknowledge Yuting Ye from FRL, Donglai Xiang and Yu Ding from CMU, Yu Xiong, Anyi Rao, and Jingbo Wang from CUHK, and Jie Chen from Microsoft.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hand-Only</head><p>Whole Body Side View <ref type="figure">Figure 16</ref>: Qualitative results for FrankMocap with wrist integration network adopted.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Hand Module</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pushing the envelope for rgb-based dense 3d hand pose estimation via neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryul</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang</forename><forename type="middle">In</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Driving-signal aware full-body avatars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uniform motion blur in poissonian noise: Blur/noise tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling the performance of image restoration from motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giacomo</forename><surname>Boracchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d hand shape and pose from images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adnane</forename><surname>Boukhayma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>De Bem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly-supervised 3d hand pose estimation from monocular rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-channel transformers for multiarticulatory sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Necati Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3d facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pose-robust face recognition via deep residual equivariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Expnet: Landmark-free, deep, 3d facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Ju</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Camera-space hand mesh recovery via semantic aggregation and adaptive 2d-1d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modelbased 3d hand reconstruction via self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefei</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Monocular expressive body regression through body-driven attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d hand shape and pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Generalized procrustes analysis. Psychometrika</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="33" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Home-based physical therapy with an interactive computer vision system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreya</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elham</forename><surname>Saraee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Nordahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3D human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Honnotate: A method for 3d annotation of hand and object poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Hampali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hand pose estimation via latent 2.5d heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Exemplar fine-tuning for 3d human pose fitting towards inthe-wild 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>In arXiv preprint arXiv:2004.03686, 2020. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3D human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weaklysupervised mesh-convolutional hand reconstruction in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Kulon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Talking with hands 16.2 m: A large-scale dataset of synchronized bodyfinger motion and audio for conversational motion analysis and synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siddhartha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning for assistive computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning a model of facial shape and expression from 4d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end human pose and mesh reconstruction with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Xnect: Real-time multi-person 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Araceli</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Piella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><forename type="middle">M</forename><surname>Sukno</surname></persName>
		</author>
		<title level="m">Survey on 3d face reconstruction from uncalibrated images</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Body2hands: Learning to infer 3d hands from conversational gesture body dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evonne</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dmd: A large-scale multi-modal driver monitoring dataset for attention and alertness analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Diego</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neslihan</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Ca?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-An</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Unnervik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oihana</forename><surname>Otaegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Salgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Real-time 2d multi-person pose estimation on cpu: Lightweight openpose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniil</forename><surname>Osokin</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1811.12004</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<imprint>
			<pubPlace>Ahmed AA Osman, Dimitrios Tzionas, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A model of dynamic human shape in motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Delving deep into hybrid annotations for 3D human recovery in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Chasing the tail in monocular 3d human reconstruction with prototype memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14739</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to regress 3d face shape and expression from an image without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiwen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Human body model fitting by learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Mofa: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3d morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Sceneaware generative network for human motion synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Monocular total capture: Posing face, body, and hands in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Ghum &amp; ghuml: Generative 3d human shape and articulated pose models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">Gabriel</forename><surname>Bazavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Denserac: Joint 3D pose and shape estimation by dense render-andcompare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">3D human mesh regression with dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Xiaobin Xu, and Qingxiong Yang. 3d hand pose tracking and estimation using stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangqiong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">End-to-end hand mesh recovery from a monocular rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Monocular realtime full body capture with inter-part correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Monocular realtime hand shape and motion capture using multi-modal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikhsanul</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d hand pose from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Freihand: A dataset for markerless capture of hand pose and shape from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Argus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">State of the art on monocular 3d face reconstruction, tracking, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zollh?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
