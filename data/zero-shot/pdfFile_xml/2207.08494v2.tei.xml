<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Alignment in Video Super-Resolution Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwei</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
							<email>jinjin.gu@sydney.edu.au</email>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
							<email>lb.xie@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
							<email>xintaowang@tencent.com</email>
							<affiliation key="aff5">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
							<email>yang.yujiu@sz.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Alignment in Video Super-Resolution Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The alignment of adjacent frames is considered an essential operation in video super-resolution (VSR). Advanced VSR models, including the latest VSR Transformers, are generally equipped with well-designed alignment modules. However, the progress of the self-attention mechanism may violate this common sense. In this paper, we rethink the role of alignment in VSR Transformers and make several counter-intuitive observations. Our experiments show that: (i) VSR Transformers can directly utilize multi-frame information from unaligned videos, and (ii) existing alignment methods are sometimes harmful to VSR Transformers. These observations indicate that we can further improve the performance of VSR Transformers simply by removing the alignment module and adopting a larger attention window. Nevertheless, such designs will dramatically increase the computational burden, and cannot deal with large motions. Therefore, we propose a new and efficient alignment method called patch alignment, which aligns image patches instead of pixels. VSR Transformers equipped with patch alignment could demonstrate state-of-the-art performance on multiple benchmarks. Our work provides valuable insights on how multi-frame information is used in VSR and how to select alignment methods for different networks/datasets. Codes and models will be released at https://github.com/XPixelGroup/RethinkVSRAlignment.</p><p>Recently, the general paradigm of vision network design has gradually shifted from CNNs to Transformers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7]</ref>. Unlike the locality property of CNNs, the self-attention operation in Transformers is very efficient for processing elements with spatially long-term distribution. VSR is no exception: Transformers have also been introduced into VSR for better results <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. However, * Jinjin Gu and Shuwei Shi contribute equally to this work. these VSR Transformers still retain complex alignment modules. The ability of Transformers to efficiently process non-local information has not yet been exploited for inter-frame information processing.</p><p>In this paper, we rethink the role of the alignment module in VSR Transformers. We make two arguments: (i) VSR Transformers can directly utilize multi-frame information from unaligned videos, and (ii) continuing to use the existing alignment methods will sometimes degrade the performance of VSR Transformers. For the first argument, we report that for Transformers using the shifted window mechanism [31, 23], misalignment within a certain range does not affect the SR performance, while alignment only has a positive effect on the pixels beyond that range. The attribution map visualization results <ref type="bibr" target="#b10">[11]</ref> also show that Transformers without alignment modules behave similarly to CNNs with alignment modules. For the second argument, we quantitatively compare the effects of various alignment methods. Using alignment will only have negative effects for pixels without large misalignment (e.g., Vimeo-90K [48]). We blame this phenomenon on the noise of optical flow and the destruction of sub-pixel information by alignment resampling operations. Although implicit alignment based on deformable convolution can minimize these negative effects, the additional cost of parameters and computation makes this approach no longer advantageous.</p><p>According to our findings, we can build an alignment-free VSR Transformer, which requires a large attention window (&gt;16) to cover misalignment between frames. However, enlarging the window size will lead to higher computational costs, making this approach no longer feasible. As a better alternative, we propose a simple yet effective alignment method called Patch Alignment, which aligns image patches instead of pixels. It will find the corresponding patches in the supporting frames and compute self-attention among them. This method uses a simple crop-then-move strategy to compensate for situations where the misalignment exceeds the Transformer's attention window. Experiments show that the proposed method achieves state-of-the-art VSR performance with a simple design and fewer parameters.</p><p>2 Related Works VSR networks. The uniqueness of the VSR task lies in the utilization of inter-frame sub-pixel information <ref type="bibr" target="#b29">[30]</ref>. Due to the motion of objects between frames, the computation and compensation for this motion is often the focus of VSR research. The pipeline of most VSR networks mainly includes an alignment module, a feature extraction module, a fusion module, and a reconstruction module. These networks are trained using a large number of low-resolution (LR) and high-resolution (HR) video sequences. The paradigm of early methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref> can be summarized as: first estimate and compensate object motion (using optical flow), then pass through different feature extraction <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b18">19]</ref>, fusion <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4]</ref>, and reconstruction modules [38], respectively. These methods can be further divided into methods for compensating motion in the image space (image alignment) and that in the feature space (feature alignment) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43]</ref>. As Chan et al. <ref type="bibr" target="#b3">[4]</ref> claims, feature alignment can achieve better results because inaccuracy in flow estimation may be mitigated after feature extraction. However, the above methods generally suffer from inaccurate optical flow estimation and defective alignment operation, resulting in noticeable artifacts in the aligned images <ref type="bibr" target="#b39">[40]</ref>. With the invention of Deformable convolutions <ref type="bibr" target="#b8">[9]</ref>, CNN networks also have the ability to model geometric transformations. Deformable convolutions gradually replace explicit warping operations in VSR networks with their learnable and flexible properties <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b39">40]</ref>. EDVR [44]  integrates deformable convolution in the VSR network for the first time and aligns features at multiple scales. Chan et al. <ref type="bibr" target="#b4">[5]</ref> incorporate optical flow as a guidance for offsets learning. The latest BasicVSR++ [6] based on the bidirectional loop structure also uses the alignment based on deformable convolution.</p><p>Differently, our paper aims to study Transformers' ability to model multi-frame information implicitly without using alignment. Before Transformer, attempts have been made to achieve the same effect in CNN, i.e., the VSR methods without alignment. Both 2D [33, 49] and 3D convolutions [41,  13, 20, 16] are used to extract correlations among frames. Similar to Transformers, some methods use correspondence calculation to build inter-frame connections without alignment. For example, MuCAN [21] uses a temporal multi-correspondence aggregation module and a cross-scale non-localcorrespondence aggregation module to replace explicit alignment. Yi et al. <ref type="bibr" target="#b50">[51]</ref> introduce an improved non-local operation to avoid the complex alignment procedure. Generally speaking, models without explicit alignment are less effective or rely on special module design. However, the Transformers discussed in this paper could efficiently handle unaligned frames without additional special design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video Super-Resolution (VSR) could provide more accurate SR results than Single-Image Super-Resolution (SISR) by exploiting the complementary sub-pixel information from multiple frames. The well-established paradigms of VSR networks usually include an alignment module to compensate for the motion of objects between frames. The alignment module is critical for CNN-based VSR networks, because the locality inductive bias of CNNs only allows them to utilize spatial-close distributed information effectively. Many VSR networks have achieved better performance by introducing more advanced alignment methods <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b23">24]</ref>.  <ref type="figure">Figure 1</ref>: A brief illustration of the VSR transformer network used. The illustrated structure is based on the sliding window mechanism. We mark the position of the existing alignments methods.   <ref type="bibr" target="#b2">[3]</ref> employ the spatial-temporal self-attention on multi-frame patches to perform implicit motion compensation. Liang et al. <ref type="bibr" target="#b21">[22]</ref> propose a parallel frame prediction Transformer to model long-range temporal dependency. Both the above Transformers consist of alignment modules. VSRT <ref type="bibr" target="#b2">[3]</ref> uses feature alignment method and VRT <ref type="bibr" target="#b21">[22]</ref> employs complex deformable convolution to perform forward and backward alignment. In this paper, we show that these complex alignment operations are unnecessary or even harmful for VSR Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary Settings</head><p>In this section, we describe the settings used in our experiments in detail, including the Transformer architecture, alignment methods, datasets, metrics and implementation details. If the readers are already familiar with the common VSR settings and VSR networks, it is okay to skip this section and go to Section 4 for experimental results and analysis. Due to space constraints, we record the detailed setup of the different experiments in Appendix G to facilitate the readers to reproduce our results. The VSR Transformer architecture. We first describe the basic VSR Transformer backbone used in this study, which follows the sliding window design. Transformer based on shifted window mechanism <ref type="bibr" target="#b22">[23]</ref> is proven to be flexible and effective for image processing tasks. We only make minimal changes when applying it to VSR to maintain its good performance without the loss of generality. This model is illustrated in <ref type="figure">Figure 1</ref>. The VSR Transformer takes a reference frame I t and 2n adjacent supporting frames {I t?n , . . . , I t?1 , I t+1 , . . . , I t+n } as input, and outputs the super-resolved reference frame I t SR . In the beginning, a feature extraction module is used to extract features for the subsequent Transformer. We use a single 2D convolution layer for feature extraction. The extracted features are denoted as 2n + 1 feature maps {X t?n , . . . , X t , . . . , X t+n }.</p><p>Then N Multi-Frame Self-Attention Blocks (MFSAB) are used as the backbone network. The MFSAB is modified from RSTB in SwinIR <ref type="bibr" target="#b22">[23]</ref>, which contains two LayerNorm <ref type="bibr" target="#b0">[1]</ref> layers, a multi-head self-attention layer and a multi-layer perceptron layer. We mainly modify the self-  <ref type="figure">Figure 4</ref>: This figure illustrates the performance differences between VSR Transformers with and without alignment module for different pixel movement conditions. Parts greater than zero indicate better performance without alignment. attention layer to make it suitable for the VSR task. Given these 2n + 1 feature maps of the frames with size H ? W ? C (H, W and C are the height, width and feature dimension), the shifted window mechanism first reshapes the feature maps of each frame to HW M 2 ? M 2 ? C features by partitioning the input into non-overlapping M ? M local windows, where HW M 2 is the total number of windows. We calculate self-attention on the features in the windows corresponding to the positions in different frames. Therefore, (2n + 1)M 2 features are involved in each standard self-attention operation, and we concatenate features from different frames to produce a local window feature X ? R (2n+1)M 2 ?C . In each self-attention layer, the query Q, key K and value V are computed as</p><formula xml:id="formula_0">Q = XW Q , K = XW K , V = XW V , where W Q , W K , W V ? R C?D</formula><p>are weight matrices, and D is the channel number of projected vectors. Then, we use Q to query K to generate the attention map</p><formula xml:id="formula_1">A = softmax( QK T / ? D + B) ? R (2n+1)M 2 ?(2n+1)M 2</formula><p>, where B is the learnable relative positional encoding. This attention map A is then used for the weighted sum of (2n + 1)M 2 vectors in V . The multi-head settings are aligned with SwinIR <ref type="bibr" target="#b22">[23]</ref> and ViT <ref type="bibr" target="#b9">[10]</ref>. In the final reconstruction module, only the features of the reference frame are used to generate the SR result. We use the pixel-shuffle layer <ref type="bibr" target="#b37">[38]</ref> to upsample the feature maps. Besides sliding window based VSR Transformer, the MFSAB can also be applied to recurrent frameworks <ref type="bibr" target="#b5">[6]</ref>. We describe the details in Appendix A. Alignment methods. Alignment methods follow a long line of research. To investigate the role of alignment, we need to implement and compare different alignment methods. These alignment methods can be classified into four types, and their respective representative methods are included in our experiments. We next describe their characteristics and then describe our implementation.</p><p>? Image alignment is the earliest and most intuitive alignment method, which is also the easiest to use. Image alignment relies on the explicitly calculated optical flow between frames. According to the estimated inter-frame motion, different frames are aligned by a warping operation. Following the existing successful experience <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b3">4]</ref>, we use the SpyNet <ref type="bibr" target="#b35">[36]</ref> to estimate the optical flow, and fine-tune the SpyNet simultaneously during training. The resampling method used is the bilinear (BI) method. ? Feature alignment also estimates the optical flow but performs the warping operation on the deep features instead of on the images. The flow estimation module still uses SpyNet, which is optimized during training. In addition to the shallow 2D convolution in <ref type="figure">Figure 1</ref>, we add five additional residual blocks <ref type="bibr" target="#b11">[12]</ref> to extract deep features. ? Deformable convolution (DC) based methods use learnable dynamic deformable convolution for alignment. Almost all state-of-the-art VSR networks use deformable convolution to perform the alignment. We employ the flow-guided deformable convolution (FGDC) alignment used in BasicVSR++ <ref type="bibr" target="#b5">[6]</ref> and VRT <ref type="bibr" target="#b21">[22]</ref> as the representative method. ? No alignment. The raw input is processed directly using the VSR Transformer.</p><p>Datasets and metrics. In the VSR literature, the REDS <ref type="bibr" target="#b33">[34]</ref> and the Vimeo-90K <ref type="bibr" target="#b47">[48]</ref> datasets are the de-facto benchmarks. REDS has 270 available video sequences, each containing 100 frames. We follow the common splitting methods and split the data into training (266 sequences) and testing (4 sequences) sets. Vimeo-90K contains 64,612 and 7,824 video sequences for training and testing, respectively. Although REDS and Vimeo-90K are widely used benchmarks, these two datasets have different motion conditions. The motion in the Vimeo-90K dataset is generally small, and movement magnitudes of 99% pixels are less than 10 (for each clip, we measure the motion of the 4th and the 7th frames). Differently, there are large motions in the REDS dataset. There are at least 20% pixels that have movement magnitudes larger than 10 (for each clip, we measure the motion of the 3rd and the 5th frames). The distributions of the movement conditions for these two datasets are shown in <ref type="table">Table 1</ref>: Quantitative comparison of different VSR methods. The results marked with * achieve similar performance as no alignment. This is due to the vanishing of optical flow in this experiment. Details are shown in <ref type="figure" target="#fig_2">Figure 5</ref>    In our experiments, we mainly study ?4 VSR task. We use bicubic interpolation to produce LR video frames. Peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) are used for evaluation.</p><p>Implementation. We implement all these methods using the BasicSR framework <ref type="bibr" target="#b44">[45]</ref>. We use the Charbonnier loss <ref type="bibr" target="#b17">[18]</ref> as the training objective. The Adam optimization <ref type="bibr" target="#b16">[17]</ref> method is used for training with ? 1 = 0.9 and ? 2 = 0.999. The initial learning rate is set to 4 ? 10 ?4 , and a Consine Annealing scheme <ref type="bibr" target="#b31">[32]</ref> is used to decay the learning rate. The total iteration number is set to 300,000. The mini-batch size is 8, and the LR patch size is 64 ? 64. The experiments are implemented based on PyTorch <ref type="bibr" target="#b34">[35]</ref> and conducted on NVIDIA A100 GPUs. The details and configurations for each experiment can be found in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Rethinking Alignment</head><p>In this section, we conduct experiments based on the above preliminary settings. We use four questions to guide our rethinking. We present our experimental results and analysis in each subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Does alignment always benefit VSR Transformers?</head><p>In previous VSR research, we often judge the pros and cons of the method based on the average performance on the test set. But for the VSR Transformers, the performance of alignment inside the local window is different from that outside the window because of the limited range of self-attention. A finer perspective on how different methods perform on different data may help us understand the effect of alignment. We investigate the performance difference between the VSR Transformer with and without alignment for different pixel movement conditions. <ref type="figure">Figure 4</ref> shows the results tested on the REDS dataset and <ref type="table">Table 1</ref> reports the quantitative results for these experiments.</p><p>One can draw the following observations. First, as can be observed from <ref type="figure">Figure 4</ref> (a), for pixels with small movements, VSR Transformer can achieve good results without alignment. Using image alignment in these cases brings negative effects. This range of pixel movement is related to the window size used by the VSR Transformer. As there is no locality inductive bias in the processing of pixels within a local window, the Transformer can handle misalignment in this range. Second, as the movement increases, the information required by VSR exceeds the scope of the local windows. In these cases, image alignment can improve performance. Third, according to <ref type="figure" target="#fig_3">Figure 2</ref>, the movement  <ref type="figure">Figure 6</ref>: The attribution results of different models. We use the local attribution map <ref type="bibr" target="#b10">[11]</ref> to highlight the responsible pixels in each frame for the SR of the selected red window. We then increase the size of the shifted window, and conduct the same experiment. The result is shown in <ref type="figure">Figure 4</ref> (b). We can see that VSR Transformer can handle a larger range of unaligned pixels with a larger window size. This shows that the processing capability of VSR Transformers for unaligned frames is related to the size of the shifted window, and also implies that this capability mostly relies on the self-attention mechanism. To investigate whether better alignment methods can eliminate the negative effects of alignment on small motion pixels, we use feature alignment to conduct the same experiment, and the result is shown in <ref type="figure">Figure 4</ref> (c). As one can see, feature alignment narrows the gap between alignment and no alignment on VSR Transformer, but it still has negative effects on the pixels of small motion.</p><p>In summary, the VSR Transformer can handle misalignment within a certain range, and using alignment at this range will bring negative effects. This range is closely related to the window size of the VSR Transformer. However, alignment is necessary for motions beyond the VSR Transformer's processing range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">What kinds of flow are better for VSR?</head><p>Although using optical flow for alignment can have a negative impact, different flows can also lead to differences in performance. This inspires us to think about what kinds of flow are better for VSR? According to <ref type="table">Table 1</ref>, one can see that optimizing the flow estimator while training the VSR network will bring better results, because the flow estimator at this time learns the optimized flow for VSR <ref type="bibr" target="#b47">[48]</ref>. The property of this task-oriented flow can imply how the VSR Transformers use multi-frame information. Our first observation is that VSR Transformers tend to use smooth flow. The flow estimator SpyNet was pre-trained with the end-point-error (EPE) loss, which does not explicitly encourage smoothness. The non-smooth flow will introduce random noise to VSR and lose sub-pixel information. To study this problem, we investigate the change of the estimated flow while training the VSR Transformer with image alignment using the REDS dataset. We can see that the flow estimated by the fine-tuned SpyNet is getting smoother, which is reflected in the decrease of the average total variation, as shown in <ref type="figure">Figure 3</ref>. Smoother flow maintains the relative relationship of adjacent pixels in the aligned frames, thus facilitating VSR processing.</p><p>Although the fine-tuned flow estimator will improve performance, there is still a gap between image alignment with flow fine-tuning and no alignment on the REDS dataset. However, we observe different results on the Vimeo-90K dataset: image alignment with flow fine-tuning is almost identical to no alignment. By observing the distribution of the estimated flow, we are surprised to find that the flow is slowly decreasing to 0 when fine-tuned with image alignment using Vimeo-90K. This is why the final result is almost the same as no alignment. In <ref type="figure" target="#fig_2">Figure 5</ref>, we show the validation curve of the related experiments and the histogram distribution of the fine-tuned flow. As one can see, there is a large gap between the VSR Transformers with and without alignment in the early stage of training. When the training reaches 25,000 iterations, the fine-tuned flow gradually shifts towards the zero We can now answer the question posed by this subsection. For most VSR Transformers, the smooth flow is better if the alignment is necessary. For situations with small motions, the flow with all zeros is the best for VSR Transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Does Transformer implicitly track the motion between unaligned frames?</head><p>HR LR <ref type="figure">Figure 10</ref>: Illustration of sub-pixel information.</p><p>We already know that VSR Transformers can handle a specific range of misalignment, but do they track these movements implicitly? Can an alignmentlike function be done inside the VSR Transformers? We next use an interpretability tool to visualize the behaviour of the VSR Transformer. Local Attribution Map (LAM) <ref type="bibr" target="#b10">[11]</ref> is an attribution method aiming to find input pixels strongly influencing network output. We first specify a target patch on the output image and then use LAM to generate the corresponding attribution maps. We will track the information used by the model, and see which parts of pixels in adjacent frames contribute the most. As objects move between frames, an ideal VSR network needs to track those movements and utilize pixels representing the same object. We show some representative results in <ref type="figure">Figure 6</ref>. It can be observed that even without the alignment module, the VSR Transformer can automatically change its attention to the most relevant pixels. Intuitively, the self-attention operation can detect the associations between pixels in different frames and utilize their additional information for VSR. This is similar to VSR CNNs with the alignment module. We also show results of VSR CNNs without the alignment module. Since CNN has the inductive bias of locality, it can only focus on the area near the current position if no alignment is used. These areas are less valuable to VSR. Thus, alignment is an essential module for CNN-based VSR methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Why do alignment methods have negative effects?</head><p>To understand the reasons for the negative impact of alignment, we need to know what sub-pixel information does VSR require. As shown in <ref type="figure">Figure 10</ref>, high-frequency information in HR frames is lost during downsampling, and only aliasing patterns are left in the LR frames. When the HR frames move, different LR frames are generated, and different aliasing patterns are produced. These  <ref type="table">Table 3</ref>: The Ablation study of Patch Alignment. We study the effect of different resampling methods (BI and NN) and different alignment positions (image space and feature space).</p><p>Frame 013, Clip 011, REDS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearest</head><p>EDVR <ref type="bibr" target="#b43">[44]</ref> BasicVSR <ref type="bibr" target="#b3">[4]</ref> IconVSR <ref type="bibr" target="#b3">[4]</ref> VRT <ref type="bibr" target="#b21">[22]</ref> BasicVSR++ <ref type="bibr" target="#b5">[6]</ref> Ours GT <ref type="figure">Figure 9</ref>: Visual comparison of VSR (?4) on REDS dataset.</p><p>patterns provide additional constraints for VSR. However, the inaccurate optical flow and the bilinear resampling operation could corrupt these patterns. First, the inaccurate flow can be viewed as a combination of the ground-truth flow and a random error term. Using such flow for alignment randomly will change the LR patterns and cause information loss. Second, the bilinear resampling operation calculates the weighted average among four adjacent pixels, and the weights are inaccessible for the VSR model. At this time, the interpolation is irreversible. The VSR model can only process the transformed LR patterns and cannot access the original patterns, resulting in information loss.</p><p>We conduct experiments to demonstrate the negative effects of optical flow and resampling methods.</p><p>The results are shown in <ref type="table" target="#tab_4">Table 2</ref>. Compared with image alignment, feature alignment improves performance by extracting part of the sub-pixel information before it is corrupted by alignment. The flow-guided deformable convolution (FGDC) reduces the negative effects of alignment by enabling the network to model geometric transformations. Changing the resampling method to nearest neighbor (NN) also improves performance because the NN method can preserve the relationship between adjacent pixels and ignore the noise of flow estimation to a certain extent. As can be seen, feature alignment using the NN resampling method achieves the same performance as the FGDC method but with a significantly reduced number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Patch Alignment</head><p>According to our observations, an inaccurate flow and the resampling operation will impair the utilization of the inter-frame information. To overcome this problem, simply increasing the Transformer's window size is a straightforward solution as Transformers are naturally good at modelling unaligned spatial dependencies within the local window. However, the time and space complexity of increasing the window size is at least O(n 2 ). We need more efficient alignment methods to improve performance for large movement pixels while introducing little negative impact and additional computational complexity. In this section, we propose a simple yet effective alignment method for VSR Transformers, called Patch Alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Method</head><p>The pipeline of the proposed method is shown in <ref type="figure">Figure 7</ref>. This method does not align individual pixels but treats the image as non-overlapping patches. The partition of patches is consistent with the partition of the Transformer's local windows. We treat the patch as a whole and perform the same operation on the pixels within the patch. In this way, the relative relationship between pixels is kept, and resampling operations will not corrupt the sub-pixel information within the patch. We locate the movement of objects based on optical flow, yet we do NOT pursue precise pixel-level alignment. We calculate the mean motion vector in each patch and find the corresponding patches in the supporting frames for each patch. Next, we use the nearest neighbor resampling method to move the entire supporting patches to their corresponding position in the reference frame. The benefits are three-fold. First, the nearest neighbor resampling ignores the fractional part of optical flow estimation and reduces the error caused by inaccurate flow estimation. Second, the entire patch is cropped and moved to the corresponding position, which preserves the relative relationship of the pixels within the patch and thus retains the sub-pixel information.</p><p>We show the comparison of image alignment with bilinear resampling and the proposed patch alignment method in <ref type="figure">Figure 8</ref>. As can be seen, image alignment introduces blurry and artifacts to the aligned image that destroy sub-pixel information. Patch alignment retains more details that can provide additional information for the VSR model. As we do not pursue pixel-level alignment, directly operating on patches will leave discontinuous artifacts along patch borders. But our experiments show that these discontinuities have little effect on VSR Transformers. Because these discontinuities do not appear in the local window of the Transformer, they do not affect the function of self-attention. It further illustrates the importance of preserving sub-pixel information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental results</head><p>We test the patch alignment method with different alignment positions and resampling methods. The experimental settings and the network configurations are the same with experiments in <ref type="table" target="#tab_4">Table 2</ref>. The results are shown in <ref type="table">Table 3</ref>. As can be seen, image-level patch alignment is already comparable to FGDC, while the latter uses 25% more parameters. Feature-level patch alignment achieves the best performance among all the tested alignment methods. We conduct an ablation experiment using bilinear resampling to study the importance of NN resampling for patch alignment. It can be seen that bilinear resampling leads to a severe performance drop. It shows that the LR patterns retained by the NN method are critical for VSR.</p><p>We use patch alignment to build VSR Transformers based on the sliding window and the recurrent frameworks, namely PSRT-sliding and PSRT-recurrent. We test their performances on REDS and Vimeo-90K datasets. The results are shown in <ref type="table" target="#tab_6">Table 4</ref>. VSR Transformers with patch alignment can achieve state-of-the-art performance with fewer parameters compared to other Transformer-based VSR methods (VSRT <ref type="bibr" target="#b2">[3]</ref> and VRT <ref type="bibr" target="#b21">[22]</ref>). Compared to BasicVSR++ <ref type="bibr" target="#b5">[6]</ref>, we use fewer frames in training, but achieve a 0.33dB improvement on REDS in terms of PSNR. Visual results of different methods are shown in <ref type="figure">Figure 9</ref>. As one can see, in accordance with its significant quantitative improvements, the proposed method can generate visually pleasing images with sharp edges and fine details. By contrast, its competitors suffer from either blurry or lost details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present two important conclusions about using Transformers in the VSR task: (i) VSR Transformers can directly utilize multi-frame information from unaligned videos, and (ii) existing alignment methods are sometimes harmful to VSR Transformers. We also propose a new patch alignment method for VSR Transformers. The proposed method demonstrates the state-of-the-art performance for VSR.</p><p>Given the current literature, our results are interesting and inspiring. They challenge our common understanding of using Transformers to process multiple spatially misaligned images. First, the analysis of alignment can provide useful insights for VSR. We need to utilize inter-frame sub-pixel information, yet many image pre-process operations interfere with our utilization of this information. Second, these observations hint that Transformer can implicitly make accurate connections for misaligned pixels. Many low-level vision tasks can take advantage of this property, such as video restoration, reference-based SR, burst image processing, stereo matching, flow estimation, etc. When designing Transformers for these tasks, we can no longer explicitly introduce the alignment modules or the cost volume modules, but give full play to the powerful modeling capabilities of the Transformer itself.</p><p>The proposed patch alignment method can also be applied to the recurrent VSR framework. Recurrent VSR methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref> use bidirectional propagation scheme to maximize information gathering in VSR and have achieved the state-of-the-art performance. By replacing the CNN backbone with the Transformer backbone, we can easily build a recurrent VSR Transformer. We employ the secondorder grid propagation framework similar to BasciVSR++ <ref type="bibr" target="#b5">[6]</ref>, where the intermediate features are propagated both forward and backward in an alternating fashion. Through propagation, information from different frames can be used for feature refinement. We replace the feature propagation bock with the MFSAB blocks presented in the main text. The architecture of this recurrent VSR Transformer is shown in <ref type="figure">Figure 11</ref>.</p><p>Alignment modules are not absent in the existing recurrent methods. In each feature propagation block, features from different frames are aligned to extract information from the adjacent frames, improving feature expressiveness. BasicVSR <ref type="bibr" target="#b3">[4]</ref> uses flow-based alignment method for both images and features and BasicVSR++ <ref type="bibr" target="#b5">[6]</ref> uses flow-guided deformable convolution (FGDC) alignment. The proposed patch alignment is also compatible with this architecture. We test different alignment methods on the recurrent VSR Transformer; the results are shown in    <ref type="figure">Figure 13</ref>: We compare the patch alignment method and no alignment on the Vimeo-90K dataset. In this case of small movements, no alignment can already achieve good performance, and patch alignment will not bring much improvement. This shows that Transformers can directly handle a small range of misalignment. In the main text, we report that the proposed PSRT-recurrent trained using 16 frames demonstrates the state-of-the-art performance on the VSR task, even compared with BasicVSR++, which was trained using 30 frames. We also tried using 30 frames when training PSRT-recurrent. The training curve is shown in <ref type="figure" target="#fig_3">Figure 12</ref>. It can be seen that PSRT-recurrent can still be greatly improved from more training frames. However, training with 30 frames takes much longer time than with 16 frames, which makes this method uneconomical. In <ref type="table">Table 8</ref> we also compare the state-of-the-art contemporaneous work RVRT <ref type="bibr" target="#b23">[24]</ref>. When RVRT uses 30 frames for training, it can achieve similar performance to PSRT-recurrent when it was trained with 16 frames. This also demonstrates the superior performance of our method.  <ref type="figure" target="#fig_2">Figure 15</ref>: The distribution of the movement for the Vid4 <ref type="bibr" target="#b27">[28]</ref> test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Patch Alignment for the Vimeo-90K Dataset</head><p>Unlike the REDS <ref type="bibr" target="#b33">[34]</ref> dataset, the motion in the Vimeo-90K dataset <ref type="bibr" target="#b47">[48]</ref> tends to be smaller. According to <ref type="figure" target="#fig_3">Figure 2</ref> in the main text, movement magnitudes of 98% pixels in the Vimeo-90K dataset are less than 8. As shown from Table 1 in the main text, VSR Transformer without alignment can outperform other alignment methods on this dataset. A natural question is whether the proposed patch alignment is still effective for a small-motion dataset. We conduct an ablation study on the Vimeo-90K and the Vid4 dataset. <ref type="table" target="#tab_9">Table 6</ref> reports the results. We also show the training curve of these two methods in <ref type="figure">Figure 13</ref> and the distribution of pixel movement for the Vid4 <ref type="bibr" target="#b27">[28]</ref> test set in Figure15. As can be observed from the distribution of movement, the Vid4 dataset contains no pixels whose movement magnitudes are larger than 5. Since most of the motion in these two datasets is within the range that VSR Transformer can handle, there is no significant difference between patch alignment and no alignment method, even no alignment version performs slightly better on Vimeo-90K. The validation curve also demonstrates that patch alignment and no alignment show comparable performance. This experiment confirms our conclusions that (1) we can get good results using VSR Transformers without additional alignment for a specific range of misalignment, and (2) the proposed patch alignment does not introduce as many negative effects as other alignment methods.  Finally, we present the results of recurrent VSR Transformer with patch alignment method trained using 14 frames in <ref type="table">Table 8</ref>. This method achieves state-of-the-art performance on both the Vimeo-90K test set and the Vid4 test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C The FLOPs and Runtime of the Proposed Method</head><p>We calculate the average FLOPs of our method and some existing methods. This FLOPs is calculated using LR frames with size 180 ? 320. We also record their average runtime. The results are shown in <ref type="table" target="#tab_10">Table 7</ref>. As can be seen, the number of parameters of our method is less than other Transformer methods. One of the reasons is that our method saves a lot of parameters on the alignment module. Our FLOPs and runtime are also within a reasonable range. As the acceleration and optimization of Transformers are still to be studied, we believe that given our relatively small FLOPs, there is room for further optimization of the runtime of our method. For the training time, only VRT reports their training time. VRT need 15 days to train and the proposed PSRT-recurrent needs 18 days to train. Our method's training time and cost are roughly the same compared with VRT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Discussion about VSRT</head><p>We notice an exception in the effect of the alignment module to a VSR Transformer, i.e., the VSRT model. Although the VSRT model <ref type="bibr" target="#b2">[3]</ref> also employs Transformer as the backbone design, the alignment module is necessary for it. Removing alignment in VSRT introduces severe performance degradation. This conflicts with our conclusion. Our discussion on this issue is as follows. The VSRT uses a token size of 8 ? 8. In the VSRT, self-attention is calculated between different tokens. This calculation is free of the indicative bias of locality. But within the 8 ? 8 token, only convolution layers and MLP layers participate in the calculation. This calculation is subject to locality bias. If the 8 ? 8 token is not well-aligned, the convolution layers and MLP layers cannot handle unaligned video frame tokens, and self-attention between tokens cannot help improve this. Therefore, the situation of VSRT does not conflict with the argument of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Limitation</head><p>Our work discusses alignment in the VSR task, whose downsampling operation leads to unique LR patterns. We believe that other downsampling methods will have similar effects, such as blurring and directly downsampling (the "BD" downsampling in other papers). For these downsampling methods, the conclusions of this paper are still valid. Some of the observations may not apply to other video restoration tasks, because the multi-frame information they need to use may differ. For other video restoration tasks, we believe that the proposed method will still lead to improvement since theoretically patch alignment preserves more information. But if patch alignment is applied without modification, the resulted improvement may not be as big as in the VSR task. Because the nature of the sub-pixel information will change for other applications, the network design can also be changed (such as adding multi-scale designs). We only have limited space in this paper. However, we emphasise the importance of research in this direction and reserve it for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More Experiments</head><p>We show more visual comparisons between the existing VSR methods and the proposed recurrent VSR Transformer with the patch alignment method. We use 16 frames to train on the REDS dataset and seven on the Vimeo-90K dataset. <ref type="figure">Figure 14</ref> shows the visual results. It can be seen that, in addition to its quantization improvement, the proposed method can generate visually pleasing images with sharp edges and fine details, such as horizontal bar patterns of buildings and numbers on license plates. In contrast, existing methods suffer from texture distortion or loss of detail in these scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Detail of Experiments</head><p>We present more details of the experiments involved in this paper so that anyone can reproduce our results. <ref type="figure" target="#fig_3">Figure 2</ref> and <ref type="figure" target="#fig_2">Figure 15</ref> illustrate the distribution of the movement for three datasets used in our work: Vimeo-90K test set <ref type="bibr" target="#b47">[48]</ref>, REDS <ref type="bibr" target="#b33">[34]</ref> test set and Vid4 <ref type="bibr" target="#b27">[28]</ref> test set. We use the pre-trained SypNet <ref type="bibr" target="#b35">[36]</ref> to calculate the optical flow. For clips in the Vimeo-90K and Vid4 test set, we measure the motion of the 4th and the 7th frames. For clips in the REDS test set, we measure the motion of the 3rd and the 5th frames. This arrangement is related to the common usage of sliding-window-based VSR models on these datasets: we use seven frames as input on Vimeo-90K, while we only use five frames on REDS. The optical flow result contains two maps, which are the movement in the x-direction W x ? R H?W and the movement in the y-direction W y ? R H?W . We use the magnitude to indicate the movement of each pixel <ref type="figure">Figure 3</ref> shows the variation curve of the total variation of the fine-tuned optical flow during training. The total variation is often used to indicate how smooth the optical flow is. The total variation of noise-contaminated optical flow is significantly larger than that of the noise-free optical flow. Given the optical flow {W x , W y }, the total variation is calculated as</p><formula xml:id="formula_2">W m i,j = |W x i,j | 2 + |W y i,j | 2 , W m ? R H?W .</formula><formula xml:id="formula_3">total variation = 1 2HW H i=1 W j=1 (|W x i,j?1 ? W x i,j | + |W y i+1,j ? W y i,j |).</formula><p>We calculate the total variation every 5000 iterations. <ref type="figure">Figure 4</ref> shows the performance differences between VSR Transformers with and without alignment modules for different pixel movements. The VSR Transformer backbone used in this figure contains 16 Multi-Frame Self-Attention Blocks (MFSABs). Similar to SwinIR <ref type="bibr" target="#b22">[23]</ref>, we add shortcut connections every 4 MFSABs. The feature dimension is 120, and the head number of the multi-head self-attention is 6. To plot the differences, we first partition the pixels into different groups according to their movement conditions and then calculate the mean square error for each group. We subtract the mean square errors of the VSR Transformer with alignment from the errors of the VSR Transformer without alignment. Thus, the parts greater than zero indicate better performance without alignment.</p><p>For the first sub-figure, we study the image alignment. The window size is set to 8. We keep the other settings for the second sub-figure and enlarge the window size to 12. For the third sub-figure, we replace the image alignment to feature alignment. In addition to the 2D convolution feature extraction, we add one CNN residual block to extract deep features. These experiments are performed under the same training settings described in Section 3 in the main text. <ref type="table">Table 1</ref> shows the quantitative comparison of different VSR methods. For VSR CNNs, we use ten residual blocks <ref type="bibr" target="#b25">[26]</ref> to extract features for all the input frames. We concatenate the features and reduce the channel number using a convolution layer. Five residual blocks are then used to conduct further processing. The VSR Transformers involved in this table share similar backbone architecture with the VSR Transformers in <ref type="figure">Figure 4</ref>, which contains 16 MFSABs with shortcut connections every 4 MFSABs. The feature dimension is 120, and the head number of the multi-head self-attention is 6. The training method is the same as described in section 3 of the main text. For the methods in which the flow network is not fixed, the learning rate for the flow network is 2.5 ? 10 ?5 . For the first 5,000 iterations, the flow network is fixed. <ref type="figure" target="#fig_2">Figure 5</ref> shows the curves of some of the methods in <ref type="table">Table 1</ref>. One can refer to <ref type="table">Table 1</ref> for the experimental details. The calculation of the movement distribution is the same as in <ref type="figure" target="#fig_3">Figure 2</ref>. The only difference is that we show the percentage in <ref type="figure" target="#fig_2">Figure 5</ref>, not the counts of pixels. <ref type="table" target="#tab_4">Table 2</ref> and <ref type="table">Table 3</ref> share the same training setting and Transformer backbone. The VSR Transformer backbone contains 36 MFSABs with shortcut connections every 6 MFSABs. The window size is set to 8. The channel size of the transformer and head size are set to 144 and 6. The training method is the same as described in Section 3 of the main text. For the methods in which the flow network is not fixed, the learning rate for the flow net is 5 ? 10 ?5 . For the first 20,000 iterations, the flow network is fixed. The implementation of different alignment methods is the same as described in Section 3 of the main text. <ref type="table" target="#tab_6">Table 4</ref> shows the quantitative comparison between the proposed method and the state-of-the-art VSR methods. Due to the space limit, we only show limited results in the main text; the full version is shown in <ref type="table">Table 8</ref>.</p><p>The architecture of the PSRT-recurrent is shown in <ref type="figure">Figure 11</ref> and described in Section A. For each feature propagation block in PSRT-recurrent, we use 18 MFSABs with shortcut connections every 6 MFSABs. The feature size is set to 120, and the number of attention heads is 6.</p><p>The PSRT-sliding backbone contains 36 MFSABs with shortcut connections every 6 MFSABs. The window size is set to 8. The channel size of the transformer and head size are set to 144 and 6.</p><p>For the PSRT-recurrent with 16 input frames and the PSRT-sliding method, the total training iteration is 600K. The initial learning rate for these experiments is set to 2 ? 10 ?4 . All other settings remain unchanged. For the PSRT-recurrent trained on Vimeo-90K, we follow <ref type="bibr" target="#b5">[6]</ref> to initialize the model with the well-trained model using REDS. We fine-tune it for the other 300K iterations. The initial learning rate is 1 ? 10 ?4 .</p><p>Frame 043, Clip 000, REDS Nearest EDVR <ref type="bibr" target="#b43">[44]</ref> BasicVSR <ref type="bibr" target="#b3">[4]</ref> IconVSR <ref type="bibr" target="#b3">[4]</ref> VRT <ref type="bibr" target="#b21">[22]</ref> BasicVSR++ <ref type="bibr" target="#b5">[6]</ref> Ours GT Frame 005, Clip 011, REDS Nearest EDVR <ref type="bibr" target="#b43">[44]</ref> BasicVSR <ref type="bibr" target="#b3">[4]</ref> IconVSR <ref type="bibr" target="#b3">[4]</ref> VRT <ref type="bibr" target="#b21">[22]</ref> BasicVSR++ <ref type="bibr" target="#b5">[6]</ref> Ours GT Sequence 00001, Clip 0837, Vimeo Nearest EDVR <ref type="bibr" target="#b43">[44]</ref> BasicVSR <ref type="bibr" target="#b3">[4]</ref> IconVSR <ref type="bibr" target="#b3">[4]</ref> VRT <ref type="bibr" target="#b21">[22]</ref> BasicVSR++ <ref type="bibr" target="#b5">[6]</ref> Ours GT Sequence 00010, Clip 0573, Vimeo Nearest EDVR <ref type="bibr" target="#b43">[44]</ref> BasicVSR <ref type="bibr" target="#b3">[4]</ref> IconVSR <ref type="bibr" target="#b3">[4]</ref> VRT <ref type="bibr" target="#b21">[22]</ref> BasicVSR++ <ref type="bibr" target="#b5">[6]</ref> Ours GT Frame 005, Clip city, Vid4</p><p>Nearest EDVR <ref type="bibr" target="#b43">[44]</ref> BasicVSR <ref type="bibr" target="#b3">[4]</ref> IconVSR <ref type="bibr" target="#b3">[4]</ref> VRT <ref type="bibr" target="#b21">[22]</ref> BasicVSR++ <ref type="bibr" target="#b5">[6]</ref> Ours GT Frame 014, Clip city, Vid4</p><p>Nearest EDVR <ref type="bibr" target="#b43">[44]</ref> BasicVSR <ref type="bibr" target="#b3">[4]</ref> IconVSR <ref type="bibr" target="#b3">[4]</ref> VRT <ref type="bibr" target="#b21">[22]</ref> BasicVSR++ <ref type="bibr" target="#b5">[6]</ref> Ours GT <ref type="figure">Figure 14</ref>: Visual comparison of VSR (?4) on REDS, Vimeo-90K and Vid4 datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " D k 5 M 8 e L l 0 v o i C J / 9 S D b C y + o i h J c = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k m k q M e i F 4 9 V 7 A e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q d e P C j i 1 X / k z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O y u r a + s b m 4 W t 4 v b O 7 t 5 + 6 e C w q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R b e a 3 n l B p H s t H M 0 7 Q j + h A 8 p A z a q z 0 c F 7 s l c p u x Z 2 B L B M v J 2 X I U e + V v r r 9 m K U R S s M E 1 b r j u Y n x J 1 Q Z z g R O i 9 1 U Y 0 L Z i A 6 w Y 6 m k E W p / M r t 0 S k 6 t 0 i d h r G x J Q 2 b q 7 4 k J j b Q e R 4 H t j K g Z 6 k U v E / / z O q k J r / 0 J l 0 l q U L L 5 o j A V x M Q k e 5 v 0 u U J m x N g S y h S 3 t x I 2 p I o y Y 8 P J Q v A W X 1 4 m z Y u K d 1 m p 3 l f L t Z s 8 j g I c w w m c g Q d X U I M 7 q E M D G I T w D K / w 5 o y c F + f d + Z i 3 r j j 5 z B H 8 g f P 5 A 6 i q j M w = &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D k 5 M 8 e L l 0 v o i C J / 9 S D b C y + o i h J c = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k m k q M e i F 4 9 V 7 A e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q d e P C j i 1 X / k z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O y u r a + s b m 4 W t 4 v b O 7 t 5 + 6 e C w q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R b e a 3 n l B p H s t H M 0 7 Q j + h A 8 p A z a q z 0 c F 7 s l c p u x Z 2 B L B M v J 2 X I U e + V v r r 9 m K U R S s M E 1 b r j u Y n x J 1 Q Z z g R O i 9 1 U Y 0 L Z i A 6 w Y 6 m k E W p / M r t 0 S k 6 t 0 i d h r G x J Q 2 b q 7 4 k J j b Q e R 4 H t j K g Z 6 k U v E / / z O q k J r / 0 J l 0 l q U L L 5 o j A V x M Q k e 5 v 0 u U J m x N g S y h S 3 t x I 2 p I o y Y 8 P J Q v A W X 1 4 m z Y u K d 1 m p 3 l f L t Z s 8 j g I c w w m c g Q d X U I M 7 q E M D G I T w D K / w 5 o y c F + f d + Z i 3 r j j 5 z B H 8 g f P 5 A 6 i q j M w = &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D k 5 M 8 e L l 0 v o i C J / 9 S D b C y + o i h J c = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k m k q M e i F 4 9 V 7 A e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q d e P C j i 1 X / k z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O y u r a + s b m 4 W t 4 v b O 7 t 5 + 6 e C w q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R b e a 3 n l B p H s t H M 0 7 Q j + h A 8 p A z a q z 0 c F 7 s l c p u x Z 2 B L B M v J 2 X I U e + V v r r 9 m K U R S s M E 1 b r j u Y n x J 1 Q Z z g R O i 9 1 U Y 0 L Z i A 6 w Y 6 m k E W p / M r t 0 S k 6 t 0 i d h r G x J Q 2 b q 7 4 k J j b Q e R 4 H t j K g Z 6 k U v E / / z O q k J r / 0 J l 0 l q U L L 5 o j A V x M Q k e 5 v 0 u U J m x N g S y h S 3 t x I 2 p I o y Y 8 P J Q v A W X 1 4 m z Y u K d 1 m p 3 l f L t Z s 8 j g I c w w m c g Q d X U I M 7 q E M D G I T w D K / w 5 o y c F + f d + Z i 3 r j j 5 z B H 8 g f P 5 A 6 i q j M w = &lt; / l a t e x i t &gt; + LayerNrom MLP &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D k 5 M 8 e L l 0 v o i C J / 9 S D b C y + o i h J c = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k m k q M e i F 4 9 V 7 A e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q d e P C j i 1 X / k z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O y u r a + s b m 4 W t 4 v b O 7 t 5 + 6 e C w q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R b e a 3 n l B p H s t H M 0 7 Q j + h A 8 p A z a q z 0 c F 7 s l c p u x Z 2 B L B M v J 2 X I U e + V v r r 9 m K U R S s M E 1 b r j u Y n x J 1 Q Z z g R O i 9 1 U Y 0 L Z i A 6 w Y 6 m k E W p / M r t 0 S k 6 t 0 i d h r G x J Q 2 b q 7 4 k J j b Q e R 4 H t j K g Z 6 k U v E / / z O q k J r / 0 J l 0 l q U L L 5 o j A V x M Q k e 5 v 0 u U J m x N g S y h S 3 t x I 2 p I o y Y 8 P J Q v A W X 1 4 m z Y u K d 1 m p 3 l f L t Z s 8 j g I c w w m c g Q d X U I M 7 q E M D G I T w D K / w 5 o y c F + f d + Z i 3 r j j 5 z B H 8 g f P 5 A 6 i q j M w = &lt; / l a t e x i t &gt; + LayerNrom MLP &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D k 5 M 8 e L l 0 v o i C J / 9 S D b C y + o i h J c = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k m k q M e i F 4 9 V 7 A e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q d e P C j i 1 X / k z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O y u r a + s b m 4 W t 4 v b O 7 t 5 + 6 e C w q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R b e a 3 n l B p H s t H M 0 7 Q j + h A 8 p A z a q z 0 c F 7 s l c p u x Z 2 B L B M v J 2 X I U e + V v r r 9 m K U R S s M E 1 b r j u Y n x J 1 Q Z z g R O i 9 1 U Y 0 L Z i A 6 w Y 6 m k E W p / M r t 0 S k 6 t 0 i d h r G x J Q 2 b q 7 4 k J j b Q e R 4 H t j K g Z 6 k U v E / / z O q k J r / 0 J l 0 l q U L L 5 o j A V x M Q k e 5 v 0 u U J m x N g S y h S 3 t x I 2 p I o y Y 8 P J Q v A W X 1 4 m z Y u K d 1 m p 3 l f L t Z s 8 j g I c w w m c g Q d X U I M 7 q E M D G I T w D K / w 5 o y c F + f d + Z i 3 r j j 5 z B H 8 g f P 5 A 6 i q j M w = &lt; / l a t e x i t &gt; + LayerNrom MLP &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D k 5 M 8 e L l 0 v o i C J / 9 S D b C y + o i h J c = " &gt; A A A B 6 X i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k m k q M e i F 4 9 V 7 A e 0 o W y 2 k 3 b p Z h N 2 N 0 I p / Q d e P C j i 1 X / k z X / j p s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O y u r a + s b m 4 W t 4 v b O 7 t 5 + 6 e C w q e N U M W y w W M S q H V C N g k t s G G 4 E t h O F N A o E t o L R b e a 3 n l B p H s t H M 0 7 Q j + h A 8 p A z a q z 0 c F 7 s l c p u x Z 2 B L B M v J 2 X I U e + V v r r 9 m K U R S s M E 1 b r j u Y n x J 1 Q Z z g R O i 9 1 U Y 0 L Z i A 6 w Y 6 m k E W p / M r t 0 S k 6 t 0 i d h r G x J Q 2 b q 7 4 k J j b Q e R 4 H t j K g Z 6 k U v E / / z O q k J r / 0 J l 0 l q U L L 5 o j A V x M Q k e 5 v 0 u U J m x N g S y h S 3 t x I 2 p I o y Y 8 P J Q v A W X 1 4 m z Y u K d 1 m p 3 l f L t Z s 8 j g I c w w m c g Q d X U I M 7 q E M D G I T w D K / w 5 o y c F + f d + Z i 3 r j j 5 z B H 8 g f P 5 A 6 i q j M w = &lt; / l a t e x i t &gt; +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The figure on the left shows the validation curves tested on Vimeo-90K test set. On the right are the histogram distribution of the estimated optical flow at different training iterations. The moment when the optical flow distribution changes corresponds to the moment when the performance of the VSR Transformer with alignment increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. In our experiments, we mainly study ?4 VSR task. We use bicubic interpolation to produce LR video frames. Peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) are used for evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>The pipeline of the proposed Patch Alignment method: x partition the input frames to patches according to the window partition of Transformer, y calculate the mean motion vector for each patch, z find the corresponding patches in the supporting frames, and { move the entire supporting patches to their corresponding position. The visualization comparison of image alignment and the proposed patch alignment.direction. When training after 50,000 iterations, the fine-tuned flow vanishes. At this time, the VSR Transformer with alignment is equivalent to the one without alignment. This phenomenon does not appear on VSR-CNN. This experiment is instructive. On the one hand, most of the movements in the Vimeo-90K dataset are smaller than the Transformer's window size. According toFigure 4 (a), alignment is detrimental to training at this situation. The fine-tuned flow estimator seems aware of this knowledge and learns to improve performance by forcing flow value to all zeros. On the other hand, the adaptability of the model is surprising. The model will choose the policy that maximizes the training objective, even if that policy completely disables part of the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 &lt; 2 MFSAB</head><label>12</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " t m 0 h p v 7 O s k H 4 L w i N k v n U S h v + e W c = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 e K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f T m F l d W 1 9 o 7 h Z 2 t r e 2 d 0 r 7 x 8 0 T Z x q x h s s l r F u B 9 R w K R R v o E D J 2 4 n m N A o k b w W j m 6 n f e u L a i F g 9 4 D j h f k Q H S o S C U b T S f f s R e + W K W 3 V n I M v E y 0 k F c t R 7 5 a 9 u P 2 Z p x B U y S Y 3 p e G 6 C f k Y 1 C i b 5 p N R N D U 8 o G 9 E B 7 1 i q a M S N n 8 1 O n Z A T q / R J G G t b C s l M / T 2 R 0 c i Y c R T Y z o j i 0 C x 6 U / E / r 5 N i e O V n Q i U p c s X m i 8 J U E o z J 9 G / S F 5 o z l G N L K N P C 3 k r Y k G r K 0 K Z T s i F 4 i y 8 v k + Z Z 1 b u o n t + d V 2 r X e R x F O I J j O A U P L q E G t 1 C H B j A Y w D O 8 w p s j n R f n 3 f m Y t x a c f O Y Q / s D 5 / A F B 5 Y 3 L &lt; / l a t e x i t &gt; X t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m / N I 8 t x 1 Y Z t + d 8 o k 1 E k h h 7 M y p A w = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 s S R S 1 G P R i 8 c K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / g g v H h T x 6 u / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f z s r q 2 v r G Z m G r u L 2 z u 7 d f O j h s m j j V j D d Y L G P d D q j h U i j e Q I G S t x P N a R R I 3 g p G t 1 O / 9 c S 1 E b F 6 w H H C / Y g O l A g F o 2 i l V v s x w 3 N v 0 i u V 3 Y o 7 A 1 k m X k 7 K k K P e K 3 1 1 + z F L I 6 6 Q S W p M x 3 M T 9 D O q U T D J J 8 V u a n h C 2 Y g O e M d S R S N u / G x 2 7o S c W q V P w l j b U k h m 6 u + J j E b G j K P A d k Y U h 2 b R m 4 r / e Z 0 U w 2 s / E y p J k S s 2 X x S m k m B M p r + T v t C c o R x b Q p k W 9 l b C h l R T h j a h o g 3 B W 3 x 5 m T Q v K t 5 l p X p f L d d u 8 j g K c A w n c A Y e X E E N 7 q A O D W A w g m d 4 h T c n c V 6 c d + d j 3 r r i 5 D N H 8 A f O 5 w / i s o 9 J &lt; / l a t e x i t &gt; X t l a t e x i t s h a 1 _ b a s e 6 4 = " q C R 5 p m A i / z R Z 6 j T j T Z i + t o D y E O U = " &gt; A AA B 7 n i c b V D L S g N B E O y N r x h f U Y 9 e B o P g x b A b g n o M e v E Y w T w g W c P s Z J I M m Z 1 d Z n q F s O Q j v H h Q x K v f 4 8 2 / c Z L s Q R M L G o q q b r q 7 g l g K g 6 7 7 7 e T W 1 j c 2 t / L b h Z 3 d v f 2 D 4 u F R 0 0 S J Z r z B I h n p d k A N l 0 L x B g q U v B 1 r T s N A 8 l Y w v p 3 5 r S e u j Y j U A 0 5 i 7 o d 0 q M R A M I p W a r U f U 7 y o T H v F k l t 2 5 y C r x M t I C T L U e 8 W v b j 9 i S c g V M k m N 6 X h u j H 5 K N Q o m + b T Q T Q y P K R v T I e 9 Y q m j I j Z / O z 5 2 S M 6 v 0 y S D S t h S S u f p 7 I q W h M Z M w s J 0 h x Z F Z 9 m b i f 1 4 n w c G 1 n w o V J 8 g V W y w a J J J g R G a / k 7 7 Q n K G c W E K Z F v Z W w k Z U U 4 Y 2 o Y I N w V t + e Z U 0 K 2 X v s l y 9 r 5 Z q N 1 k c e T i B U z g H D 6 6 g B n d Q h w Y w G M M z v M K b E z s v z r v z s W j N O d n M M f y B 8 / k D 5 D e P S g = = &lt; / l a t e x i t &gt; X t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :Figure 12 : 5 VSR</head><label>11125</label><figDesc>The framework of the used recurrent-based VSR Transformer. The comparison of with different training frames. Training with 30 frames leads to better performance at the cost of a larger training cost. Trans., patch alignment VSR Trans., no alignment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>and discussed in Section 4.2</figDesc><table><row><cell></cell><cell>Exp. Index</cell><cell>Method</cell><cell>Alignment</cell><cell>Remark</cell><cell cols="2">Vimeo90K-T PSNR SSIM</cell><cell>REDS4 PSNR SSIM</cell></row><row><cell></cell><cell>1</cell><cell>VSR-CNN</cell><cell>Image alignment</cell><cell>Finetune flow</cell><cell>36.13</cell><cell>0.9342</cell><cell>29.81 0.8541</cell></row><row><cell></cell><cell>2</cell><cell>VSR-CNN</cell><cell>No alignment</cell><cell></cell><cell>36.24</cell><cell>0.9359</cell><cell>28.95 0.8280</cell></row><row><cell></cell><cell>3</cell><cell cols="2">VSR Transformer Image alignment</cell><cell>Fix flow</cell><cell>36.87</cell><cell>0.9429</cell><cell>30.25 0.8637</cell></row><row><cell></cell><cell>4</cell><cell cols="2">VSR Transformer Image alignment</cell><cell>Finetune flow</cell><cell cols="2">37.44  *  0.9472  *</cell><cell>30.43 0.8677</cell></row><row><cell></cell><cell>5</cell><cell cols="3">VSR Transformer Feature alignment Finetune flow</cell><cell>37.36</cell><cell>0.9468</cell><cell>30.74 0.8740</cell></row><row><cell></cell><cell>6</cell><cell cols="2">VSR Transformer No alignment</cell><cell>Window size 8</cell><cell>37.43</cell><cell>0.9470</cell><cell>30.56 0.8696</cell></row><row><cell></cell><cell>7</cell><cell cols="2">VSR Transformer No alignment</cell><cell>Window size 16</cell><cell>37.46</cell><cell>0.9474</cell><cell>30.81 0.8745</cell></row><row><cell></cell><cell cols="2">Validation Performance on Vimeo90K</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>37.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>37.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>36.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PNSR</cell><cell>36.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>35.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>35.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>34.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10 4</cell><cell>10 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Iterations</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>VSR Trans. w/ image alignment VSR Trans. w/o alignment VSR Trans. w/ image alignment, flow fixed VSR CNN w/o alignment VSR CNN w/ image alignment</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of VSR Transformers with different alignment methods on the REDS4 dataset.</figDesc><table><row><cell>#</cell><cell>Alignment Method No Ali. Img. Ali. Feat. Ali. FGDC Img. Feat. BI Position Resampling Params. NN (M)</cell><cell>REDS4 PSNR / SSIM</cell></row><row><cell>1</cell><cell>12.9</cell><cell>30.92 / 0.8759</cell></row><row><cell>2</cell><cell>12.9</cell><cell>30.84 / 0.8752</cell></row><row><cell>3</cell><cell>14.8</cell><cell>31.06 / 0.8792</cell></row><row><cell>4</cell><cell>14.8</cell><cell>31.11 / 0.8801</cell></row><row><cell>5</cell><cell>16.1</cell><cell>31.11 / 0.8804</cell></row></table><note>of about 70% pixels is less than eight on the REDS test set. This makes the model without alignment still perform better than the image alignment on this test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison (PSNR? and SSIM?) on the REDS4<ref type="bibr" target="#b33">[34]</ref> dataset, Vid4<ref type="bibr" target="#b27">[28]</ref>, Vimeo-90K-T [48] dataset for 4? VSR task. Red indicates the best and blue indicates the second best performance (best view in color) in each group of experiments.</figDesc><table><row><cell>Method</cell><cell>Frames REDS/Vimeo</cell><cell>Params (M)</cell><cell>REDS4 PSNR SSIM</cell><cell cols="2">Vimeo-90K-T PSNR SSIM</cell><cell cols="2">Vid4 PSNR SSIM</cell></row><row><cell>EDVR [44]</cell><cell>5/7</cell><cell>20.6</cell><cell>31.09 0.8800</cell><cell cols="2">37.61 0.9489</cell><cell cols="2">27.35 0.8264</cell></row><row><cell>VSR-T [3]</cell><cell>5/7</cell><cell>32.6</cell><cell>31.19 0.8815</cell><cell cols="2">37.71 0.9494</cell><cell cols="2">27.36 0.8258</cell></row><row><cell>PSRT-sliding</cell><cell>5/-</cell><cell>14.8</cell><cell>31.32 0.8834</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VRT</cell><cell>6/-</cell><cell>30.7</cell><cell>31.60 0.8888</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PSRT-recurrent</cell><cell>6/-</cell><cell>10.8</cell><cell>31.88 0.8964</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BasicVSR [4]</cell><cell>15/14</cell><cell>6.3</cell><cell>31.42 0.8909</cell><cell cols="2">37.18 0.9450</cell><cell cols="2">27.24 0.8251</cell></row><row><cell>IconVSR [4]</cell><cell>15/14</cell><cell>8.7</cell><cell>31.67 0.8948</cell><cell cols="2">37.47 0.9476</cell><cell cols="2">27.39 0.8279</cell></row><row><cell>BasicVSR++ [6]</cell><cell>30/14</cell><cell>7.3</cell><cell>32.39 0.9069</cell><cell cols="2">37.79 0.9500</cell><cell cols="2">27.79 0.8400</cell></row><row><cell>VRT</cell><cell>16/7</cell><cell>35.6</cell><cell>32.19 0.9006</cell><cell cols="2">38.20 0.9530</cell><cell cols="2">27.93 0.8425</cell></row><row><cell>PSRT-recurrent</cell><cell>16/14</cell><cell>13.4</cell><cell>32.72 0.9106</cell><cell cols="2">38.27 0.9536</cell><cell cols="2">28.07 0.8485</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>In the recurrent VSR Transformers in this Table, we use 12 MFSABs with shortcut connections every 3 MFSABs for each feature propagation block. The feature size is set to 100, and the number of attention heads is 4. The baseline is the original BasicVSR++ model that uses FGDC and CNN backbone. Replacing the CNN with Transformer blocks can bring a PSNR improvement of 0.5dB on the REDS test set.</figDesc><table><row><cell>Input Frame Sequence</cell></row><row><cell>Reconstruction</cell></row><row><cell>Output HR Frame Sequence</cell></row></table><note>However, the FGDC alignment used 7.8M parameters, accounting for almost half of all parameters. Replacing the FGDC alignment with the proposed patch alignment achieves competitive results without introducing additional parameters -our method saves 7.8M of parameters. This experiment illustrates the effectiveness of the proposed patch alignment method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the different alignment methods and backbone networks. The results are tested on REDS4<ref type="bibr" target="#b33">[34]</ref> dataset for 4? video super-resolution on RGB channels.</figDesc><table><row><cell>Method</cell><cell cols="3">Frames Params(M) PSNR</cell><cell>SSIM</cell></row><row><cell>BasicVSR++ [6], The baseline model</cell><cell>6</cell><cell>7.3</cell><cell cols="2">31.38 0.8898</cell></row><row><cell>Flow-guided Deformable Alignment + Transformer</cell><cell>6</cell><cell>18.6</cell><cell cols="2">31.89 0.8967</cell></row><row><cell>Patch Alignment + Transformer</cell><cell>6</cell><cell>10.8</cell><cell cols="2">31.88 0.8964</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of patch alignment method trained using the Vimeo-90K dataset. The results are tested on Vimeo-90K-T and Vid4 for 4? video super-resolution on Y channel. The experiments with 7 training frames were only trained from scratch for 300K iterations.</figDesc><table><row><cell>Method</cell><cell cols="2">Frames Params</cell><cell>Vimeo-90K-T PSNR SSIM</cell><cell>Vid4 PSNR SSIM</cell></row><row><cell>PSRT-recurrent w/o alignment</cell><cell>7</cell><cell>12.0</cell><cell>37.87 0.9508</cell><cell>27.71 0.8403</cell></row><row><cell>PSRT-recurrent</cell><cell>7</cell><cell>13.4</cell><cell>37.80 0.9502</cell><cell>27.72 0.8409</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>The comparison of the parameter numbers, FLOPs and the runtime for different methods.</figDesc><table><row><cell cols="2">Method</cell><cell cols="6">Parameters (M) FLOPs (T) Runtime (ms)</cell></row><row><cell>DUF</cell><cell></cell><cell></cell><cell>5.8</cell><cell cols="2">2.34</cell><cell>974</cell><cell></cell></row><row><cell>RBPN</cell><cell></cell><cell></cell><cell>12.2</cell><cell cols="2">8.51</cell><cell>1507</cell><cell></cell></row><row><cell cols="2">EDVR [44]</cell><cell></cell><cell>20.6</cell><cell cols="2">2.95</cell><cell>378</cell><cell></cell></row><row><cell cols="2">VSRT [3]</cell><cell></cell><cell>32.6</cell><cell>1.6</cell><cell></cell><cell>-</cell><cell></cell></row><row><cell cols="2">VRT [22]</cell><cell></cell><cell>35.6</cell><cell>1.3</cell><cell></cell><cell>243</cell><cell></cell></row><row><cell cols="2">PSRT-recurrent (Ours)</cell><cell></cell><cell>13.4</cell><cell>1.5</cell><cell></cell><cell>812</cell><cell></cell></row><row><cell cols="9">Table 8: Quantitative comparison (PSNR? and SSIM?) on the REDS4 [34] dataset, Vid4 [28], Vimeo-</cell></row><row><cell cols="9">90K-T [48] dataset for 4? VSR task. Red indicates the best and blue indicates the second best</cell></row><row><cell cols="5">performance (best view in color) in each group of experiments.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Frames REDS/Vimeo</cell><cell>Params (M)</cell><cell cols="2">REDS4 PSNR SSIM</cell><cell cols="2">Vimeo-90K-T PSNR SSIM</cell><cell cols="2">Vid4 PSNR SSIM</cell></row><row><cell>Bicubic</cell><cell>-/-</cell><cell>-</cell><cell cols="2">26.14 0.7292</cell><cell cols="2">31.32 0.8684</cell><cell cols="2">23.78 0.6347</cell></row><row><cell>RCAN [55]</cell><cell>-/-</cell><cell>-</cell><cell cols="2">28.78 0.8200</cell><cell cols="2">35.35 0.9251</cell><cell cols="2">25.46 0.7395</cell></row><row><cell>SwinIR [23]</cell><cell>-/-</cell><cell>11.9</cell><cell cols="2">29.05 0.8269</cell><cell cols="2">35.67 0.9287</cell><cell cols="2">25.68 0.7491</cell></row><row><cell>TOFlow [48]</cell><cell>5/7</cell><cell>-</cell><cell cols="2">27.98 0.7990</cell><cell cols="2">33.08 0.9054</cell><cell cols="2">25.89 0.7651</cell></row><row><cell>DUF</cell><cell>7/7</cell><cell>5.8</cell><cell cols="2">28.63 0.8251</cell><cell>-</cell><cell>-</cell><cell cols="2">27.33 0.8319</cell></row><row><cell>PFNL</cell><cell>7/7</cell><cell>3.0</cell><cell cols="2">29.63 0.8502</cell><cell cols="2">36.14 0.9363</cell><cell cols="2">26.73 0.8029</cell></row><row><cell>RBPN</cell><cell>7/7</cell><cell>12.2</cell><cell cols="2">30.09 0.8590</cell><cell cols="2">37.07 0.9435</cell><cell cols="2">27.12 0.8180</cell></row><row><cell>EDVR [44]</cell><cell>5/7</cell><cell>20.6</cell><cell cols="2">31.09 0.8800</cell><cell cols="2">37.61 0.9489</cell><cell cols="2">27.35 0.8264</cell></row><row><cell>MuCAN [21]</cell><cell>5/7</cell><cell>-</cell><cell cols="2">30.88 0.8750</cell><cell cols="2">37.32 0.9465</cell><cell>-</cell><cell>-</cell></row><row><cell>VSR-T [3]</cell><cell>5/7</cell><cell>32.6</cell><cell cols="2">31.19 0.8815</cell><cell cols="2">37.71 0.9494</cell><cell cols="2">27.36 0.8258</cell></row><row><cell>PSRT-sliding</cell><cell>5/-</cell><cell>14.8</cell><cell cols="2">31.32 0.8834</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VRT</cell><cell>6/-</cell><cell>30.7</cell><cell cols="2">31.60 0.8888</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PSRT-recurrent</cell><cell>6/-</cell><cell>10.8</cell><cell cols="2">31.88 0.8964</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BasicVSR [4]</cell><cell>15/14</cell><cell>6.3</cell><cell cols="2">31.42 0.8909</cell><cell cols="2">37.18 0.9450</cell><cell cols="2">27.24 0.8251</cell></row><row><cell>IconVSR [4]</cell><cell>15/14</cell><cell>8.7</cell><cell cols="2">31.67 0.8948</cell><cell cols="2">37.47 0.9476</cell><cell cols="2">27.39 0.8279</cell></row><row><cell>BasicVSR++ [6]</cell><cell>30/14</cell><cell>7.3</cell><cell cols="2">32.39 0.9069</cell><cell cols="2">37.79 0.9500</cell><cell cols="2">27.79 0.8400</cell></row><row><cell>VRT</cell><cell>16/7</cell><cell>35.6</cell><cell cols="2">32.19 0.9006</cell><cell cols="2">38.20 0.9530</cell><cell cols="2">27.93 0.8425</cell></row><row><cell>RVRT [24]</cell><cell>30/14</cell><cell>10.8</cell><cell cols="2">32.75 0.9113</cell><cell cols="2">38.15 0.9527</cell><cell cols="2">27.99 0.8462</cell></row><row><cell>PSRT-recurrent</cell><cell>16/14</cell><cell>13.4</cell><cell cols="2">32.72 0.9106</cell><cell cols="2">38.27 0.9536</cell><cell cols="2">28.07 0.8485</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Patch Alignment for Recurrent-based VSR Transformer</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="933" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06847</idno>
		<title level="m">Video super-resolution transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Basicvsr: The search for essential components in video super-resolution and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding deformable alignment in video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Basicvsr++: Improving video super-resolution with enhanced propagation and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross aggregation transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linghe</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interpreting super-resolution networks with local attribution maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghyun</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiboot vsr: Multi-stage multi-reference bootstrapping for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratheesh</forename><surname>Kalarot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video super-resolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on computational imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatio-temporal transformer network for video restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tae Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video super-resolution using non-simultaneous fully recurrent convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1342" to="1355" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fast spatio-temporal residual network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mucan: Multi-correspondence aggregation network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12288</idno>
		<title level="m">Vrt: A video restoration transformer</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02146</idno>
		<title level="m">Recurrent video restoration transformer with guided deformable attention</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video super-resolution via deep draft-ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Flow-guided sparse transformer for video deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On bayesian adaptive video super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="346" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust video super-resolution with learned temporal dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video super-resolution based on deep learning: a comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhubo</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanhua</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="page" from="1" to="55" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generative adversarial networks and perceptual losses for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Lopez-Tapia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3312" to="3327" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Baik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokil</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Frame-recurrent video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tdan: Temporally-deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformable non-local network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dewei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuangchuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longcun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianfang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="177734" to="177744" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">BasicSR: Open source image and video restoration toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
		<ptr target="https://github.com/xinntao/BasicSR" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03106</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-memory convolutional neural network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2530" to="2544" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video enhancement with task-oriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1106" to="1125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Frame and feature-context video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deformable 3d convolution for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1500" to="1504" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Restormer: Efficient transformer for high-resolution image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linghe</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01427</idno>
		<title level="m">Accurate image restoration with attention retractable transformer</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
