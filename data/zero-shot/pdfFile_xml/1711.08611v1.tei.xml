<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Relevance Matching Model for Ad-hoc Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-23">23 Nov 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
							<email>guojiafeng@ict.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
							<email>fanyixing@software.ict.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Intelligent Information Retrieval</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
							<email>croft@cs.umass.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Center for Intelligent Information Retrieval</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cas</forename><surname>Key</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Lab of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep Relevance Matching Model for Ad-hoc Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-23">23 Nov 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2983323.2983769</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Relevance Matching</term>
					<term>Semantic Matching</term>
					<term>Neural Models</term>
					<term>Ad-hoc Retrieval</term>
					<term>Ranking Models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Machine learning methods have been successfully applied to information retrieval (IR) in recent years. Typically, a ranking function which produces a relevance score given a Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. query and document pair is learned based on a set of human defined features. However, handcrafting features can be time-consuming, incomplete and over-specified. On the other hand, deep neural networks, as a representation learning method, are able to discover from the training data the hidden structures and features at different levels of abstraction that are useful for the tasks. Recently, deep models have been applied to a variety of applications in computer vision <ref type="bibr" target="#b15">[16]</ref>, speech recognition <ref type="bibr" target="#b9">[10]</ref> and NLP <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b16">17]</ref>, and have yielded significant performance improvements. Given the success of deep learning in these domains, it seems that deep learning should have a major impact on IR. However, there have been few positive results of deep models on IR tasks, especially ad-hoc retrieval tasks, until now.</p><p>Without loss of generality, when applying deep models to ad-hoc retrieval, the task is typically formalized as a matching problem between two pieces of text (i.e., the query and document). Such a matching problem formalization is often considered general in the sense that it can cover both ad-hoc retrieval tasks as well as many NLP tasks such as paraphrase identification, question answering (QA), and automatic conversation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11]</ref>. A variety of deep matching models have been proposed to solve this matching problem, which can be categorized into two types according to their model architecture. One is the representation-focused model, which tries to build a good representation for a single text with a deep neural network, and then conducts matching between the compositional and abstract text representations. Examples include DSSM <ref type="bibr" target="#b11">[12]</ref>, C-DSSM <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref> and ARC-I <ref type="bibr" target="#b10">[11]</ref>. The other is the interaction-focused model, which first builds local interactions (i.e., local matching signals) between two pieces of text, and then uses deep neural networks to learn hierarchical interaction patterns for matching. Examples include DeepMatch <ref type="bibr" target="#b16">[17]</ref>, ARC-II <ref type="bibr" target="#b10">[11]</ref> and MatchPyramid <ref type="bibr" target="#b18">[19]</ref>.</p><p>However, in this work, we argue that the matching problems in many NLP tasks and the ad-hoc retrieval task are fundamentally different. Most NLP tasks concern semantic matching, i.e., identifying the semantic meaning and inferring the semantic relations between two pieces of text, while the ad-hoc retrieval task is mainly about relevance matching, i.e., identifying whether a document is relevant to a given query. We point out three major differences between these two matching problems which may lead to significantly different architecture design for the deep matching models. We also show that most existing deep matching models are designed for semantic matching rather than relevance matching.</p><p>Based on these differences, we propose a deep relevance matching model (DRMM) for ad-hoc retrieval by explicitly modeling the three major factors in relevance matching. Overall, our model is an interaction-focused model which employs a joint deep architecture at the query term 1 level for relevance matching. Specifically, we first build local interactions between each pair of terms from a query and a document based on term embeddings. For each query term, we map the variable-length local interactions into a fixed-length matching histogram. Based on this fixed-length matching histogram, we then employ a feed forward matching network to learn hierarchical matching patterns and produce a matching score. Finally, the overall matching score is generated by aggregating the scores from each query term with a term gating network computing the aggregation weights. We show how our major model designs, including matching histogram mapping, a feed forward matching network, and a term gating network, address the three key factors in relevance matching for ad-hoc retrieval. We evaluate the effectiveness of the proposed DRMM based on two representative ad-hoc retrieval benchmark collections. For comparison, we take into account some wellknown traditional retrieval models, as well as several stateof-the-art deep matching models either designed for the general matching problem or proposed specifically for the adhoc retrieval task. The empirical results show that the existing deep matching models cannot compete with the traditional retrieval models on these benchmark collections, while our model can outperform all the baseline models significantly in terms of all the evaluation metrics.</p><p>The major contributions of this paper include:</p><p>1. We point out three major differences between semantic matching and relevance matching, which may lead to significantly different architecture design of the deep matching models.</p><p>2. We propose a novel deep relevance matching model for ad-hoc retrieval by explicitly addressing the three key factors of relevance matching.</p><p>3. We conduct rigorous comparisons over state-of-the-art retrieval models on benchmark collections and analyze the deficiencies of existing deep matching models and advantages of the DRMM. <ref type="bibr" target="#b0">1</ref> Here we use term to denote the indexed units in search systems, which could be stemmed words or phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">AD-HOC RETRIEVAL AS A MATCHING PROBLEM</head><p>According to existing literature <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>, the core problem in ad-hoc retrieval, i.e., the computation of the relevance for a document given a particular query, can be formalized as a text matching problem as follows. Given two texts T1 and T2, the degree of matching is typically measured as a score produced by a scoring function based on the representation of each text:</p><formula xml:id="formula_0">match(T1, T2) = F (?(T1), ?(T2)),</formula><p>where ? is a function to map each text to a representation vector, and F is the scoring function based on the interactions between them. Such a text matching problem is considered general since it also describes many NLP tasks, such as paraphrase identification, question answering, and automatic conversation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11]</ref>. A variety of deep matching models have been proposed either for the specific ad-hoc retrieval task or for the general matching problem.</p><p>Depending on how you choose the two functions, existing deep matching models can be categorized into two types. The first one, the representation-focused model, tries to build a good representation for a single text with a deep neural network, and then conducts matching between two compositional and abstract text representations. In this approach, ? is a complex representation mapping function while F is a relatively simple matching function. For example, in DSSM <ref type="bibr" target="#b11">[12]</ref>, ? is a feed forward neural network, while F is the cosine similarity function. In C-DSSM <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref>, ? is a convolutional neural network (CNN) <ref type="bibr" target="#b15">[16]</ref>, while F is the cosine similarity function. In ARC-I <ref type="bibr" target="#b10">[11]</ref>, ? is a CNN, while F is a multi-layer perceptron (MLP). Without loss of generality, all the model architectures of representationfocused models can be viewed as a Siamese (symmetric) architecture over the text inputs, as shown in <ref type="figure" target="#fig_1">Figure 1</ref> The second one, the interaction-focused model, first builds the local interactions between two texts based on some basic representations, and then uses deep neural networks to learn the hierarchical interaction patterns for matching. In this approach, ? is usually a simple mapping function while F is a complex deep model. For example, in DeepMatch <ref type="bibr" target="#b16">[17]</ref>, ? simply maps each text to a sequence of words, while F is a feed forward neural network powered by a topic model over the word interaction matrix. In ARC-II <ref type="bibr" target="#b10">[11]</ref> and MatchPyramid <ref type="bibr" target="#b18">[19]</ref>, ? maps each text to a sequence of word vectors, while F is a CNN over the interaction matrix between word vectors from the two texts. Without loss of generality, all the model architectures of interaction-focused models can be viewed as a hierarchical deep architecture over the local interaction matrix, as shown in <ref type="figure" target="#fig_1">Figure 1</ref></p><formula xml:id="formula_1">(b).</formula><p>Although various deep matching models have been proposed under such a general matching problem formalization, most of them have only been demonstrated to be effective on a set of NLP tasks such as paraphrase identification and QA <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>. There have been few positive results on the ad-hoc retrieval task. Even the deep models specially designed for Web search, e.g., DSSM and C-DSSM, were only evaluated on &lt;query, doc title&gt; pairs which are not a typical ad-hoc retrieval setting. If we directly apply these deep matching models on some benchmark retrieval collections, e.g. TREC collections, we find relatively poor performance compared to traditional ranking models, such as the language model <ref type="bibr" target="#b30">[31]</ref> and BM25 <ref type="bibr" target="#b21">[22]</ref>. All these observations raise some questions such as: Is matching in ad-hoc retrieval really the same as that in NLP tasks? Are the existing deep matching models suitable for the ad-hoc retrieval task?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SEMANTIC MATCHING VS. RELEVANCE MATCHING</head><p>In this section, we discuss the differences between text matching in ad-hoc retrieval and other NLP tasks. The matching in many NLP tasks, such as paraphrase identification, question answering and automatic conversation, is mainly concerned with semantic matching, i.e., identifying the semantic meaning and inferring the semantic relations between two pieces of text. In these semantic matching tasks, the two texts are usually homogeneous and consist of a few natural language sentences, such as questions/answer sentences, or dialogs. To infer the semantic relations between natural language sentences, semantic matching emphasizes the following three factors:</p><p>Similarity matching signals: It is important, or critical to capture the semantic similarity/relatedness between words, phrases and sentences, as compared with exact matching signals. For example, in paraphrase identification, one needs to identify whether two sentences convey the same meaning with different expressions. In automatic conversation, one aims to find a proper response semantically related to the previous dialog, which may not share any common words or phrases between them.</p><p>Compositional meanings: Since texts in semantic matching usually consist of natural language sentences with grammatical structures, it is more beneficial to use the compositional meaning of the sentences based on such grammatical structures rather than treating them as a set/sequence of words <ref type="bibr" target="#b24">[25]</ref>. For example, in question answering, most questions have clear grammatical structures which can help identify the compositional meaning that reflects what the question is about.</p><p>Global matching requirement: Semantic matching usually treats the two pieces of text as a whole to infer the semantic relations between them, leading to a global matching requirement. This is partially related to the fact that most texts in semantic matching have limited lengths and thus the topic scope is concentrated. For example, two sentences are considered as paraphrases if the whole meaning is the same, and a good answer fully answers the question.</p><p>The matching in ad-hoc retrieval, on the contrary, is mainly about relevance matching, i.e., identifying whether a document is relevant to a given query. In this task, the query is typically short and keyword based, while the document can vary considerably in length, from tens of words to thousands or even tens of thousands of words. To estimate the relevance between a query and a document, relevance matching is focused on the following three factors:</p><p>Exact matching signals: Although term mismatch is a critical problem in ad-hoc retrieval and has been tackled using different semantic similarity signals, the exact matching of terms in documents with those in queries is still the most important signal in ad-hoc retrieval due to the indexing and search paradigm in modern search engines. For example, Fang and Zhai <ref type="bibr" target="#b6">[7]</ref> proposed the semantic term matching constraint which states that matching an original query term exactly should always contribute no less to the relevance score than matching a semantically related term multiple times. This also explains why some traditional retrieval models, e.g., BM25, can work reasonably well purely based on exact matching signals.</p><p>Query term importance: Since queries are mainly short and keyword based without complex grammatical structures in ad-hoc retrieval, it is important to take into account term importance, while the compositional relation among the query terms is usually the simple "and" relation in operational search. For example, given the query "bitcoin news", a relevant document is expected to be about "bitcoin" and "news", where the term "bitcoin" is more important than "news" in the sense that a document describing other aspects of "bitcoin" would be more relevant than a document describing "news" of other things. In the literature, there have been many formal studies on retrieval models showing the importance of term discrimination <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Diverse matching requirement: In ad-hoc retrieval, a relevant document can be very long and there have been different hypotheses concerning document length <ref type="bibr" target="#b21">[22]</ref> in the literature, leading to a diverse matching requirement. Specifically, the Verbosity Hypothesis assumes that a long document is like a short document, covering a similar scope but with more words. In this case, the relevance matching might be global if we assume short documents have a concentrated topic. On the contrary, the Scope Hypothesis assumes a long document consists of a number of unrelated short documents concatenated together. In this way, the relevance matching could happen in any part of a relevant document, and we do not require the document as a whole to be relevant to a query.</p><p>As we can see, there are significant differences between relevance matching in ad-hoc retrieval and semantic matching in many NLP tasks. These differences affect the design of deep model architectures and it may be difficult to find a "one-fit-all" solution to such different matching problems. If we revisit the existing deep matching models, we find that most of them concern semantic matching rather than relevance matching. For example, the representation-focused models such as DSSM, C-DSSM and ARC-I focus on the compositional meaning of the texts and fit the global matching requirement. In these models, detailed matching signals and, especially, exact matching signals are lost since they defer the interaction between two texts until their individual representations have been created <ref type="bibr" target="#b10">[11]</ref>. Although the interaction-focused models such as DeepMatch, ARC-II and MatchPyramid preserve both exact and similarity matching signals, they do not differentiate these signals but treat them as equally important. These models focus on learning the composition of local interactions without addressing term importance. In particular, the convolutional structures in ARC-II and MatchPyramid are designed to learn positional regularities, which may work well under the global matching requirement but fail under the diverse matching requirement.(There is more discussion on this in Section 4.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DEEP RELEVANCE MATCHING MODEL</head><p>Based on the above analysis, we propose a novel deep matching model specifically designed for relevance matching in ad-hoc retrieval by explicitly addressing the three factors described in Section 3. We refer to our model as a deep relevance matching model (DRMM). Overall, our model is similar to interaction-focused models rather than representation-focused models since the latter would inevitably lose the detailed matching signals which are critical for relevance matching in ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level over the local interactions between query and document terms for relevance matching. We first build local interactions between each pair of terms from a query and a document based on term embeddings. For each query term, we then transform the variable-length local interactions into a fixed-length matching histogram. Based on the fixed-length matching histogram, we employ a feed forward matching network to learn hierarchical matching patterns and produce a matching score for each query term. Finally, the overall matching score is generated by aggregating the scores from each single query term with a term gating network computing the aggregation weights. The model architecture is depicted in <ref type="figure" target="#fig_3">Figure 2</ref>.</p><p>More formally, suppose both query and document are represented as a set of term vectors denoted by q={w</p><formula xml:id="formula_2">(q) 1 , . . . , w (q) M } and d = {w (d) 1 , . . . , w (d) N }, where w (q) i , i = 1, . . . , M and w (d)</formula><p>j , j = 1, . . . , N denotes a query term vector and a document term vector, respectively, and s denotes the final rel-evance score, we have</p><formula xml:id="formula_3">z (0) i = h(w (q) i ? d), i= 1, . . . , M z (l) i = tanh(W (l) z (l?1) i + b (l) ), i= 1, . . . , M, l= 1, . . . , L s = M i=1 giz (L) i</formula><p>where ? denotes the interaction operator between a query term and the document terms, h denotes the mapping function from local interactions to matching histogram, z (l)</p><p>i , l = 0, . . . , L denotes the intermediate hidden layers for the i-th query term, and gi, i = 1, . . . , M denotes the aggregation weight produced by the term gating network. W (l) denotes the l-th weight matrix and b (l) denotes the l-th bias term, which are shared across different query terms. Note that we adopt cosine similarity, a widely used measure for semantic closeness in neural embeddings <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, as the interaction operator between each pair of term vectors from a query and a document. In our work, we assume the term vectors are learned a priori using existing neural embedding models such as Word2Vec <ref type="bibr" target="#b17">[18]</ref>. We do not learn term vectors in our deep relevance matching model for the following reasons: 1) Reliable term representations can be better acquired from large scale unlabeled text collections rather than from the limited ground truth data for ad-hoc retrieval; 2) By using the a priori learned term vectors, we can focus the learning of our model on relevance matching patterns and considerably reduce the model complexity. In the following, we will describe the major components of our model, including the matching histogram mapping, feed forward matching network, and term gating network in detail, and discuss how they address the three key factors of relevance matching in ad-hoc retrieval.</p><p>Matching Histogram Mapping: The input of our deep relevance matching model is the local interactions between each pair of terms from a query and a document. A major problem is that the size of local interactions is not fixed due to the varied lengths of queries and documents. Previous interaction-based models view the local interactions as a matching matrix by preserving the sequential term orders in both queries and documents. Clearly the matching matrix is a position preserving representation, which is useful if the learning task is position related. However, according to the diverse matching requirement, relevance matching is not position related since it could happen in any position in a long document. Thus the matching matrix may not be a suitable representation for ad-hoc retrieval due to the potentially noisy positional signals in it.</p><p>In our work, we adopt a strength preserving representation, namely a matching histogram, which groups local interactions according to different levels of signal strengths rather than their positions. Specifically, since the local interaction (i.e., cosine similarity between term vectors) is within the interval [?1, 1], we discretize the interval into a set of ordered bins and accumulate the count of local interactions in each bin. In this work, we consider fixed bin size and treat exact matching as a separate bin. Other discretization schemes could be explored in future work. For example, suppose the bin size is set as 0.5, we will obtain five bins <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref>} in an ascending order. Given a query term "car " and a document (car, rent, truck, bump, injunction, runway), and the corresponding local interactions based on cosine similarity are (1, 0.2, 0.7, 0.3, ?0.1, 0.1), we will obtain a matching histogram as [0, 1, 3, 1, 1]. We explore three ways of the matching histogram mapping:</p><formula xml:id="formula_4">{[?1, ?0.5), [?0.5, ?0), [0, 0.5), [0.5, 1),</formula><p>Count-based Histogram (CH): This is the simplest way of transformation as described above which directly takes the count of local interactions in each bin as the histogram value.</p><p>Normalized Histogram (NH): We normalize the count value in each bin by the total count to focus on the relative rather than the absolute number of different levels of interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LogCount-based Histogram (LCH):</head><p>We apply logarithm over the count value in each bin, both to reduce the range, and to allow our model to more easily learn multiplicative relationships <ref type="bibr" target="#b0">[1]</ref>.</p><p>We compare our matching histogram representation with previous matching matrix representations to show the advantages. Firstly, by setting exact matching as a separate bin, the matching histogram clearly distinguishes the exact matching signals from similarity matching signals, while in a matching matrix all the signals are mixed together. Secondly, to solve the problem of variable size in the matching matrix, a zero-padding scheme is often adopted in previous methods <ref type="bibr" target="#b10">[11]</ref>. However, the zero-padding scheme introduces additional interaction signals which may be unfair for short documents. In contrast, we map the variable-size interactions into a fixed-length matching histogram without introducing any additional signals.</p><p>Feed forward Matching Network: Based on the matching histogram above, we employ a feed forward matching network to learn the hierarchical matching patterns and produce a matching score for each query term. Since our model follows the approach of interaction-focused models, we discuss the major differences between the learning of our feed forward matching network and that in previous interactionfocused models.</p><p>Existing interaction-focused models, e.g., ARC-II and MatchPyramid, employ a CNN to learn hierarchical matching pat-terns over the matching matrix. These models are basically position-aware using convolutional units with a local "receptive field" and learning positional regularities in matching patterns. This may be suitable for the image recognition task, and work well on semantic matching problems due to the global matching requirement (i.e., all the positions are important). However, it may not be suitable for the ad-hoc retrieval task, since such positional regularity may not exist in relevance matching due to the diverse matching requirement discussed in Section 3. Besides, since CNN parameters are position related, these models will treat both exact matching and similarity matching signals equally.</p><p>Our deep relevance matching model, on the contrary, aims to extract hierarchical matching patterns from different levels of interaction signals rather than different positions. The position-free and strength-focused property makes it better at handling the diverse matching requirement in ad-hoc retrieval. Meanwhile, since the matching histogram directly distinguishes exact matching signals from the rest, our model can naturally learn the importance of exact matching signals.</p><p>There have been some interaction-focused models that employ special pooling strategies to turn the position-aware interactions into strength-based fixed-length representations. For example, MV-LSTM <ref type="bibr" target="#b25">[26]</ref> used K-max pooling strategy <ref type="bibr" target="#b12">[13]</ref> to select the top K strongest interaction signals from the matching matrix as the input of a MLP. However, such a pooling strategy simply truncates the signals and thus will be strongly biased to long documents since it is more likely for long documents to contain more strong signals. The pooling strategy is applied over the entire matching matrix in MV-LSTM, making it possible that the top K strongest signals all come from the interactions between a single query term and the document terms. In contrast, our model does not rely on any pooling strategy to truncate the interactions so that we can avoid these problems.</p><p>Term Gating Network: One significant difference of our model from existing interaction-focused models is that we employ a joint deep architecture at the query term level. In this way, our model can explicitly model query term importance. This is achieved by using the term gating network, which produces an aggregation weight for each query term controlling how much the relevance score on that query term contributes to the final relevance score. Specifically, we employ the softmax function as the gating function.</p><formula xml:id="formula_5">gi = exp(wgx (q) i ) M j=1 exp(wgx (q) j ) , i = 1, . . . , M,</formula><p>where wg denotes the weight vector of the term gating network and x (q) i , i = 1, . . . , M denotes the i-th query term input. We tried different inputs for the gating function as follows:</p><p>Term Vector (TV): Inspired by the work <ref type="bibr" target="#b31">[32]</ref> where term embeddings can be leveraged to learn the term weights in queries, we use query term vectors as the input of the gating function. In this method, x (q) i denotes the i-th query term vector, and wg is a weight vector with the same dimensionality of term vectors.</p><p>Inverse Document Frequency (IDF): An important signal of term importance in ad-hoc retrieval is the inverse document frequency. We also tried this simple but powerful signal in the gating function. In this method, x (q) i denotes the inverse document frequency of the i-th query term, and wg reduces to a single parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Training</head><p>Since the ad-hoc retrieval task is fundamentally a ranking problem, we employ a pairwise ranking loss such as hinge loss to train our deep relevance matching model. Given a triple (q, d + , d ? ), where document d + is ranked higher than document d ? with respect to query q, the loss function is defined as:</p><formula xml:id="formula_6">L(q, d + , d ? ; ?) = max(0, 1 ? s(q, d + ) + s(q, d ? ))</formula><p>where s(q, d) denotes the predicted matching score for (q, d), and ? includes the parameters for the feed forward matching network and those for the term gating network. The optimization is relatively straightforward with standard backpropagation <ref type="bibr" target="#b28">[29]</ref>. We apply stochastic gradient descent method Adagrad <ref type="bibr" target="#b3">[4]</ref> with mini-batches (20 in size), which can be easily parallelized on single machine with multi-cores. For regularization, we find that the early stopping <ref type="bibr" target="#b8">[9]</ref> strategy works well for our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we conduct experiments to demonstrate the effectiveness of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Sets</head><p>To conduct experiments, we use two TREC collections, Robust04 and ClueWeb-09-Cat-B. The details of the two collections are provided in <ref type="table" target="#tab_0">Table 1</ref>. As we can see, they represent different sizes and genres of heterogeneous text collections. Robust04 is a small news dataset. Its topics are collected from TREC Robust Track 2004. ClueWeb-09-Cat-B, on the other hand, is a large Web collection, whose topics are accumulated from TREC Web Tracks 2009, 2010, and 2011. Note that ClueWeb-09-Cat-B is filtered to the set of documents with spam scores in the 60 th percentile, using the Waterloo Fusion spam scores <ref type="bibr" target="#b2">[3]</ref>. For both datasets, we made use of both the title and the description of each TREC topic in our experiments. The retrieval experiments described in this section are implemented using the Galago Search Engine 2 . During indexing and retrieval, both documents and query words are white-space tokenized, lowercased, and stemmed using the Krovetz stemmer <ref type="bibr" target="#b14">[15]</ref>. Stopword removal is performed on query words during retrieval using the INQUERY stop list <ref type="bibr" target="#b1">[2]</ref>. We adopt three types of baseline methods for comparison, including traditional retrieval models, representationfocused deep matching models and interaction-focused deep matching models. Traditional retrieval models include QL: Query likelihood model based on Dirichlet smoothing <ref type="bibr" target="#b30">[31]</ref> is one of the best performing language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines and Experimental Settings</head><p>BM25: The BM25 formula <ref type="bibr" target="#b21">[22]</ref> is another highly effective retrieval model that represents the classical probabilistic retrieval model.</p><p>Representation-focused deep matching models include DSSMT /DSSMD: DSSM <ref type="bibr" target="#b11">[12]</ref> is a state-of-the-art deep matching model for Web search. In the original paper, the model was evaluated based on &lt;query, doc title&gt; pairs where doc title is extracted from the title field. We denote this model as DSSMT . Since other baseline models and our model are based on the full text of the documents, we also evaluated the DSSM model under the same setting, denoted by DSSMD. Since DSSM needs large scale training data due to its huge parameter size, we directly used the released model 3 (trained on large click-through dataset) in our experiments.</p><p>C-DSSMT /C-DSSMD: C-DSSM <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8]</ref> is a similar deep matching model to DSSM for Web search, replacing the feed forward neural network with a convolutional neural network. For the same reason as DSSM, we also made use of the released model 3 directly and adopt two versions of the C-DSSM model, one based on title fields of documents denoted as C-DSSMT and the other based the whole document denoted as C-DSSMD.</p><p>ARC-I: ARC-I <ref type="bibr" target="#b10">[11]</ref> is a general representation-focused deep matching model that has been tested on a set of NLP tasks including sentence completion, response matching, and paraphrase identification. We implemented the ARC-I model according to the original paper since there is no publicly available code.</p><p>Interaction-focused deep matching models are as follows: ARC-II: ARC-II <ref type="bibr" target="#b10">[11]</ref> was proposed by the authors of the model ARC-I, but focuses on learning hierarchical matching patterns from local interactions using a CNN. We also implemented ACR-II since there is no publicly available code.</p><p>MP: MatchPyramid <ref type="bibr" target="#b18">[19]</ref> is another state-of-the-art interaction-focused deep matching model and has been tested on two NLP tasks including paraphrase identification and paper citation matching. There are three variants of the model based on different interaction operators, denoted as MPIND, MPCOS, and MPDOT . We obtained the original implementation of the model from the authors for comparison.</p><p>We refer to our proposed deep relevance matching model as DRMM. With different types of histogram mapping functions (i.e., CH, NH and LCH) and term gating functions (i.e., TV and IDF), we obtained six different variants of our proposed model. For example, by DRMMCH?IDF we refer to DRMM with Count-based histogram and term gating network using inverse document frequency.</p><p>Term Embeddings: For all the models based on term embedding inputs, including ARC-I, ARC-II, MatchPyramid and DRMM, we used 300-dimensional term vectors trained with the Continuous Bag-of-Words (CBOW) Model <ref type="bibr" target="#b17">[18]</ref> on the Robust04 and ClueWeb-09-Cat-B collections, respectively. Specifically, we used 10 as the context window size and used 10 negative samples and a subsampling of fre-quent words with sampling threshold of 10 ?4 as suggested by Word2Vec 4 . Each corpus was pre-processed by removing HTML tags and stemming. We also discarded from the vocabulary all the terms that occur less than 10 times in the corpus, which resulted in a vocabulary of size 0.1M and 4.1M on the Robust04 and ClueWeb-09-Cat-B collections, respectively. To address the out-of-vocabulary (OOV) terms (i.e., some rare terms or numbers not trained by CBOW) in queries, we follow the practice in previous work <ref type="bibr" target="#b13">[14]</ref> to only allow exact matching between such query terms and document terms.</p><p>Network Configurations: For network configurations (e.g., numbers of layers and hidden nodes), we tune the hyper parameters on a validation set (as part of the training set). For ARC-I, ARC-II and MatchPyramid, we tried both the default configurations in their original paper and other settings. We find that models with less layers and feature maps perform better, probably due to the limited training data in TREC collections. Specifically, for ARC-I and ARC-II, we use 3-word windows, 64 feature maps and 6 layers (two for convolutions, two for max-pooling and two full connection). For MatchPyramid, we use one convolutional layer, one dynamic pooling layer and two full connection layers. The number of feature maps is 8 and the kernel size is set to be 3 ? 3. For DRMM, we also use a four-layer architecture throughout all experiments, i.e., one histogram input layer (30 nodes), two hidden layers in the feed forward matching network (5 nodes and 1 node respectively), and one output layer (1 node) with the term gating network for the final matching score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Methodology</head><p>Given the limited number of queries for each collection, we conduct 5-fold cross-validation to minimize over-fitting without reducing the number of learning instances. Topics for each collection are randomly divided into 5 folds. The parameters for each model are tuned on 4-of-5 folds. The final fold in each case is used to evaluate the optimal parameters. This process is repeated 5 times, once for each fold. Mean average precision (MAP) is the optimized metric for all retrieval models. Throughout this paper each displayed evaluation statistic is the average of the five foldlevel evaluation values. For evaluation, the top-ranked 1, 000 documents are compared using the mean average precision (MAP), normalized discounted cumulative gain at rank 20 (nDCG@20), and precision at rank 20 (P@20). Statistical differences between models are computed using the Fisher randomization test <ref type="bibr" target="#b23">[24]</ref> (? = 0.05). Note that for all the deep matching models, we adopt a re-ranking strategy for efficient computation. An initial retrieval is performed using the QL model to obtain the top 2, 000 ranked documents. We then use the deep matching models to re-rank these top results. The top-ranked 1, 000 documents are then used for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Retrieval Performance and Analysis</head><p>This section presents the performance results of different retrieval models over the two benchmark datasets. A summary of results is displayed in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>As we can see, all the representation-focused models perform significantly worse than the traditional retrieval models, demonstrating the unsuitability of these models for rel-4 https://code.google.com/p/word2vec/ evance matching. Both DSSMT and C-DSSMT can work better than their counterpart on the whole document on ClueWeb-09-Cat-B, showing that models designed for global matching requirement cannot handle the diverse matching requirement in long documents. Note that we do not report the performance of DSSMT and C-DSSMT on Robust04 since there is no title field in many subsets in this collection. The ARC-I model, although trained on the corresponding corpus, performs even worse than DSSM and C-DSSM. A possible reason is that ARC-I concatenates the query and document representation for computing the matching score, which may be less effective than the cosine function in DSSM and C-DSSM.</p><p>When we look at the interaction-focused models, we find that these baseline models cannot compete with the traditional retrieval models either. Among these models, ARC-II can outperform ARC-I by directly learning from local interactions, but performs worse than MatchPyramid models due to the indirect local interactions (i.e., local interaction is based on the weighted sum of query and document term vectors rather than cosine similarity or dot product), which is consistent with previous results in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref>. Moreover, the best performing interaction-focused model, MPCOS, can consistently outperform all the representation-focused models on both test collections. When comparing the MatchPyramid models, we find that both MPIND and MPCOS perform much better than MPDOT . Note that MPIND is purely based on exact matching signals, MPCOS and MPDOT involve both exact and similarity matching signals where exact matching signals are always stronger than similarity signals in MPCOS, but this may not be true in MPDOT . The performance gap between MPDOT and the other two MPs indicates the importance of the exact matching signals in relevance matching. In fact, when evaluated on the semantic matching tasks in <ref type="bibr" target="#b18">[19]</ref>, MPDOT performed better than the other two MPs even though it cannot differentiate the exact matching signals from the rest, demonstrating the significant differences between semantic matching and relevance matching.</p><p>As for our proposed DRMMs, we have the following observations: (1) NH-based models perform significantly worse than CH-based models, while LCH-based models achieve the best performance on both collections. The low performance of NH-based models may be related to the loss of document length information after normalization which is important in ad-hoc retrieval <ref type="bibr" target="#b5">[6]</ref>. Meanwhile, the good performance of LCH-based models indicates that deep neural networks can benefit from input signals with reduced range and nonlinear transformation useful for learning multiplicative relationships <ref type="bibr" target="#b0">[1]</ref>; <ref type="bibr" target="#b1">(2)</ref> The term gating function based on inverse document frequency works better than that based on term vectors. There are two possible reasons for this result. Firstly, term vectors do not contain sufficient information for the term importance. Secondly, the learning of the model might be dominated by the term gating network when we use term vectors as the input since there are more parameters (i.e., 300 parameters) in the gating network compared to the feed forward matching network (i.e., 155 parameters).</p><p>Finally, we can see that the best performing DRMM (i.e., DRMMLCH?IDF ) is significantly better than all the existing deep matching models as well as traditional retrieval models. For example, on ClueWeb-09-Cat-B topic titles, the relative improvement of our model over the best perform-   ing baseline (i.e., BM25) is about 11.9%, 14.7%, and 12% in terms of MAP, nDCG@20 and P@20, respectively. Another interesting finding is that on the Robust04 collection, the performance of DRMMLCH?IDF on topic descriptions can be comparable to that on topic titles, which is seldom observed on previous models. This also demonstrates the potential of our model in handling long queries in ad-hoc retrieval.</p><formula xml:id="formula_7">DSSMD 0.095 ? 0.201 ? 0.171 ? 0.078 ? 0.169 ? 0.145 ? CDSSMD 0.067 ? 0.146 ? 0.125 ? 0.050 ? 0.113 ? 0.093 ? ARC-I 0.041 ? 0.066 ? 0.065 ? 0.030 ? 0.047 ? 0.045 ? Interaction-Focused Matching Baselines ARC-II 0.067 ? 0.147 ? 0.128 ? 0.042 ? 0.086 ? 0.074 ? MPIND 0.169 ? 0.319 ? 0.281 ? 0.067 ? 0.142 ? 0.118 ? MPCOS 0.189 ? 0.330 ? 0.290 ? 0.094 ? 0.190 ? 0.162 ? MPDOT 0.083 ? 0.159 ? 0.155 ? 0.047 ? 0.104 ? 0.092 ? Our Approach DRMMCH?T V 0.</formula><formula xml:id="formula_8">DSSMT 0.054 ? 0.132 ? 0.185 ? 0.046 ? 0.119 ? 0.143 ? DSSMD 0.039 ? 0.099 ? 0.131 ? 0.034 ? 0.078 ? 0.103 ? CDSSMT 0.064 ? 0.153 ? 0.214 ? 0.055 ? 0.139 ? 0.171 ? CDSSMD 0.054 ? 0.134 ? 0.177 ? 0.049 ? 0.125 ? 0.160 ? ARC-I 0.024 ? 0.073 ? 0.089 ? 0.017 ? 0.036 ? 0.051 ? Interaction-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Analysis on DRMM model</head><p>We conducted experiments to verify the effectiveness of different components in the DRMM and analyze the effect of term embedding dimensions. Through these experiments, we try to gain a better understanding of the DRRM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Impact of Different Model Components</head><p>To study the effect of different model components, we compare the original DRMMLCH?IDF with several simpler versions of the model. Firstly, we removed the term gating network and used a simple sum to aggregate the scores from all the query terms. Since the aggregation weight is uniform, we denote this model as DRMMLCH?UNI . We also tried removing the histogram mapping layer but kept the rest unchanged. To turn the variable-length local interactions into a fixed-length representation, we adopted two pooling strategies. One is dynamic pooling as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19]</ref> which keeps the position information, and the other is Kmax pooling as in <ref type="bibr" target="#b25">[26]</ref> which turns the positional signals into strength related signals. For a fair comparison, we require the size of the representation after pooling to be the same as the size of the matching histogram (i.e., <ref type="bibr" target="#b29">30)</ref>. Note that although the matching network structure is the same, the learned model is significantly different due to the change of the input. The matching model based on dynamic pooling is a position-aware model, while the model based on K-max pooling is learned with respect to the top strong interaction signals. We denote the former model as DRMMDY N?IDF and the latter as DRMMKMAX?IDF .</p><p>The comparison results over the topic titles on the two test collections in terms of MAP are depicted in <ref type="figure" target="#fig_5">Figure 3</ref>. As we can see, without the term gating network, DRMMLCH?UNI performs slightly worse than the original DRMM. Specifically, the relative MAP drop of DRMMLCH?UNI compared with DRMMLCH?IDF is about 6.8% and 3.5% on Robust04 and ClueWeb-09-Cat-B, respectively. The results demonstrate the effectiveness of the differentiation of query term importance in relevance matching. Besides, we find that DRMMDY N?IDF based on position-related signals performs significantly worse than the other two models based on strengthrelated signals (i.e., DRMMLCH?IDF and DRMMKMAX?IDF ).</p><p>The results indicate that ad-hoc retrieval is more likely to be a strength-related task rather than a position-related task. When comparing DRMMKMAX?IDF and the original DRMMLCH?IDF , we find that DRMMKMAX?IDF works quite well on Robust04 but fails on ClueWeb-09-Cat-B. The possible reason is that the document length variation on Web data (i.e., ClueWeb-09-Cat-B) is much larger than that on news data (i.e., Robust04), leading to the failure of the Kmax pooling method which has potential bias towards very long documents. This further demonstrates the effectiveness of our matching histogram mapping and the corresponding histogram based feed forward matching network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Impact of Term Embeddings</head><p>Since we leverage a priori learned term embeddings in our model, we further study the effect of embedding dimensionality on the retrieval performance. Here we report the performance results on the Robust04 collection using term embeddings trained by CBOW model with 50, 100, 300, and 500 dimensions, respectively. As shown in <ref type="table" target="#tab_4">Table  3</ref>, the performance first increases and then slightly drops with the increase of dimensionality. Term embeddings of different dimensionality provide different granularity of semantic similarity; they may also require different amounts of training data. With lower dimensionality, the similarity between term embeddings might be coarse and hurt the relevance matching performance. However, with larger dimensionality, one may need more data to train reliable term </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>By formalizing ad-hoc retrieval as a text matching problem, deep matching models can be applied to this task so that features can be automatically acquired in an end-to-end way. In recent years, a variety of deep matching models have been proposed for the text matching problems. As mentioned before, we can categorize the existing deep matching models into two major types, namely representationfocused models and interaction-focused models. We have described several representative deep matching models in these two classes in previous sections including DSSM, C-DSSM, ARC-I, ARC-II and MatchPyramid. Here we will discuss some other related work in this direction.</p><p>In the class of representation-focused models, Qiu et al. <ref type="bibr" target="#b20">[21]</ref> proposed Convolutional Neural Tensor Network (CNTN) for community-based question answering. The CNTN model is similar to ARC-I, using CNN to build the representations for each piece of texts. The major difference between CNTN and ARC-I is that CNTN employs a tensor layer rather than MLP on top of the two CNNs to compute the matching score between the two pieces of text. In <ref type="bibr" target="#b24">[25]</ref>, Socher et al. proposed an Unfolding Recursive Autoencoder (uRAE) for paraphrase identification. They first employed recursive autoencoders to build the hierarchical compositional text representations based on syntactic trees, and then conducted matching at different levels for the identification task. In <ref type="bibr" target="#b29">[30]</ref>, Yin et al. introduced MultiGranCNN which employs a CNN to obtain hierarchical representations of texts, and then computes the matching score based on the interactions between these multigranular representations.</p><p>In the class of interaction-focused models, Wang et al. <ref type="bibr" target="#b27">[28]</ref> proposed Deep Match Tree (DeepMatchtree) for the short text matching problem. Different from DeepMatch <ref type="bibr" target="#b16">[17]</ref> which builds local interactions between texts based on semantic topics, DeepMatchtree defines interactions in the product space of dependency trees. A deep neural network is then leveraged for making a matching decision on the two short texts, on the basis of these local interactions. In <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr">Wan et al. introduced</ref> Match-SRNN to model the recursive matching structure in the local interactions so that long-distance dependency between the interactions can be captured. The proposed model was evaluated on two tasks, including community based question answering and paper citation matching.</p><p>Most of these deep matching models are designed for the semantic matching problem, which is significantly different from the relevance matching problem in ad-hoc retrieval. In this work, we introduce a model specifically designed for the relevance matching problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In this paper, we point out that there are significant differences between semantic matching for many NLP tasks and relevance matching for the ad-hoc retrieval task. Many existing deep matching models designed for the semantic matching problem thus may not fit the ad-hoc retrieval task. Based on this analysis, we propose a novel deep relevance matching model for ad-hoc retrieval, by explicitly addressing the three factors in relevance matching. The proposed model contains three major components, i.e., matching histogram mapping, a feed forward matching network, and a term gating network. Experimental results on two representative benchmark datasets show that our model can significantly outperform traditional retrieval models as well as state-of-the-art deep matching models.</p><p>For future work, we would like to leverage larger training data, e.g. click-through logs, to train deeper DRMM so that we can further explore the potential of the proposed model on ad-hoc retrieval. We may also include phrase embeddings so that phrases can be treated as a whole rather than separate terms. In this way, we expect the local interactions can better reflect the meaning by using the proper semantic units in language, leading to better retrieval performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>CIKM' 16 ,</head><label>16</label><figDesc>October 24-28, 2016, Indianapolis, IN, USA c 2016 ACM. ISBN 978-1-4503-4073-1/16/10. . . $15.00 DOI: http://dx.doi.org/10.1145/2983323.2983769</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Two types of deep matching models: (a) Representation-focused models employ a Siamese (symmetric) architecture over the text inputs; (b) Interaction-focused models employ a hierarchical deep architecture over the local interaction matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Architecture of the Deep Relevance Matching Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2</head><label></label><figDesc>http://www.lemurproject.org/galago.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of several simpler versions of DRMM over topic titles of the two test collections in terms of MAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>This work was supported in part by the Center for Intelligent Information Retrieval, in part by the 973 Program of China under Grant No. 2014CB340401 and 2013CB329606, in part by the National Natural Science Foundation of China under Grant No. 61232010, 61472401, 61425016, and 61203298, and in part by the Youth Innovation Promotion Association CAS under Grant No. 20144310 and 2016102.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the TREC collections used in this study. The ClueWeb-09-Cat-B collection has been filtered to the set of documents in the 60 th percentile of spam scores.</figDesc><table><row><cell></cell><cell cols="2">Robust04 ClueWeb-09-Cat-B</cell></row><row><cell>Vocabulary</cell><cell>0.6M</cell><cell>38M</cell></row><row><cell>Document Count</cell><cell>0.5M</cell><cell>34M</cell></row><row><cell>Collection Length</cell><cell>252M</cell><cell>26B</cell></row><row><cell>Query Count</cell><cell>250</cell><cell>150</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of different retrieval models over the Robust-04 and ClueWeb-09-Cat-B collections. Significant improvement or degradation with respect to QL is indicated (+/-) (p-value ? 0.05).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Robust-04 collection</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Topic titles</cell><cell></cell><cell></cell><cell cols="2">Topic descriptions</cell></row><row><cell>Model Type</cell><cell>Model Name</cell><cell>MAP</cell><cell cols="2">nDCG@20 P@20</cell><cell>MAP</cell><cell cols="2">nDCG@20 P@20</cell></row><row><cell>Traditional Retrieval</cell><cell>QL</cell><cell>0.253</cell><cell>0.415</cell><cell>0.369</cell><cell>0.246</cell><cell>0.391</cell><cell>0.334</cell></row><row><cell>Baselines</cell><cell>BM25</cell><cell>0.255</cell><cell>0.418</cell><cell>0.370</cell><cell>0.241</cell><cell>0.399</cell><cell>0.337</cell></row><row><cell>Representation-Focused</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Matching Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of DRMM over different dimensionality of term embeddings trained by CBOW on the Robust04 collection.</figDesc><table><row><cell>Topic</cell><cell>Embedding</cell><cell cols="3">MAP NDCG@20 P@20</cell></row><row><cell></cell><cell>CBOW-50d</cell><cell>0.268</cell><cell>0421</cell><cell>0.375</cell></row><row><cell>Titles</cell><cell cols="2">CBOW-100d 0.270 CBOW-300d 0.279</cell><cell>0.427 0.431</cell><cell>0.379 0.381</cell></row><row><cell></cell><cell cols="2">CBOW-500d 0.277</cell><cell>0.430</cell><cell>0.381</cell></row><row><cell></cell><cell>CBOW-50d</cell><cell>0.268</cell><cell>0.431</cell><cell>0.365</cell></row><row><cell>Descriptions</cell><cell cols="2">CBOW-100d 0.271 CBOW-300d 0.275</cell><cell>0.433 0.437</cell><cell>0.367 0.371</cell></row><row><cell></cell><cell cols="2">CBOW-500d 0.274</cell><cell>0.435</cell><cell>0.370</cell></row><row><cell cols="5">embeddings. Our results suggest that 300 dimensions is suf-</cell></row><row><cell cols="5">ficient for learning term embeddings effective for relevance</cell></row><row><cell cols="3">matching on the Robust04 collection.</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://research.microsoft.com/enus/downloads/731572aa-98e4-4c50-b99d-ae3f0c9562b9/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trec and tipster experiments with inquery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Broglio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPM</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="343" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient and effective spam filtering and re-ranking for large web datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A formal study of information retrieval heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diagnostic evaluation of information retrieval models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic term matching in axiomatic approaches to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling interestingness with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C S L L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">402</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.2188</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Short text similarity with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1411" to="1420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Viewing morphology as an inference process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
	<note>The handbook of brain theory and neural networks</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A deep architecture for matching short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1367" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<title level="m">Text matching as image recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural tensor network architecture for community-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1305" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comparison of statistical significance tests for information retrieval evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Carterette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08277</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Match-srnn: Modeling the recursive matching structure with spatial rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02427</idno>
		<title level="m">Syntax-based deep matching of short texts</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R G H R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multigrancnn: An architecture for general matching of text chunks on multiple levels of granularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="63" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to reweight terms with distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
