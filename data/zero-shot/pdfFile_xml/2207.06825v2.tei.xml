<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bruggemann</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Prune</roleName><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
							<email>csakarid@vision.ee.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truong</forename><surname>Luc</surname></persName>
							<email>truongp@vision.ee.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
							<email>vangool@vision.ee.ethz.ch</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Switzerland</surname></persName>
						</author>
						<title level="a" type="main">Refign: Align and Refine for Adaptation of Semantic Segmentation to Adverse Conditions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Due to the scarcity of dense pixel-level semantic annotations for images recorded in adverse visual conditions, there has been a keen interest in unsupervised domain adaptation (UDA) for the semantic segmentation of such images. UDA adapts models trained on normal conditions to the target adverse-condition domains. Meanwhile, multiple datasets with driving scenes provide corresponding images of the same scenes across multiple conditions, which can serve as a form of weak supervision for domain adaptation. We propose Refign, a generic extension to self-training-based UDA methods which leverages these cross-domain correspondences. Refign consists of two steps: (1) aligning the normal-condition image to the corresponding adverse-condition image using an uncertaintyaware dense matching network, and (2) refining the adverse prediction with the normal prediction using an adaptive label correction mechanism. We design custom modules to streamline both steps and set the new state of the art for domain-adaptive semantic segmentation on several adverse-condition benchmarks, including ACDC and Dark Zurich. The approach introduces no extra training parameters, minimal computational overhead-during training only-and can be used as a drop-in extension to improve any given self-training-based UDA method. Code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation is arguably one of the most important tasks for scene understanding. Accurate and reliable pixel-level semantics are required for fully autonomous vehicle systems. In such safety-critical applications, robustness of the segmentation model to adverse visual conditions is pivotal. Since state-of-the-art semantic segmentation models are typically trained on clear-weather domains <ref type="bibr" target="#b5">[6]</ref>, where detailed pixel-level annotations are available, they have proven to be frail <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref> with respect to changes in image quality, illumination, or weather. Accordingly, a large body of research has focused on unsupervised domain adaptation (UDA) to adapt these models to different domains in which no labels are available <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b69">70]</ref>.</p><p>In this paper, we propose an extension to UDA approaches, which leverages additional reference-or normalcondition-images to improve the predictions of the targetdomain images (see <ref type="figure" target="#fig_0">Fig. 1</ref>). The reference image depicts the same scene as the target image, albeit from a different viewpoint and under favorable conditions (daytime and clear weather). For driving datasets, such image pairs can be collected with minimal extra effort by capturing the same route twice and matching frames via GPS. In recent years, a number of driving datasets have followed this procedure, e.g. RobotCar <ref type="bibr" target="#b30">[31]</ref>, Dark Zurich <ref type="bibr" target="#b41">[42]</ref>, ACDC <ref type="bibr" target="#b40">[41]</ref>, and Boreas <ref type="bibr" target="#b3">[4]</ref>. When adapting a semantic segmentation model from a normal-condition dataset (e.g. Cityscapes <ref type="bibr" target="#b5">[6]</ref>) to adverse conditions, the reference frames represent an intermediate domain. While they overlap with the target frames in terms of sensor and region characteristics, they share source domain weather and time of day. This can bolster the domain adaptation process by providing complementary, more easily learnable information, even if reference and target images might differ slightly in semantic content.</p><p>Current state-of-the-art UDA approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b74">75]</ref> rely on self-training <ref type="bibr" target="#b70">[71]</ref>, where the network is trained with its own target domain predictions as self-supervision. The surprising effectiveness of self-training can be largely at-tributed to clever regularization strategies <ref type="bibr" target="#b61">[62]</ref>, in the absence of which it suffers from confirmation bias. These regularization strategies aim at correctly propagating available true labels to neighboring unlabeled samples in an iterative fashion. A critical issue in this procedure is the error propagation of noisy labels, leading to a drift in pseudo-labels if unmitigated. It has been shown that large neural networks easily overfit to label noise, which deteriorates their generalization performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b73">74]</ref>. Our method ameliorates this error propagation issue by incorporating the predictions of two separate views to reason about the labels of a given scene. Broadly speaking, it could thus be considered an instance of Multi-View Learning <ref type="bibr" target="#b68">[69]</ref>. More specifically, we consider the target prediction as a noisy label to be modified by the complementary reference class-wise probabilities, posing the fusion process as self-label correction <ref type="bibr" target="#b59">[60]</ref>. Echoing recent advances in that field, we design an adaptive label refinement mechanism, which allows the learner to ignore or revise noisy labels.</p><p>Considering that semantic segmentation requires precise, pixel-level predictions, we hypothesize that the reference-target label fusion benefits greatly from spatial alignment of the two frames. Thus, in a preliminary step to refinement, we warp the reference prediction to align it with the target. In anticipation that such alignment is imperfect-due to dynamic objects, occlusions, and warp inaccuracies-we jointly estimate a confidence for each warped pixel, which serves as guidance in the downstream refinement. To streamline this process, we design a probabilistic extension to the geometric matching framework WarpC <ref type="bibr" target="#b54">[55]</ref>, and show its effectiveness in terms of accuracy and uncertainty awareness.</p><p>Altogether, Refign consists of the alignment module and the refinement module. Both modules introduce limited computational overhead during training and are shown to boost the performance of baseline UDA methods significantly. When added on top of DAFormer <ref type="bibr" target="#b14">[15]</ref>, Refign achieves 65.6% and 56.2% mIoU for semantic segmentation on ACDC and Dark Zurich respectively, setting the new state of the art on these adverse-condition benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Adaptation to Adverse Domains. Several works on domain adaptation for semantic segmentation have been developed with the synthetic-to-real adaptation setting in mind, focusing on adversarial alignment of the source and target domains <ref type="bibr">[13, 14, 29, 43, 57-59, 61, 76]</ref>, self-training with pseudo-labels in the target domain <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b80">81]</ref>, and combining self-training with adversarial adaptation <ref type="bibr" target="#b24">[25]</ref> or with pixel-level adaptation via explicit transforms from source to target <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b69">70]</ref>. Significantly fewer methods have been presented for adaptation from normal to adverse domains, which is highly relevant for practical scenarios such as au-tomated driving, in which the perception system needs to be robust to unfavorable conditions such as fog, night, rain, and snow. Generating partially synthetic data with simulated adverse weather from clear-weather counterparts has proven to improve performance on real adverse-condition sets featuring fog <ref type="bibr" target="#b39">[40]</ref> and rain <ref type="bibr" target="#b51">[52]</ref>. Sequences of domains of progressively increasing adversity have been leveraged in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b64">65]</ref> via curriculum-based schemes. Light-weight input adapters <ref type="bibr" target="#b33">[34]</ref> and adversarial style transfer <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b47">48]</ref> are proposed as a generic pre-processing step at test time before predicting with source-domain models. Shared characteristics between different domains, such as sensor and time of day <ref type="bibr" target="#b8">[9]</ref> or visibility <ref type="bibr" target="#b22">[23]</ref>, are leveraged to learn consistent representations across different datasets. The recent introduction of adverse-condition semantic segmentation datasets with image-level cross-condition correspondences, such as Dark Zurich <ref type="bibr" target="#b37">[38]</ref> and ACDC <ref type="bibr" target="#b40">[41]</ref>, has enabled the development of approaches which leverage these correspondences as weak supervision for adapting to adverse conditions. Sparse, pixel-level correspondences are used in <ref type="bibr" target="#b21">[22]</ref> to enforce consistency of the predictions across different conditions. DANIA <ref type="bibr" target="#b63">[64]</ref> warps daytime predictions into the nighttime image viewpoint and applies a consistency loss for static classes only. Closely related to our work, MGCDA <ref type="bibr" target="#b41">[42]</ref> fuses cross-time-of-day predictions after two-view-geometry-based alignment. Differently from their work, we directly warp the two corresponding images with an uncertainty-aware dense matching network. The warp uncertainty provides guidance for the downstream fusion, which enables more nuanced refinement, even incorporating dynamic objects. Finally, while most of the aforementioned approaches are tailored to specific conditions, our method can address arbitrary adverse conditions. Dense Geometric Matching. Dense correspondence estimation aims at finding pixel-wise matches relating a pair of images. Approaches such as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b55">56]</ref> predict a 4D correlation volume, from which the dense correspondences are extracted as the argmax of the matching scores. Our matching network instead follows a recent line of works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref> which directly regress the dense flow field or correspondence map. DGC-Net <ref type="bibr" target="#b31">[32]</ref> employs a coarse-to-fine approach where a global cost volume is constructed at the coarsest scale to handle large motions. However, it can only handle input images of a fixed, low resolution, which significantly impacts the accuracy of the predicted flow. To circumvent this issue, GLU-Net <ref type="bibr" target="#b52">[53]</ref> integrates both local and global correlation layers. RANSAC-Flow <ref type="bibr" target="#b44">[45]</ref> is based on a two-stage refinement strategy, first estimating homographies relating the pair and then refining them by a predicted residual flow. COTR <ref type="bibr" target="#b17">[18]</ref> relies on a transformer-based architecture to perform matching. Similarly to us, PDC-Net <ref type="bibr" target="#b53">[54]</ref> adopts a probabilistic framework to regress both the flow and its uncertainty. Differently from  <ref type="figure">Figure 2</ref>. Generic self-training for UDA, complemented by the proposed Refign (gray-shaded area). Refign leverages an additional reference image to improve target pseudo-labels with two modules. The ALIGN module is a pre-trained dense matching network (Sec. 3.1) which (1) warps the reference prediction to align it with the target (Q a R ) and (2) estimates a confidence map of the warp (PR). The REFINE module (Sec. 3.2) combines the above quantities to improve the target pseudo-labels YT for self-training. Only solid arrows backpropagate gradients. our work, it requires sparse ground-truth matches obtained through structure from motion (SfM) for training. Instead, we present a probabilistic extension of the warp consistency framework <ref type="bibr" target="#b54">[55]</ref>, which leverages both synthetic flow and real image pairs and requires no training annotations. Label Correction. Label correction (LC) approaches aim to improve learning from noisy labels by modifying the one-hot labels, e.g. through convex combinations with predicted distributions. Response-based knowledge distillation (KD) <ref type="bibr" target="#b11">[12]</ref> methods are a prominent example for LC. While the correction is initiated by a separate teacher model in KD, Self-LC methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b72">73]</ref> rely on the learner itself to correct erroneous labels. Similarly to our method, multiple works have used Self-LC to improve self-training for domain-adaptive semantic segmentation <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79]</ref>. In contrast to these works, our method uses complementary information gained from two different views of the same scene to correct the labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Refign</head><p>Given labeled images from a source domain S (e.g. Cityscapes <ref type="bibr" target="#b5">[6]</ref>), and unlabeled, corresponding pairs of images from a target domain T (e.g. adverse-condition images of ACDC <ref type="bibr" target="#b40">[41]</ref>) and a reference domain R (e.g. normalcondition images of ACDC), we aim to learn a model which predicts semantic segmentation maps for target-domain images. Ground-truth semantic segmentation labels are only available for the source-domain images during training. The reference image is assumed to depict the same scene as the target image, but from a different viewpoint and under better visual conditions.</p><p>Our method-Refign-is a framework-agnostic exten-Algorithm 1. Generic UDA self-training, complemented with Refign (shaded in gray).</p><p>Require: Samples D S , D T , D R , initialized network f ? , pretrained alignment module ALIGN 1: for i = 0 to N do <ref type="bibr">2:</ref> update/initialize teacher network f EM A <ref type="bibr">3:</ref> sample</p><formula xml:id="formula_0">(I S , Y S ) from D S , I T from D T , and I R from D R 4:? S ? f ? (I S ) 5:</formula><p>if RAND(0, 1) &lt; 0.5 then // Adapt to T 6:</p><formula xml:id="formula_1">Q T ? f EM A (I T ) and Q R ? f EM A (I R ) 7: Q a R , P R ? ALIGN(Q R , I R , I T ) // Warp Q R 8: Q r T ? REFINE(Q T , Q a R , P R ) // Eq. 6 9: Y T ? PSEUDO(Q r T ) // e.g. with argmax 10:? T ? f ? (I T ) 11: l = L(? S , Y S ) + L(? T , Y T ) 12: else // Adapt to R 13: Y R ? PSEUDO(f EM A (I R )) 14:? R ? f ? (I R ) 15: l = L(? S , Y S ) + L(? R , Y R ) 16:</formula><p>end if <ref type="bibr">17:</ref> apply gradient descent to ? with ? ? l 18: end for sion to self-training-based UDA methods, leveraging the additional reference image. Underpinning Refign is the hypothesis that R can serve as an intermediate domain between S and T . It is a well-established fact that intermediate domains can bolster UDA <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39]</ref>. In our case, we hypothesize that the higher-quality predictions for R can be used to guide self-training in T . <ref type="figure">Fig. 2</ref> shows a generic self-training UDA setup, with our Refign module shaded in gray. In each training iteration, the model f ? is trained both with the source ground-truth labels Y S and the target pseudo-labels Y T . Most state-of-theart UDA approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b65">66]</ref> generate the pseudo-labels with a Mean Teacher <ref type="bibr" target="#b49">[50]</ref> (f EM A ), using the exponential moving average (EMA) weights of f ? . This increases pseudo-label accuracy and mitigates confirmation bias <ref type="bibr" target="#b49">[50]</ref>. As depicted in <ref type="figure">Fig. 2</ref> and summarized in Alg. 1, Refign introduces two additional steps at training time to improve the pseudo-labels: (1) A pre-trained alignment module (Line 7) computes the flow from target to reference image and warps the reference prediction accordingly. The alignment module also estimates a pixel-wise warp confidence map P R .</p><p>(2) A non-parametric refinement module (Line 8) fuses the target and warped reference predictions-using P R for the fusion weights-to produce refined target predictions. The target predictions are subsequently converted to pseudolabels according to the base UDA method (Line 9, e.g. through argmax and confidence weighting if Refign is built on DACS <ref type="bibr" target="#b50">[51]</ref>). Since Refign hinges on high-quality reference predictions, we adapt f ? to R in every second training iteration via the employed UDA base method (Lines 12-15, omitted in <ref type="figure">Fig. 2</ref>).</p><p>Refign does not introduce any additional training parameters, since the alignment module is pre-trained and frozen, and the refinement module is non-parametric. Consequently, the memory and computation overhead during training is minor since no additional backpropagation is required. During inference, Refign is removed altogether. We describe the main two components of Refign-the alignment and refinement modules-in more detail in Sec. 3.1 and Sec. 3.2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Alignment</head><p>Exact spatial alignment of the target and reference images is a crucial preliminary step for precise, pixel-wise semantic label refinement. Our alignment module warps the reference image to align it with the target and estimates a confidence map of the warp, which is an important asset to guide the downstream label refinement. To fulfill these requirements, we extend the warp consistency (WarpC) framework of <ref type="bibr" target="#b54">[55]</ref> with uncertainty prediction. WarpC. We first recap WarpC, referring to the original work <ref type="bibr" target="#b54">[55]</ref> for a more in-depth discussion. Given two images I, J ? R h?w?3 depicting a similar scene, the goal is to find a dense displacement field F J?I ? R h?w?2 relating pixels in J to I. WarpC exploits the consistency graph shown in <ref type="figure">Fig. 3</ref> to train the flow estimator. I is augmented heavily-e.g. through a randomly sampled homography-to yield I . The synthetic augmentation warp W subsequently supervises two objectives: (1) the direct esti-mateF I ?I of the flow F I ?I ,</p><formula xml:id="formula_2">L I ?I = F I ?I ? W 2 ,<label>(1)</label></formula><p>and (2) estimation of the composite flow F I ?J?I formed by chaining F I ?J and F J?I :</p><formula xml:id="formula_3">L I ?J?I = V ? F I ?J + ?F I ?J (F J?I ) ? W 2 = V ? F I ?J?I ? W 2 .</formula><p>(2) ? F (T) defines the warp of T by the flow F and V ? {0, 1} h?w is the estimated visibility mask. V aims to mask out all pixels in I which have no correspondence in J due to occlusion, image boundary, etc. We estimate V analogously to <ref type="bibr" target="#b54">[55]</ref> based on the Cauchy-Schwarz inequality (see appendix, Sec. B.1). The two loss terms in (1) and (2) complement each other: L I ?I promotes convergence and favors smooth solutions, while L I ?J?I learns realistic motion patterns and appearance changes. The overall network is trained via L align = L I ?I + ?L I ?J?I , where ? is a weighting term balancing the individual losses. UAWarpC. Our extension, Uncertainty-Aware WarpC (UAWarpC) adds predictive uncertainty estimation <ref type="bibr" target="#b32">[33]</ref> to WarpC. We model the flow conditioned on the image inputs I, J via a Gaussian p(F J?I |I, J) = N (F J?I ;F J?I ,? J?I ), implying that the predicted flow is J I I W <ref type="figure">Figure 3</ref>. Warp consistency graph of <ref type="bibr" target="#b54">[55]</ref>. Synthetic warp W supervises two flows: (1) the direct flow I ? I, and (2) the composite flow I ? J ? I. We propose a probabilistic extension to <ref type="bibr" target="#b54">[55]</ref> by predicting parameterized flow distributions (shown as ).</p><p>corrupted with additive Gaussian noise. Note that a different Gaussian is predicted for each pixel. To accommodate x and y flow directions, the distributions are bivariate. We assume for simplicity that the variance is equal in both directions. Thus, the network is trained to output meanF ij ? R 2 and log-variance log? ij ? R at each spatial location ij. Gaussianity allows to easily incorporate the composite flow F I ?J?I into this probabilistic framework. Assuming that F I ?J and F J?I are conditionally independent given the images, we can infer 1 that p(F I ?J?I |I, J, I ) = N (F I ?J?I ;F I ?J?I ,? I ?J?I ) is another Gaussian. Analogously to the composite flow mean in <ref type="formula">(2)</ref>, the composite flow variance is calculated through warping.</p><formula xml:id="formula_4">? I ?J?I =? I ?J + ?F I ?J (? J?I )<label>(3)</label></formula><p>We follow the principle of maximum log-likelihood estimation to train our model (derivation in appendix, Sec. A).</p><formula xml:id="formula_5">L prob I ?I = ? log p(W|I, I ) ? 1 2? I ?I L I ?I + log? I ?I<label>(4)</label></formula><p>The formulation for L prob I ?J?I is obtained simply by replacing the subscripts. Although the negative log-likelihood of a Gaussian corresponds to the squared error loss, in practice we use a Huber loss <ref type="bibr" target="#b16">[17]</ref> in <ref type="formula" target="#formula_2">(1)</ref> and <ref type="formula">(2)</ref> to increase robustness to outliers. Alignment for Label Refinement. The alignment module is trained separately on the large-scale MegaDepth <ref type="bibr" target="#b25">[26]</ref> dataset and it is subsequently frozen during self-training of the segmentation network. During self-training, it estimates the flow F I T ?I R and accordingly warps the reference class-wise probability map Q R ? R h?w?c , yielding Q a R (see <ref type="figure">Fig. 2</ref>). In addition, it estimates a warp confidence map P R ? [0, 1] h?w . To obtain P R from our probabilistic model, we compute the probability of the true flow F I T ?I R being within a radius r of the estimated flowF I T ?I R , as in <ref type="bibr" target="#b53">[54]</ref> (derivation in appendix, Sec. A).</p><formula xml:id="formula_6">P R = p( F I T ?I R ?F I T ?I R ? r) = 1 ? exp ?r 2 2? I T ?I R<label>(5)</label></formula><p>We set r = 1. The elements of P R corresponding to invalid warp regions are set to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Refinement</head><p>The refinement module aims to improve target class-wise probabilities Q T using the aligned reference class probabilities Q a R and the matching confidence map P R . The refined target class weights Q r T are then converted to pseudo-labels for self-training. The refinement is a convex combination with element-wise weights ? ? R h?w?c :</p><formula xml:id="formula_7">Q r T = (1 ? ?) Q T + ? Q a R ,<label>(6)</label></formula><p>where denotes element-wise multiplication. Our construction of ? builds on principles of self-label correction <ref type="bibr" target="#b59">[60]</ref>, as we describe in the following. Confidence. During early training stages, the network's predictions are unreliable, especially in the more challenging adverse domain. In line with the principle of curriculum learning <ref type="bibr" target="#b2">[3]</ref>, the model should thus rely more heavily on the "easier" reference images first. Even if the reference prediction guidance is inaccurate-e.g. due to erroneous warping-degradation during early training is limited, as deep networks tend to learn simple patterns first before memorizing noise <ref type="bibr" target="#b0">[1]</ref>. Later in training, on the other hand, the model should be allowed to ignore or revise faulty reference predictions. This progression can be captured via the model confidence, which increases steadily during training. More formally, we gauge the model confidence with the normalized entropy of the target probability map</p><formula xml:id="formula_8">H(Q T ) = H(Q T )</formula><p>Hmax ? [0, 1] h?w . We take the mean over all pixels to obtain a global image-level estimate, and introduce a hyperparameter ? as an exponent to allow for tuning. This yields a trust score s:</p><formula xml:id="formula_9">? ? s(Q T ) = mean ? (Q T ) ? .<label>(7)</label></formula><p>Large Static Classes. Based on the average size of connected class segments, we hereafter refer to the three classes pole, traffic light, and traffic sign as small static classes (avg. size of 8k pixels on Cityscapes <ref type="bibr" target="#b5">[6]</ref>), while the other eight static classes are named large static classes (avg. size of 234k pixels). We experimentally observe that large static classes are more accurately matched by the alignment module compared to small static classes (see appendix, Sec. C).</p><p>In fact, guiding the refinement through P R for large static classes might be overly pessimistic. The matching network learns to be uncertain in non-textured regions (e.g. road, sky), where it is unable to identify distinct matches <ref type="bibr" target="#b53">[54]</ref>. However, even though P R is low for these regions, the broader semantic class is still matched correctly, due to smooth interpolation learned by the alignment network. We propose more aggressive refinement for large static classes to compensate for this effect. To rule out unwanted drift towards large classes, we restrict the aggressive mixing both spatially and across channels via a binary mask M ? {0, 1} h?w?c with elements m ijk . We define A as the set of large static classes, Z T = argmax c Q T as the target predictions, and Z R as the reference predictions.</p><formula xml:id="formula_10">m ijk = 1 if k ? A and Z ij T ? A and Z ij R ? A , 0 otherwise .<label>(8)</label></formula><p>M restricts aggressive mixing only to tensor elements fulfilling two criteria: <ref type="bibr" target="#b0">(1)</ref> The element belongs to the channel of a large static class. <ref type="formula">(2)</ref> The corresponding pixel is labeled with a large static class in both domains. Aggressive mixing is incorporated as follows:</p><formula xml:id="formula_11">? ? max(P R , M).<label>(9)</label></formula><p>As slight abuse of notation, we use max(?, ?) to denote the element-wise maximum of two tensors, which are broadcast to the same shape. Here, P R is stacked c times along the third dimension to match the shape of M. Refinement Equation.</p><p>Combining the two propositions, we obtain adaptive pseudo-label refinement:</p><formula xml:id="formula_12">? = s(Q T ) max(P R , M).<label>(10)</label></formula><p>Owing to pixel-wise modulation by P R , this refinement scheme can ignore dynamic objects and small static objects, which are difficult to align. On the other hand, if e.g. two cars are coincidentally present at the same location, information transfer is still possible. Furthermore, the scheme allows for easy tuning of the degree of mixing with a single hyperparameter-the exponent ? of trust score s. Finally, since entries of P R corresponding to invalid warp regions are zero, no mixing happens if no match can be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present extensive experiments for both UDA and geometric matching. Sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Datasets. For the source domain we use Cityscapes <ref type="bibr" target="#b5">[6]</ref>. For the target and reference domains, we use ACDC <ref type="bibr" target="#b40">[41]</ref>, Dark Zurich <ref type="bibr" target="#b41">[42]</ref>, RobotCar Correspondence <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>, or CMU Correspondence <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. Each of these four targetdomain datasets contains adverse-normal condition street scene image pairs in the training set. ACDC contains    <ref type="bibr" target="#b25">[26]</ref> and evaluate using the test split of <ref type="bibr" target="#b44">[45]</ref>. To test the ability of the alignment module to generalize to road scenes, we additionally evaluate it on the sparse ground-truth matches provided by <ref type="bibr" target="#b21">[22]</ref> for the RobotCar and CMU Correspondence datasets.</p><p>Architectures. To showcase the flexibility of Refign, we combine it with state-of-the-art UDA methods. We choose DACS <ref type="bibr" target="#b50">[51]</ref> (using DeepLabv2 <ref type="bibr" target="#b4">[5]</ref>) and DAFormer <ref type="bibr" target="#b14">[15]</ref> (based on SegFormer <ref type="bibr" target="#b66">[67]</ref>) as base methods. Our alignment network follows almost exactly the same architecture as WarpC <ref type="bibr" target="#b54">[55]</ref> (VGG-16 <ref type="bibr" target="#b45">[46]</ref> encoder and GLU-Net <ref type="bibr" target="#b52">[53]</ref> decoder), complemented with the uncertainty decoder of <ref type="bibr" target="#b53">[54]</ref>.</p><p>Metrics. For evaluating segmentation results, we use mean intersection over union (mIoU). Geometric matching accuracy is evaluated using the percentage of correct keypoints at a given pixel threshold T (PCK-T ). The quality of matching uncertainty estimates is evaluated using sparsification error, specifically the area under the sparsification error curve (AUSE) <ref type="bibr" target="#b53">[54]</ref> for average end-point error (AEPE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to the State of the Art in UDA</head><p>ACDC. We present comparisons to several state-of-the-art methods on the ACDC test set in <ref type="table" target="#tab_0">Table 1</ref>. Applying Refign on top of DAFormer <ref type="bibr" target="#b14">[15]</ref> results in a mIoU of 65.5%, setting the new state of the art in domain adaptation from Cityscapes to ACDC. Refign boosts the performance of DAFormer by a substantial 10.1%. Besides static classes, we observe substantial improvement for dynamic classes as well, owing to our adaptive refinement method. Among DeepLabv2-based methods, our method Refign-DACS is the second best after DANIA <ref type="bibr" target="#b63">[64]</ref>. Note that Refign boosts the mIoU of DACS [51] by 6.8%. We present qualitative comparisons with FDA, DANIA, and DAFormer in <ref type="figure">Fig. 4</ref>. Our Refign-DAFormer consistently produces more accurate segmentation maps than the Image FDA <ref type="bibr" target="#b69">[70]</ref> DANIA (PSPNet) <ref type="bibr" target="#b63">[64]</ref> DAFormer <ref type="bibr" target="#b14">[15]</ref> Refign-DAFormer Ground Truth <ref type="figure">Figure 4</ref>. Qualitative segmentation results on two images of the ACDC validation set for models adapted from Cityscapes to ACDC.   <ref type="figure">Figure 6</ref>. Prediction diversity on the ACDC validation set during training. We use normalized entropy as a diversity index. Refign preserves higher prediction diversity compared to a naive refinement scheme, which consists of simple averaging (? = 0.5 in <ref type="formula" target="#formula_7">(6)</ref>  <ref type="table" target="#tab_2">Table 3</ref> lists semi-supervised domain adaptation results on the RobotCar and CMU Correspondence datasets. We compare to DAFormer <ref type="bibr" target="#b14">[15]</ref> and the three PSPNetbased <ref type="bibr" target="#b76">[77]</ref> models proposed in <ref type="bibr" target="#b21">[22]</ref>. The models in <ref type="bibr" target="#b21">[22]</ref> rely on sparse 2D matches obtained via a multi-stage pipeline involving camera pose estimation, 3D point cloud generation and matching, match pruning, and 2D reprojection. In contrast, our alignment module directly establishes correspondences through a dense matching network. Our models achieve the best score on both datasets, demonstrating the generality of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semi-Supervised Domain Adaptation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study and Further Analysis</head><p>We perform an extensive analysis of our method on the ACDC validation set. To obtain more reliable performance estimates, all experiments in this section are repeated three times and we report the average performance. <ref type="table" target="#tab_3">Table 4</ref> shows the ablation study of different components of our refinement scheme <ref type="bibr" target="#b9">(10)</ref>. The first row lists a naive refinement scheme, where ? = 0.5 and no alignment is performed. Adding alignment (row 2; +21.9% mIoU) improves performance substantially across all classes. Further adding P R (row 3; +9.6% mIoU) sharply improves performance for dynamic classes and small static classes, but some large static classes are insufficiently mixed and <ref type="table">Table 6</ref>. Comparison to the state of the art in geometric matching. All methods are trained on MegaDepth and evaluated on MegaDepth, RobotCar, and CMU. "w/o SfM": trained without sparse structure from motion matches, "UA": uncertainty-aware matching network.  their performance decreases due to low warping confidence within non-textured regions (Sec. 3.2). Performance for large static classes is fully recovered by incorporating aggressive large static class mixing via M, without sacrificing performance on small static classes or dynamic classes (row 4; +3.9% mIoU). Introducing confidence-adaptive mixing with trust score s increases performance further for 17 of the 19 classes (row 5; +1.8% mIoU). We arrive at the final configuration by adding concurrent adaptation to R-as detailed in Lines 12-15 of Alg. 1-which adds another +1.0% mIoU in performance. <ref type="table" target="#tab_4">Table 5</ref> shows the sensitivity of our method to different values of its only hyperparameter ? (mean-entropy exponent, see <ref type="bibr" target="#b6">(7)</ref>). The method is fairly robust to decreasing ?. In <ref type="figure">Fig. 5</ref>, we compare values of trust score s during training across different conditions. We observe that s remains larger for images of more challenging conditions (night, snow). Thus, the refinement scheme automatically relies more heavily on the reference predictions for these conditions. Finally, we plot in <ref type="figure">Fig. 6</ref> the diversity of validation set predictions during training, measured by the normalized entropy of the predicted class histogram. Compared to a naive mixing scheme consisting of simple averaging, Refign preserves considerably higher prediction diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Geometric Matching Results</head><p>In <ref type="table">Table 6</ref> we compare our alignment strategy UAWarpC to state-of-the-art geometric matching methods. All reported methods are trained on MegaDepth. To estimate their suitability for deployment as an alignment module in Refign, we report generalization performance on two road datasets, RobotCar and CMU. UAWarpC improves the generalization accuracy compared to the WarpC <ref type="bibr" target="#b54">[55]</ref> baseline, demonstrating that our uncertainty modeling increases the robustness of the flow estimator. In terms of uncertainty estimation, our method matches or even outperforms the recent probabilistic PDC-Net+ <ref type="bibr" target="#b53">[54]</ref> in AUSE.</p><p>Finally, we show qualitative warp results in <ref type="figure" target="#fig_3">Fig. 7</ref>. Notably, the alignment module successfully manages to identify dynamic objects and assigns them a low confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present Refign, a generic add-on to self-trainingbased UDA methods that leverages an additional reference image for each target-domain image. Refign consists of two steps: (1) uncertainty-aware alignment of the reference prediction with the target prediction, and (2) adaptive refinement of the target predictions with the aligned reference predictions. To enable step (1), we propose UAWarpC, a probabilistic extension to the matching method WarpC <ref type="bibr" target="#b54">[55]</ref>. UAWarpC reaches state-of-the-art performance in both flow accuracy and uncertainty estimation. Step (2) consists of a non-parametric label correction scheme. We apply Refign to two existing UDA methods-DACS <ref type="bibr" target="#b50">[51]</ref> and DAFormer <ref type="bibr" target="#b14">[15]</ref>-and report state-of-the-art normal-toadverse domain adaptation results for semantic segmentation on multiple driving scene datasets.</p><formula xml:id="formula_13">1 2? v I ?I (F v I ?I ?W v ) 2 ? ? = ? log 1 2?? I ?I e ? 1 2? I ?I F I ?I ?W 2 ? 1 2? I ?I F I ?I ? W 2 + log? I ?I = 1 2? I ?I L I ?I + log? I ?I<label>(11)</label></formula><p>Derivation of Confidence Map We integrate the bivariate Gaussian density function over a circle with radius r (subscripts are omitted).</p><formula xml:id="formula_14">P R = p( F ?F ? r) = 2? 0 r 0 1 2?? e ? 1 2? ? 2 ?d?d? = 1 ? exp ?r 2 2?<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Details</head><p>In this section, we describe training settings and implementation details. Both alignment and segmentation network were trained using Automatic Mixed Precision on a single consumer RTX 2080 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Alignment Network</head><p>UAWarpC training almost exactly follows the setup of <ref type="bibr" target="#b54">[55]</ref>. The training consists of two stages: In the first stage, the network is trained without the visibility mask, as the visibility mask estimate is still inaccurate. In the second stage, the visibility mask is activated and more data augmentation is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Handling</head><p>The alignment network is trained using MegaDepth <ref type="bibr" target="#b25">[26]</ref>, consisting of 196 scenes reconstructed from 1,070,468 internet photos with COLMAP <ref type="bibr" target="#b43">[44]</ref>. 150 scenes are used for training, encompassing around 58,000 sampled image pairs. 1800 image pairs sampled from 25 different scenes are used for validation. No ground-truth correspondences from SfM reconstructions are used to train UAWarpC.</p><p>During training, the image pairs I, J are resized to 750?750 pixels, and a dense flow W is sampled to create I . Finally, all three images I, J, I are center-cropped to resolution 520?520. In the first training stage, W consists of sampled color jitter, Gaussian blur, homography, TPS, and affine-TPS transformations. In the second stage, local elastic transformations are added, and the strength of the transformations is increased. For the detailed augmentation parameters, we refer to <ref type="bibr" target="#b54">[55]</ref>.</p><p>Architecture and Loss Function Again following <ref type="bibr" target="#b54">[55]</ref>, a modified GLU-Net <ref type="bibr" target="#b52">[53]</ref> is used as a base architecture for flow prediction. GLU-Net is a four-level pyramidal network with a VGG-16 <ref type="bibr" target="#b45">[46]</ref> encoder. The encoder is initialized with ImageNet weights and frozen. GLU-Net requires an additional low-resolution input of 256?256 to establish global correlations, followed by repeated levels of upscaling and local feature correlations. As in <ref type="bibr" target="#b54">[55]</ref>, our flow decoder uses residual connections for efficiency. In addition, we replace all transposed convolutions with bilinear upsampling, and normalize all encoder feature maps, to increase the convergence rate.</p><p>The uncertainty estimate is produced using the uncertainty decoder proposed in <ref type="bibr" target="#b53">[54]</ref>. However, instead of predicting the parameters of several mixture components, we simply output a single value per pixel-the log-variance.</p><p>As in <ref type="bibr" target="#b54">[55]</ref>, the loss is applied at all four levels of the pyramidal GLU-Net. We simply add the four components. The employed loss functions are explained in Sec. 3.1 of the main paper. To obtain the visibility mask for the second training stage, we use the Cauchy-Schwarz inequality, analogously to <ref type="bibr" target="#b54">[55]</ref>.</p><formula xml:id="formula_15">V = 1 F I ?J + ?F I ?J (F J?I ) ? W 2 &lt; ? 2 + ? 1 F I ?J 2 + ?F I ?J (F J?I ) 2 + W 2<label>(13)</label></formula><p>1 denotes the element-wise indicator function. We use ? 1 = 0.03 and ? 2 = 0.05.</p><p>Optimization Schedule For the first training stage, the alignment network is trained with a batch size of 6 for 400k iterations. We use the Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with weight decay 4 ? 10 ?4 . The initial learning rate is 10 ?4 , and is halved after 250k and 325k iterations. For the second training stage, we use 225k training steps with initial learning rate 5 ? 10 ?5 , halved after 100k, 150k, and 200k iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Segmentation Network</head><p>For training the domain adaptive segmentation network, we follow the employed base UDA method, respectively. We summarize here the settings used with DAFormer <ref type="bibr" target="#b14">[15]</ref>.  Data Handling Input images are resized to half resolution for Cityscapes <ref type="bibr" target="#b5">[6]</ref>, ACDC <ref type="bibr" target="#b40">[41]</ref>, and Dark Zurich <ref type="bibr" target="#b41">[42]</ref>. For RobotCar Correspondence <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref> and CMU Correspondence <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>, we resize to 720?720 and 540?720, respectively. Data augmentation consists of random cropping to 512?512 and random horizontal flipping. For the coarsely labeled extra target images in the semi-supervised domain adaptation for RobotCar and CMU, we additionally apply random rotation with maximum 10?and color jittering.</p><p>Optimization Schedule We use the AdamW <ref type="bibr" target="#b27">[28]</ref> optimizer with a weight decay of 0.01. The learning rate follows a linear warmup for 1500 steps, followed by linear decay. The peak learning rate is 6 ? 10 ?4 . On ACDC and Dark Zurich, we train for 40k iterations; on RobotCar and CMU, we train for 20k iterations. A batch size of 2 is used throughout.</p><p>To mitigate the risk of overfitting, we use the coarsely labeled extra target images in semi-supervised domain adaptation on RobotCar and CMU only in every second training iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Small vs. Large Static Classes</head><p>To motivate the distinction between small and large static classes (as defined in Sec. 3.2), we generate ACDC <ref type="bibr" target="#b40">[41]</ref> reference image predictions using a SegFormer <ref type="bibr" target="#b66">[67]</ref> trained on Cityscapes <ref type="bibr" target="#b5">[6]</ref>, and warp them onto the corresponding adverse-image viewpoint. As shown in <ref type="figure" target="#fig_0">Fig. C-1</ref>, we observe a correlation between the resulting IoU and the average size of the connected class component for static classes (pearson correlation coeff. of 0.70). The classes pole, traffic light, and traffic sign are drastically smaller than the rest, and consequentially have lower accuracy. On the other hand, such indiscriminate warping (i.e., without P R ) is surprisingly accurate for the large static classes.</p><p>Furthermore, we analyze the mIoU improvement when only considering pixels above a certain P R threshold for the above mentioned warped SegFormer predictions, see <ref type="figure">Fig. C-2</ref>. While the performance increases monotonically for both dynamic and small static classes, it remains mostly flat for large static classes. This suggests that large static classes are largely insensitive to the warping confidence, while both dynamic and small static classes benefit greatly from confidence guidance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Experimental Results</head><p>Due to space restrictions, we present the full class-wise performances of state-of-the-art UDA methods on Dark Zurich-test here in Table D-1. The models reported in Tables 1, 2, and D-1 all use the same image input size at testtime for fairness of comparison. Table D-2 presents models which do not follow that protocol. Using Cityscapespretrained weights for initialization, Refign added on top of HRDA <ref type="bibr" target="#b15">[16]</ref> achieves 72.1 mIoU and 63.9 mIoU on ACDC and Dark Zurich-test, respectively, ranking first on the public leaderboards of these benchmarks at the time of publication.</p><p>In <ref type="table" target="#tab_8">Table D</ref>-3, we report the performance of City-scapes?ACDC Refign-DAFormer on the four different conditions of the ACDC validation set. Refign improves markedly over the baseline for all conditions.</p><p>We also compare the Cityscapes?ACDC Refign-DAFormer model with state-of-the-art foggy scene understanding methods in Table D-4. All methods are trained with Cityscapes as source domain, however the foggy scene understanding methods utilize both synthetic foggy data and a larger pool of real foggy data as targets. Surprisingly, our model achieves state-of-the-art performance despite this  handicap. Finally, we conduct experiments substituting the Seg-Former <ref type="bibr" target="#b66">[67]</ref> based architecture of DAFormer <ref type="bibr" target="#b14">[15]</ref> with DeepLabv2 <ref type="bibr" target="#b4">[5]</ref>. On both ACDC and Dark Zurich validation sets, this version of Refign improves substantially over the baseline, as reported in <ref type="table" target="#tab_8">Table D-</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Refign at Test-Time</head><p>Although designed to refine pseudo-labels during online self-training, Refign can also be applied at test-time to arbitrary, trained models, if a reference image is available. We report ACDC and Dark Zurich validation set scores in Table E-1. The performance gain is more moderate than if Refign is applied at training-time. This is unsurprising, given that we only conduct a single refinement iteration in that case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Results</head><p>We show more qualitative results in this section. <ref type="figure" target="#fig_0">Fig. F-1</ref> shows more qualitative segmentation results for randomly selected ACDC validation samples. <ref type="figure">Fig. F-2</ref> shows the warps and corresponding confidence maps for randomly selected ACDC samples. Finally, in <ref type="figure">Fig. F-3</ref>, we show some warp failures. Importantly, the confidence map correctly blends out the inaccurate warps. Image FDA <ref type="bibr" target="#b69">[70]</ref> DANIA (PSPNet) <ref type="bibr" target="#b63">[64]</ref> DAFormer <ref type="bibr" target="#b14">[15]</ref> Refign-DAFormer Ground Truth <ref type="figure">Figure</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>By leveraging a reference image depicting a similar scene as the target, Refign improves target predictions in two steps: (1) The reference predictions are spatially aligned with the target.(2) The target predictions are refined via adaptive label correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. 4.1 provides an overview of the experimental setup. Sec. 4.2 and Sec. 4.3 present comparisons with state-of-the-art methods in UDA and semi-supervised domain adaptation, respectively. Sec. 4.4 discusses ablations and Sec. 4.5 shows geometric matching comparisons. Training settings and implementation details are discussed in Sec. B of the appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Visualizations of warped reference images and the corresponding confidence maps (white ? low confidence) from ACDC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure C-1. Correlation between the average size of connected components (on Cityscapes [6]) and mIoU score of warped reference image predictions for static classes. Larger classes benefit heavily from indiscriminate warping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>F- 1 .Figure F- 2 .</head><label>12</label><figDesc>Prediction samples of the ACDC validation set. Example visualizations of warped reference images and the corresponding confidence maps from ACDC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison to the state of the art in Cityscapes?ACDC domain adaptation on the ACDC test set. Methods above the double line use a DeepLabv2 model. "Ref.": for each adverse input image a reference frame from the same geo-location is used. 32.5 68.1 20.1 17.4 15.8 30.2 28.7 59.9 25.3 37.7 28.7 25.5 70.2 39.6 40.5 52.7 29.2 38.4 37.7 FDA [70] 73.2 34.7 59.0 24.8 29.5 28.6 43.3 44.9 70.1 28.2 54.7 47.0 28.5 74.6 44.8 52.3 63.3 28.3 39.5 45.7 DANNet (DeepLabv2) [63] 82.9 53.1 75.3 32.1 28.2 26.5 39.4 40.3 70.0 39.7 83.5 42.8 28.9 68.0 32.0 31.6 47.0 21.5 36.7 46.3 DANIA (DeepLabv2) [64] 87.8 57.1 80.3 36.2 31.4 28.6 49.5 45.8 76.2 48.8 90.2 47.9 31.1 75.5 36.5 36.5 47.8 32.5 44.1 51.8 DACS [51] 58.5 34.7 76.4 20.9 22.6 31.7 32.7 46.8 58.7 39.0 36.3 43.7 20.5 72.3 39.6 34.8 51.1 24.6 38.2</figDesc><table><row><cell>Method</cell><cell>Ref.</cell><cell>road</cell><cell>sidew.</cell><cell>build.</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veget.</cell><cell>terrain</cell><cell>IoU ? sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorc.</cell><cell>bicycle</cell><cell>mean</cell></row><row><cell>AdaptSegNet [57]</cell><cell></cell><cell cols="5">69.4 34.0 52.8 13.5 18.0</cell><cell cols="2">4.3 14.9</cell><cell cols="12">9.7 64.0 23.1 38.2 38.6 20.1 59.3 35.6 30.6 53.9 19.8 33.9</cell><cell>33.4</cell></row><row><cell>BDL [25]</cell><cell></cell><cell cols="20">56.0 41.2</cell></row><row><cell>Refign-DACS (ours)</cell><cell></cell><cell cols="19">49.5 56.7 79.8 31.2 25.7 34.1 48.0 48.7 76.2 42.5 38.5 48.3 24.7 75.3 46.5 43.9 64.3 34.1 43.6</cell><cell>48.0</cell></row><row><cell>MGCDA (RefineNet) [42]</cell><cell></cell><cell cols="19">73.4 28.7 69.9 19.3 26.3 36.8 53.0 53.3 75.4 32.0 84.6 51.0 26.1 77.6 43.2 45.9 53.9 32.7 41.5</cell><cell>48.7</cell></row><row><cell>DANNet (PSPNet) [63]</cell><cell></cell><cell cols="19">84.3 54.2 77.6 38.0 30.0 18.9 41.6 35.2 71.3 39.4 86.6 48.7 29.2 76.2 41.6 43.0 58.6 32.6 43.9</cell><cell>50.0</cell></row><row><cell>DANIA (PSPNet) [64]</cell><cell></cell><cell cols="19">88.4 60.6 81.1 37.1 32.8 28.4 43.2 42.6 77.7 50.5 90.5 51.5 31.1 76.0 37.4 44.9 64.0 31.8 46.3</cell><cell>53.5</cell></row><row><cell>DAFormer [15]</cell><cell></cell><cell cols="19">58.4 51.3 84.0 42.7 35.1 50.7 30.0 57.0 74.8 52.8 51.3 58.3 32.6 82.7 58.3 54.9 82.4 44.1 50.7</cell><cell>55.4</cell></row><row><cell>Refign-DAFormer (ours)</cell><cell></cell><cell cols="19">89.5 63.4 87.3 43.6 34.3 52.3 63.2 61.4 86.9 58.5 95.7 62.1 39.3 84.1 65.7 71.3 85.4 47.9 52.8</cell><cell>65.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">mIoU ?</cell><cell></cell></row><row><cell></cell><cell>Dark Zurich-test [42]</cell><cell>ND [8]</cell><cell>Bn [42, 72]</cell></row><row><cell>DMAda (RefineNet) [8]</cell><cell>32.1</cell><cell>36.1</cell><cell>28.3</cell></row><row><cell>GCMA (RefineNet) [38]</cell><cell>42.0</cell><cell>45.6</cell><cell>33.2</cell></row><row><cell>MGCDA (RefineNet) [42]</cell><cell>42.5</cell><cell>49.4</cell><cell>34.9</cell></row><row><cell>CDAda (RefineNet) [68]</cell><cell>45.0</cell><cell>50.9</cell><cell>33.8</cell></row><row><cell>DANNet (PSPNet) [63]</cell><cell>45.2</cell><cell>47.7</cell><cell>28.0</cell></row><row><cell>DANIA (PSPNet) [64]</cell><cell>47.0</cell><cell>48.4</cell><cell>27.0</cell></row><row><cell>CCDistill (RefineNet) [9]</cell><cell>47.5</cell><cell>46.2</cell><cell>33.0</cell></row><row><cell>DACS (DeepLabv2) [51]</cell><cell>36.7</cell><cell>39.5</cell><cell>25.3</cell></row><row><cell>Refign-DACS (DeepLabv2, ours)</cell><cell>41.2</cell><cell>41.5</cell><cell>26.2</cell></row><row><cell>DAFormer [15]</cell><cell>53.8</cell><cell>54.1</cell><cell>33.8</cell></row><row><cell>Refign-DAFormer (ours)</cell><cell>56.2</cell><cell>56.8</cell><cell>35.2</cell></row></table><note>Comparison of Cityscapes?Dark Zurich methods on Dark Zurich-test. Trained models are tested for generalization on the Nighttime Driving (ND) and BDD100k-night (Bn) test sets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Semi-supervised domain adaptation on City-scapes?RobotCar and Cityscapes?CMU. "Ref.": a reference frame is used for each adverse input image.</figDesc><table><row><cell>Method</cell><cell>Ref.</cell><cell>mIoU ?</cell></row><row><cell></cell><cell cols="2">RobotCar [22, 31]</cell><cell>CMU [2, 22]</cell></row><row><cell>PSPNet [77]</cell><cell>45.8</cell><cell></cell><cell>73.6</cell></row><row><cell>Cross-Season, CE [22]</cell><cell>53.8</cell><cell></cell><cell>79.3</cell></row><row><cell>Cross-Season, Hinge C [22]</cell><cell>50.6</cell><cell></cell><cell>72.4</cell></row><row><cell>Cross-Season, Hinge F [22]</cell><cell>55.4</cell><cell></cell><cell>75.3</cell></row><row><cell>DAFormer [15]</cell><cell>51.7</cell><cell></cell><cell>75.6</cell></row><row><cell>Refign-DAFormer (ours)</cell><cell>60.5</cell><cell></cell><cell>83.6</cell></row><row><cell cols="4">1600 training, 406 validation, and 2000 test images dis-</cell></row><row><cell cols="4">tributed equally among fog, night, rain, and snow. Dark</cell></row><row><cell cols="4">Zurich contains 2416 training, 50 validation, and 151 test</cell></row><row><cell cols="4">images for nighttime. RobotCar (resp. CMU) Correspon-</cell></row><row><cell cols="4">dence contains 6511 (28766) training, 27 (25) validation,</cell></row></table><note>and 27 (33) test images, captured at various conditions. The RobotCar and CMU Correspondence datasets additionally have 40 and 66 coarsely annotated images, enabling semi- supervised domain adaptation. For training the alignment network, we use MegaDepth</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on the ACDC validation set (metric: IoU) for different components of Refign, as detailed in<ref type="bibr" target="#b9">(10)</ref>. Default values in case of component omission are: PR = 1 2 , M = 0, s = 1. "R-ad": concurrent adaptation to R, i.e., Lines 12-15 of Alg. 1.</figDesc><table><row><cell>ALIGN</cell><cell>PR</cell><cell>M</cell><cell>s</cell><cell>R-ad</cell><cell>road</cell><cell>sidew.</cell><cell>build.</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veget.</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorc.</cell><cell>bicycle</cell><cell>mean</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64.3</cell><cell>25.4</cell><cell>63.3</cell><cell>26.5</cell><cell>18.0</cell><cell>5.5</cell><cell>6.4</cell><cell>9.3</cell><cell>63.7</cell><cell>13.3</cell><cell>79.1</cell><cell>6.9</cell><cell>0.8</cell><cell>23.8</cell><cell>24.8</cell><cell>39.5</cell><cell>8.3</cell><cell>6.7</cell><cell>23.7</cell><cell>26.8</cell></row><row><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>86.9</cell><cell>60.0</cell><cell>82.7</cell><cell>49.0</cell><cell>32.3</cell><cell>43.9</cell><cell>58.1</cell><cell>40.7</cell><cell>83.3</cell><cell>38.3</cell><cell>94.4</cell><cell>12.8</cell><cell>7.2</cell><cell>52.7</cell><cell>43.4</cell><cell>50.4</cell><cell>15.1</cell><cell>30.0</cell><cell>43.6</cell><cell>48.7</cell></row><row><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>67.6</cell><cell>52.0</cell><cell>83.8</cell><cell>47.8</cell><cell>36.7</cell><cell>56.0</cell><cell>69.3</cell><cell>51.7</cell><cell>73.7</cell><cell>37.0</cell><cell>63.7</cell><cell>46.0</cell><cell>27.2</cell><cell>78.9</cell><cell>67.6</cell><cell>75.1</cell><cell>83.2</cell><cell>42.6</cell><cell>48.7</cell><cell>58.3</cell></row><row><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>87.8</cell><cell>57.3</cell><cell>84.1</cell><cell>47.7</cell><cell>33.1</cell><cell>55.3</cell><cell>69.8</cell><cell>51.9</cell><cell>84.5</cell><cell>37.8</cell><cell>94.9</cell><cell>45.4</cell><cell>28.5</cell><cell>78.3</cell><cell>68.7</cell><cell>78.7</cell><cell>83.2</cell><cell>45.0</cell><cell>49.2</cell><cell>62.2</cell></row><row><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>88.5</cell><cell>58.9</cell><cell>85.0</cell><cell>48.4</cell><cell>34.2</cell><cell>57.2</cell><cell>71.3</cell><cell>54.1</cell><cell>85.2</cell><cell>40.1</cell><cell>95.1</cell><cell>55.4</cell><cell>36.5</cell><cell>82.9</cell><cell>67.6</cell><cell>79.3</cell><cell>83.4</cell><cell>45.5</cell><cell>47.9</cell><cell>64.0</cell></row><row><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>89.4</cell><cell>62.4</cell><cell>85.5</cell><cell>48.6</cell><cell>36.6</cell><cell>57.7</cell><cell>71.0</cell><cell>55.0</cell><cell>85.3</cell><cell>41.0</cell><cell>95.1</cell><cell>57.3</cell><cell>33.1</cell><cell>82.9</cell><cell>73.6</cell><cell>82.5</cell><cell>86.0</cell><cell>43.9</cell><cell>48.1</cell><cell>65.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Hyperparameter study of the mean-entropy exponent ? on the ACDC validation set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mIoU ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? = 1</cell><cell></cell><cell>? = 1 2</cell><cell>? = 1 4</cell><cell>? = 1 8</cell><cell>? = 1 16</cell></row><row><cell cols="4">Refign-DAFormer</cell><cell></cell><cell>59.2</cell><cell></cell><cell>61.8</cell><cell>65.0</cell><cell>64.3</cell><cell>63.5</cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Figure 5. Trust score s(QT )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>during training for images of the</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>different conditions in ACDC.</cell></row><row><cell>) trust score s(QT</cell><cell>0.4 0.6</cell><cell></cell><cell cols="2">night; 34.8 mIoU</cell><cell></cell><cell></cell><cell>On average, difficult conditions (night, snow-lower mIoU) ex-hibit a higher s, meaning their target predictions undergo more</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell cols="2">snow; 56.3 mIoU rain; 58.5 mIoU</cell><cell></cell><cell></cell><cell>intensive correction by refer-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">fog; 67.9 mIoU</cell><cell></cell><cell></cell><cell>ence predictions. Shown are</cell></row><row><cell></cell><cell>0.0</cell><cell>0</cell><cell>10k</cell><cell>20k</cell><cell>30k</cell><cell>40k</cell><cell>mIoU validation scores of the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>iteration</cell><cell></cell><cell></cell><cell>DAFormer [15] baseline.</cell></row><row><cell></cell><cell cols="2">0.64</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.62</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>diversity index</cell><cell cols="2">0.58 0.60</cell><cell></cell><cell cols="2">Ground Truth</cell><cell></cell></row><row><cell></cell><cell cols="2">0.56</cell><cell></cell><cell cols="3">DAFormer [15] Refign (ours)</cell></row><row><cell></cell><cell cols="2">0.54</cell><cell></cell><cell cols="2">Naive</cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>10k</cell><cell>20k</cell><cell>30k</cell><cell>40k</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>iteration</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table D -</head><label>D</label><figDesc>1. State-of-the-art comparison on Dark Zurich-test for Cityscapes?Dark Zurich domain adaptation. Methods above the double line all use a DeepLabv2 [5] model. "Ref.": For each adverse input image a reference image at similar geo-location is used. Table D-2. State-of-the-art comparison of models which do not follow the common image input resizing protocol. Refign-HRDA currently ranks first on public leaderboards.</figDesc><table><row><cell>Method</cell><cell>Ref.</cell><cell>road</cell><cell>sidew.</cell><cell>build.</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veget.</cell><cell>terrain</cell><cell>IoU ? sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>motorc.</cell><cell>bicycle</cell><cell>mean</cell></row><row><cell>Method</cell><cell>Cityscapes?ACDC</cell><cell></cell><cell></cell><cell cols="3">Cityscapes?Dark Zurich</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ACDC [41]</cell><cell></cell><cell cols="6">Dark Zurich-test [42] ND [8] Bn [42, 72]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SePiCo (DAFormer) [66]</cell><cell>-</cell><cell></cell><cell></cell><cell>54.2</cell><cell></cell><cell>57.1</cell><cell></cell><cell>36.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HRDA [16]</cell><cell>68.0</cell><cell></cell><cell></cell><cell>55.9</cell><cell></cell><cell>55.6</cell><cell></cell><cell>39.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Refign-HRDA</cell><cell>72.1</cell><cell></cell><cell></cell><cell>63.9</cell><cell></cell><cell>57.8</cell><cell></cell><cell>40.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table D -</head><label>D</label><figDesc>3. Performance of Cityscapes?ACDC models for different conditions on the validation set.</figDesc><table><row><cell>Method</cell><cell></cell><cell>mIoU ?</cell><cell></cell><cell></cell></row><row><cell></cell><cell>night</cell><cell>snow</cell><cell>rain</cell><cell>fog</cell></row><row><cell>DAFormer [15]</cell><cell>34.8</cell><cell>56.3</cell><cell>58.5</cell><cell>67.9</cell></row><row><cell>Refign-DAFormer</cell><cell>48.1</cell><cell>65.0</cell><cell>65.2</cell><cell>73.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table D -</head><label>D</label><figDesc>4. Performance comparison with specialized foggy scene understanding methods on the Foggy Zurich<ref type="bibr" target="#b6">[7]</ref> and Foggy Driving<ref type="bibr" target="#b39">[40]</ref> test sets.Table D-5. Performance of Refign vs. DAFormer baseline with a DeepLabv2 model on the ACDC and Dark Zurich validation sets.</figDesc><table><row><cell>Method</cell><cell>Target Domain Training Data</cell><cell></cell><cell>mIoU ?</cell></row><row><cell></cell><cell cols="2">Foggy CS-DBF [7] Foggy Zurich [7] ACDC [41]</cell><cell>Foggy Zurich [7] FoggyDriving [40]</cell></row><row><cell>CMAda3+ [7]</cell><cell></cell><cell></cell><cell>46.8</cell><cell>49.8</cell></row><row><cell>FIFO [23]</cell><cell></cell><cell></cell><cell>48.4</cell><cell>50.7</cell></row><row><cell>CuDA-Net+ [30]</cell><cell></cell><cell></cell><cell>49.1</cell><cell>53.5</cell></row><row><cell>TDo-Dif [27]</cell><cell></cell><cell></cell><cell>51.9</cell><cell>50.7</cell></row><row><cell>Refign-DAFormer</cell><cell></cell><cell></cell><cell>51.4</cell><cell>53.9</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>mIoU ?</cell></row><row><cell></cell><cell></cell><cell cols="2">ACDC [41]</cell><cell>Dark Zurich [42]</cell></row><row><cell cols="2">DAFormer (DeepLabv2) [15]</cell><cell>46.4</cell><cell>24.8</cell></row><row><cell cols="2">Refign-DAFormer (DeepLabv2)</cell><cell>55.6</cell><cell>38.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table E-1. Applying Refign only for one refinement iteration at test-time to DAFormer on the ACDC and Dark Zurich validation sets.</figDesc><table><row><cell>Method</cell><cell>mIoU ?</cell><cell></cell></row><row><cell></cell><cell>ACDC [41]</cell><cell>Dark Zurich [42]</cell></row><row><cell>DAFormer [15]</cell><cell>55.6</cell><cell>34.1</cell></row><row><cell>DAFormer + Test-Time Refign</cell><cell>56.8</cell><cell>38.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The sum of independent normally distributed random variables is normally distributed with mean/var. being the sum of the means/vars.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/lhoyer/DAFormer, https://github.com/vikolss/DACS</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was supported by the ETH Future Computing Laboratory (EFCL), financed by a donation from Huawei Technologies.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual topometric localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hern?n</forename><surname>Badino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent vehicles symposium (IV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ronan Collobert, and Jason Weston. Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Boreas: A multi-season autonomous driving dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenan</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Zou</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Kang</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Curriculum model adaptation with synthetic and real data for semantic foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="1182" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-domain correlation distillation for unsupervised domain adaptation in nighttime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1789" to="1819" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hrda: Context-aware high-resolution domain-adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="492" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">COTR: correspondence transformer for matching across images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Benchmarking the robustness of semantic segmentation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kamann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning texture invariant representation for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeongjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A cross-season correspondence dataset for robust semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Mans Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FIFO: Learning fog-invariant features for foggy scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyoung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dualresolution correspondence networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Megadepth: Learning singleview depth prediction from internet photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised foggy scene understanding via self spatial-temporal label diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3525" to="3540" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Both style and fog matter: Cumulative domain adaptation for semantic foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacheng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">1 year, 1000 km: The oxford robotcar dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DGC-Net: Dense geometric correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iaroslav</forename><surname>Melekhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Tiulpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Estimating the mean and variance of the target probability distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weigend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1994 ieee international conference on neural networks (ICNN&apos;94)</title>
		<meeting>1994 ieee international conference on neural networks (ICNN&apos;94)</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Don&apos;t worry about the weather: Unsupervised condition-dependent domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horia</forename><surname>Porav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bruls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bridging the day and night domain gap for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Barea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Guided curriculum model adaptation and uncertainty-aware evaluation for semantic nighttime image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic foggy scene understanding with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="973" to="992" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ACDC: The Adverse Conditions Dataset with Correspondences for semantic driving scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Map-guided curriculum domain adaptation and uncertaintyaware evaluation for semantic nighttime image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3139" to="3153" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning from synthetic data: Addressing domain shift for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structurefrom-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ransac-flow: generic two-stage image alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Darmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">See clearer at night: towards robust nighttime semantic segmentation through day-night image conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaite</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning in Defense Applications</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Joint optimization framework for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoharu</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dacs: Domain adaptation via crossdomain mixed sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rain rendering for evaluating and improving robustness to bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raoul</forename><forename type="middle">De</forename><surname>Shirsendu Sukanta Halder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Fran?ois</forename><surname>Charette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="360" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Glunet: Global-local universal network for dense flow and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Pdc-net+: Enhanced probabilistic dense correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Warp consistency for unsupervised learning of dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Probabilistic warp consistency for weaklysupervised semantic correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prune</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Proselflc: Progressive self label correction for training robust deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil M</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Theoretical analysis of self-training with deep networks on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kendrick</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">DANNet: A one-stage domain adaptation network for unsupervised nighttime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A one-stage domain adaptation network with image alignment for unsupervised nighttime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Incremental adversarial domain adaptation for continually changing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Wulfmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICRA</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binhui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoren</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Cdada: A curriculum domain adaptation for nighttime semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengnian</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep multi-view learning methods: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangdong</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">448</biblScope>
			<biblScope unit="page" from="106" to="129" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd annual meeting of the association for computational linguistics</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Understanding deep learning (still) requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Fully convolutional adaptation networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1106" to="1120" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Unsupervised scene adaptation with memory regularization in vivo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dannet</surname></persName>
		</author>
		<idno>DeepLabv2) [63] 88.6 53.4 69.8 34.0 20.0 25.0 31.5 35.9 69.5 32.2 82.3 44.2 43.7 54.1 22.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dania (deeplabv2</surname></persName>
		</author>
		<idno>64] 89.4 60.6 72.3 34.5 23.7 37.3 32.8 40.0 72.1 33.0 84.1 44.7 48.9 59.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gcma (refinenet</surname></persName>
		</author>
		<idno>38] 81.7 46.9 58.8 22.0 20.0 41.2 40.5 41.6 64.8 31.0 32.1 53.5 47.5 75.5 39.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cdada</surname></persName>
		</author>
		<idno>68] 90.5 60.6 67.9 37.0 19.3 42.9 36.4 35.3 66.9 24.4 79.8 45.4 42.9 70.8 51.7</idno>
		<imprint/>
	</monogr>
	<note>RefineNet</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dannet</surname></persName>
		</author>
		<idno>63] 90.4 60.1 71.0 33.6 22.9 30.6 34.3 33.7 70.5 31.8 80.2 45.7 41.6 67.4 16.8</idno>
		<imprint/>
	</monogr>
	<note>PSPNet</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ccdistill</surname></persName>
		</author>
		<idno>9] 89.6 58.1 70.6 36.6 22.5 33.0 27.0 30.5 68.3 33.0 80.9 42.3 40.1 69.4 58.1</idno>
		<imprint/>
	</monogr>
	<note>RefineNet</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dania (pspnet</surname></persName>
		</author>
		<idno>64] 91.5 62.7 73.9 39.9 25.7 36.5 35.7 36.2 71.4 35.3 82.2 48.0 44.9 73.7 11.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daformer</surname></persName>
		</author>
		<idno>15] 93.5 65.5 73.3 39.4 19.2 53.3 44.1 44.0 59.5 34.5 66.6 53.4 52.7 82.1 52.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
