<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
							<email>wangwenhai362@163.com</email>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfeng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the "global" property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within "local" windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM) strategy, successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed "UM-MAE" for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples 1 random patch from each 2 ? 2 grid, and a Secondary Masking (SM) which randomly masks a portion of (usually 25%) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up by ? 2? and reduces the GPU memory by at least ? 2?) of Pyramid-based ViTs, but maintains the competitive (or even better) fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The code and pre-trained models are available at https://github.com/implus/UM-MAE. Global window Pyramid-based ViT (e.g., PVT, Swin) ? Local window (a) (b) 75% mask drop 75% mask token Pyramid-based ViT (e.g., PVT, Swin) ? Local window (c) Pyramid-based ViT (e.g., PVT, Swin) MAE Decoder Uniform Sampling Secondary Masking 75% mask drop 25% mask token MAE-style Pre-training target input (d) Encoder: Figure 1: Illustration of various input strategy for supporting Vanilla ViT and Pyramid-based ViT. (a) The "global" window of Vanilla ViT can receive arbitrary subset of image patches by skipping random 75% of the total, whilst (b) skipping these 75% patches is unacceptable for Pyramid-based ViT as patch elements are not equivalent across the "local" windows. (c) A straightforward solution is to adopt the mask token for the encoder (e.g., SimMIM [42]) at the cost of slower training. (d) Our Uniform Masking (UM) approach (including Uniform Sampling and Secondary Masking) enables the efficient MAE-style pre-training for Pyramid-based ViTs while keeping its competitive fine-tuning accuracy.</p><p>2 Related Work Masked Image Modeling (MIM). The past few months have witnessed a huge surge in MIM. BEiT [2] first introduces the idea of BERT [11], which dominates the field of Natural Language Processing (NLP), into computer vision. Due to the large structural difference between images and languages, they adopt the dVAE [32] model to discretize image patches to obtain their class labels. The algorithm randomly masks the input image blocks, and expects the network to restore the categories of masked image patches according to the context. To further improve the quality of the discretized labels, PeCo [13] proposes to replace dVAE with VQ-VAE [36] model, and introduces perceptual loss in the reconstruction stage. iBOT [49] and data2vec <ref type="bibr" target="#b0">[1]</ref> attempt to use the mean teacher network to build an online discretization module, avoiding the additional and separate presteps of training the discretization model. CAE <ref type="bibr" target="#b5">[6]</ref> proposes to separate representation learning from the image reconstruction task as much as possible, resulting in more robust feature representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-Supervised Learning (SSL) uses auxiliary tasks to mine their own supervision from large-scale unlabeled data, and learns representations that are transferable to downstream tasks. SSL first shows great potential in Natural Language Processing (NLP) field by the "masked autoencoding" solutions of GPT <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3]</ref> and BERT <ref type="bibr" target="#b10">[11]</ref>. These techniques, through learning to predict the removed portion of data based on the available context, have innovated the paradigm of NLP pipelines.</p><p>Inspired by the success of BERT, the vision community has recently raised great interest in imitating its formulation (i.e., masked autoencoding) for image understanding. A series of works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref> has been proposed in past months, where Masked AutoEncoder (MAE) <ref type="bibr" target="#b18">[19]</ref> becomes one of the most representative methods which significantly optimizes both the pre-training efficiency and fine-tuning accuracy, successfully leading the new trend of SSL across vision tasks.</p><p>One of the most impactful designs in MAE is the asymmetric encoder-decoder architecture. Different from the decoder part where it receives entire sequence of patch tokens, the encoder part only takes the visible image patches (usually only 25% of the total) as input. Interestingly, such design not only significantly reduces the pre-training complexity, but also obtains superior fine-tuning performance.</p><p>It is worth noting that the successful application of the asymmetric structure relies on the "global" property of Vanilla Vision Transformer (ViT) <ref type="bibr" target="#b13">[14]</ref>, whose self-attention mechanism can reason over arbitrary subset of discrete image patches (see <ref type="figure">Fig. 1 (a)</ref>). Nevertheless, the "global" property of Vanilla ViT is a double-edged sword: When transferring Vanilla ViT to downstream vision tasks with considerably larger input resolution (e.g., ? 1000 2 in object detection), its large memory requirement is challenging for modern GPUs due to the quadratic complexity of the global self-attention. Although ViTDet <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> attempts to partially restrict the local, windowed self-attention for certain blocks of Vanilla ViT during fine-tuning, the optimal architecture is unknown considering that the information flow can be arbitrarily different between pre-training and fine-tuning stage. Therefore, to address the above issue, the Pyramid-based ViTs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> can be the preferred options since they are more storage-friendly via naturally introducing the local window operators, and they have already demonstrated the great compatibility and advance in transferring to downstream vision tasks <ref type="bibr" target="#b20">[21]</ref>.</p><p>However, it is still unclear to effectively pre-train Pyramid-based ViTs using the efficient asymmetric structure likewise in MAE, as they are commonly equipped with operators in "local" windows. Specifically, the amount of visible elements in each local window is usually not equal, which prevents the effective parallel computation of window-based operators (see <ref type="figure">Fig. 1 (b)</ref>). A compromise solution is to recall back the dropped patches as proposed in SimMIM <ref type="bibr" target="#b41">[42]</ref>, where all the masked regions PVT Forward (reorganized compact input) <ref type="figure">Figure 2</ref>: The equivalence between forwarding original size input with masked place holders and reorganized compact input for PVT <ref type="bibr" target="#b36">[37]</ref>. We typical show three stages with pyramid spatial sizes. The spatial reduction window is performed over non-overlapped regions as in PVT.</p><p>are modelled as learnable, shared mask tokens (see <ref type="figure">Fig. 1 (c)</ref>). Consequently, it sacrifices a lot of efficiency due to the considerably large computation and storage complexity for the encoders.</p><p>To successfully enable MAE pre-training (i.e., adopt the efficient asymmetric structure) for Pyramidbased ViTs with locality, in this paper, we propose the Uniform Masking (UM) strategy that contains the Uniform Sampling (US) and Secondary Masking (SM) step. As illustrated in <ref type="figure">Fig. 1 (d)</ref>, US first strictly samples 1 random patch from each 2 ? 2 grid, resulting in 75% drop of the image. Compared to the completely random 75% drop in MAE, US has more risks to leak semantic clues as its sampled patches are distributed more uniformly than the Random Sampling (RS) of MAE, which reduces the difficulty of the pixel recovery pre-task (see the "Pre-train Loss" of <ref type="table">Table 1</ref>) and hinders the representation learning (see <ref type="table">Table 1</ref>). Therefore, SM is further introduced to randomly mask a (relatively small, e.g., 25%) portion of the already sampled regions by US as shared, learnable tokens. Finally, the uniform-sampled patches, together with the secondary-masked tokens, are reorganized as a compact 2D input under a quarter of original image resolution, to sequentially feed through the Pyramid-based ViT encoder and MAE decoder for the reconstruction of the raw image pixels.</p><p>Our contributions are summarized as follows: (1) We propose Uniform Masking, which successfully enables MAE pre-training (i.e., UM-MAE) for popular Pyramid-based ViTs; <ref type="bibr" target="#b1">(2)</ref> We empirically show that UM-MAE considerably speeds up pre-training efficiency by ? 2? and reduces the GPU memory consumption by at least ? 2? compared to the existing sota Masked Image Modelling (MIM) framework, whilst maintaining the competitive fine-tuning performance; (3) We reveal and discuss several notable different behaviors between Vanilla ViT and Pyramid-based ViTs under MIM. The equivalence between forwarding original size input with masked place holders and reorganized compact input for Swin <ref type="bibr" target="#b27">[28]</ref>. We typically show the case of non-overlapped local self-attention window along with its shifted version in consecutive blocks of the same stage. Different from recovering discretized labels (i.e., high-dimensional semantic labels) of image patches, the Masked Autoencoder <ref type="bibr" target="#b18">[19]</ref> (MAE) method is the first to demonstrate that high-quality image representations can also be obtained by directly recovering the original pixels of an image. MAE utilizes an asymmetric encoder-decoder design, that is, the encoder only receives visible image blocks as input in the feed-forward stage, which simultaneously improves the pre-training efficiency and achieves SOTA results on multiple image classification datasets. Concurrently, SimMIM <ref type="bibr" target="#b41">[42]</ref> applies learnable mask tokens on the masked input patches and performs a similar self-supervised scheme to Swin Transformer <ref type="bibr" target="#b27">[28]</ref>, a Pyramid-based variant of ViT <ref type="bibr" target="#b13">[14]</ref>, achieving a competitive improvement. MaskFeat <ref type="bibr" target="#b38">[39]</ref> proves that the feature descriptor of the predicted image (e.g., HOG <ref type="bibr" target="#b8">[9]</ref>), instead of raw pixels, can further improve the fine-tuning performance.</p><p>Pyramid-based Vison Transformer. Ever since the successful application of Vanilla ViT <ref type="bibr" target="#b13">[14]</ref> in image classification, the vision community has put a lot of effort into researching its pyramid-based (i.e., hierarchical) variants <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b15">16]</ref>, to inherit the advantages of the pyramid structure <ref type="bibr" target="#b24">[25]</ref> in the downstream vision tasks. There are two representative series among these variants: PVT <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> and Swin <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>. Generally, PVT introduces non-overlapped Spatial-Reduction Window (SRW) to reduce the complexity of global self-attention mechanism, whilst Swin restrains self-attention operator inside non-overlapped, shifted local windows. Given their popularity, this paper focuses on these two representative architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We propose Uniform Masking (UM) to enable the support of MAE-style pre-training for Pyramidbased ViTs. UM is a simple and two-stage strategy that converts dense image tokens into sparse ones but spatially preserves their uniform distribution. It consists of two stages including Uniform Sampling (US) and Secondary Masking (SM), which together guarantee the efficiency and quality of self-supervised learning representations. We introduce them respectively in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Uniform Sampling</head><p>As illustrated in <ref type="figure">Fig. 1</ref>, UM firstly perform Uniform Sampling (US) that samples 25% 2 visible image patches with a uniform constraint: 1 of every 2 ? 2 blocks is strictly sampled across the image space. Similarly following MAE <ref type="bibr" target="#b18">[19]</ref>, the left 75% masked patches are dropped and would not participate in the feed-forward process of the encoders. US brings a desirable property to input data: the amount of elements is ensured equal across the shifted (and non-overlapped <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b27">28]</ref>) local windows. This property thus makes it possible for most popular Pyramid-based ViTs (e.g., PVT <ref type="bibr" target="#b36">[37]</ref> and Swin <ref type="bibr" target="#b27">[28]</ref>) to process the discrete subset of image patch sequence, as these sampled patches can be reorganized as a compact 2D image (see <ref type="figure">Fig. 1 (d)</ref>, 2, 3) thanks to the equivalence of the operated elements. Taking the representative Pyramid-based ViTs: PVT <ref type="bibr" target="#b36">[37]</ref> and Swin <ref type="bibr" target="#b27">[28]</ref> as examples, we elaborate on the details of how US makes the uniform-distributed, sparse patches compatible for these architectures with locality.</p><p>Compatibility with PVT <ref type="bibr" target="#b36">[37]</ref>. As shown in <ref type="figure">Fig. 2</ref>, PVT introduces Spatial-Reduction Window (SRW) to reduce the complexity of the original self-attention block which builds relations over each feature pair. The information in SRW would be aggregated to present the keys and values with considerably reduced scales. In <ref type="figure">Fig. 2</ref>, starting from the input image with uniformly sampled patches, we show the first three stages of PVT and mark its typical spatial-reduction hyper-parameters as {8, 4, 2} sequentially in the upper pipeline. The masked patches are considered as blank place holders inside the original image/feature space. We demonstrate that for PVT, by simultaneously reorganizing the original visible input to its compact form and halving the edge sizes of spatial-reduction windows (i.e., {4, 2, 1}) accordingly, the effective elements are equivalent between corresponding local windows across the two pipelines. Therefore, US is compatible with PVT, and the input elements are reduced by 75% in case of the encoder part when using the reorganized compact 2D image as input.</p><p>Compatibility with Swin <ref type="bibr" target="#b27">[28]</ref>. Swin has two major differences from PVT: (1) the Local Self-Attention Window (LSAW) size is usually fixed across stages; and (2) it has a Shifted version of the Local Self-Attention Window (SLSAW) in successive blocks, which introduces connections between neighboring non-overlapping windows. Based on these differences, we deduce that Swin has more restrictions on the selection of window size and input image scale during pre-training: a window (and input image) size of 16 ? 2 n ? 16 ? 2 n (n ? N) is necessary to ensure the equivalence when considering the shifted cases with a shift offset being 8 ? 2 n ? 8 ? 2 n , as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. Under the above constraints, the effective elements are equivalent between corresponding (shifted) local windows across the two pipelines, similar to PVT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Secondary Masking</head><p>Compared to Random Sampling as adopted in MAE <ref type="bibr" target="#b18">[19]</ref>, Uniform Sampling (US) samples the image patches uniformly distributed over the 2D space that makes it compatible for representative Pyramid-based ViTs. However, US can potentially make the self-supervisory task less challenging by providing shortcuts for pixel reconstruction purely through the neighbouring low-level image statistics. It has already been evidenced in MAE, where a similar sampling strategy termed "Grid-wise Sampling" largely hinders the representation quality. It is also observed that in <ref type="table">Table 1</ref>, the uniform distribution by US indeed reduces the difficulty of the pre-training -the pre-train loss decreases from 0.4256 (MAE Baseline) to 0.3858, but impedes the quality of learned representations for downstream tasks -the fine-tuning accuracy is dropped by ?1 absolute mIoU point in semantic segmentation.</p><p>To address the degradation issues brought by US, we further propose the Secondary Masking (SM) strategy that performs a secondary random mask among the already sampled visible patches of US, as illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref> from (c) to (d). Different from the US stage where the masked patches are entirely dropped, SM keeps the masked patches by using shared mask tokens for the compatibility of Pyramid-based ViTs with locality (see Sec. 3.1). The simple operation thus increases the difficulty of semantic recovery pre-task, which focuses the network on learning high-quality representations over the incomplete visual context, without heavily relying on the neighbouring low-level pixels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">UM-MAE Pipeline with Pyramid-based ViT</head><p>Fig <ref type="figure" target="#fig_3">. 5</ref> demonstrates the detailed asymmetric design (i.e., the MAE-style pipeline) of our approach when applied to typical Pyramid-based ViTs (e.g., PVT and Swin). For easier reference, we term our method as UM-MAE and describe its major components as follows.</p><p>Encoder. We follow MAE <ref type="bibr" target="#b18">[19]</ref> where the operation units are 16 ? 16 image patches. The proposed Uniform Masking is performed to obtain the compact, reorganized 2D input (consisting of both visible patches and mask tokens). It is with reduced scale (i.e., 25% of the full set) as the input for the encoder. Each mask token <ref type="bibr" target="#b10">[11]</ref> is a shared, learned embedding vector that indicates the presence of a miss. Whether to append the positional embeddings here is decided by the specific architectures used in the encoder (e.g., "yes" for PVT and "no" for Swin by default). However, hierarchical ViTs usually have four stages with a total stride being 32, which makes the 16 ? 16 unit become a sub-pixel finally. Therefore, the sub-pixel convolution <ref type="bibr" target="#b32">[33]</ref> is adopted (i.e., PixelShuffle operator in Pytorch <ref type="bibr" target="#b28">[29]</ref>) to recover its resolution before feeding the learned representations to the decoder.</p><p>Decoder. The decoder part is similar to that of MAE. The input to the decoder is the full set of tokens including (i) encoded patches from UM, and (ii) mask tokens. Positional embeddings are added to all tokens and the architecture of decoder refers to the lightweight Vanilla ViT as adopted in MAE.</p><p>Reconstruction Target. We reconstruct the input by predicting the pixel values (the normalized version in MAE) for each dropped patch during Uniform Masking. A linear projection is applied on the last layer of the decoder whose number of elements equals the number of pixel values in a patch. The loss function is Mean Squared Error (MSE) and it is computed only on the dropped patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this paper, as suggested in MAE <ref type="bibr" target="#b18">[19]</ref>, we focus on the fine-tuning accuracy (instead of linear probing) over a series of downstream tasks, including image classification using ImageNet-1K (IN1K) <ref type="bibr" target="#b9">[10]</ref>, semantic segmentation using ADE20K <ref type="bibr" target="#b46">[47]</ref> and object detection using COCO <ref type="bibr" target="#b25">[26]</ref>. The pre-training is conducted on IN1K training set with 256 2 resolution by default. Unless otherwise stated, we perform supervised fine-tuning (or learning from scratch) over image classification for 100 epochs with 256 2 resolution, semantic segmentation for 160K iterations with 512 2 resolution, and object detection for 25 epochs with 1024 2 resolution. We adopt UperNet <ref type="bibr" target="#b40">[41]</ref> as the segmentor and GFL <ref type="bibr" target="#b21">[22]</ref> as the object detector. The depth of decoder in UM-MAE is set to 2 for faster ablation iterations. To show the efficiency and effectiveness of our method, we mainly compare against the existing sota self-supervised approach, i.e., SimMIM <ref type="bibr" target="#b41">[42]</ref> that has already been adopted successfully in pre-training Pyramid-based ViT like Swin <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>. Two representative models from Pyramidbased ViTs, PVT-Small (PVT-S) and Swin-Tiny (Swin-T) are adopted in most ablation studies for experimental efficiency. Our codebase is built upon the official repositories of MAE, SimMIM, MMDetection and MMSegmentation <ref type="bibr" target="#b2">3</ref>  "Pyramid Support" denotes whether the strategy supports the Pyramid-based ViTs. "SM Ratio" denotes the Secondary Masking Ratio. "Pretrain Loss" refers to the converged pre-training loss value which can represent the difficulty of the self-supervisory task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pilot Experiments with Vanilla ViT</head><p>Instead of directly applying UM to Pyramid-based ViTs, we first conduct pilot experiments using UM strategy in pre-training Vanilla ViT to validate its effectiveness and capture a brief understanding of its mechanism. It is believed that "UM works for Vanilla ViT" is a necessary (but not sufficient) condition for "UM works for Pyramid-based ViTs".</p><p>Pilot Settings. ViT-Base (ViT-B/16) <ref type="bibr" target="#b13">[14]</ref> is adopted as the backbone in our pilot experiments. Following the official codes of MAE, we first do self-supervised pre-training (under 200 epochs) on the ImageNet-1K (IN1K) <ref type="bibr" target="#b9">[10]</ref> training set using different sampling strategies as illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. For fair comparisons, the number of input image patches and self-supervised signals are kept exactly the same across the sampling strategies during pre-training. Then we perform supervised fine-tuning over image classification (100 epochs, 224 2 resolution), semantic segmentation (160K iterations, 512 2 resolution) and object detection (25 epochs, 896 2 resolution). We adopt UperNet <ref type="bibr" target="#b40">[41]</ref> as the segmentor and GFL <ref type="bibr" target="#b21">[22]</ref> as the object detector. The other pre-training/fine-tuning hyper-parameters follow the descriptions from MAE <ref type="bibr" target="#b18">[19]</ref>. More details can be found in Supplementary Material. For the mask ratio of Secondary Masking in UM, we ablate their values in {15%, 25%, 35%}.</p><p>Results. In <ref type="table">Table 1</ref>, it is observed that the proposed UM with 25% SM Ratio is as competitive as the RS (MAE Baseline), whilst other variants lead to performance degradation during fine-tuning. The results of (a), (b), (c) show the more uniform the distribution, the simpler the pre-training task and the worse the transfer ability. Our proposed UM (with 25% SM Ratio) achieves the best trade-off.  Ablation Study on Secondary Masking Ratio. According to the optimal mask ratio (i.e., 25%) validated in the pilot experiment using Vanilla ViT, we continue to perform ablation study on Secondary Masking Ratio around 25% (i.e., {20%, 25%, 30%}) for Pyramid-based ViTs (here we take Swin-T as an example), which are the focus of this paper. The models fine-tuned on ImageNet are also used for fine-tuning on COCO object detection and ADE20K semantic segmentation tasks, following the intermediate fine-tuning scheme as proposed in BEiT <ref type="bibr" target="#b1">[2]</ref> and adopted in SwinV2 <ref type="bibr" target="#b26">[27]</ref>. The pre-training schedule is set as 200 epoch for efficiency. As shown in <ref type="table" target="#tab_2">Table 2</ref>, comprehensively considering the performance of multiple downstream tasks, a good choice of SM Ratio for Pyramid-based ViT is 25%, as same as that of Vanilla ViT. The ratio is thus fixed as 25% by default in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments with Pyramid-based ViT</head><p>Ablation Study on Pre-training Schedules. We ablate the pre-training schedules of both UM-MAE (with 25% SM Ratio) and SimMIM (with the optimal 25% RS as in MAE) from 200 to 800 to show its influence on the IN1K fine-tuning accuracy using PVT-S. We fix the fine-tuning epoch as 100. The supervised baseline of learning from "Scratch" for 100 epochs is also provided for a better comparison. <ref type="figure" target="#fig_4">Fig. 6</ref> shows that more pre-training epochs lead to better fine-tuning accuracy.</p><p>Ablation Study on Fine-tuning Schedules. We empirically discover that the self-supervised Pyramid-based ViTs using MIM can have consistent benefits from more fine-tuning epochs. Based on PVT-S which is first pre-trained using SimMIM and UM-MAE on ImageNet-1K for 200 epochs, we fine-tune the model again on ImageNet-1K for 100, 200 and 300 epochs, respectively. The    direct supervised baseline (i.e., learning from "Scratch") is also provided. <ref type="figure" target="#fig_5">Fig. 7</ref> demonstrates that UM-MAE performs on par with SimMIM, and they both outperform the supervised baseline by a non-trivial margin.</p><p>Efficiency. Compared to the SimMIM framework for self-supervising Pyramid-based ViTs, the core advantage of the proposed UM-MAE is the memory and runtime efficiency. In <ref type="table" target="#tab_4">Table 3</ref>  Performance on Large Models. The above ablation study mainly focuses on relatively small models for experimental efficiency. We are interested whether the proposed approach can scale to large architectures, e.g., Swin-Large (Swin-L) <ref type="bibr" target="#b27">[28]</ref>. Following the settings of SimMIM <ref type="bibr" target="#b41">[42]</ref>, we apply UM-MAE pre-training for 800 epochs and perform fine-tuning for 100 epochs on IN1K. <ref type="table" target="#tab_6">Table 4</ref> reports the Top-1 accuracy, showing that our UM-MAE maintains the competitive performance on large-scale models. The model is further fine-tuned on COCO under the same HTC++ <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref> framework and evaluated on COCO validation set.   MAE has 18.75% valid input image patches in fact). In <ref type="figure" target="#fig_6">Fig. 8</ref>, we observe that they all roughly recover the semantic details close to the original images, whilst the results of SimMIM may be over-smooth sometimes, e.g., the long right leg of the frog.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion: Different Behaviors between Vanilla ViT and Pyramid-based ViT under MIM</head><p>For downstream tasks with dense predictions, we empirically discover several notable differences between Vanilla ViT and Pyramid-based ViT under the framework of MIM. Take the semantic segmentation task on ADE20K as an example, we mainly demonstrate the following two aspects:</p><p>(1) The performance of direct fine-tuning self-supervised Pyramid-based ViTs under MIM lags far behind the intermediate fine-tuning scheme as proposed in BEiT <ref type="bibr" target="#b1">[2]</ref>. From  <ref type="bibr" target="#b18">[19]</ref>). Considering the lack of intrinsic inductive bias in modeling local visual structures, we deduce that the Vanilla ViT can benefit much more from the pre-training MIM stage as it usually suffers from many optimization problems (e.g., attention collapse <ref type="bibr" target="#b47">[48]</ref>). MIM tends to provide a fantastic initialization and probably brings effective inductive bias (see the next paragraph) for Vanilla ViT through meaningful semantic reconstruction over the visual context. Therefore, it has a great chance to avoid the bad local minimums and fully unleash the potential of Vanilla ViT during supervised fine-tuning on downstream tasks with dense predictions.</p><p>(2) Layer-wise learning rate decay <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2]</ref> is crucial for Vanilla ViT to obtain the optimal performance, but is harmful for Pyramid-based ViTs. As demonstrated in <ref type="table" target="#tab_12">Table 7</ref>, when fine-tuning models on ADE20K, Vanilla ViT relies heavily on the layer-wise learning rate decay strategy, whilst Pyramid-based ViTs have the opposite effect. We suspect that the pre-training MIM step probably brings effective inductive bias for Vanilla ViT especially under its early layers, thereby a relatively small learning rate at early stages would not dramatically change the pre-learned implicit local structures which preserve the generalization ability. In contrast, Pyramid-based ViTs have already introduced the inductive bias via local window operators, thus there is probably no need for them to apply the layer-wise learning rate decay strategy in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In     Swin Architecture. The Swin architecture generally follows the first version of Swin <ref type="bibr" target="#b27">[28]</ref>. To flexibly transfer the relative position biases to downstream tasks with arbitrary input scales, we refer to the continuous approach of <ref type="bibr" target="#b26">[27]</ref> where a small meta network is adopted to produce the biases based on the log-spaced relative coordinates. The architectures and pre-training/fine-tuning resolutions are kept the same under different MIM methods in our ablation experiments for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Pre-training and Fine-tuning Settings on ImageNet-1K</head><p>ImageNet-1K [10] contains 1.3M images of 1K categories for training and 50K images for validation. The larger dataset, ImageNet-22K (IN22K), which contains 14.2M images and 22K classes, is not used at all in our method. In this paper, the pre-training is conducted on ImageNet-1K training set. The default setting is in <ref type="table" target="#tab_13">Table 8</ref> in case of pre-training for 200 epochs, where most of the configuration values refer to that of MAE <ref type="bibr" target="#b18">[19]</ref>. The warmup epoch is linearly scaled when the pre-training epoch increases to 400 or 800. To train the models which exceeds the GPU memory, we halve the batch size per GPU (e.g., 128 ? 64) and double the accumulation step (e.g., 4 ? 8) for maintaining the same effective global batch size. We follow MAE and use the linear lr scaling rule: lr = base_lr?globalbatchsize / 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Object Detection on COCO</head><p>The COCO benchmark <ref type="bibr" target="#b25">[26]</ref> consists of trainval35k (115K images) for training and minival (5K images) for validation. We adopt the one-stage GFL <ref type="bibr" target="#b21">[22]</ref> detector on COCO dataset. Specifically, we replace the backbone of GFL with the pre-trained Vanilla ViT, PVT and Swin, respectively. The hyper-parameters mainly follow the default settings of GFL for PVT in mmdetection <ref type="bibr" target="#b4">[5]</ref>. We report the performance using AdamW optimizer under a 25 epochs cosine schedule with a base learning rate of 1e-4, a weight decay of 0.05, and augmentation by large-scale jitter with a scale range of [0.1, 2.0]. The batch size is 16, distributed across 8 GPUs (i.e., 2 images per GPU).</p><p>For Vanilla ViT (ViT-B in this paper), we use the local-global adaptation during fine-tuning as proposed in <ref type="bibr" target="#b23">[24]</ref>, where the network blocks are divided into 4 subsets, each consisting of a last global-window block and several local-window blocks otherwise. The intermediate feature maps at the tail of each subset are applied with convolutions to upsample or downsample to different scales for matching the input requirement of FPN <ref type="bibr" target="#b24">[25]</ref>. The default training resolution is 1024 2 as suggested in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23]</ref>. However, even under the memory-friendly implementation of the local-global adaptation <ref type="bibr" target="#b23">[24]</ref>, 1024 2 input resolution exceeds the memory limitation of our GPU (24 GB), thus we choose a slightly smaller size 896 2 during training.</p><p>For PVT (PVT-S) and Swin (Swin-T), the input resolution is 1024 2 during training. As these hierarchical ViTs already contain pyramid feature maps that are compatible with FPN, there is no need to apply further adaptations like ViT-B does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Semantic Segmentation on ADE20K</head><p>ADE20K has 20K training images and 2K validation images of 150 categories. We use UperNet <ref type="bibr" target="#b40">[41]</ref> following the code of <ref type="bibr" target="#b1">[2]</ref>. The 16K-iteration polynomial learning rate schedule is applied with the first warm-up 1500 iterations. The AdamW optimizer is adopted with an initial learning rate of 1e-4, a weight decay of 0.05 and a batch size of 16 across 8 GPUs. The training resolution is 512 2 .</p><p>For Vanilla ViT (ViT-B in this paper), following MAE, the relative position bias is turned on during transfer learning, initialized as zero. The layer-wise lr decay <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2]</ref> is set as 0.65 by default.</p><p>For PVT (PVT-S) and Swin (Swin-T), the layer-wise lr decay is not applied by simply setting it as 1.0, which always performs best in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Visualization Results</head><p>We demonstrate more visualization results based on Swin-T under UM-MAE and SimMIM framework in <ref type="figure" target="#fig_7">Fig. 9</ref>. It is observed that the results of SimMIM are smoother than that of UM-MAE usually, yet are sometimes too smooth causing distortion. Although there are some differences in their visual reconstructions, the performances of fine-tuning are very close.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The equivalence between forwarding original size input with masked place holders and reorganized compact input for Swin [28]. We typically show the case of non-overlapped local self-attention window along with its shifted version in consecutive blocks of the same stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Different sampling strategy with a sample ratio of 25%. (a) Random Sampling (RS) in MAE [19]; (b) Grid-wise Sampling (GS) in MAE; and the proposed (c) Uniform Sampling (US); (d) Uniform Masking (UM) that includes US and Secondary Masking (SM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>The Uniform Masking MAE pipeline (UM-MAE) for Pyramid-based ViTs. The image patches are reorganized as 2D compact input after Uniform Masking as the input of Pyramid-based ViT Encoder (e.g., PVT, Swin). A Pixel Shuffle<ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29]</ref> layer is applied to recover the feature resolution due to the default large stride of the encoder backbone. The resulted representations are further fed into the MAE Decoder (lightweight Vanilla ViT) with mask tokens for the reconstruction of the original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Pre-training schedules. Longer pre-training epoch brings a slight improvement on fine-tuning accuracy. All the models (including the "Scratch") are fine-tuned/supervised for 100 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Fine-tuning schedules. Longer fine-tuning epoch leads to consistent improvement, while the direct supervised scheme (i.e., Scratch) lags behind the self-supervised counterparts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Uncurated reconstruction visualizations under the same 75% mask pattern. The models are both pre-trained for 800 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Uncurated random samples on ImageNet validation images. The masking ratio is 75%. The Swin-T models under UM-MAE and SimMIM are both pre-trained for 800 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Secondary Masking Ratio. Based on Swin-T, we pre-train models using different SM ratios for 200 epochs. The Top-1 Accuracy for IN1K, mIoU for ADE20K and AP for COCO are reported. 25% performs good overall considering multiple tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Efficiency comparisons with representative fine-tuning performance. The real time (hours) and memory consumption (GB) are evaluated on 8 GeForce RTX 3090 GPUs with 128 images per GPU ( * denotes the estimation due to memory limitation 24 GB). To train Swin-T under SimMIM which exceeds the GPU memory, we halve the batch size per GPU and double the accumulation step for maintaining the same effective global batch size. The models fine-tuned on ImageNet are also used for fine-tuning on COCO object detection and ADE20K semantic segmentation (i.e., the intermediate fine-tuning scheme [2, 27]). Compared to SimMIM, UM-MAE speeds up by ? 2? and reduces the memory by at least ? 2?, whilst performing competitively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, we show their clear comparisons based on PVT-S and Swin-T in case of pre-training over 200 epochs. The fine-tuning and supervised-from-scratch performances are as well demonstrated. The statistics of pre-training time and memory usage are evaluated on 8 GeForce RTX 3090 GPUs. It is observed that the proposed UM-MAE significantly saves almost half of the pre-training hours and around 1 2 to 2 3 GPU memory budgets against SimMIM, where their performances under multiple downstream tasks are comparable (and sometimes better, e.g., 45.96 vs. 45.35 mIoU on ADE20K).</figDesc><table><row><cell cols="4">Method (Swin-L) P-Size EP-Size IN1K</cell></row><row><cell>SimMIM [42]</cell><cell>192 2</cell><cell>192 2</cell><cell>85.4</cell></row><row><cell>UM-MAE (ours)</cell><cell>256 2</cell><cell>128 2</cell><cell>85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>IN1K Performance on large models. The result of SimMIM is borrowed from the paper<ref type="bibr" target="#b41">[42]</ref>. "EP-Size" denotes the Effective Pre-training Size. The reorganized compact 2D input in UM-MAE quarters the Pretraining Size from 256 2 to 128 2 by dropping 75% tokens.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>demonstrates that the pre-trained backbone self-supervised under UM-MAE only in IN1K can even outperform the one supervised in IN22K, using only half the training epochs of Baseline.</figDesc><table><row><cell>HTC++ (Swin-L)</cell><cell>Pre-train</cell><cell cols="3">Epoch AP box AP mask</cell></row><row><cell>Baseline [28]</cell><cell>IN22K, sup</cell><cell>72</cell><cell>57.1</cell><cell>49.5</cell></row><row><cell cols="2">UM-MAE (ours) IN1K, unsup</cell><cell>36</cell><cell>57.4</cell><cell>49.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>COCO Performance on large models. The result of the Baseline refers to the paper<ref type="bibr" target="#b27">[28]</ref>. "Epoch": training schedule on COCO.</figDesc><table><row><cell>Visualization of Reconstructions. Based on</cell></row><row><cell>Swin-T, we visualize the image reconstruction</cell></row><row><cell>results for both UM-MAE and SimMIM, using</cell></row><row><cell>a shared mask drawn from Uniform Sampling.</cell></row><row><cell>Note that for UM-MAE we further perform Sec-</cell></row><row><cell>ondary Masking at a ratio of 25% as input mask</cell></row><row><cell>embeddings for final reconstruction (i.e., UM-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>, we observe that the self-supervised Pyramid-based ViTs under MIM framework are not competitive in directly fine-tuning downstream tasks with dense predictions. However, an intermediate finetuning process on IN1K can significantly improve its performance, which even outperforms the supervised baseline by a considerable margin (44.51 ? 45.96). Following BEiT [2], the intermediate fine-tuning further trains the model on an intermediate dataset after MIM pre-training, and then fine-tunes the model on the target downstream tasks. The similar scheme is also supported in SwinV2 [27], where all the demonstrated top results of Swin Transformers on downstream dense prediction tasks contain an intermediate fine-tuning step, instead of directly fine-tuning the model self-supervised by SimMIM [42] on the target tasks. This phenomenon is quite different from the case of Vanilla ViT, where the self-supervised Vanilla ViT after MIM pre-training can directly beats the supervised counterparts (e.g. on ADE20K, 45.3 ? 45.6 in BEiT [2] and 47.4 ? 48.1 in MAE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>The</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="3">Pre-train lw-lr decay mIoU mAcc aAcc</cell></row><row><cell></cell><cell>ViT-B</cell><cell>1600, MAE</cell><cell>1.00 0.85 0.65</cell><cell>45.87 56.19 82.46 47.80 58.26 83.27 48.15 58.99 83.05</cell></row><row><cell>effectiveness of intermediate fine-tuning based on</cell><cell>Swin-T</cell><cell>200, UM-MAE</cell><cell>1.00 0.85 0.65</cell><cell>45.96 56.55 81.58 45.80 56.68 81.45 45.42 56.43 80.74</cell></row><row><cell>Swin-T when transferring it to downstream semantic segmen-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>tation tasks. The intermediate fine-tuning is crucial for Pyramid-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>based ViTs self-supervised by various MIM frameworks. "Inter-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mediate" denotes the usage of intermediate fine-tuning [2, 27] on</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IN1K for 100 epochs, after the self-supervising process.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>The effectiveness of layer-wise learning rate decay ("lwlr decay" for short). Under MIM, lw-lr decay is crucial for Vanilla ViT, but is harmful for Pyramid-based ViTs. "lw-lr decay" being 1.00 means no decay applied. The pre-trained ViT-B model is downloaded from the official MAE github.SimMIM, UM-MAE significantly improves the pre-training efficiency in both memory and runtime of Pyramid-based ViTs but maintains the competitive fine-tuning performance. We also discuss several empirical findings about the different behaviors between Vanilla ViT and Pyramid-based ViT under the framework of MIM. We hope our approach and discovery can faithfully inspire the vision community and push forward the topic of MIM.</figDesc><table><row><cell>config</cell><cell>Pre-train Value</cell><cell>Fine-tune Value</cell></row><row><cell>optimizer</cell><cell>AdamW</cell><cell>AdamW</cell></row><row><cell>base learning rate</cell><cell>1.5e-4</cell><cell>5e-4</cell></row><row><cell>weight decay</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>optimizer momentum</cell><cell>?1, ?2=0.9, 0.95</cell><cell>?1, ?2=0.9, 0.999</cell></row><row><cell>layer-wise lr decay</cell><cell>1.0</cell><cell>0.7 (Swin-L), 0.8 (ViT-B), 0.85 (Swin-T, PVT-S)</cell></row><row><cell cols="2">global batch size (over 8 GPUs) 4096</cell><cell>1024</cell></row><row><cell>batch size per GPU</cell><cell>128</cell><cell>64</cell></row><row><cell>accumulated iteration</cell><cell>4</cell><cell>2</cell></row><row><cell>learning rate schedule</cell><cell>cosine decay</cell><cell>cosine decay</cell></row><row><cell>warmup epochs</cell><cell>10</cell><cell>5</cell></row><row><cell>augmentation</cell><cell cols="2">RandomResizedCrop RandAug (9, 0.5)</cell></row><row><cell>label smoothing</cell><cell>-</cell><cell>0.1</cell></row><row><cell>mixup</cell><cell>-</cell><cell>0.8</cell></row><row><cell>cutmix</cell><cell>-</cell><cell>1.0</cell></row><row><cell>drop path</cell><cell>-</cell><cell>0.1 (ViT-B, Swin-T, PVT-S), 0.2 (Swin-L)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Pre-training and fine-tuning settings on ImageNet-1K dataset.</figDesc><table><row><cell>Origin</cell><cell>Mask</cell><cell>UM-MAE SimMIM</cell><cell>Origin</cell><cell>Mask</cell><cell>UM-MAE SimMIM</cell><cell>Origin</cell><cell>Mask</cell><cell>UM-MAE SimMIM</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here 25% sample ratio empirically refers to the optimal mask ratio 75% in MAE<ref type="bibr" target="#b18">[19]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/mae, https://github.com/microsoft/SimMIM, https://github.com/open-mmlab</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Broader Impacts</head><p>The proposed method may generate inexistent images and introduce biases from the learned training dataset, which would contain possible negative societal impacts. These issues warrant further research and consideration when using the pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Architectures</head><p>Vanilla ViT Architecture. We follow the standard Vanilla ViT architecture adopted in MAE <ref type="bibr" target="#b18">[19]</ref>. The encoder and decoder share the similar structures which consist of a stack of Transformer blocks. The positional embeddings use the sine-cosine version as well. To fine-tune image classification tasks with Vanilla ViT, we extract globally averaged feature vector from the encoder output.</p><p>PVT Architecture. The PVT architecture generally follows the first version of PVT <ref type="bibr" target="#b36">[37]</ref>. The positional embeddings use the sine-cosine version as well, and they are adopted only in the input stage. In order to flexibly adjust the operation size of SRW (e.g., from window size 4 to 8) when transferring to downstream tasks, we adopt the spatial pooling operator as proposed in <ref type="bibr" target="#b37">[38]</ref> for the implementation of SRW. The architectures and pre-training/fine-tuning resolutions are kept the same under different MIM methods in our ablation experiments for fair comparisons.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03555,2022.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Context autoencoder for self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shentong</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03026</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Electra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno>CVPR, 2022. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Peco: Perceptual codebook for bert pre-training of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12710</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno>ICCV, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Container: Context aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno>ICCV, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cmt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06263,2021.4</idno>
		<title level="m">Convolutional neural networks meet vision transformers</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<idno>ICCV, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploring plain vision transformer backbones for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16527</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Benchmarking detection transfer learning with vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11429</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Swin transformer v2: Scaling up capacity and resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners. OpenAI blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>CVPR, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12602</idno>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pvt v2: Improved baselines with pyramid vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Visual Media</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Masked feature prediction for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09133</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ICCV, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00641,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno>ICCV, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rest: An efficient transformer for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinglong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Bin</forename><surname>Yang</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Nested hierarchical transformer: Towards accurate, data-efficient and interpretable visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sercan</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno>2022. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">ibot: Image bert pre-training with online tokenizer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
