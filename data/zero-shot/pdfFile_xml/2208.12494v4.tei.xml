<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRASP: Guiding model with RelAtional Semantics using Prompt for Dialogue Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Son</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<address>
									<country>Korea University Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<address>
									<country>Korea University Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwoo</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<address>
									<country>Korea University Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuiseok</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<address>
									<country>Korea University Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GRASP: Guiding model with RelAtional Semantics using Prompt for Dialogue Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The dialogue-based relation extraction (Dialo-gRE) task aims to predict the relations between argument pairs that appear in dialogue. Most previous studies utilize fine-tuning pretrained language models (PLMs) only with extensive features to supplement the low information density of the dialogue by multiple speakers. To effectively exploit inherent knowledge of PLMs without extra layers and consider scattered semantic cues on the relation between the arguments, we propose a Guiding model with RelAtional Semantics using Prompt (GRASP). We adopt a promptbased fine-tuning approach and capture relational semantic clues of a given dialogue with 1) an argument-aware prompt marker strategy and 2) the relational clue detection task. In the experiments, GRASP achieves state-of-theart performance in terms of both F1 and F1 c scores on a DialogRE dataset even though our method only leverages PLMs without adding any extra layers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The relation extraction (RE) task aims to extract semantic relations from unstructured text such as a sentence, a document, or even a dialogue. RE plays a critical role in information extraction and knowledge base construction as it can extract structured relational information <ref type="bibr" target="#b8">(Ji et al., 2010;</ref><ref type="bibr" target="#b19">Swampillai and Stevenson, 2010)</ref>. However, the utilization of sentence-level RE in a conversational setting is limited because numerous relational facts appear across multiple sentences with more than one speaker in a dialogue . Thus, the dialogue-based relation extraction (DialogRE) task, which includes argument pairs and their corresponding relations, has been proposed to encourage building a model that captures the underlying se-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R1</head><p>(Pheebs, PER) sister per:siblings (Ursula, PER)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R2</head><p>(S3, PER) none per:alternate_names (Pheebs, PER)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R3</head><p>(Rift's, ORG) works over at per:employees_or_members (Ursula, PER) <ref type="table">Table 1</ref>: Example of DialogRE data. The arguments are bold, and the triggers are underlined. The arguments and triggers are scattered throughout the dialogue, which leads to low information density. The triggers determine the direction of the proper relation indirectly. mantic spread in the dialogue, as presented in <ref type="table">Table  1</ref>  <ref type="bibr" target="#b23">(Yu et al., 2020)</ref>.</p><p>In previous studies where state-of-the-art (SoTA) performance is achieved on DialogRE benchmarks, fine-tuning is employed on pre-trained language models (PLMs) <ref type="bibr" target="#b10">(Lee and Choi, 2021;</ref><ref type="bibr" target="#b14">Long et al., 2021)</ref>, such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019)</ref>. As fine-tuning requires the addition of extra layers on top of the PLMs and the training objectives are different from those used in the pre-training phase, PLMs cannot effectively exploit their learned knowledge in the downstream task, resulting in less generalized capability .</p><p>To effectively utilize knowledge from PLMs, several studies involving prompt-based fine-tuning have been conducted. They employ the PLM di- (1) Fine-tuning</p><p>(2) Prompt-based Fine-tuning : Tokens in the vocabulary : Learnable Continuous tokens : Arguments <ref type="figure">Figure 1</ref>: An illustration of (1) standard fine-tuning approach, and (2) prompt-based fine-tuning approach. In prompt-based fine-tuning approach, a set of label words are mapped into a class set by a certain mapping function.</p><p>rectly as a predictor and completing a cloze task to bridge the gap between pre-training and finetuning <ref type="bibr" target="#b5">(Gao et al., 2020;</ref><ref type="bibr" target="#b7">Han et al., 2021)</ref>. As presented in <ref type="figure">Figure 1</ref>, prompt-based fine-tuning treats the downstream task as a masked language modeling (MLM) problem by directly generating the textual response to a given template. In specific, prompt-based fine-tuning updates the original input on the basis of the template and predicts the label words with the [MASK] token. Afterwards, the model maps predicted label words to corresponding task-specific class sets. However, the prompt-based fine-tuning approach is still not sufficient in terms of performance compared with the fine-tuning-based approach <ref type="bibr" target="#b7">(Han et al., 2021;</ref>. We attribute this phenomenon to the following properties of conversation: higher person-pronoun frequency <ref type="bibr" target="#b20">(Wang and Liu, 2011)</ref> and lower information density <ref type="bibr" target="#b2">(Biber, 1991)</ref> by multiple speakers 1 . Therefore, a prompt-based fine-tuning approach that collects the sparse semantics in the dialogue is required to understand relation between the arguments.</p><p>We propose a method named Guiding model with RelAtional Semantics using Prompt (GRASP) for DialogRE. To maximize the advantages of the prompt-based fine-tuning approach for the DialogRE task, we suggest an argument-aware prompt marking (APM) strategy and a relational clue detection (RCD) task. The APM strategy guides the model to the significant arguments scattered in the dialogue by carefully considering arguments. For our APM strategy, we conduct empirical study based on the diverse marker types to validate our APM strategy. Along with the APM strategy, the suggested RCD task with a training objective leads the model to pay attention to significant relational clues. Specifically, the model is trained to determine whether each token in a dialogue belongs to a subject, object, or trigger. As a result, the PLM is trained on RCD and MLM jointly. In the experiments, our method achieves SoTA performance at a significant level in the DialogRE task for both the full-shot and few-shot settings. Only PLMs are employed without the addition of an extra layer as a predictor, and GRASP exhibits a higher performance than other baselines. The significant performance improvement indicates that attending to significant semantic clues guides the PLMs to predict the correct relation with its inherent knowledge in both full-shot and few-shot settings. Moreover, we provide ablation studies and qualitative analysis on the robustness of GRASP.</p><p>Our contributions are as follows:</p><p>? We adopt a prompt-based fine-tuning approach to utilize a PLM's inherent knowledge directly for dialogues with relatively low information density.</p><p>? We introduce an APM strategy and a RCD task that guide PLMs on the significant relational clues, which are semantic information to predict relations.</p><p>? We demonstrate that our proposed method achieves SoTA performance on the DialogRE task in both full-shot and few-shot settings.</p><p>? We conduct ablation studies and qualitative analysis to validate the robustness of GRASP.</p><p>The remainder of this paper is organized as follows. In Section 3, we present the entire process of our method in detail. The experimental setup and the results are explained in Section 4. The further analyses is provided in Section 5, and Section 6 presents the conclusions. Appendix 2 provides related works including the prompt-based learning, and the DialogRE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Prompt-based learning Prompt-based learning is a method of reducing the gap between the pretraining objective and that of fine-tuning. For example, language models such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>   . Also, prompt-based learning shows better performance than fine-tuning especially in the few-shot setting <ref type="bibr" target="#b5">(Gao et al., 2020;</ref><ref type="bibr" target="#b17">Schick and Sch?tze, 2021;</ref><ref type="bibr" target="#b11">Li and Liang, 2021;</ref>.</p><p>DialogRE Recent studies on the DialogRE dataset show a tendency to fine-tune the PLM with task-specific objectives and use the model with high-complexity <ref type="bibr" target="#b21">(Xue et al., 2021;</ref><ref type="bibr" target="#b14">Long et al., 2021;</ref><ref type="bibr" target="#b10">Lee and Choi, 2021)</ref>. In detail, <ref type="bibr" target="#b10">Lee and Choi (2021)</ref> shows considerable performance with the contextualized turn representations from the diverse type of nodes and edges. Moreover, taskspecific objectives of fine-tuning lead to a gap between pre-training and fine-tuning. To overcome the limitation, <ref type="bibr" target="#b7">Han et al. (2021)</ref> utilizes multiple [MASK] tokens for each argument and the relation with logical rules by concentrating subject and object in DialogRE. There also exists an approach that incorporates potential knowledge contained in relation labels into prompt construction with trainable virtual type words and answers words. This approach also carefully initializes the virtual tokens with implicit semantic words and employs prior distributions estimated from the data .</p><p>Despite the prompt-based approach's high potential, few prompting studies sufficiently consider low information density and difficulty of capturing intrinsic relational information of the data between the argument pair of dialogue relation extraction task. We focus on building a light model with prompt-based fine-tuning with implicit semantic information of the relation which alleviates the sparsity problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>An overview of GRASP is illustrated in <ref type="figure">Figure  2</ref>. First, an input with a prompt template is constructed by using the APM strategy. Then, the PLM receives the constructed input for prompt-based fine-tuning and estimates probability distributions of the model's vocabulary by using contextualized representations of the PLM. The RCD task lets the model predict the relational clue type of each token, and the model takes the [MASK] representation to predict a final relation in the MLM task. In other words, our model is trained through multitask learning to encourage mutual communication between relational clues and the final relation for the argument pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Each example X includes dialogue D, subject a 1 , and object a 2 . Note that D denotes {s 1 : u 1 , s 2 : u 2 , . . . , s N : u N }, where s n is the nth speaker and u n is the corresponding utterance. Given X = {D, a 1 , a 2 }, the goal of DialogRE is to predict relation y between arguments a 1 and a 2 by leveraging D. To describe the DialogRE task in terms of prompt-based fine-tuning, a template function, T (?), is defined to map each example to X prompt = T (X ). A [MASK] is inserted into X prompt , and used to predicting the label words of relation y. The formulation of T (X ) is as follows:</p><formula xml:id="formula_0">T (X ) = "[CLS]D[SEP][subj]a 1 [subj] [MASK][obj]a 2 [obj][SEP]".</formula><p>(1)</p><p>Based on the structure of T (?), we construct our template function, T (?), by applying two steps: transformation of D to D with an argument-aware prompt marker, described in 3.2, and prompt initialization for [subj] and [obj], explained in 3.3. Subsequently, we introduce the RCD task in 3.4 to train the model by utilizing T (?) with a multitask training strategy on MLM, as detailed in 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Argument-aware Prompt Marker</head><p>We propose an APM strategy that considers both speaker and non-speaker arguments. In previous studies, a dialogue is encoded by focusing on speaker information <ref type="bibr" target="#b10">(Lee and Choi, 2021;</ref><ref type="bibr" target="#b23">Yu et al., 2020;</ref> without focusing nonspeaker arguments. However, in the DialogRE dataset, approximately 77.4% of relation triples include at least one non-speaker argument, implying that consideration of non-speaker arguments is also inevitable to enhance the model's argumentawareness. Our methods is inspired by the previous works of Soares et al. <ref type="formula">(2019)</ref> and <ref type="bibr" target="#b7">Han et al. (2021)</ref>; the former addresses the importance of entity markers regarding recognition of the entity position, and the latter improves the model performance by using specific additional punctuation in the model's original vocabulary as the entity marker. Accordingly, we insert the argument-aware prompt marker token, [p], as an entity marker. The argument-aware prompt markers allow our model to obtain informative signs to determine which token is the indicative component for relation prediction. We initialize the feature of [p] as the embedding of the space token in the vocabulary of the model. Our empirical experiments reveal that the space token can perceive the start position of the arguments. Consequently, our prompt marker enhances the model to discriminate which part of the dialogue plays a critical role in predicting a relation.</p><formula xml:id="formula_1">[CLS] + APM ( " ) + [SEP] + [subj] + S2 + [subj] + [MASK] + [obj] + Frank + [obj] + [SEP] PLM e([CLS]) e (APM( " )) e([SEP]) e % ([subj]) e(S2) e % ([subj]) e([MASK]) e % ([obj]) e(Frank) e % ([</formula><p>Using the proposed argument-aware prompt marker, we strengthen the token replacement method of BERT s <ref type="bibr" target="#b23">(Yu et al., 2020)</ref>. Given example X , BERT s constructsX = {D,? 1 ,? 2 }, wher? D = {s 1 : u 1 ,s 2 : u 2 , . . . ,s N : u N } ands n is</p><formula xml:id="formula_2">s n = ? ? ? ? ? [S 1 ] if s n = a 1 [S 2 ] if s n = a 2 s n otherwise.</formula><p>(2)</p><formula xml:id="formula_3">[S 1 ] and [S 2 ] are special tokens for speakers. In addition,? m (m ? {1, 2}) is defined as [S m ] if ? n (s n = a m )</formula><p>and a m otherwise. Even though BERT s prevents the model from overfitting and demonstrates higher generalization capacity <ref type="bibr" target="#b23">(Yu et al., 2020)</ref>, BERT s does not consider non-speaker arguments. To explore the disregarded arguments, the APM strategy expands BERT s by considering both types of arguments in the Dialo-gRE task. We define function APM(?) that encodes an utterance by inserting [p] in front of each argument token. Given dialogueD constructed by using BERT s , we construct D = {s 1 : u 1 ,s 2 : u 2 , . . . ,s N : u N } by applying APM(u n ) for each utterance inD. Consequently, the APM strategy constructs X = {D ,? 1 ,? 2 } based onX .</p><formula xml:id="formula_4">u n = APM(u n ) if ? m (? m ? u n ) (m ? {1, 2}) u n otherwise<label>(3)</label></formula><p>For instance, APM(?) encodes the text "I am Tom Gordon" to <ref type="bibr">[I, am, [p]</ref>, Tom, Gordon], inserting [p] in front of "Tom Gordon."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prompt Construction</head><p>We update the constructed input X with a template function and conduct a deliberate initialization. We utilize the prior distribution of argument types for initialization inspired by the study conducted by . Prompt tokens [subj] and [obj] are used to inject argumenttype information. We define the argument-type set, AT = {"PER," "ORG," "GPE," "VALUE,"</p><p>"STRING"}, using the types pre-defined in the dataset as depicted in <ref type="table">Table 1</ref>. We calculate the distributions of argument types ? <ref type="bibr">[subj]</ref> and ? <ref type="bibr">[obj]</ref> over AT by using frequency statistics. We aggregate each argument type, at ? AT , with the corresponding prior distribution to initialize prompt tokens [subj] and [obj]. The specific initialization equations are as follows:</p><formula xml:id="formula_5">e([subj]) = at ?AT ? [subj]</formula><p>at ? e(at)</p><formula xml:id="formula_6">e([obj]) = at ?AT ? [obj]</formula><p>at ? e(at),</p><p>where e(?) is the embedding from the PLM of an input token and?(?) is the initialized embedding of the prompt token. Suppose the subject has prior distribution, ? [subj] = {"PER": 0.5,''ORG":0.5, "GPE":0.0,"VALUE":0.0,"STRING":0.0}. The initial embedding of the [subj] token can be calculated as a weighted average, i.e.,?([subj]) = 0.5 ? e("PER") + 0.5 ? e("ORG").</p><p>Consequently, we can formalize T (?), which converts X to X prompt , where X prompt is an argument-enhanced input example, by using the APM strategy and prompt construction with deliberate initializations, i.e., X prompt = T (X ). Then, the final input structure for prompt-based fine-tuning is as follows:</p><formula xml:id="formula_8">T (X ) = "[CLS]D [SEP][subj]? 1 [subj] [MASK][obj]? 2 [obj][SEP]".<label>(5)</label></formula><p>In addition, for relation prediction by applying MLM, we also define V rel as a set of label words in the model's vocabulary as illustrated in <ref type="figure">Figure 2</ref>. In detail, we utilize its metadata for the initialization of each relation representation to inject its semantics. For instance, we add a special token to the model's vocabulary, [per:date_of_birth] as a label word, and initialize this token by aggregating the embeddings of the words in the metadata, i.e., {"person," "date," "of," "birth"} for a class "per:date_of_birth."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Relational Clue Detection task</head><p>To improve the understanding capability on relational clues by employing a prompt-based finetuning approach, we introduce a RCD task. We define a set of label words, V rcd ={[subject], [object], [trigger], [outside]}, and add to the model's vocabulary. Then, we construct a sequence of label words for RCD, C rcd , by assigning each token in X prompt to the corresponding clue type word from V rcd . For instance, C rcd is constructed as follows: {[subject], [trigger], [object]} when the token sequence is given by {"Pheebs", "lives in", "LA"}, where "Pheebs" is a subject and "LA" is an object argument, and "lives in" is a trigger.</p><p>The RCD task exploits the MLM head that is used to predict the [MASK] token. Except for the [MASK] token, each token is sequentially tagged with V rcd using the meta-data provided in the dataset. In other words, the RCD task allows the model to identify which non-[MASK] tokens correspond to certain relational clue types. RCD supports the model in collecting scattered information from the entire dialogue by indicating where to focus in the dialogue to predict a relation. In this respect, the model pays considerably more attention to the semantic clues of the relation, such as triggers. Moreover, our model maintains a lightweight complexity by conducting the RCD task without an additional classifier. The loss for the RCD task over each token x ? X prompt is aggregated as follows:</p><formula xml:id="formula_9">L RCD = ? x?X prompt log P (x = C rcd (x)|X prompt ) . (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Training</head><p>Before the final relation-prediction, we mark the position of the trigger using the [p] token, i.e., the argument-aware prompt marker, based on the results of the RCD task. Given example X prompt , the model predicts the label words of V rcd for all non-[MASK] tokens. Subsequently, [p] is appended in front of the words predicted as a trigger to train the model to distinguish essential clues that encourage determining the relation. To improve the model's ability to capture relational clues through the interaction between the MLM task for relation prediction and the RCD  task, we employ a multitask learning to train the GRASP model by using the joint loss expressed in Equations <ref type="formula">(6)</ref> and <ref type="formula">(7)</ref>. Therefore, the final learning objective is to minimize the joint loss, where ? 1 and ? 2 are hyperparameters.</p><formula xml:id="formula_10">L GRASP = ? 1 ? L RCD + ? 2 ? L REL<label>(8)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>For the base PLMs, RoBERTa-base and RoBERTalarge are adopted, as denoted by GRASP base and GRASP large , respectively. The evaluation metrics are the F1 and F1 c <ref type="bibr" target="#b23">(Yu et al., 2020)</ref> scores. F1 c is an evaluation metric for supplementing the F1 score in a conversational setting and is computed by employing part of the dialogue necessary for predicting the relation between given arguments as input instead of the entire dialogue. The detailed settings for GRASP can be found in Appendix A.</p><p>In a full-shot setting, GRASP is compared with both fine-tuning-based and prompt-based finetuning approaches. TUCORE-GCN <ref type="bibr" target="#b10">(Lee and Choi, 2021</ref>) is a typical fine-tuning-based model using turn-level features with a graph convolution network <ref type="bibr" target="#b9">(Kipf and Welling, 2016)</ref>. CoIn <ref type="bibr" target="#b14">(Long et al., 2021)</ref> employs utterance-aware and speaker-aware representations, and Dual <ref type="bibr" target="#b0">(Bai et al., 2021)</ref> models the relational semantics using abstract meaning representations <ref type="bibr" target="#b1">(Banarescu et al., 2013)</ref>. Moreover, PTR <ref type="bibr" target="#b7">(Han et al., 2021)</ref> and KnowPrompt  are the prompt-based fine-tuning baseline models. In the few-shot setting, 8-, 16-, and 32-shot experiments were conducted based on LM-BFF <ref type="bibr" target="#b5">(Gao et al., 2020)</ref> by using three different randomly sampled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Full-shot setting As presented in <ref type="table" target="#tab_4">Table 2</ref>, it is shown that GRASP large surpasses all of the baseline models, including the current SoTA models, that is, CoIn and TUCORE-GCN. From the result in the full-shot setting, the baselines of the finetuning-based approach, such as TUCORE-GCN or CoIn, show better performance than those of the prompt-based fine-tuning approach. In particular, CoIn outperforms all of the other baselines, including PTR and KnowPrompt on V1, and TUCORE-GCN exhibits the best performance on V2. Interestingly, even though the performance of the promptbased fine-tuning baselines is much lower than finetuning based models, our GRASP large outperforms regardless of the way of training approach.</p><p>In addition, GRASP large shows its efficiency in conversational settings by exceeding all the baselines in terms of F1 c , thereby indicating that our method effectively overcomes the low information density of dialogues. These results imply that guiding the model to pay attention to relational clues with a prompt-based fine-tuning approach can be more effective than adding additional features and layers. The slightly low performance of GRASP base is attributed to the gap in the model size; for example, TUCORE-GCN has 401M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-shot Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Shot K=8 K=16 K=32</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning based approach</head><p>RoBERTa  29.8 40.8 49.7 TUCORE-GCN <ref type="bibr" target="#b10">(Lee and Choi, 2021)</ref>   weight parameters with RoBERTa-large model, and GRASP base has 125M weight parameters with RoBERTa-base. Nevertheless, GRASP base demonstrates 6.0%p and 0.6%p improvements compared with the other prompt-based fine-tuning baselines, i.e., PTR and KnowPrompt, respectively.</p><p>Few-shot setting As presented in <ref type="table" target="#tab_6">Table 3</ref>, GRASP still exhibits robust performance in fewshot settings. GRASP base outperforms the baselines of both the fine-tuning and prompt-based finetuning methods regardless of the number of shots, demonstrating 20%p or higher performance in the 8-shot setting compared with TUCORE-GCN and indicating that our method is more efficient than the fine-tuning method in a low-resource setting. GRASP base also demonstrates improved performance compared with KnowPrompt in all-shot settings, indicating the effectiveness of the considerations on the properties of the dialogue with promptbased fine-tuning. Except for the 8-shot setting, GRASP large presents outstanding performance, achieving up to 15.3%p of absolute improvement in the 16-shot setting. Although GRASP large outperforms the fine-tuning-based models and PTR, the limited performance of GRASP large in the 8-shot setting can be attributed to an insufficient number of examples. We also observe that the fine-tuning-based models, such as TUCORE-GCN, perform at least 5.7%p worse than the prompt-based fine-tuning models, such as PTR, in the 8-shot setting, indicating that the fine-tuning-based models may have difficulty in sufficiently capturing relational semantics when the data are extremely scarce. In particular, TUCORE-GCN indicates a 5.2%p lower perfor-  mance than the fine-tuned RoBERTa, indicating that the high complexity requires a larger amount of data than the other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We conduct an ablation study to validate the effectiveness of the proposed modules. As shown in <ref type="table" target="#tab_8">Table 4</ref>, each of the proposed modules improves the overall performance for both F1 and F1 c settings. Without the APM strategy, the performance of GRASP base decreases the F1 score by 0.8%p and the F1 c score by 0.4%p on the development set, and the F1 score drops sharply by 2.1%p and F1 c by 1.0%p on the test set. This result indicates that argument-awareness can be obtained through both speaker and non-speaker argument information. When the RCD task is excluded, the performance of GRASP base decreases the F1 score by 1.1%p and F1 c by 0.5%p on the development set and the F1 score by 1.3%p and F1 c by 0.7%p for the test set. These results demonstrate that the RCD task alleviates the low information density of the dialogue by guiding the model to focus on relational clues. In addition, the performance without the manual initialization of prompt construction of GRASP base is reduced by 2.2%p for the F1 score and 1.4%p for the F1 c score on the development set and by 3.1%p for the F1 score and 1.9%p for the F1 c score on the test set. This result suggests that prompt construction is a basic step in training the model in the prompt-based fine-tuning case. The deliberate initialization of prompts is critical for modeling the tasks in an appropriate direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis on marker type for APM</head><p>To analyze argument-awareness regarding the types of markers, we conduct experiments on diverse prompt markers, as shown in <ref type="table" target="#tab_10">Table 5</ref>. The result reveals that considering argument types leads to performance improvement in the model. Punctuation marker ";" shows comparable performance among other punctuation markers, and "!" and "@"   achieve a limited score. We presume that the higher frequency of ";" acts as a delimiter, which results in decent performance. We also observe that the APM marker (front) performs the best, with a 69.0% F1 score among all other marker types. In addition, we conduct an experiment based on the position of the prompt marker, [p], by comparing two versions of the APM marker: APM marker (front) and APM marker (surrounding). The APM marker (surrounding) display 3.1%p lower performance than the APM marker (front). Based on these results, we empirically adopt the embedding initialization of our [p] prompt marker using the space token and located it in front of the arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Analysis on GRASP</head><p>Since we train GRASP attending on relational clues through the APM strategy and the RCD task, we further conduct analysis to investigate that the relational clues such as triggers contribute to predicting a relation between the arguments in a prompt-based manner, as shown in <ref type="table" target="#tab_11">Table 6</ref>. Specifically, we compare GRASP large with a fine-tuned RoBERTa-large model to validate our method.</p><p>We observe that the fine-tuned RoBERTa model struggles to capture the symmetrical relations including the trigger. The fine-tuned RoBERTa model fail to capture relational clues, such as "got married", misleading the model into predicting an inappropriate relations for both symmetrical relations between "Frank Jr." and "Alice." "got married" is a critical cue to distinguish the relations between "per:spouse" and "per:siblings" because this phrase implies a romantic relationship in a dictionary definition. In contrast, GRASP, which is trained using the RCD task and the APM strategy in a prompt-based manner, predicted the correct relations, capturing the correct relational clues including the trigger "got married" for both symmetrical argument pairs. This result presents the effectiveness of GRASP designed to guide the model on the relational clues, alleviating the difficulties of low information density in dialogues. Additional</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MELD EmoryNLP</head><p>RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019)</ref> 62.0 37.3 COSMIC <ref type="bibr" target="#b6">(Ghosal et al., 2020)</ref> 65.  examples demonstrating similar phenomena for the symmetrical relations are provided in the Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis on the applicability of GRASP</head><p>To demonstrate the robustness of our APM strategy and RCD task, we evaluated GRASP on MELD <ref type="bibr" target="#b16">(Poria et al., 2019)</ref> and EmoryNLP (Zahiri and Choi, 2018) datasets, which are designed for emotion recognition in conversations (ERC). MELD <ref type="bibr" target="#b16">(Poria et al., 2019</ref>) is a multimodal dataset collected from a TV show named Friends and consists of seven emotion labels and 2,458 dialogues with only textual modality. EmoryNLP (Zahiri and Choi, 2018) is also collected from Friends and comprises seven emotion labels and 897 dialogues. Each utterance in these datasets is annotated with one of the seven emotion labels. The weighted-F1 is calculated to evaluate the MELD and EmoryNLP datasets.</p><p>For baselines, we employ the fine-tuned RoBERTa <ref type="bibr" target="#b13">(Liu et al., 2019)</ref>, COSMIC <ref type="bibr" target="#b6">(Ghosal et al., 2020)</ref>, and TUCORE-GCN models <ref type="bibr" target="#b10">(Lee and Choi, 2021)</ref>. COSMIC <ref type="bibr" target="#b6">(Ghosal et al., 2020)</ref> uses RoBERTa-large as the encoder. It is a framework that models various aspects of commonsense knowledge by considering mental states, events, actions, and cause-effect relations for emotional recognition in conversations.</p><p>As presented in <ref type="table" target="#tab_13">Table 7</ref>, GRASP is applied to other dialogue-based tasks by alleviating the low information density of the given dialogue. In particular, the performance of GRASP surpasses that of TUCORE-GCN, which is the current SoTA model in DialogRE, and those of the baseline specialized on ERC tasks, such as COSMIC in both MELD and EmoryNLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed GRASP, which is a method for guiding PLMs to relational semantics using prompt-based fine-tuning for the DialogRE task. We focused on alleviating the critical challenge in dialogues, that is, low information density, by effectively capturing relational clues. In GRASP, we constructed prompts with deliberate initialization and suggested 1) an APM strategy considering both speaker and non-speaker arguments and 2) the RCD task, which guides the model to determine which token belongs to the relational clues. Experimental results on the DialogRE dataset revealed that GRASP achieved SoTA performance in terms of the F1 and F1 c scores, even though our method only leveraged a PLM without adding any extra layers.  GRASP is trained using AdamW <ref type="bibr" target="#b15">(Loshchilov and Hutter, 2017)</ref> as an optimizer with no weight decay. The number of training epochs is set to 30 with early stopping, and the ratio of ? 1 and ? 2 for the joint loss is 0.7 to 0.3. A learning rate of 5e?5, batch size of 8, and maximum sequence length of 512 are adopted for RoBERTa-base with identical parameters for RoBERTa-large, except for the learning rate of 5e?6. <ref type="table">Table 9</ref> shows additional examples to demonstrate the prediction tendency of GRASP and the finetuned RoBERTa models on the symmetrical relations described in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed experimental settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Qualitative Analysis Examples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Multitask Learning M rel : Y ? V rel is a mapping function that converts a class set, Y, into a set of label words, V rel . For each input X prompt , the purpose of MLM is to fill [MASK] with the relation label words in the model's vocabulary. As the model predicts the correct label word at the position of [MASK], we can formulate p(y|x) = P ([MASK] = M rel (y)|X prompt ). The training objective of the relation prediction is to minimize L REL = ? log P ([MASK] = M rel (y)|X prompt ). (7)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>]</head><label></label><figDesc>Jeff Bezos is founder of Amazon [SEP] Jeff Bezos is founder of Amazon [SEP] [subj] Amazon [subj] [MASK] [obj] Jeff Bezos [obj] [SEP]</figDesc><table><row><cell></cell><cell></cell><cell>Label words</cell><cell>Class Set</cell></row><row><cell></cell><cell></cell><cell>[per:founded_by]</cell><cell>per:founded_by</cell></row><row><cell></cell><cell></cell><cell>[per:children]</cell><cell>per:children</cell></row><row><cell>CLS</cell><cell>per:children per:founded_by Class Set</cell><cell>? [per:friends]</cell><cell>per:friends</cell></row><row><cell>Head</cell><cell>per:friends</cell><cell>MLM Head</cell><cell></cell></row></table><note>[CLS? [CLS]</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performances of GRASP on test set of DialogRE. V1 and V2 represent the version of the dataset. The underlined scores are the previous SoTA performances. Subscript in parentheses represents advantages of GRASP over the best results of baselines (the underlined). Best results are bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Low-resource RE performance of F1 scores</cell></row><row><cell>(%) on different test sets. We use K = 8, 16, 32 (# of ex-</cell></row><row><cell>amples per class) for few-shot experiments. Best results</cell></row><row><cell>are bold and the second place results are underlined.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on DialogRE dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Frank [/E1] lives in [E2] Montauk [/E2] . [SEP] 65.9 Type marker [CLS] [E1:PER] Frank [/E1:PER] lives in [E2:GPE] Montauk [/E2:GPE] . [SEP]</figDesc><table><row><cell>Marker Type</cell><cell>Input Example</cell><cell>F1</cell></row><row><cell>Entity marker</cell><cell cols="2">[CLS] [E1] 66.7</cell></row><row><cell>Punctuation marker (!)</cell><cell>[CLS] ! Frank ! lives in ! Montauk ! . [SEP]</cell><cell>66.7</cell></row><row><cell>Punctuation marker (@)</cell><cell>[CLS] @ Frank @ lives in @ Montauk @ . [SEP]</cell><cell>65.3</cell></row><row><cell>Punctuation marker (;)</cell><cell>[CLS] ; Frank ; lives in ; Montauk ; . [SEP]</cell><cell>67.1</cell></row><row><cell>APM marker (front)</cell><cell>[CLS] [p] Frank lives in [p] Montauk . [SEP]</cell><cell>69.0</cell></row><row><cell cols="2">APM marker (surrounding) [CLS] [p] Frank [p] lives in [p] Montauk [p] . [SEP]</cell><cell>65.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>The performance based on the marker type. The arguments are bold. The embedding of [p] is initialized with the space token from the model's vocabulary. In type marker, [E1:PER] represents a start position of subject which has a person type and [/E1:PER] represents an end position of object that has the same type.</figDesc><table><row><cell>Dialogue</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S1: Hey!!</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S2: Hey!</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">S1: Guess what. Frank Jr., and Alice got married!</cell><cell></cell><cell></cell></row><row><cell>S2: Oh my God!!</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">S1: And! And, they're gonna have a baby! And! And, they want me to grow it for them in my uterus.</cell></row><row><cell>S3: My God!</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">S4: Are you serious?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>S1: Yeah</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">S5: You're really thinking about having sex with your brother?!</cell><cell></cell></row><row><cell cols="5">S1: Ewww! And "Oh no!" It's-they just want me to be the surrogate. It's her-it's her egg and her sperm, and I'm-I'm just the</cell></row><row><cell cols="2">oven, it's totally their bun.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>S5: Huh.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Argument pair Ground Truth</cell><cell cols="2">RoBERTa Predicted Relation Predicted (subject, object, trigger) GRASP</cell><cell>Predicted Relation</cell></row><row><cell>(Frank Jr., Alice)</cell><cell>per:spouse</cell><cell>unanswerable</cell><cell>(Frank Jr, Alice, got married)</cell><cell>per:spouse</cell></row><row><cell>(Alice, Frank Jr.)</cell><cell>per:spouse</cell><cell>per:siblings</cell><cell>(Alice, Frank Jr, got married)</cell><cell>per:spouse</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>The qualitative analysis on the prediction of GRASP based on the comparison with the RoBERTa model. Predicted (subject, object, trigger) is that GRASP predicted on RCD task.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Experimental results of GRASP base on MELD and EmoryNLP tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>HyperparametersGRASP base GRASP large</figDesc><table><row><cell>Learning rate</cell><cell>5e ? 5</cell><cell>5e ? 6</cell></row><row><cell>Max seq. len</cell><cell>512</cell><cell></cell></row><row><cell>Batch size</cell><cell>8</cell><cell></cell></row><row><cell>Num. epochs</cell><cell>30</cell><cell></cell></row><row><cell>Joint ratio (? 1 &amp; ? 2 )</cell><cell>0.7 / 0.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameter values used in prompt-tuning process on test set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In the DialogRE dataset, 65.9% of relational triples involve arguments that never appear in the same utterance, demonstrating that multi-turn reasoning plays an important role.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Argument pair</head><p>Ground Truth RoBERTa GRASP Predicted Relation Predicted <ref type="bibr">(subject, object, trigger)</ref> Predicted Relation  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic representation for dialogue modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.342</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4430" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Variation across speech and writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Biber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Knowprompt: Knowledgeaware prompt-tuning with synergistic optimization for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07650</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">COSMIC: COmmonSense knowledge for eMotion identification in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.224</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2470" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11259</idno>
		<title level="m">Ptr: Prompt tuning with rules for text classification</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of the tac 2010 knowledge base population track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><forename type="middle">Ellis</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third text analysis conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph based network with contextualized representations of turns in dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongseok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Suk</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.36</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="443" to="455" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Dominican Republic</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Prefixtuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Gpt understands, too. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Consistent inference for dialogue relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/535</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3885" to="3891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MELD: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1050</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Intersentential relations in information extraction corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumutha</forename><surname>Swampillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A pilot study of opinion summarization in conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the Association for Computational Linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the Association for Computational Linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gdpnet: Refining latent multi-view graph for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng Siong</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Codred: A crossdocument relation extraction dataset for acquiring knowledge in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4452" to="4472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dialogue-based relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.444</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4927" to="4940" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Emotion detection on tv show transcripts with sequence-based convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho D</forename><surname>Zahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshops at the thirty-second aaai conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chandler and I are moving in together. S1: Oh my God. Ohh, my little sister and my best friend. . . shaking up. Oh, that&apos;s great. That&apos;s great. S3: Guys, I&apos;m happy too. S2: Okay, come here! S3: Wow! Big day huh? People moving in</title>
	</analytic>
	<monogr>
		<title level="m">Dialogue S1: Hey! Hi! S2: Hey! S1: What&apos;s up? S2: Well umm</title>
		<imprint/>
	</monogr>
	<note>people getting annulled</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">S2: Okay, I gotta go find Rachel but umm, if you guys see her could you please try to give her some really bad news so that mine doesn&apos;t seem so bad</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
