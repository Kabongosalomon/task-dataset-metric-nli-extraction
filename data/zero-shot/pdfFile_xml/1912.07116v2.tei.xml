<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Processing Using Multi-Code GAN Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
							<email>jinjingu@link.cuhk.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
							<email>bzhou@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Processing Using Multi-Code GAN Prior</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) Image Reconstruction (b) Image Colorization (c) Image Super-Resolution (d) Image Denoising (e) Image Inpainting (f) Semantic Manipulation Figure 1: Multi-code GAN prior facilitates many image processing applications using the reconstruction from fixed PGGAN [23] models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing. 1 1 Code is available at this link.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b15">[16]</ref> have advanced image generation by improving the synthesis quality <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref> and stabilizing the training process <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>. The capability to produce high-quality images makes GANs applicable to many image processing tasks, such as semantic face editing <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">36]</ref>, super-resolution <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42]</ref>, image-to-image translation <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref>, etc. However, most of these GAN-based approaches require special design of network structures <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b52">53]</ref> or loss functions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28]</ref> for a particular task, limiting their generalization ability. On the other hand, the large-scale GAN models, like StyleGAN <ref type="bibr" target="#b23">[24]</ref> and BigGAN <ref type="bibr" target="#b7">[8]</ref>, can synthesize photo-realistic images after being trained with millions of diverse images. Their neural representations are shown to contain various levels of semantics underlying the observed data <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>. Reusing these models as prior to real image processing with minor effort could potentially lead to wider applications but remains much less explored.</p><p>The main challenge towards this goal is that the standard GAN model is initially designed for synthesizing images from random noises, thus is unable to take real images for any post-processing. A common practice is to invert a given image back to a latent code such that it can be reconstructed by the generator. In this way, the inverted code can be used for further processing. To reverse the generation process, existing approaches fall into two types. One is to directly optimize the latent code by minimizing the reconstruction error through back-propagation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref>. The other is to train an extra encoder to learn the mapping from the image space to the latent space <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>. However, the reconstructions achieved by both methods are far from ideal, especially when the given image is with high resolution. Consequently, the reconstructed image with low quality is unable to be used for image processing tasks.</p><p>In principle, it is impossible to recover every detail of any arbitrary real image using a single latent code, otherwise, we would have an unbeatable image compression method. In other words, the expressiveness of the latent code is limited due to its finite dimensionality. Therefore, to faithfully recover a target image, we propose to employ multiple latent codes and compose their corresponding feature maps at some intermediate layer of the generator. Utilizing multiple latent codes allows the generator to recover the target image using all the possible composition knowledge learned in the deep generative representation. The experiments show that our approach significantly improves the image reconstruction quality. More importantly, being able to better reconstruct the input image, our approach facilitates various real image processing applications by using pre-trained GAN models as prior without retraining or modification, which is shown in <ref type="figure" target="#fig_1">Fig.1</ref>. We summarize our contributions as follows:</p><p>? We propose mGANprior, shorted for multi-code GAN prior, as an effective GAN inversion method by using multiple latent codes and adaptive channel importance. The method faithfully reconstructs the given real image, surpassing existing approaches. ? We apply the proposed mGANprior to a range of realworld applications, such as image colorization, superresolution, image inpainting, semantic manipulation, etc, demonstrating its potential in real image processing. ? We further analyze the internal representation of different layers in a GAN generator by composing the features from the inverted latent codes at each layer respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related</head><p>Work GAN Inversion. The task of GAN inversion targets at reversing a given image back to a latent code with a pretrained GAN model. As an important step for applying GANs to real-world applications, it has attracted increasing attention recently. To invert a fixed generator in GAN, existing methods either optimized the latent code based on gradient descent <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32]</ref> or learned an extra encoder to project the image space back to the latent space <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>. Bau et al. <ref type="bibr" target="#b2">[3]</ref> proposed to use encoder to provide better initialization for optimization. There are also some models taking invertibility into account at the training stage <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. However, all the above methods only consider using a single latent code to recover the input image and the reconstruction quality is far from ideal, especially when the test image shows a huge domain gap to training data. That is because the input image may not lie in the synthesis space of the generator, in which case the perfect inversion with a single latent code does not exist. By contrast, we propose to increase the number of latent codes, which significantly improve the inversion quality no matter whether the target image is in-domain or out-of-domain.</p><p>Image Processing with GANs. GANs have been widely used for real image processing due to its great power of synthesizing photo-realistic images. These applications include image denoising <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref>, image inpainting <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref>, super-resolution <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42]</ref>, image colorization <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b19">20]</ref>, style mixing <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b9">10]</ref>, semantic image manipulation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b28">29]</ref>, etc. However, current GAN-based models are usually designed for a particular task with specialized architectures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref> or loss functions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b9">10]</ref>, and trained with paired data by taking one image as input and the other as supervision <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b19">20]</ref>. Differently, our approach can reuse the knowledge contained in a well-trained GAN model and further enable a single GAN model as prior to all the aforementioned tasks without retraining or modification. It is worth noticing that our method can achieve similar or even better results than existing GAN-based methods that are particularly trained for a certain task.</p><p>Deep Model Prior. Generally, the impressive performance of the deep convolutional model can be attributed to its capacity of capturing statistical information from large-scale data as prior. Such prior can be inversely used for image generation and image reconstruction <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b1">2]</ref>. Upchurch et al. <ref type="bibr" target="#b39">[40]</ref> inverted a discriminative model, starting from deep convolutional features, to achieve semantic image transformation. Ulyanov et al. <ref type="bibr" target="#b38">[39]</ref> reconstructed the target image with a U-Net structure to show that the structure of a generator network is sufficient to capture the lowlevel image statistics prior to any learning. Athar et al.</p><p>[2] learned a universal image prior for a variety of image restoration tasks. Some work theoretically explored the prior provided by deep generative models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b17">18]</ref>, but the results using GAN prior to real image processing are still unsatisfying. A recent work <ref type="bibr" target="#b2">[3]</ref> applied generative image prior to semantic photo manipulation, but it can only edit some partial regions of the input image yet fails to apply to other tasks like colorization or super-resolution. That is because it only inverts the GAN model to some intermediate feature space instead of the earliest hidden space. By contrast, our method reverses the entire generative process, i.e., from the image space to the initial latent space, which supports more flexible image processing tasks. </p><formula xml:id="formula_0">G (`) 2 F (`) 1 z 1 ? 1 z 2 ? 2 ? z N ? N } Inversion Result Target Image } MSE + Perceptual Loss N X n=1 F (`) n ? n x x inv F (`) N ? G (`) 1 ? F (`) 2 } }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Code GAN Prior</head><p>A well-trained generator G(?) of GAN can synthesize high-quality images by sampling codes from the latent space Z. Given a target image x, the GAN inversion task aims at reversing the generation process by finding the adequate code to recover x. It can be formulated as</p><formula xml:id="formula_1">z * = arg min z?Z L(G(z), x),<label>(1)</label></formula><p>where L(?, ?) denotes the objective function. However, due to the highly non-convex natural of this optimization problem, previous methods fail to ideally reconstruct an arbitrary image by optimizing a single latent code. To this end, we propose to use multiple latent codes and compose their corresponding intermediate feature maps with adaptive channel importance, as illustrated in <ref type="figure" target="#fig_2">Fig.2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">GAN Inversion with Multiple Latent Codes</head><p>The expressiveness of a single latent code may not be enough to recover all the details of a certain image. Then, how about using N latent codes {z n } N n=1 , each of which can help reconstruct some sub-regions of the target image? In the following, we introduce how to utilize multiple latent codes for GAN inversion. Feature Composition. One key difficulty after introducing multiple latent codes is how to integrate them in the generation process. A straightforward solution is to fuse the images generated by each z n from the image space X . However, X is not naturally a linear space such that linearly combining synthesized images is not guaranteed to produce a meaningful image, let alone recover the input in detail. A recent work <ref type="bibr" target="#b4">[5]</ref> pointed out that inverting a generative model from the image space to some intermediate feature space is much easier than to the latent space. Accordingly, we propose to combine the latent codes by composing their intermediate feature maps. More concretely, the generator G(?) is divided into two sub-networks, i.e., G ( )</p><formula xml:id="formula_2">1 (?) and G ( ) 2 (?).</formula><p>Here, is the index of the intermediate layer to perform feature composition. With such a separation, for any z n , we can extract the corresponding spatial feature F ( )</p><formula xml:id="formula_3">n = G ( ) 1 (z n ) for further composition.</formula><p>Adaptive Channel Importance. Recall that we would like each z n to recover some particular regions of the target image. Bau et al. <ref type="bibr" target="#b3">[4]</ref> observed that different units (i.e., channels) of the generator in GAN are responsible for generating different visual concepts such as objects and textures. Based on this observation, we introduce the adaptive channel importance ? n for each z n to help them align with different semantics. Here, ? n ? R C is a Cdimensional vector and C is the number of channels in the -th layer of G(?). We expect each entry of ? n to represent how important the corresponding channel of the feature map F ( ) n is. With such composition, the reconstructed image can be generated with</p><formula xml:id="formula_4">x inv = G ( ) 2 ( N n=1 F ( ) n ? n ),<label>(2)</label></formula><p>where denotes the channel-wise multiplication as</p><formula xml:id="formula_5">{F ( ) n ? n } i,j,c = {F ( ) n } i,j,c ? {? n } c .<label>(3)</label></formula><p>Here, i and j indicate the spatial location, while c stands for the channel index. Optimization Objective. After introducing the feature composition technique together with the introduced adaptive channel importance to integrate multiple latent codes, there are 2N sets of parameters to be optimized in total. Accordingly we reformulate Eq. <ref type="formula" target="#formula_1">(1)</ref> as</p><formula xml:id="formula_6">{z * n } N n=1 , {? * n } N n=1 = arg min {zn} N n=1 ,{?n} N n=1 L(x inv , x). (4)</formula><p>To improve the reconstruction quality, we define the objective function by leveraging both low-level and high-level information. In particular, we use pixel-wise reconstruction error as well as the l 1 distance between the perceptual features <ref type="bibr" target="#b21">[22]</ref> extracted from the two images 2 . Therefore, the objective function is as follows:</p><formula xml:id="formula_7">L(x 1 , x 2 ) = ||x 1 ? x 2 || 2 2 + ||?(x 1 ), ?(x 2 )|| 1 ,<label>(5)</label></formula><p>where ?(?) denotes the perceptual feature extractor. We use the gradient descent algorithm to find the optimal latent codes as well as the corresponding channel importance scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Code GAN Prior for Image Processing</head><p>After inversion, we apply the reconstruction result as multi-code GAN prior to a variety of image processing tasks. Each task requires an image as a reference, which is the input image for processing. For example, image colorization task deals with grayscale images and image inpainting task restores images with missing holes. Given an input, we apply the proposed multi-code GAN inversion method to reconstruct it and then post-process the reconstructed image to approximate the input. When the approximation is close enough to the input, we assume the reconstruction before post-processing is what we want. Here, to adapt mGANprior to a specific task, we modify Eq.(5) based on the post-processing function:</p><p>? For image colorization task, with a grayscale image I gray as the input, we expect the inversion result to have the same gray channel as I gray with</p><formula xml:id="formula_8">L color = L(gray(x inv ), I gray ),<label>(6)</label></formula><p>where gray(?) stands for the operation to take the gray channel of an image. ? For image super-resolution task, with a low-resolution image I LR as the input, we downsample the inversion result to approximate I LR with</p><formula xml:id="formula_9">L SR = L(down(x inv ), I LR ),<label>(7)</label></formula><p>where down(?) stands for the downsampling operation. ? For image inpainting task, with an intact image I ori and a binary mask m indicating known pixels, we only reconstruct the incorrupt parts and let the GAN model fill in the missing pixels automatically with</p><formula xml:id="formula_10">L inp = L(x inv ? m, I ori ? m),<label>(8)</label></formula><p>where ? denotes the element-wise product. <ref type="bibr" target="#b1">2</ref> In this experiment, we use pre-trained VGG-16 model <ref type="bibr" target="#b36">[37]</ref> as the feature extractor, and the output of layer conv 43 is used. learning an encoder <ref type="bibr" target="#b51">[52]</ref>, (c) using the encoder as initialization for optimization <ref type="bibr" target="#b4">[5]</ref>, and (d) our proposed mGANprior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct extensive experiments on state-of-the-art GAN models, i.e., PGGAN <ref type="bibr" target="#b22">[23]</ref> and StyleGAN <ref type="bibr" target="#b23">[24]</ref>, to verify the effectiveness of mGANprior. These models are trained on various datasets, including CelebA-HQ <ref type="bibr" target="#b22">[23]</ref> and FFHQ <ref type="bibr" target="#b23">[24]</ref> for faces as well as LSUN <ref type="bibr" target="#b45">[46]</ref> for scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with Other Inversion Methods</head><p>There are many attempts on GAN inversion in the literature. In this section, we compare our multi-code inversion approach with the following baseline methods: (a) optimizing a single latent code z as in Eq.(1) <ref type="bibr" target="#b31">[32]</ref>, (b) learning an encoder to reverse the generator <ref type="bibr" target="#b51">[52]</ref>, and (c) combing (a) and (b) by using the output of the encoder as the initialization for further optimization <ref type="bibr" target="#b4">[5]</ref>. <ref type="table">Table 1</ref>: Quantitative comparison of different GAN inversion methods: including (a) optimizing a single latent code <ref type="bibr" target="#b31">[32]</ref>, (b) learning an encoder <ref type="bibr" target="#b51">[52]</ref>, (c) using the encoder as initialization for optimization <ref type="bibr" target="#b4">[5]</ref>, and (d) our proposed mGANprior. ? means the higher the better while ? means the lower the better. To quantitatively evaluate the inversion results, we introduce the Peak Signal-to-Noise Ratio (PSNR) to measure the similarity between the original input and the reconstruction result from pixel level, as well as the LPIPS metric <ref type="bibr" target="#b48">[49]</ref> which is known to align with human perception. We make comparisons on three PGGAN <ref type="bibr" target="#b22">[23]</ref> models that are trained on LSUN bedroom (indoor scene), LSUN church (outdoor scene), and CelebA-HQ (human face) respectively. For each model, we invert 300 real images for testing.</p><p>Tab.1 and <ref type="figure" target="#fig_3">Fig.3</ref> show the quantitative and qualitative comparisons respectively. From Tab.1, we can tell that mGANprior beats other competitors on all three models from both pixel level (PSNR) and perception level (LPIPS). We also observe in <ref type="figure" target="#fig_3">Fig.3</ref> that existing methods fail to recover the details of the target image, which is due to the limited representation capability of a single latent code. By contrast, our method achieves much more satisfying reconstructions with most details, benefiting from multiple latent codes. We even recover an eastern face with a model trained on western data (CelebA-HQ <ref type="bibr" target="#b22">[23]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Analysis on Inverted Codes</head><p>As described in Sec.3, our method achieves high-fidelity GAN inversion with N latent codes and N importance factors. Taking PGGAN as an example, if we choose the 6th layer (i.e., with 512 channels) as the composition layer with N = 10, the number of parameters to optimize is 10 ? (512 + 512), which is 20 times the dimension of the original latent space. In this section, we perform detailed analysis on the inverted codes. Number of Codes. Obviously, there is a trade-off between the dimension of the optimization space and the inversion quality. To better analysis such trade-off, we evaluate our method by varying the number of latent codes to optimize. <ref type="figure" target="#fig_5">Fig.4</ref> shows that the more latent codes used, the better reconstruction we are able to obtain. However, it does not imply that the performance can be infinitely improved by increasing the number of latent codes. From <ref type="figure" target="#fig_5">Fig.4</ref>, we can see that after the number reaches 20, there is no significant improvement via involving more latent codes. Different Composition Layers. On which layer to perform feature composition also affects the performance of the    proposed mGANprior. We thus compose the latent codes on various layers of PGGAN (i.e., from 1st to 8th) and compare the inversion quality, as shown in <ref type="figure" target="#fig_5">Fig.4</ref>. In general, a higher composition layer could lead to a better inversion effect. However, as revealed in <ref type="bibr" target="#b3">[4]</ref>, higher layers contain the information of local pixel patterns such as edges and colors rather than the high-level semantics. Composing features at higher layers is hard to reuse of the semantic knowledge learned by GANs. This will be discussed more in Sec.4.4. Role of Each Latent Code. We employ multiple latent codes by expecting each of them to take charge of inverting a particular region and hence complement with each other. In this part, we visualize the roles that different latent codes play in the inversion process. As pointed out by <ref type="bibr" target="#b3">[4]</ref>, for a particular layer in a GAN model, different units (channels) control different semantic concepts. Recall that mGANprior uses adaptive channel importance to help determine what kind of semantics a particular z should focus on. Therefore, for each z n , we set the elements in ? n that are larger than 0.2 as 0, getting ? n . Then we compute the difference map between the reconstructions using ? n and ? n . With the help of a segmentation model <ref type="bibr" target="#b50">[51]</ref>, we can also get the segmentation maps for various visual concepts, such as tower and tree. We finally annotate each latent code based on the Intersection-over-Union (IoU) metric between the corresponding difference map and all candidate segmentation maps. <ref type="figure" target="#fig_6">Fig.5</ref> shows the segmentation result  and the IoU maps of some chosen latent codes. It turns out that the latent codes are specialized to invert different meaningful image regions to compose the whole image. This is also a huge advantage of using multiple latent codes over using a single code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Image Processing Applications</head><p>With the high-fidelity image reconstruction, our multicode inversion method facilitates many image processing tasks with pre-trained GANs as prior. In this section, we apply the proposed mGANprior to a variety of real-world applications to demonstrate its effectiveness, including image colorization, image super-resolution, image inpainting and denoising, as well as semantic manipulation and style mixing. For each application, the GAN model is fixed. Image Colorization. Given a grayscale image as input, we can colorize it with mGANprior as described in Sec.3.2. We compare our inversion method with optimizing the intermediate feature maps <ref type="bibr" target="#b2">[3]</ref>. We also compare with DIP <ref type="bibr" target="#b38">[39]</ref>, which uses a discriminative model as prior, and Zhang et al. <ref type="bibr" target="#b47">[48]</ref>, which is specially designed for colorization task. We do experiments on PGGAN models trained for bedroom and church synthesis, and use the area under the curve of the cumulative error distribution over ab color space as the evaluation metric, following <ref type="bibr" target="#b47">[48]</ref>. Tab.2 and <ref type="figure" target="#fig_7">Fig.6</ref> show the quantitative and qualitative comparisons respectively. It turns out that using the discriminative model as prior fails to colorize the image adequately. That is because discriminative models focus on learning high-level representation which are not suitable for low-level tasks. On the contrary, using the generative model as prior leads to much more satisfying colorful images. We also achieve comparable results as the model whose primary goal is image colorization <ref type="figure" target="#fig_7">(Fig.6 (c) and (d)</ref>). This benefits from the rich knowledge learned by GANs. Note that Zhang et al. <ref type="bibr" target="#b47">[48]</ref> is proposed for general image colorization, while our approach can be only applied to a certain image category corresponding to the given GAN model. A larger GAN model trained on a more diverse dataset should improve its generalization ability.</p><p>Image Super-Resolution. We also evaluate our approach on the image super-resolution (SR) task. We do experiments on the PGGAN model trained for face synthesis and set the SR factor as 16. Such a large factor is very challenging for the SR task. We compare with DIP <ref type="bibr" target="#b38">[39]</ref> as well as the state-of-the-art SR methods, RCAN <ref type="bibr" target="#b49">[50]</ref> and ESRGAN <ref type="bibr" target="#b41">[42]</ref>. Besides PSNR and LPIPS, we introduce Naturalness Image Quality Evaluator (NIQE) <ref type="bibr" target="#b32">[33]</ref> as an extra metric. Tab.3 shows the quantitative comparison. We can con-  <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>, (b) inversion by optimizing feature maps <ref type="bibr" target="#b2">[3]</ref>, (c) DIP <ref type="bibr" target="#b38">[39]</ref>, and (d) our mGANprior.  clude that our approach achieves comparable or even better performance than the advanced learning-based competitors. A visualization example is also shown in <ref type="figure" target="#fig_8">Fig.7</ref>, where our method reconstructs the human eye with more details. Compared to existing learning-based models, like RCAN and ESRGAN, our mGANprior is more flexible to the SR factor. This suggests that the freely-trained PGGAN model has spontaneously learned rich knowledge such that it can be used as prior to enhance a low-resolution (LR) image.</p><p>Image Inpainting and Denoising. We further extend our approach to image restoration tasks, like image inpainting and image denoising. We first corrupt the image contents by randomly cropping or adding noises, and then use different algorithms to restore them. Experiments are conducted on PGGAN models and we compare with several baseline inversion methods as well as DIP <ref type="bibr" target="#b38">[39]</ref>. PSNR and Structural SIMilarity (SSIM) <ref type="bibr" target="#b42">[43]</ref> are used as evaluation metrics.   Tab.4 shows the quantitative comparison, where our approach achieves the best performances on both settings of center crop and random crop. <ref type="figure" target="#fig_9">Fig.8</ref> includes some examples of restoring corrupted images. It is obvious that both existing inversion methods and DIP fail to adequately fill in the missing pixels or completely remove the added noises. By contrast, our method is able to use well-trained GANs as prior to convincingly repair the corrupted images with meaningful filled content. Semantic Manipulation. Besides the aforementioned lowlevel applications, we also test our approach with some high-level tasks, like semantic manipulation and style mixing. As pointed out by prior work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>, GANs have already encoded some interpretable semantics inside the latent space. From this point, our inversion method provides a feasible way to utilize these learned semantics for real image manipulation. We apply the manipulation framework based on latent code proposed in <ref type="bibr" target="#b34">[35]</ref> to achieve semantic facial attribute editing. <ref type="figure" target="#fig_10">Fig.9</ref> shows the manipulation results. We see that mGANprior can provide rich enough information for semantic manipulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Knowledge Representation in GANs</head><p>As discussed above, the major limitation of using single latent code is its limited expressiveness, especially when the test image presents domain gap to the training data. Here we verify whether using multiple codes can help alleviate this problem. In particular, we try to use GAN models trained for synthesizing face, church, conference room, and bedroom, to invert a bedroom image. As shown in <ref type="figure" target="#fig_1">Fig.10</ref>, when using a single latent code, the reconstructed image still lies in the original training domain (e.g., the inversion  <ref type="figure" target="#fig_1">Figure 11</ref>: Colorization and inpainting results with mGANprior using different composition layers. AuC (the higher the better) for colorization task are 86.83%, 87.44%, 90.02% with respect to the 2nd, 4th, and 8th layer respectively. PSNR (the higher the better) for inpainting task are <ref type="bibr">21.19db, 22.11db, 20.</ref>70db with respect to the 2nd, 4th, and 8th layer respectively. Images in green boxes indicate the best results.</p><p>with PGGAN CelebA-HQ model looks like a face instead of a bedroom). On the contrary, our approach is able to compose a bedroom image no matter what data the GAN generator is trained with.</p><p>We further analyze the layer-wise knowledge of a welltrained GAN model by performing feature composition at different layers. <ref type="figure" target="#fig_1">Fig.10</ref> suggests that the higher layer is used, the better the reconstruction will be. That is because reconstruction focuses on recovering low-level pixel values, and GANs tend to represent abstract semantics at bottom layers while represent content details at top layers. We also observe that the 4th layer is good enough for the bedroom model to invert a bedroom image, but the other three models need the 8th layer for satisfying inversion. The reason is that bedroom shares different semantics from face, church, and conference room, therefore the high-level knowledge (contained in bottom layers) from these models cannot be reused. We further make per-layer analysis by applying our approach to image colorization and image inpainting tasks, as shown in <ref type="figure" target="#fig_1">Fig.11</ref>. The colorization task gets the best result at the 8th layer while the inpainting task at the 4th layer. That is because colorization is more like a low-level rendering task while inpainting requires the GAN prior to fill in the missing content with meaningful objects. This is consistent with the analysis from <ref type="figure" target="#fig_1">Fig.10</ref>, which is that lowlevel knowledge from GAN prior can be reused at higher layers while high-level knowledge at lower layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present mGANprior that employs multiple latent codes for reconstructing real images with a pre-trained GAN model. It enables these GAN models as powerful prior to a variety of image processing tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>jinjingu@link.cuhk.edu.cn, {sy116, bzhou}@ie.cuhk.edu.hk (a) Image Reconstruction (b) Image Colorization (c) Image Super-Resolution (d) Image Denoising (e) Image Inpainting (f) Semantic Manipulation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Multi-code GAN prior facilitates many image processing applications using the reconstruction from fixed PGGAN<ref type="bibr" target="#b22">[23]</ref> models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Pipeline of GAN inversion using multiple latent codes {zn} N n=1 . The generative features from these latent codes are composed at some intermediate layer (i.e., the -th layer) of the generator, weighted by the adaptive channel importance scores {?n} N n=1 . All latent codes and the corresponding channel importance scores are jointly optimized to recover a target image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Qualitative comparison of different GAN inversion methods, including (a) optimizing a single latent code [32], (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Effects on inversion performance by the number of latent codes used and the feature composition position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the role of each latent code. On the top row are the target image, inversion result, and the corresponding segmentation mask, respectively. On the bottom row are several latent codes annotated with a specific semantic label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparison of different colorization methods, including (a) inversion by optimizing feature maps<ref type="bibr" target="#b2">[3]</ref>, (b) DIP<ref type="bibr" target="#b38">[39]</ref>, (c) Zhang et al.<ref type="bibr" target="#b47">[48]</ref>, and (d) our mGANprior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparison of different super-resolution methods with SR factor 16. Competitors include DIP<ref type="bibr" target="#b38">[39]</ref>, RCAN<ref type="bibr" target="#b49">[50]</ref>, and ESRGAN<ref type="bibr" target="#b41">[42]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Qualitative comparison of different inpainting methods, including (a) inversion by optimizing a single latent code</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Real face manipulation with respect to four various attributes. In each four-element tuple, from left to right are: input face, inversion result, and manipulation results by making a particular semantic more negative and more positive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Comparison of the inversion results using different GAN models as well as performing feature composition at different layers. Each row stands for a PGGAN model trained on a specific dataset as prior, while each column shows results by composing feature maps at a certain layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation results on colorization task with bedroom and church images. AuC refers to the area under the curve of the cumulative error distribution over ab color space<ref type="bibr" target="#b47">[48]</ref>. ? means higher score is better.</figDesc><table><row><cell></cell><cell>Bedroom</cell><cell>Church</cell></row><row><cell>Method</cell><cell>AuC (%)?</cell><cell>AuC (%)?</cell></row><row><cell>Grayscale input</cell><cell>88.02</cell><cell>85.50</cell></row><row><cell>(a) Optimizing feature maps [3]</cell><cell>85.41</cell><cell>86.10</cell></row><row><cell>(b) DIP [39]</cell><cell>84.33</cell><cell>83.31</cell></row><row><cell>(c) Zhang et al. [48]</cell><cell>88.55</cell><cell>89.13</cell></row><row><cell>(d) Ours</cell><cell>90.02</cell><cell>89.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison of different super-resolution methods with SR factor 16. Competitors include DIP<ref type="bibr" target="#b38">[39]</ref>, RCAN<ref type="bibr" target="#b49">[50]</ref>, and ESRGAN<ref type="bibr" target="#b41">[42]</ref>. ? means the higher the better while ? means the lower the better.</figDesc><table><row><cell>Method</cell><cell>PSNR?</cell><cell>LPIPS?</cell><cell>NIQE?</cell></row><row><cell>(a) DIP [39]</cell><cell>26.87</cell><cell>0.4236</cell><cell>4.66</cell></row><row><cell>(b) RCAN [50]</cell><cell>28.82</cell><cell>0.4579</cell><cell>5.70</cell></row><row><cell>(c) ESRGAN [42]</cell><cell>25.26</cell><cell>0.3862</cell><cell>3.27</cell></row><row><cell>(d) Ours</cell><cell>26.93</cell><cell>0.3584</cell><cell>3.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Quantitative comparison of different inpainting methods. We do test with both centrally cropping a 64 ? 64 box and randomly cropping 80% pixels. ? means higher score is better.</figDesc><table><row><cell></cell><cell>Center Crop</cell><cell>Random Crop</cell></row><row><cell>Method</cell><cell cols="2">PSNR? SSIM? PSNR? SSIM?</cell></row><row><cell>(a) Single latent code [30, 32]</cell><cell cols="2">10.37 0.1672 12.79 0.1783</cell></row><row><cell cols="3">(b) Optimizing feature maps [3] 14.75 0.4563 18.72 0.2793</cell></row><row><cell>(c) DIP [39]</cell><cell cols="2">17.92 0.4327 18.02 0.2823</cell></row><row><cell>(d) Ours</cell><cell cols="2">21.43 0.5320 22.11 0.5532</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at this link.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and L?on Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent convolutional models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahrukh</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic photo manipulation with a generative image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gan dissection: Visualizing and understanding generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inverting layers of a large generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Seeing what a gan cannot generate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image blind denoising with generative adversarial network based noise modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gated-gan: Adversarial gated networks for multi-collection style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minje</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inverting the generator of a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil Anthony</forename><surname>Bharath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ganalyze: Toward visual definitions of cognitive image properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lore</forename><surname>Goetschalckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global guarantees for enforcing deep generative priors by empirical risk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Voroninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mixgan: learning concepts from different domains for mixture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang-Yuan</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the&quot;steerability&quot; of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grdn: Grouped residual dense network for real image denoising and gan-based real-world noise modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Ryun</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Won</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fader networks: Manipulating images by sliding attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generative semantic manipulation with mask-contrasting gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Precise recovery of latent vectors from generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zachary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Few-shot unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Invertibility of convolutional generative networks from partial measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulas</forename><surname>Ayaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Karaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Making a completely blind image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Invertible conditional gans for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guim</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M?lvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interpreting the latent space of gans for semantic face editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faceid-gan: Learning a symmetry three-player gan for identity-preserving face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Infrared image colorization based on a triplet dcgan architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><forename type="middle">X</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vintimilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep feature interpolation for image content changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09267</idno>
		<title level="m">Semantic hierarchy emerges in deep generative representations for scene synthesis</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic image inpainting with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teck Yian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
