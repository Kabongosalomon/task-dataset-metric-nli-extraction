<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards An End-to-End Framework for Flow-Guided Video Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TMCC</orgName>
								<orgName type="institution" key="instit2">Nankai University</orgName>
								<address>
									<region>CS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Ze</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TMCC</orgName>
								<orgName type="institution" key="instit2">Nankai University</orgName>
								<address>
									<region>CS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Qin</surname></persName>
							<email>qinjianhua@hisilicon.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Hisilicon Technologies Co. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Le</forename><surname>Guo</surname></persName>
							<email>guochunle@nankai.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TMCC</orgName>
								<orgName type="institution" key="instit2">Nankai University</orgName>
								<address>
									<region>CS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TMCC</orgName>
								<orgName type="institution" key="instit2">Nankai University</orgName>
								<address>
									<region>CS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards An End-to-End Framework for Flow-Guided Video Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optical flow, which captures motion information across frames, is exploited in recent video inpainting methods through propagating pixels along its trajectories. However, the hand-crafted flow-based processes in these methods are applied separately to form the whole inpainting pipeline. Thus, these methods are less efficient and rely heavily on the intermediate results from earlier stages. In this paper, we propose an End-to-End framework for Flow-Guided Video Inpainting (E 2 FGVI) through elaborately designed three trainable modules, namely, flow completion, feature propagation, and content hallucination modules. The three modules correspond with the three stages of previous flowbased methods but can be jointly optimized, leading to a more efficient and effective inpainting process. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods both qualitatively and quantitatively and shows promising efficiency. The code is available at https://github.com/MCG-NKU/E2FGVI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video inpainting aims to fill up the "corrupted" regions with plausible and coherent content throughout video clips. It is widely applied to real-world applications such as object removal <ref type="bibr" target="#b15">[16]</ref>, video restoration <ref type="bibr" target="#b27">[28]</ref>, and video completion <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref>. Despite the significant progress made in image inpainting <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>, video inpainting remains full of challenges due to the complex video scenarios and deteriorated video frames. Directly performing image inpainting on each frame independently tends to generate temporally inconsistent videos and results in severe artifacts. Both spatial structure and temporal coherence are required to be considered in high-quality video inpainting. Recent progress in deep learning motivates researchers to exploit more effective solutions <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>Among them, typical flow-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b56">57]</ref> consider video inpainting as a pixel propagation problem to naturally preserve the temporal coherence. As shown in * Equal contribution ? C.L. Guo is the corresponding author.  <ref type="figure" target="#fig_7">Figure 1</ref>. (a) The general pipelines of flow-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b56">57]</ref> and ours. While previous flow-based methods conduct the three stages separately, our corresponding modules work in an end-toend manner. (b) A qualitative comparison of our approach with a state-of-the-art flow-based method FGVC <ref type="bibr" target="#b16">[17]</ref>. Due to the error accumulation and ignoring temporal information during content hallucination, FGVC fails to generate faithful and temporally consistent results compared with our method. <ref type="figure" target="#fig_7">Fig. 1 (a)</ref>, these methods can be decomposed into three inter-related stages. (1) Flow completion: The estimated optical flow needs to be completed first because the absence of flow fields in corrupted regions will influence the latter processes. (2) Pixel propagation: They fill the holes in corrupted videos by bidirectionally propagating pixels in the visible areas with the guidance of the completed optical flow. (3) Content hallucination: After propagation, the remaining missing regions can be hallucinated by a pretrained image inpainting network <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>. Unfortunately, even though impressive results can be obtained, the whole flow-based inpainting process must be carried out separately as many hand-crafted operations (e.g., Poisson blending, solving sparse linear equations, and indexing per-pixel flow trajectories) are involved in the first two stages. The isolated processes raise two main problems. One is that the errors that occur at earlier stages would be accumulated and amplified at subsequent stages, which further influences the final performance significantly. Specifically, the inaccurate flow estimation would mislead the propagation of pixels and further confuse the stage of content hallucination, producing unfaithful inpainting results. Second, these complex hand-designed operations only can be processed without GPU acceleration. The whole procedure of inferring video sequences, therefore, is very timeconsuming. Taking DFVI <ref type="bibr" target="#b56">[57]</ref> as an example, completing one video with the size of 432 ? 240 from DAVIS <ref type="bibr" target="#b43">[44]</ref>, which contains about 70 frames, needs about 4 minutes 1 , which is unacceptable in most real-world applications. Besides, except for the above-mentioned drawbacks, only using a pretrained image inpainting network at the content hallucination stage ignores the content relationships across temporal neighbors, leading to inconsistent generated content in videos (see <ref type="figure" target="#fig_7">Fig. 1</ref> </p><formula xml:id="formula_0">(b)).</formula><p>To address these flaws, in this paper, we carefully design three trainable modules, including (1) flow completion, (2) feature propagation, and (3) content hallucination modules which simulate corresponding stages in flow-based methods and further constitute an End-to-End framework for Flow-Guided Video Inpainting (E 2 FGVI). Such close collaboration between the three modules alleviates the excessive dependence of intermediate results in the previously independently developed system <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b66">67]</ref> and works in a more efficient manner.</p><p>To be specific, for the flow completion module, we directly employ it on the masked videos for one-step completion instead of multiple complex steps. For the feature propagation module, in contrast to the pixel-level propagation, our flow-guided propagation process is conducted in the feature space with the assistance of deformable convolution. With more learnable sampling offsets and feature-level operations, the propagation module releases the pressure of inaccurate flow estimation. For the content hallucination module, we propose a temporal focal transformer to effectively model long-range dependencies on both spatial and temporal dimensions. Both local and non-local temporal neighbors are considered in this module, leading to more temporally coherent inpainting results.</p><p>Experimental results demonstrate that our framework enjoys the following two strengthens:</p><p>? State-of-the-art accuracy: Taking comparisons with previous state-of-the-art (SOTA) methods, the proposed E 2 FGVI achieves significant improvements on two common distortion-oriented metrics (i.e., PSNR and SSIM <ref type="bibr" target="#b52">[53]</ref>), one popular perception-oriented index (i.e., VFID <ref type="bibr" target="#b50">[51]</ref>), and one temporal consistency measurement (i.e., E warp <ref type="bibr" target="#b24">[25]</ref>). ? High efficiency: Our method processes 432 ? 240 videos at 0.12 seconds per frame on a Titan Xp GPU, which is nearly 15? faster than previous flow-based methods. In contrast to methods that also can be endto-end deployed, our method shows comparable inference time. Besides, our method has the lowest computational complexity (FLOPs) among all compared SOTA methods. We hope the proposed end-to-end framework with the aforementioned advantages could serve as a strong baseline for the video inpainting community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video inpainting. Building upon the development of deep learning, great progress has been made in video inpainting. These methods can be roughly divided into three classes: 3D convolution-based <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b49">50]</ref>, flow-based <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b56">57]</ref>, and attention-based methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b62">63]</ref>. Some methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b49">50]</ref> employing 3D convolution and attention usually yield temporally inconsistent results due to the limited temporal receptive fields. To generate more temporal coherence results, many works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b66">67]</ref> regard optical flows as strong priors for video inpainting and incorporate them into the network. However, directly computing optical flows between images within invalid regions is extremely difficult as these regions themselves become occlusion factors, restricting the performance. Recent flow-based methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b56">57]</ref> perform flow completion first and use the completed optical flows to propagate indexed pixels along their trajectories. Instead of conducting hand-crafted pixel-level propagation, we design an end-to-end trainable framework that performs the propagation process at the feature space. Besides, our method benefits from recent advances in using transformers to improve the inpainting results <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b62">63]</ref>. Flow-based video processing. The motion information across frames well assists many video-related tasks, such as video understanding <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref>, video segmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b47">48]</ref>, video object detection <ref type="bibr" target="#b65">[66]</ref>, depth estimation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b35">36]</ref>, video super-resolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b57">58]</ref>, frame interpolation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27]</ref>, etc. Specifically, many video restoration and enhancement algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b57">58]</ref> rely on optical flow to perform alignment for compensating the information between frames. Recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref> leverage deformable convolution <ref type="bibr" target="#b63">[64]</ref> to simulate the behavior of optical flow but with more learnable offsets for more effective alignment. Our works also share the same merit as these works. Vision transformer. Recently, Transformer <ref type="bibr" target="#b48">[49]</ref> has gained much attention in the vision community. Vision Transformer <ref type="bibr" target="#b14">[15]</ref> and its follow-ups <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b61">62]</ref> achieve an impressive performance on image and video representation learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref>, image generation <ref type="bibr" target="#b40">[41]</ref>, object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b64">65]</ref>, and many other applications <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. Because of the quadratic complexity of selfattention, many works deployed effective window-based at- tentions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b58">59]</ref> to reduce its computational complexity while improving the model's capability with the limited receptive fields. Swin Transformer <ref type="bibr" target="#b33">[34]</ref> strengthens local connections by computing self-attention through shifting local windows. Focal Transformer <ref type="bibr" target="#b58">[59]</ref> introduces focal selfattention, which enhances the global-local interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given a corrupted video sequence {X t ? R H?W ?3 | t = 1 . . . T } with sequence length T and corresponding frame-wise binary masks {M t ? R H?W ?1 | t = 1 . . . T }, we aim at synthesizing faithful content which is consistent in both space and time dimensions within the corrupted (masked) areas. In the following, we discuss the main components of our method. First, we use a context encoder, which encodes all corrupted frames into lowerresolution features for computational efficiency at subsequent processing. Second, we extract and complete the optical flow between local neighbors through a flow completion module (Sec. 3.1). Third, the completed optical flow assists the features extracted from local neighbors to accomplish feature alignment and bidirectional propagation (Sec. 3.1). Fourth, multi-layer temporal focal transformers perform content hallucination by combining propagated local neighboring features with non-local reference features. (Sec. 3.2). Finally, a decoder up-scales the filled features and reconstructs them to a final video sequence <ref type="figure" target="#fig_1">Fig. 2</ref> shows the whole pipeline of the proposed E 2 FGVI. It is worth noticing that all modules are differentiable and constitute an end-to-end trainable architecture.</p><formula xml:id="formula_1">{? t ? R H?W ?3 | t = 1 . . . T }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Flow completion and feature propagation</head><p>In this section, we will detail the proposed flow-related operations. Note that we only apply flow-based modules on the features extracted from local neighboring frames because the flow estimation is substantially degraded or even fails because of the presence of large motion, which frequently occurs in non-local frames. Besides, the flow-related operations are given at lower-resolution space for computational efficiency. End-to-end flow completion. Before flow prediction, we first downsample the original corrupted frames X t at 1/4 resolution, which matches the spatial resolution of encoded low-resolution features. The downsampled frames are denoted as X t</p><formula xml:id="formula_2">? ? R H 4 ? W 4 ?3 .</formula><p>The flow prediction between adjacent frames i and j is computed by a flow estimation network F:</p><formula xml:id="formula_3">F i?j = F(X i ? , X j ? ).<label>(1)</label></formula><p>We initialize the network using pretrained weights from a lightweight flow estimation network to resort to its rich knowledge about optical flows. Following most flow-based video inpainting methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b56">57]</ref>, we estimate both forward flowF t?t+1 and backward flowF t?t?1 through Eq. (1) for flow-guided bidirectional propagation. Since the missing areas in corrupted videos become occlusion factors for flow estimation, which severely affects the quality of estimated flow, we need to restore the forward and backward flow before using them for feature propagation. For simplicity, we use L1 loss 2 to restore the bidirectional flows:</p><formula xml:id="formula_4">L f low = T ?1 t=1 F t?t+1 ?Ft?t+1 1 + T t=2 F t?t?1 ?Ft?t?1 1,<label>(2)</label></formula><p>where F t?t+1 and F t?t?1 are the ground truth forward and backward flow, respectively, which are calculated from original uncorrupted videos.</p><p>Our flow completion module differs from DFVI <ref type="bibr" target="#b56">[57]</ref> and FGVC <ref type="bibr" target="#b16">[17]</ref> from two main aspects. (1) DFVI and FGVC deploy the flow completion network and propagation algorithm separately. In contrast, our flow completion module can be trained with other network components in an end-toend manner, which facilitates the module to generate taskoriented flows <ref type="bibr" target="#b57">[58]</ref>. <ref type="bibr" target="#b1">(2)</ref> The flow completion in DFVI and FGVC is less efficient (&gt; 0.4s/flow) because they need to initialize the flow first and then refine the initialized flow <ref type="figure">Figure 3</ref>. An example of using the completed forward flowFt?t+1 to guide the feature backward propagation, where ? and ? denote an addition operation and a concatenation operation, respectively. Note that the backward flow will act in the opposite direction.</p><formula xml:id="formula_5">c Conv LReLU Conv LReLU c Conv Offsets &amp; Weight masks D b? t+1 b E t b E tF t?t+1 W</formula><p>with multiple stages, while we estimate and complete the flow in only one feed-forward pass with much faster speed</p><formula xml:id="formula_6">(&lt; 0.01s/flow). Flow-guided feature propagation. Suppose {E t ? R H 4 ? W 4 ?C | t = 1 .</formula><p>. . T l } are the local temporal neighboring features extracted from the context encoder, where T l denotes the length of local neighboring frames. Taking the forward flowF t?t+1 as an example, it assists us in capturing the motion of the corrupted regions from the t-th frame to the (t+1)-th frame. Once the pixels in the corrupted regions at the t-th content feature is known in the valid area at the (t+1)-th feature, we can intuitively exploit this valid information through warping the (t+1)-th backward propagation feature? t+1 b to current time step with the help of the forward flowF t?t+1 . The warped feature can be further merged with current content feature E t and updated through a backward propagation function P b (?):</p><formula xml:id="formula_7">E t b = P b (E t , W(? t+1 b ,Ft?t+1)),<label>(3)</label></formula><p>where W(?) denotes the spatial warping operation based on optical flow,? t b is the backward propagation feature at the t-th time step, and the propagation function P b (?) represents two convolutional layers with a LeakyReLU <ref type="bibr" target="#b36">[37]</ref> activation.</p><p>The warping and merging operations in Eq. (3) are approximate to the whole propagation process in DFVI and FGVC, but we conduct them in the feature space rather than the image space. The propagation feature? t b is updated step by step as faithful content is gradually involved in the corrupted area for each content feature, which also facilitates the connection across all local neighboring features with flow guidance. Unlike the hand-crafted pixel-level propagation in flow-based methods, which is very time-consuming and depends heavily on the quality of estimated flow, the feature-level propagation adaptively merges the flow-traced information with larger receptive fields using convolutional layers and can be speeded up by GPUs.</p><p>Although the feature-level propagation can be much faster and more effective than FGVC and DFVI, it still needs to face the problem caused by the inaccurate flow estimation results in Eq. <ref type="formula" target="#formula_3">(1)</ref>, which will bring irrelevant information in the propagation process and further hamper the final performance. To mitigate this problem, inspired by <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b51">52]</ref>, we employ modulated deformable convolution <ref type="bibr" target="#b63">[64]</ref> to further index and weight the candidate feature points. As shown in <ref type="figure">Fig. 3</ref>, we first calculate the weight mask W t?t+1 and the offsets ?F t?t+1 relative to the estimated optical flow with:</p><formula xml:id="formula_8">[Wt?t+1, ?Ft?t+1] = C b (E t , W(? t+1 b ,Ft?t+1),Ft?t+1),<label>(4)</label></formula><p>where C b (?) denotes multiple cascading convolutional layers. Both the size of computed weight mask M t?t+1 and offset ?F t?t+1 are H 4 ? W 4 ? K 2 ? G, where K and G are the kernel size and the group number of deformable convolution, respectively. We can further generate K 2 ? G candidate feature points for each spatial location by adding the offset ?F t?t+1 to the completed optical flowF t?t+1 . The relationship between the offset ?F t?t+1 and the completed optical flowF t?t+1 are mutually beneficial. On the one hand, more flexible sampling locations could well compensate for the inaccurate flow completion. On the other hand, the completed flow provides promising initial sampling locations, which make it easily find more meaningful content within their surroundings. Then, we use a deformable convolutional layer to warp the backward feature? t+1 b instead of optical flow-based warping in Eq. (3) and further obtain the backward propagation feature? t b through:</p><formula xml:id="formula_9">E t b = P b (E t , D b (? t+1 b</formula><p>, Wt?t+1,Ft?t+1 + ?Ft?t+1)), <ref type="bibr" target="#b4">(5)</ref> where D b denotes the operation of the deformable convolutional layer. The weight mask W t?t+1 , whose values are normalized via a sigmoid function, can be applied to each sampling pixel for measuring its validity. The aforementioned operations are employed bidirectionally following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b56">57]</ref>, while the forward propagation feature? t f can be obtained in the same way but in the opposite direction. Finally, we use a learnable 1 ? 1 sized convolution layer to fuse the forward and backward propagation features adaptively instead of using a pre-defined rule to combine the bidirectional flow traced pixels in <ref type="bibr" target="#b56">[57]</ref>.</p><formula xml:id="formula_10">E t = I(? t f ,? t b ),<label>(6)</label></formula><p>where I denotes a 1 ? 1 sized convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal focal transformer</head><p>Only using the information provided by local temporal neighbors is not enough for video inpainting. As discussed in <ref type="bibr" target="#b16">[17]</ref>, the corrupted content at local neighbors may appear in the non-local ones. Thus, the information in the nonlocal temporal neighbors can be regarded as a promising reference for these missing regions in local neighbors. Here we stack multiple temporal focal transformer blocks to effectively combine the information from local and non-local temporal neighbors for performing content hallucination.</p><p>Suppose T nl is the number of selected non-local frames.</p><formula xml:id="formula_11">E nl ? R T nl ? H 4 ? W 4 ?C is the encoded features of all non- local neighbors.? l ? R T l ? H 4 ? W 4</formula><p>?C is the local temporal feature through concatenating the results in Eq. (6) at the temporal dimension. We use a soft split operation <ref type="bibr" target="#b32">[33]</ref> to perform overlapped patch embedding on the concatenated local and non-local temporal features:</p><formula xml:id="formula_12">Z 0 = SS([? l , E nl ]) ? R (T l +T nl )?M ?N ?Ce ,<label>(7)</label></formula><p>where SS denotes the operation of soft split. Z 0 is the embedded token that contains both local and non-local temporal information. M ? N is the embedded spatial dimension, and C e is the feature dimension. Instead of vanilla vision transformer <ref type="bibr" target="#b14">[15]</ref>, which is frequently employed in recent works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b62">63]</ref>, we use focal transformer <ref type="bibr" target="#b58">[59]</ref> to search from both local and non-local neighbors to fill missing contents. The reasons are listed as follows: (1) Compared with performing fine-grained global attention, the computational and memory cost can be effectively reduced through window-based attention <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b58">59]</ref>. <ref type="formula" target="#formula_4">(2)</ref> For each token in the missing regions, it is reasonable to perform the fine-grained self-attention only in local regions while the coarse-grained attentions globally because of the local self-similarity of an image.</p><p>Since the original focal transformer is unable to process sequence data, we propose a temporal focal transformer that essentially extends the size of focal windows from 2D to 3D. Specifically, we first split the input token Z n?1 , where n ? [1, N ] and N is the stacking number of focal transformer blocks, into a grid of subwindows with size s t ? s h ? s w . The split token? n?1 ?</p><formula xml:id="formula_13">R ( (T l +T nl ) s t ? M s h ? N sw ?Ce)?(st?s h ?sw)</formula><p>can be directly used for computing fine-grained local attentions. To perform global attention at the coarse granularity, a linear embedding layer f p is used to pool the sub-windows spatially vi?</p><formula xml:id="formula_14">Z n?1 g = f p (? n?1 ) ? R ( (T l +T nl ) s t ? M s h ? N sw ?Ce)?st .</formula><p>We then calculate the query, key, and value through two linear projection layers f q , f kv :</p><formula xml:id="formula_15">Q n = fq(? n?1 ), {K n l , K n g , V n l , V n g } = f kv ({? n?1 ,? n?1 g }).<label>(8)</label></formula><p>To calculate attentions with local-global interactions, for the queries inside the i-th sub-window Q n i ? R st?s h ?sw?Ce , we gather the keys not only from the i-th local window K n l,i ? R st?s h ?sw?Ce but also from the ith unfolded coarse-grained window K n g,i ? R st?s h ?sw?Ce .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sub-window pooling</head><p>Flatten Flatten Multi-head Self-Attention <ref type="figure">Figure 4</ref>. Illustration of temporal focal self-attention. Here we use the window size of 2 ? 2 ? 2 as an example. We can see that the keys and values {K n , V n } contain both fine-grained local information and coarse-grained global information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local window</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-grained window</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input tokens</head><formula xml:id="formula_16">T fq f kv Z n?1 Z n?1 g Q n {K n , V n }</formula><p>This operation can be processed in parallel. We concatenate corresponding keys and values respectively by K n = {K n l , K n g } and V n = {V n l , V n g }, and then calculate the focal self-attention for Q l i :</p><formula xml:id="formula_17">Attention (Q n , K n , V n ) = Softmax Q n (K n ) T ? Ce V n . (9)</formula><p>Note that the attention function also can work in a multihead manner. An example is shown in <ref type="figure">Fig. 4</ref>. Finally, the whole process in the n-th focal transformer block is formulated as</p><formula xml:id="formula_18">Z n = MFSA(LN1(Z n?1 )) + Z n?1 ,<label>(10)</label></formula><formula xml:id="formula_19">Z n = F3N(LN2(Z n )) + Z n ,<label>(11)</label></formula><p>where MFSA and LN denote the multi-head focal selfattention and layer normalization <ref type="bibr" target="#b0">[1]</ref>, respectively. We use F3N <ref type="bibr" target="#b32">[33]</ref> to link the connections across embedded patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training objectives</head><p>We employ three loss functions to optimize our model. The first is the reconstruction loss which measures pixellevel differences between synthetic videos? and the original ones Y through L1 distance:</p><formula xml:id="formula_20">Lrec = ? ? Y 1.<label>(12)</label></formula><p>The second is the adversarial loss which has been proven to be useful for the generation of high-quality and realistic content. We employ a T-PatchGAN <ref type="bibr" target="#b6">[7]</ref> based discriminator to make the model focus on both global and local features across all temporal neighbors. The training objective of this discriminator D is:</p><formula xml:id="formula_21">LD = E x?P Y (x) [ReLU(1 ? D(x))]+ E z?P? (z) [ReLU(1 + D(z))],<label>(13)</label></formula><p>For video inpainting generator, the adversarial loss is formulated as:  </p><formula xml:id="formula_22">L adv = ?E z?P? (z) [D(z)],<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>Dataset. To show the effectiveness of the proposed method, we evaluate it on two popular video object segmentation datasets, i.e., YouTube-VOS <ref type="bibr" target="#b55">[56]</ref> and DAVIS <ref type="bibr" target="#b43">[44]</ref>. YouTube-VOS, with diverse scenes, consists of 3471, 474, and 508 video clips for training, validation, and test, respectively. We follow the original split mode and report the experimental metrics on the test set for YouTube-VOS. DAVIS is composed of 60 video clips for training and 90 video clips for testing. Following FuseFormer <ref type="bibr" target="#b32">[33]</ref>, 50 video clips from the test set are used for calculating metrics. We train our model on the YouTube-VOS dataset and evaluate it on both YouTube-VOS and DAVIS datasets. As for masks, during training, we generate stationary and object-like masks to simulate video completion and object removal applications following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b62">63]</ref>. For evaluation, stationary masks are used to calculate objective metrics, and objectlike masks are adopted for qualitative comparisons because of the lack of references. Metrics. We choose PSNR, SSIM <ref type="bibr" target="#b52">[53]</ref>, VFID <ref type="bibr" target="#b50">[51]</ref>, and flow warping error E warp <ref type="bibr" target="#b24">[25]</ref> to evaluate the performance of recent video inpainting methods. Specifically, PSNR and SSIM are frequently used metrics for distortion-oriented image and video assessment. VFID measures the perceptual similarity between two input videos and has been adopted in recent video inpainting works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b62">63]</ref>. Flow warping error E warp is employed to measure the temporal stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison</head><p>Quantitative results. We report quantitative results on YouTube-VOS <ref type="bibr" target="#b55">[56]</ref> and DAVIS <ref type="bibr" target="#b43">[44]</ref> under the stationary masks and compare our method with previous video inpainting methods, including VINet <ref type="bibr" target="#b22">[23]</ref>, DFVI <ref type="bibr" target="#b56">[57]</ref>,</p><p>LGTSM <ref type="bibr" target="#b7">[8]</ref>, CAP <ref type="bibr" target="#b27">[28]</ref>, STTN <ref type="bibr" target="#b62">[63]</ref>, FGVC <ref type="bibr" target="#b16">[17]</ref>, and Fuseformer <ref type="bibr" target="#b32">[33]</ref>. As shown in Tab. 1, our method substantially surpasses all previous SOTA algorithms on all four quantitative metrics. The superior results demonstrate that our method can generate videos with less distortion (PSNR and SSIM), more visually plausible content (VFID), and better spatial and temporal coherence (E warp ), which verifies the superiority of the proposed method. Qualitative results. We choose three representative methods, including CAP <ref type="bibr" target="#b27">[28]</ref>, FGVC <ref type="bibr" target="#b16">[17]</ref>, and Fuseformer <ref type="bibr" target="#b32">[33]</ref>, to conduct visual comparisons. <ref type="figure">Fig. 6</ref> shows both video completion and object removal results. While the compared methods are hard to recover reasonable details in the masked regions, the proposed method can generate faithful textural and structure information. This demonstrates the effectiveness of the proposed method.</p><p>For further comprehensive comparisons, a user study is conducted on both object removal and video completion applications. We select five methods including two flowbased methods (i.e., DFVI <ref type="bibr" target="#b56">[57]</ref> and FGVC <ref type="bibr" target="#b16">[17]</ref>), and three attention-based methods (i.e., CAP <ref type="bibr" target="#b27">[28]</ref>, STTN <ref type="bibr" target="#b62">[63]</ref>, and Fuseformer <ref type="bibr" target="#b32">[33]</ref>). We invite 20 participants for the user study totally. Every volunteer is shown randomly sampled 40 video triplets and asked to select a visually better inpainting video. Each triplet is composed of one original video, one from our method, and one from a randomly cho-Masked Frames CAP <ref type="bibr" target="#b27">[28]</ref> FGVC <ref type="bibr" target="#b16">[17]</ref> FuseFormer <ref type="bibr" target="#b32">[33]</ref> E 2 FGVI (Ours) <ref type="figure">Figure 6</ref>. Qualitative results compared with CAP <ref type="bibr" target="#b27">[28]</ref>, FGVC <ref type="bibr" target="#b16">[17]</ref>, FuseFormer <ref type="bibr" target="#b32">[33]</ref>.  sen method. The user study results are shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. As we can see, volunteers obviously favor our results over those from almost all methods. Although such significant preference does not exist in the comparisons with FGVC, the proposed method still receives a majority of votes. This demonstrates that the proposed method could generate more visually pleasant results than compared methods. Efficiency comparisons. We use FLOPs and inference time to measure the efficiency of each method. The FLOPs are calculated using the temporal size of 8, and the runtime is measured on a single Titan Xp GPU using DAVIS dataset. The compared results are shown in Tab. 1. The proposed method shows comparable running time with transformerbased methods and is nearly ?15 faster than flow-based methods. Besides, it holds the lowest FLOPs in contrast to all other methods. This indicates that the proposed method is highly efficient for video inpainting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations</head><p>We perform three ablation studies on flow completion, feature propagation, and attention mechanism to verify the effectiveness of proposed modules in our framework. All ablation studies are conducted on the DAVIS dataset. Study of flow completion module. First, we investigate that the importance of motion information for video inpainting. By only removing the flow consistency loss L f low , our flow completion module no longer provides information about object motions (see <ref type="figure" target="#fig_4">Fig. 7</ref>), resulting in a large performance decrease, as shown in Tab. 2. Second, we study the necessity of completing the optical flow through fixing the pretrained weights in the flow completion module. With the preliminary knowledge about optical flow, the flow completion module regards the masked regions as occlusion factors and provides initial flow estimation for visible regions (see <ref type="figure" target="#fig_4">Fig. 7)</ref>. In contrast to the model without motion information, the performance has an obvious improvement. However, such model ignores the motion information in the masked regions. After we complete the flow by training the  flow completion module towards minimizing the flow consistency loss, we obtain larger PSNR and SSIM values than before. As shown in <ref type="figure" target="#fig_4">Fig. 7</ref>, the model with completed flows recovers more faithful content about the human arm. Additionally, in Tab. 2 and <ref type="figure" target="#fig_4">Fig. 7</ref>, we also show the potential upper bound of our method which estimates the optical flow between uncorrupted frames. Study of feature propagation module. After we remove the feature propagation module from the model (case (a) in Tab. <ref type="bibr" target="#b2">3)</ref>, the values of quantitative metrics are decreased dramatically. From <ref type="figure" target="#fig_5">Fig. 8 (a)</ref>, we can see that the results generated by this model exist severe artifacts and discontinuous content. After adding flow-based warping and propagation (see Eq. (3)) to this model (case (b) in Tab. 3), since we could bring valid pixels from adjacent frames to unseen regions with the assistance of optical flow, the generated content becomes more faithful as shown in <ref type="figure" target="#fig_5">Fig. 8 (b)</ref>, and the PSNR value is increased by a large margin (0.42dB). However, it is hard for flow-based warping and propagation to recover the content that cannot be traced by optical flow (the white line in <ref type="figure" target="#fig_5">Fig. 8 (b)</ref>). Besides, for the feature propagation module, which only involves deformable convolutionbased warping (case (c) in Tab. 3), the structure details can be more clearly recovered with the help of more learnable offsets, but more artifacts are involved due to the lack of faithful information warped from adjacent frames in contrast to flow-based warping. By combining deformable convolution with flow guidance (case (d) in Tab. 3), the PSNR and SSIM values can be further improved. In <ref type="figure" target="#fig_5">Fig. 8 (d)</ref>, this model achieves the visually best results among all variants while preserving promising structure details. This demonstrates the effectiveness of the feature propagation module. Study of attention mechanism. We remove the flow completion and feature propagation modules to purely compare different attention mechanisms, including vanilla global attention (FuseFormer <ref type="bibr" target="#b32">[33]</ref>), local window attention, and fo-   <ref type="figure" target="#fig_6">Fig. 9</ref> shows two failure cases. When encountering large motion or a large amount of missing object details across frames, our method produces implausible content and many artifacts in masked regions as well as FGVC <ref type="bibr" target="#b16">[17]</ref> and Fuse-Former <ref type="bibr" target="#b32">[33]</ref> do. This demonstrates that these situations are still challenging for video inpainting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Limitation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed an end-to-end trainable flow-based model for video inpainting named E 2 FGVI. The elaborately designed three modules (i.e., flow completion, feature propagation, and content hallucination modules) are collaborated together and address many bottlenecks of previous methods. Experimental results have shown that our method achieves state-of-the-art quantitative and qualitative performance on two benchmark datasets and is efficient in terms of inference time and computational complexity. We hope it can serve as a strong baseline for future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture and Training Details</head><p>Architecture. In our model, the encoder and the decoder use the same architecture as FuseFormer <ref type="bibr" target="#b32">[33]</ref>. The channel dim C of the encoder and the decoder is set as 128.</p><p>A lightweight model SPyNet <ref type="bibr" target="#b44">[45]</ref> is employed as our flow completion module for computational efficiency. To utilize the learned flow prior in original SPyNet, we use pre-trained weights to initialize this module. The architecture details of the T-PatchGAN are identical to previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b62">63]</ref>. The kernel size K and the group number G of deformable convolution are set as 3 and 16, respectively. The number of focal transformer blocks N is set as 8 and the embedded dim of tokens C e is set as 512. The embedded spatial dimension M ? N is 20 ? 36. The size of partitioned subwindow s t ? s h ? s w is set to (T l + T nl ) ? 5 ? 9. At the end of the content hallucination module, we use a soft composite operator <ref type="bibr" target="#b32">[33]</ref> to composite the embedded tokens to features, which share the same spatial size as the original ones.</p><p>Training details. For training objectives, the weights of L rec , L adv , and L f low are 1, 10 ?2 , and 1, respectively. Taking the memory limitations of GPUs into account, we resize all frames from videos into 432 ? 240 for training, evaluation, and test. During training, the numbers of local (T l ) and non-local frames (T nl ) are 5 and 3, respectively. Local frames are continuous clips, while non-local frames are randomly sampled from videos for training. Following STTN <ref type="bibr" target="#b62">[63]</ref> and FuseFormer <ref type="bibr" target="#b32">[33]</ref>, during evaluation and test, we use a sliding window with the size of 10 to get local neighboring frames and uniformly sample the non-local neighboring frames with a sampling rate of 10. We adopt Adam optimizer with ? 1 = 0 and ? 2 = 0.99. The final model is trained for 500K iterations, and the initial learning rate is set as 0.0001 for all modules and reduced by the factor of 10 at 400K iteration. In our ablation studies, we train the model for 250K iterations. We use 8 NVIDIA Tesla V100 GPUs for training and the batch size is set as 8. Our code is available 3 for reproducibility. To verify the effectiveness of online flow completion, we prepare completed flows using the FGVC <ref type="bibr" target="#b24">[25]</ref> flow completion module in an offline manner. We then retrain a model with the FGVC completed flows. The PSNR value of this model is slightly higher than our end-to-end setting (32.38 3 https://github.com/MCG-NKU/E2FGVI vs. <ref type="bibr">32.35 (dB)</ref>). However, the inference speed is much slower than ours (1.21 vs. 0.16 (s/frame)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Taking a deeper look to flow-guided feature propagation module</head><p>To further investigate the effectiveness of the feature propagation module, we visualize averaged local neighboring features with the temporal size of 5 before conducting content hallucination in <ref type="figure" target="#fig_7">Fig. 10</ref>. The four cases in <ref type="figure" target="#fig_7">Fig. 10</ref> correspond to the four variants in the Tab. 3 of our main paper. For the model without feature propagation ( <ref type="figure" target="#fig_7">Fig. 10(a)</ref>), obviously, we can see that corrupted regions from all frames still exist in these features, further restricting the performance of content hallucination. For the model only using flow-based warping ( <ref type="figure" target="#fig_7">Fig. 10(b)</ref>) or deformable convolutionbased warping <ref type="figure" target="#fig_7">(Fig. 10(c)</ref>), corrupted regions are filled with the contents warped from adjacent frames. And the deformable convolution-based warping can generate smoother content than flow-based one due to more sampling feature points. However, especially for the last two temporal features (last two columns in <ref type="figure" target="#fig_7">Fig. 10</ref>), the regions filled by the model without flow guidance have more distinct boundaries in contrast to flow-based warping, which implies that less faithful content are propagated without motion information. Through adopting deformable convolution with flow guidance, the final propagation module ( <ref type="figure" target="#fig_7">Fig. 10(d)</ref>) fills the holes with the most reasonable and natural content among all cases. This is a promising demonstration of the mutually beneficial relationship between deformable offsets and completed flow fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Study of the hallucination ability</head><p>To purely evaluate the hallucination ability of our method, we first pre-fill the pixels which can be traced by flow fields <ref type="bibr" target="#b16">[17]</ref>. The remaining unfill pixels are thus most likely not visible in other video frames. We then feed the pre-filled videos to an image inpainting model <ref type="bibr" target="#b60">[61]</ref> and our model, respectively. Our hallucinated result has a much larger PSNR value than the image inpainting model on DAVIS dataset (31.74 vs. 30.80 (dB)). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Parameter comparison</head><p>We report the parameters in Tab. 5. Although our method consumes ?14% more parameters than the SOTA method (i.e., FuseFormer <ref type="bibr" target="#b32">[33]</ref>), it achieves a great trade-off between performance and computational complexity among other methods (see Tab. 5). For further comparison, we add residual blocks in FuseFormer to achieve similar parameters with ours. Our method still performs better than the larger version of FuseFormer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. More Qualitative Results</head><p>In this section, we provide additional visual results on two benchmark datasets, including YouTube-VOS <ref type="bibr" target="#b55">[56]</ref> and DAVIS <ref type="bibr" target="#b43">[44]</ref>, to further show the superiority of the proposed E 2 FGVI. The reconstruction results of CAP <ref type="bibr" target="#b27">[28]</ref>, FGVC <ref type="bibr" target="#b16">[17]</ref>, and FuseFormer <ref type="bibr" target="#b32">[33]</ref> are presented for comparisons. As shown in <ref type="figure" target="#fig_7">Fig. 11-14</ref>, our E 2 FGVI can generate more faithful textural and structural information and more coherent contents in masked regions than other methods. Our demo is shown in our project page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAP</head><p>FGVC FuseFormer E 2 FGVI(Ours) <ref type="figure" target="#fig_7">Figure 11</ref>. Qualitative video completion results on YouTube-VOS <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAP</head><p>FGVC FuseFormer E 2 FGVI(Ours) CAP FGVC FuseFormer E 2 FGVI(Ours) <ref type="figure" target="#fig_7">Figure 13</ref>. Qualitative object removal results on DAVIS <ref type="bibr" target="#b43">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CAP</head><p>FGVC FuseFormer E 2 FGVI(Ours) <ref type="figure" target="#fig_7">Figure 14</ref>. Qualitative video completion results on DAVIS <ref type="bibr" target="#b43">[44]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed End-to-End framework for Flow-Guided Video Inpainting (E 2 FGVI). It consists of 1) a frame-level content encoder, 2) a flow completion module, 3) a feature propagation module, 4) a content hallucination module which is composed of multiple temporal focal transformer blocks, and 5) a frame-level decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>User study results. The vertical axis indicates the percentage of favoring our method compared to other methods.The third loss is the flow consistency loss shown in Eq.<ref type="bibr" target="#b1">(2)</ref>. Training details can be found in supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>motion info w/o completed flow w/ completed flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Ablation studies on the flow completion module. The first row shows the results generated from the flow completion modules under different situations. The second row visualizes corresponding inpainting frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results of the ablation studies on the feature propagation module. The last four columns correspond to four cases in Tab. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Two failure cases (car drifting). Current video inpainting methods fail to deal with large motion or a large number of missing object details and may produce severe artifacts. cal attention. As shown in Tab. 4, vanilla global attention achieves the best quantitative performance while suffering from the heavy computation. Local attention introduces local windows as Video Swin Transformer<ref type="bibr" target="#b34">[35]</ref> does. Although the FLOPs are decreased by 34%, the attention calculation is limited in the local window, leading to poor performance. Focal attention shows a good trade-off between performance and computation. Its PSNR and SSIM values are comparable to FuseFormer, and the computational cost is only increased by 12% in contrast to the local one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>B. 1 .</head><label>1</label><figDesc>Completing flows in a offline manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Visualization of the frame-wise average features before feeding into the content hallucination stage under different experimental settings: (a) without flow-guided feature propagation, (b) flow-guided feature propagation without deformable convolution (Eq. 3 of the main paper), (c) feature propagation without flow guidance, and (d) final flow-guided feature propagation module with the assistance of both flow fields and deformable convolution. (Zoom-in for best view)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 .</head><label>12</label><figDesc>Qualitative video completion results on YouTube-VOS<ref type="bibr" target="#b55">[56]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons with SOTA video inpainting models on YouTube-VOS<ref type="bibr" target="#b55">[56]</ref> and DAVIS<ref type="bibr" target="#b43">[44]</ref> datasets. ? indicates higher is better. ? indicates lower is better. Ewarp * denotes Ewarp ? 10 ?2 . Each method is evaluated following the procedures in FuseFormer<ref type="bibr" target="#b32">[33]</ref>. VINet, DFVI, and FGVC are not end-to-end training methods. Their FLOPs, thus, are not projectable. SSIM ? VFID ? E warp</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy</cell><cell></cell><cell></cell><cell></cell><cell>Efficiency</cell></row><row><cell cols="2">Models</cell><cell></cell><cell cols="8">YouTube-VOS PSNR ? ? DAVIS</cell><cell>FLOPs</cell><cell>Runtime (s/frame)</cell></row><row><cell cols="2">VINet [23]</cell><cell></cell><cell>29.20</cell><cell>0.9434</cell><cell>0.072</cell><cell>0.1490</cell><cell>28.96</cell><cell>0.9411</cell><cell>0.199</cell><cell>0.1785</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">DFVI [57]</cell><cell></cell><cell>29.16</cell><cell>0.9429</cell><cell>0.066</cell><cell>0.1509</cell><cell>28.81</cell><cell>0.9404</cell><cell>0.187</cell><cell>0.1608</cell><cell>-</cell><cell>2.56</cell></row><row><cell cols="2">LGTSM [8]</cell><cell></cell><cell>29.74</cell><cell>0.9504</cell><cell>0.070</cell><cell>0.1859</cell><cell>28.57</cell><cell>0.9409</cell><cell>0.170</cell><cell>0.1640</cell><cell>1008G</cell><cell>0.23</cell></row><row><cell cols="2">CAP [28]</cell><cell></cell><cell>31.58</cell><cell>0.9607</cell><cell>0.071</cell><cell>0.1470</cell><cell>30.28</cell><cell>0.9521</cell><cell>0.182</cell><cell>0.1533</cell><cell>861G</cell><cell>0.40</cell></row><row><cell cols="2">FGVC [17]</cell><cell></cell><cell>29.67</cell><cell>0.9403</cell><cell>0.064</cell><cell>0.1022</cell><cell>30.80</cell><cell>0.9497</cell><cell>0.165</cell><cell>0.1586</cell><cell>-</cell><cell>2.44</cell></row><row><cell cols="2">STTN [63]</cell><cell></cell><cell>32.34</cell><cell>0.9655</cell><cell>0.053</cell><cell>0.0907</cell><cell>30.67</cell><cell>0.9560</cell><cell>0.149</cell><cell>0.1449</cell><cell>1032G</cell><cell>0.12</cell></row><row><cell cols="3">FuseFormer [33]</cell><cell>33.29</cell><cell>0.9681</cell><cell>0.053</cell><cell>0.0900</cell><cell>32.54</cell><cell>0.9700</cell><cell>0.138</cell><cell>0.1362</cell><cell>752G</cell><cell>0.20</cell></row><row><cell cols="3">E 2 FGVI (Ours)</cell><cell>33.71</cell><cell>0.9700</cell><cell>0.046</cell><cell>0.0864</cell><cell>33.01</cell><cell>0.9721</cell><cell>0.116</cell><cell>0.1315</cell><cell>682G</cell><cell>0.16</cell></row><row><cell></cell><cell>90% 100%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>removal stationary</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Percentage</cell><cell>60% 70% 80%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>50%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40%</cell><cell>CAP</cell><cell>DFVI</cell><cell>STTN</cell><cell cols="2">FGVC FuseFormer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* ? PSNR ? SSIM ? VFID ? E warp*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation studies on the flow completion module.</figDesc><table><row><cell>Case</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell>w/o motion information</cell><cell>32.08</cell><cell>0.9673</cell></row><row><cell>w/o completed flow</cell><cell>32.23</cell><cell>0.9682</cell></row><row><cell>w/ completed flow</cell><cell>32.35</cell><cell>0.9688</cell></row><row><cell>Flow GT</cell><cell>32.54</cell><cell>0.9698</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Investigation on the feature propagation module. 'Flow' indicates the flow-based warping function W in Eq. (4). 'DCN' denotes modulated deformable convolution<ref type="bibr" target="#b63">[64]</ref>.</figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell></cell><cell>(d)</cell></row><row><cell>Flow</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DCN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">PSNR 31.73/0.9653 32.15/.9677 32.17/0.9676 32.35/.9688</cell></row><row><cell>Masked Frames</cell><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell><cell>(d)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on various attention mechanisms. Fuse-Former<ref type="bibr" target="#b32">[33]</ref> is the current SOTA method that uses vanilla global attention.</figDesc><table><row><cell></cell><cell>Case</cell><cell>PSNR SSIM FLOPs</cell><cell></cell></row><row><cell></cell><cell>FuseFormer</cell><cell>31.74 0.9662 752G</cell><cell></cell></row><row><cell></cell><cell cols="2">Local attention 31.57 0.9648 497G</cell><cell></cell></row><row><cell></cell><cell cols="2">Focal attention 31.73 0.9653 560G</cell><cell></cell></row><row><cell>Masked Frames</cell><cell>FGVC</cell><cell>FuseFormer</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Parameters comparisons. FuseFormer* denotes a larger version of original FuseFormer.</figDesc><table><row><cell></cell><cell cols="2">FuseFormer [33] FuseFormer*</cell><cell>E 2 FGVI</cell></row><row><cell>Params. (M)</cell><cell>36.6</cell><cell>41.6</cell><cell>41.8</cell></row><row><cell>PSNR/SSIM</cell><cell>31.74/0.9662</cell><cell cols="2">31.91/0.9669 32.35/0.9688</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We test it on Intel(R) Core(TM) i7-6700K CPU with a single NVIDIA Titan Xp GPU.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Other loss functions can also be used in Eq. (2), but we do not observe significant improvements on the final inpainting performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Basicvsr: The search for essential components in video super-resolution and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding deformable alignment in video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Basicvsr++: Improving video superresolution with enhanced propagation and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
		<idno>CVPR, 2022. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Free-form video inpainting with 3d gated convolution and temporal patchgan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Liang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learnable gated temporal shift module for deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Liang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Virtex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cswin transformer: A general vision transformer backbone with cross-shaped windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00652</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video inpainting with short-term windows: Application to object removal and error concealment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounira</forename><surname>Ebdelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Guillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flow-edge guided video completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Ze</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m">Visual attention network</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Xing</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang-Jiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Hai</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<title level="m">Attention mechanisms in computer vision: A survey. Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Proposal-based video completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatio-temporal transformer network for video restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tae Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flow-guided video inpainting with scene templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Sundaramoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adacof: Adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Copy-and-paste networks for deep video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyeun</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Short-term and long-term context aggregation network for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramamohanarao</forename><surname>Kotagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Decoupled spatial-temporal transformer for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanming</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06637</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fuseformer: Fusing fine-grained information in transformers for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanming</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Video swin transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Consistent video depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video inpainting of complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alasdair</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Fradet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam journal on imaging sciences</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Onion-peel networks for deep video completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cascaded deep video deblurring using temporal sharpness prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra Florian Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tdan: Temporally-deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Video inpainting by jointly learning temporal structure and spatial details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video-tovideo synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Zooming slow-mo: Fast and accurate one-stage space-time video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">P</forename><surname>Allebach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Temporal modulation network for controllable space-time video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep flow-guided video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
		<title level="m">Video enhancement with task-oriented flow. IJCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Focal attention for long-range interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Progressive temporal feature alignment network for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
