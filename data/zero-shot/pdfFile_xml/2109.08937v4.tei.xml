<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Laboratory of Natural Resources Monitoring in Tropical and Subtropical Area of South China</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Lancaster Environment Centre</orgName>
								<orgName type="department" key="dep2">Centre for Ecology &amp; Hydrology</orgName>
								<orgName type="institution">Lancaster University</orgName>
								<address>
									<addrLine>Library Avenue</addrLine>
									<postCode>LA1 4YQ, LA1 4AP</postCode>
									<settlement>Lancaster, Lancaster</settlement>
									<region>UK. 4)</region>
									<country>UK, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghui</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Duan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Geo-Information Science and Earth Observation</orgName>
								<orgName type="institution">University of Twente</orgName>
								<address>
									<settlement>Enschede</settlement>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Remote Sensing and Information Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<addrLine>129 Luoyu Road</addrLine>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<region>Hubei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Lancaster Environment Centre</orgName>
								<orgName type="department" key="dep2">Centre for Ecology &amp; Hydrology</orgName>
								<orgName type="institution">Lancaster University</orgName>
								<address>
									<addrLine>Library Avenue</addrLine>
									<postCode>LA1 4YQ, LA1 4AP</postCode>
									<settlement>Lancaster, Lancaster</settlement>
									<region>UK. 4)</region>
									<country>UK, UK</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">) Geography and Environmental Science</orgName>
								<orgName type="institution">University of Southampton</orgName>
								<address>
									<addrLine>Highfield</addrLine>
									<postCode>SO17 1BJ</postCode>
									<settlement>Southampton</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>11A Datun Road</addrLine>
									<postCode>100101</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<postCode>510000</postCode>
									<settlement>Guangzhou, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Laboratory of Natural Resources Monitoring in Tropical and Subtropical Area of South China</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 *Corresponding author. 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic Segmentation</term>
					<term>Remote Sensing</term>
					<term>Vision Transformer</term>
					<term>Hybrid Structure</term>
					<term>Global-local Context</term>
					<term>Urban Scene 3</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation of remotely sensed urban scene images is required in a wide range of practical applications, such as land cover mapping, urban change detection, environmental protection, and economic assessment. Driven by rapid developments in deep learning technologies, the convolutional neural network (CNN) has dominated semantic segmentation for many years. CNN adopts hierarchical feature representation, demonstrating strong capabilities for local information extraction. However, the local property of the convolution layer limits the network from capturing the global context. Recently, as a hot topic in the domain of computer vision, Transformer has demonstrated its great potential in global information modelling, boosting many vision-related tasks such as image classification, object detection, and particularly semantic segmentation. In this paper, we propose a Transformer-based decoder and construct an UNet-like Transformer (UNetFormer) for real-time urban scene segmentation. For efficient segmentation, the UNetFormer selects the lightweight ResNet18 as the encoder and develops an efficient global-local attention mechanism to model both global and local information in the decoder. Extensive experiments reveal that our method not only runs faster but also produces higher accuracy compared with state-of-the-art lightweight models. Specifically, the proposed UNetFormer achieved 67.8% and 52.4% mIoU on the UAVid and LoveDA datasets, respectively, while the inference speed can achieve up to 322.4 FPS with a 512?512 input on a single NVIDIA GTX 3090 GPU. In further exploration, the proposed Transformer-based decoder combined with a Swin Transformer encoder also achieves the state-of-the-art result (91.3% F1 and 84.1% mIoU) on the Vaihingen dataset. The source code will be freely available at https://github.com/WangLibo1995/GeoSeg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Driven by advances in sensor technology, fine-resolution remotely sensed urban scene images have been captured increasingly across the globe, with abundant spatial details and rich potential semantic contents. Urban scene images have been subjected extensively to semantic segmentation, the task of pixel-level segmentation and classification, leading to various urban-related applications, including land cover mapping <ref type="bibr" target="#b32">(Li et al., 2022b;</ref><ref type="bibr" target="#b42">Maggiori et al., 2016;</ref><ref type="bibr" target="#b43">Marcos et al., 2018)</ref>, change detection <ref type="bibr" target="#b73">(Xing et al., 2018;</ref><ref type="bibr" target="#b77">Yin et al., 2018)</ref>, environmental protection <ref type="bibr" target="#b56">(Samie et al., 2020)</ref>, road and building extraction <ref type="bibr" target="#b15">(Griffiths and Boehm, 2019;</ref><ref type="bibr" target="#b57">Shamsolmoali et al., 2020;</ref><ref type="bibr" target="#b66">Vakalopoulou et al., 2015)</ref> and many other practical applications <ref type="bibr" target="#b51">(Picoli et al., 2018;</ref><ref type="bibr" target="#b58">Shen et al., 2019)</ref>. Recently, a growing wave of deep learning technology <ref type="bibr" target="#b27">(LeCun et al., 2015)</ref>, in particular the convolutional neural network (CNN), has dominated the task of semantic segmentation <ref type="bibr" target="#b6">(Chen et al., 2014;</ref><ref type="bibr" target="#b8">Chen et al., 2018b;</ref><ref type="bibr" target="#b38">Long et al., 2015;</ref><ref type="bibr" target="#b55">Ronneberger et al., 2015;</ref><ref type="bibr" target="#b86">Zhao et al., 2017a)</ref>.</p><p>Compared with traditional machine learning methods for segmentation, such as the support vector machine (SVM) <ref type="bibr" target="#b16">(Guo et al., 2018)</ref>, random forest <ref type="bibr" target="#b49">(Pal, 2005)</ref> and conditional random field (CRF) <ref type="bibr" target="#b26">(Kr?henb?hl and Koltun, 2011)</ref>, CNN-based methods are capable of capturing more fine-grained local context information, which underpins its huge capabilities in feature representation and pattern recognition <ref type="bibr" target="#b82">(Zhang et al., 2020a;</ref><ref type="bibr" target="#b83">Zhang et al., 2020b)</ref>.</p><p>Despite the above advantages, the convolution operation with a fixed receptive view is designed to extract local patterns and lacks the ability to model global contextual information or long-range dependencies in its nature. As for semantic segmentation, per-pixel classification is often ambiguous if only local information is modelled, while the semantic content of each pixel becomes more accurate with the help of global contextual information <ref type="bibr" target="#b75">(Yang et al., 2021a)</ref>  <ref type="bibr" target="#b34">(Li et al., 2021c)</ref>. The global and local contextual information is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Although the selfattention mechanism alleviates the above issue <ref type="bibr" target="#b67">(Vaswani et al., 2017)</ref>  , they normally require significant computational time and memory to capture the global context, thus, reducing their efficiency and restricting their potential for real-time urban applications. In this paper, we aim to achieve precise urban scene segmentation while ensuring the efficiency of the network simultaneously. Inspired by the recent breakthrough of Transformers in computer vision, we propose a UNet-like Transformer (UNetFormer) to address such a challenge. The UNetFormer innovatively adopts a hybrid architecture consisting of a CNN-based encoder and a specifically designed Transformer-based decoder. Specifically, we adopt the ResNet18 as the encoder and design a global-local Transformer block (GLTB) to construct the decoder. Unlike the conventional self-attention block in the standard Transformer, the proposed GLTB develops an efficient global-local attention mechanism with an attentional global branch and a convolutional local branch to capture both global and local contexts for visual perception, as illustrated in <ref type="figure" target="#fig_1">Fig.   2</ref>. In the global branch, the window-based multi-head self-attention and cross-shaped window context interaction module are introduced to capture global contexts with low complexity . In the global branch, convolutional layers are applied to extract the local context.</p><p>Finally, to effectively fuse the spatial details and context information as well as further refine the feature maps, a feature refinement head (FRH) is proposed and attached at the end of the network.</p><p>The trade-off between accuracy and efficiency as well as effective feature refinement allows the proposed method to exceed the state-of-the-art lightweight networks for efficient segmentation of remotely sensed urban scene images, demonstrated by four public datasets: the UAVid <ref type="bibr" target="#b39">(Lyu et al., 2020)</ref>, ISPRS Vaihingen and Potsdam datasets, as well as the LoveDA <ref type="bibr" target="#b68">(Wang et al., 2021a)</ref>.</p><p>The remainder of this paper is organized as follows. In Section 2, we review the related work on CNN-based and Transformer-based urban scene segmentation and global context modelling.</p><p>In Section 3, we present the structure of our UNetFormer and introduce the proposed GLTB and FRH. In Section 4, we conduct an ablation study to demonstrate the effectiveness of GLTB and FRH as well as the novel hybrid structure and compare the results with a set of state-of-the-art models applied to the four datasets. In Section 5, we provide a comprehensive discussion. Section 6 is a summary and conclusion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CNN-based semantic segmentation methods</head><p>The fully convolutional network (FCN) <ref type="bibr" target="#b38">(Long et al., 2015)</ref> is the first effective CNN structure to address semantic segmentation problems in an end-to-end manner. Since then, CNN-based methods have dominated the semantic segmentation task in the remote sensing field <ref type="bibr" target="#b23">(Kemker et al., 2018;</ref><ref type="bibr" target="#b25">Kotaridis and Lazaridou, 2021;</ref><ref type="bibr" target="#b41">Ma et al., 2019;</ref><ref type="bibr" target="#b65">Tong et al., 2020;</ref><ref type="bibr" target="#b87">Zhao and Du, 2016;</ref><ref type="bibr" target="#b94">Zhu et al., 2017)</ref>. However, the over-simplified decoder of FCN leads to a coarse-resolution segmentation, limiting the fidelity and accuracy.</p><p>To address this problem, an encoder-decoder network, i.e., the UNet, was proposed for semantic segmentation, with two symmetric paths named the contracting path and the expanding path <ref type="bibr" target="#b55">(Ronneberger et al., 2015)</ref>. The contracting path extracts hierarchical features by gradually downsampling the spatial resolution of the feature maps, while the expanding path learns more contextual information by progressively restoring the spatial resolution. Subsequently, the encoder-decoder framework has become the standard structure of remote sensing image segmentation networks <ref type="bibr" target="#b1">(Badrinarayanan et al., 2017;</ref><ref type="bibr" target="#b7">Chen et al., 2018a)</ref>  <ref type="bibr" target="#b63">(Sun et al., 2019)</ref>. Based on encoder-decoder structure, <ref type="bibr" target="#b10">(Diakogiannis et al., 2020;</ref><ref type="bibr" target="#b81">Yue et al., 2019;</ref><ref type="bibr" target="#b92">Zhou et al., 2018)</ref> designed different skip connections to capture more abundant context, while <ref type="bibr" target="#b36">(Liu et al., 2018;</ref><ref type="bibr" target="#b88">Zhao et al., 2017b)</ref>  <ref type="bibr" target="#b58">(Shen et al., 2019)</ref> developed various decoders to retain semantic information.</p><p>The encoder-decoder CNN-based methods, although have achieved encouraging performance, encounter bottlenecks in urban scene interpretation <ref type="bibr" target="#b59">(Sherrah, 2016)</ref>  <ref type="bibr" target="#b44">(Marmanis et al., 2018;</ref><ref type="bibr" target="#b47">Nogueira et al., 2019)</ref>. To be specific, CNN-based segmentation networks with limited receptive fields can only extract local semantic features and lack the capability to model the global information from the whole image. However, within fine-resolution remotely sensed urban scene images, complicated patterns and human-made objects occur frequently <ref type="bibr" target="#b22">(Kampffmeyer et al., 2016;</ref><ref type="bibr" target="#b43">Marcos et al., 2018)</ref>  <ref type="bibr" target="#b0">(Audebert et al., 2018)</ref>. It is difficult to identify these complex objects if only relying on the local infromation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Global contextual information modelling</head><p>To liberate the network from the local pattern focus of CNNs, many attempts have been conducted to modelling global contextual information, while the most popular way is incorporating attention mechanisms into networks. For example, Wang et al. modified the dotproduct self-attention mechanism and applied it to computer vision domains .</p><p>Fu et al. appended two types of attention modules on top of a dilated FCN to adaptively integrate local features with their global dependencies <ref type="bibr" target="#b13">(Fu et al., 2019)</ref>. Huang et al. proposed a criss-cross attention block to aggregate informative global features <ref type="bibr" target="#b21">(Huang et al., 2020)</ref>. <ref type="bibr">Yuan et al.</ref> developed an object context block to explore object-based global relations <ref type="bibr" target="#b80">(Yuan et al., 2020)</ref>.</p><p>Attention mechanisms also improve the performance of remote sensing image segmentation networks. <ref type="bibr">Yang et al.</ref> proposed an attention-fused network to fuse high-level and low-level semantic features and obtain state-of-the-art results in the semantic segmentation of fineresolution remote sensing images <ref type="bibr" target="#b76">(Yang et al., 2021b)</ref>. Li et al. integrated lightweight spatial and channel attention modules to refine semantic features adaptively for high-resolution remotely sensed image segmentation . Ding et al. designed a local attention block with an embedding module to capture richer contextual information <ref type="bibr" target="#b11">(Ding et al., 2021)</ref>. <ref type="bibr">Li et al. developed</ref> a linear attention mechanism to reduce the computational complexity while improving performance <ref type="bibr" target="#b31">(Li et al., 2021a)</ref>. However, the above attention modules restrict the global feature representation due to over-reliance on convolutional operations. Furthermore, a single attention module cannot model the global information at multi-level semantic features in the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformer-based semantic segmentation methods</head><p>Recently, several attempts were made to apply the Transformer for global information extraction <ref type="bibr" target="#b67">(Vaswani et al., 2017)</ref>. Different from the CNN structure, the Transformer translates 2D image-based tasks into 1D sequence-based tasks. Due to the powerful sequence-to-sequence modelling ability, the Transformer demonstrates superior characterization of extracting global context than the above-mentioned attention-alone models and obtains state-of-the-art results on fundamental vision tasks, such as image classification <ref type="bibr" target="#b12">(Dosovitskiy et al., 2020)</ref>, object detection  and semantic segmentation . Driven by this, many researchers in the remote sensing field have applied the Transformer for remote sensing image scene classification <ref type="bibr" target="#b2">(Bazi et al., 2021;</ref><ref type="bibr" target="#b9">Deng et al., 2021)</ref>, hyperspectral image classification <ref type="bibr" target="#b19">(Hong et al., 2021)</ref>  <ref type="bibr" target="#b18">(He et al., 2021)</ref>, object detection <ref type="bibr" target="#b30">(Li et al., 2022a)</ref>, change detection <ref type="bibr" target="#b4">(Chen et al., 2021a)</ref>, and especially semantic segmentation <ref type="bibr" target="#b70">Wang et al., 2021b)</ref>.</p><p>Most of the existing Transformers for semantic segmentation still follow the encoder-decoder framework. According to different encoder-decoder combinations, they can be divided into two categories. The first is constructed by a Transformer-based encoder and a Transformer-based decoder, namely the pure Transformer structure. Typical models include the Segmenter <ref type="bibr" target="#b61">(Strudel et al., 2021)</ref>, SegFormer <ref type="bibr" target="#b72">(Xie et al., 2021)</ref> and SwinUNet . The second adopts a hybrid structure, which is composed of a Transformer-based encoder and a CNN-based decoder.</p><p>Transformer-based semantic segmentation methods commonly follow the second structure. For example, the TransUNet employed the hybrid vision Transformer <ref type="bibr" target="#b12">(Dosovitskiy et al., 2020)</ref> as the encoder for stronger feature extraction and obtains state-of-the-art results in medical image segmentation . The DC-Swin introduced Swin Transformer  as the encoder and designs a densely connected convolutional decoder for fine-resolution remote sensing image segmentation, surpassing the CNN-based methods by a large gap . <ref type="bibr" target="#b50">(Panboonyuen et al., 2021)</ref> also selected the Swin Transformer as the encoder and utilizes various CNN-based decoders, such as UNet <ref type="bibr" target="#b55">(Ronneberger et al., 2015)</ref>, FPN <ref type="bibr" target="#b24">(Kirillov et al., 2019)</ref> and PSP <ref type="bibr" target="#b86">(Zhao et al., 2017a)</ref>, for semantic segmentation of remotely sensed images, obtaining advanced accuracy.</p><p>Despite the above advantages, the computational complexity of the Transformer-based encoder is much higher than the CNN-based encoder due to its square-complexity self-attention mechanism <ref type="bibr" target="#b67">(Vaswani et al., 2017)</ref>, which seriously affects its potential and feasibility for urbanrelated real-time applications. Thus, to fully harness the global context extraction ability of Transformers without resulting in high computational complexity, in this paper, we present a UNet-like Transformer with a CNN-based encoder and a Transformer-based decoder for efficient semantic segmentation of remotely sensed urban scene images. Specifically, for our UNetFormer, we select the lightweight backbone, i.e. ResNet18, as the encoder and develop an efficient globallocal attention mechanism to construct Transformer blocks in the decoder. The proposed efficient global-local attention mechanism adopts a dual-branch structure, i.e. a global branch and a local branch. Such a structure allows the attention block to capture both global and local contexts, thereby surpassing the single-branch efficient attention mechanisms in Transformers that only capture global contexts <ref type="bibr" target="#b85">Zhang and Yang, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, the proposed UNetFormer is constructed using a CNN-based encoder and a Transformer-based decoder. A detailed description of each component is given in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CNN-based encoder</head><p>As the ResNet18 <ref type="bibr" target="#b17">(He et al., 2016)</ref> has demonstrated effectiveness and efficiency simultaneously in a wide range of real-time semantic segmentation tasks, we select the pre-trained ResNet18 as the encoder here to extract multi-scale semantic features with significantly low computational cost. ResNet18 consists of four-stage Resblocks, with each stage down-sampling the feature map with a scale factor of 2. In the proposed UNetFormer, the feature maps generated by each stage are fused with the corresponding feature maps of the decoder by a 1?1 convolution with the channel dimension in 64, i.e., the skip connection. Specifically, the semantic features produced by the Resblocks are aggregated with the features generated by the GLTB of the decoder using a weighted sum operation. The weighted sum operation weights the two features selectively based on their contributions to segmentation accuracy, thereby learning more generalized fusion features <ref type="bibr" target="#b64">(Tan et al., 2020)</ref>. The formulation of the weighted sum operation can be denoted as:</p><formula xml:id="formula_0">= ? ? + (1 ? ?) ?<label>(1)</label></formula><p>where represents the fused feature, denotes the feature produced by the Resblocks, and indicates the feature generated by the global-local Transformer block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer-based decoder</head><p>Complicated human-made objects occur frequently in fine-resolution remotely sensed urban images, which makes it difficult to achieve precise real-time segmentation without global semantic information. To capture the global context, mainstream solutions focus on attaching a single attention block at the end of the network  or introducing Transformers as the encoder . The former cannot capture multi-scale global features, whereas the latter significantly increases the complexity of the network and loses spatial details.</p><p>In contrast, in the proposed UNetFormer, we utilize three global-local Transformer blocks and a feature refinement head to build a lightweight Transformer-based decoder, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>With such a hierarchical and lightweight design, the decoder is capable of capturing both global and local contexts at multiple scales while maintaining high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Global-local Transformer block (GLTB)</head><p>The global-local Transformer block consists of the global-local attention, multilayer perceptron, two batch normalization layers and two additional operations, as shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Efficient Global-local attention:</head><p>Although the global context is crucial for semantic segmentation of complex urban scenes, local information is still essential to preserve rich spatial details. In this regard, the proposed efficient global-local attention constructs two parallel branches to extract the global and local contexts, respectively, as shown in <ref type="figure" target="#fig_3">Fig. 4 (a)</ref>.</p><p>As a relatively shallow structure, the local branch employs two parallel convolutional layers with kernel sizes of 3 and 1 to extract the local context. Two batch normalization operations are then attached before the final sum operation.</p><p>The global branch deploys the window-based multi-head self-attention to capture global context. As illustrated in <ref type="figure" target="#fig_3">Fig 4. (b)</ref>, we first use a standard 1?1 convolution to expand the channel dimension of the input 2D feature map ? ? ? ? ? to three times. Then, we apply the window partition operation to split the 1D sequence ? ? ?3? ? ? ????( ? )? ? into the query (Q), key (K) and value (V) vectors. The channel dimension C is set to 64. The window size w and the number of heads h are both set to 8. The details of the window-based multi-head self-attention can refer to Swin Transformer . Performing self-attention in a non-overlapping local window, although being efficient, can destroy the spatial consistency of urban scenes due to the lack of interactions across windows.</p><p>The Swin Transformer introduces an extra shifted Transformer block to mine the relationship between local windows. Although the ability to capture cross-window relations increases, the computation significantly surges accordingly. In this paper, we propose a cross-shaped window context interaction module to capture the cross-window relations with high computational efficiency. As illustrated in <ref type="figure" target="#fig_3">Fig. 4 (c)</ref>, the cross-shaped window context interaction module fuses the two feature maps produced by a horizontal average pooling layer and a vertical average pooling layer, thereby capturing the global context. Specifically, the horizontal average pool layer establishes the horizontal relationship between Windows, such as 1 = ( 2 ). For any point P 1 ( , ) in Window 1, its dependency with P 2 ( + , ) in Window 2 can be modelled as:</p><formula xml:id="formula_1">P 1 ( , ) = ? P 1 ( + , ) ? ?1 =0 + ? P 2 ( + ? , ) =0 (2) P 1 ( + , ) = ?P 1 ( , ) ? (3) P 2 ( + ? , ) = ?P 2 ( + , ) ? (4) P 1 ( , ) = ? ?P 1 ( , ) ? ? ?1 =0 + ? ?P 2 ( + , ) ? =0 (5)</formula><p>Where w is the window size. D denotes the self-attention computation, which can model dependencies of pixel pairs in a local window. Thus, for any other point P 1 ( + , ) in the red path  <ref type="formula" target="#formula_2">(  4 )</ref>) . Generalized to an M?M input (M denotes the number of windows), by connecting more intermedia windows like Window 2 and Window 3, the long-range dependency between any two windows can be modelled. Thus, the cross-shaped window context interaction module can model the window-wise long-range dependencies, thereby capturing the global context.</p><p>Besides, the global context in the global branch is further aggregated with the local context in the local branch to produce the global-local context. Finally, we employ a depth-wise convolution, a batch normalization operation and a standard 1?1 convolution to characterize the fine-grained global-local context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feature refinement head (FRH)</head><p>The shallow feature produced by the first Resblock preserves rich spatial details of urban scenes, but lacks semantic content, while the deep global-local feature provides precise semantic information, but with a coarse spatial resolution. Hence, a direct sum operation on these two features, although fast, can reduce segmentation accuracy <ref type="bibr" target="#b52">(Poudel et al., 2018;</ref><ref type="bibr" target="#b53">Poudel et al., 2019;</ref><ref type="bibr" target="#b79">Yu et al., 2018)</ref>. In this paper, we develop a feature refinement head to shrink the semantic gap between the two features for further accuracy improvement.</p><p>As can be seen in <ref type="figure" target="#fig_5">Fig. 5</ref>, we perform a weighted sum operation on the two features first to take full advantage of the precise semantic information and spatial details. The fused feature is then selected as the input of the FRH, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. Second, we construct two paths to strengthen the channel-wise and spatial-wise feature representation. Specifically, the channel path employs  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss function</head><p>In the training phase, we employ not only the primary feature refinement head but also build an extra auxiliary head to optimize the global-local Transformer blocks, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. This multi-head segmentation architecture has been demonstrated to be effective in previous research <ref type="bibr" target="#b78">(Yu et al., 2020;</ref><ref type="bibr" target="#b95">Zhu et al., 2019)</ref>. Based on the multi-head design, we apply a principal loss and an auxiliary loss to train the entire network. The principal loss ? is a combination of a dice loss ? and a cross-entropy loss ? , which can be formulated as: <ref type="bibr">8)</ref> where N and K denote the number of samples and the number of categories, respectively. ( ) and ? ( ) represent the one-hot encoding of the true semantic labels and the corresponding softmax output of the network, n ? [1, ? , N]. ? ( ) is the confidence of sample n belonging to the category k. We select the cross-entropy loss as the auxiliary loss ? and deploy it on the auxiliary head. The auxiliary head takes the fused feature of the three global-local Transformer blocks as the input and constructs a 3?3 convolution layer with batch normalization and ReLU, a 1?1 convolution layer and an upsampling operation to generate the output. For a better combination with the principle loss, the auxiliary is further multiplied by a factor ?. Thus, the overall loss ? can be formulated as:</p><formula xml:id="formula_2">? = ? 1 ? ? ( ) log ? ( ) =1 =1 (6) ? = 1 ? 2 ? ? ? ( ) ( ) ? ( ) + ( ) =1 =1 (7) ? = ? + ?<label>(</label></formula><formula xml:id="formula_3">? = ? + ? ? ? (9)</formula><p>where ? is set to 0.4 by default.  <ref type="bibr">4,</ref><ref type="bibr">6,</ref><ref type="bibr">8,</ref><ref type="bibr">10,</ref><ref type="bibr">12,</ref><ref type="bibr">14,</ref><ref type="bibr">16,</ref><ref type="bibr">20,</ref><ref type="bibr">22,</ref><ref type="bibr">24,</ref><ref type="bibr">27,</ref><ref type="bibr">29,</ref><ref type="bibr">31,</ref><ref type="bibr">33,</ref><ref type="bibr">35,</ref><ref type="bibr">38</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Evaluation metrics</head><p>The evaluation metrics used in our experiments included two major categories. The first one was to evaluate the accuracy of the network including the overall accuracy (OA), mean F1 score (F1) and mean intersection over union (mIoU). The second one was to evaluate the scale of the network, including the floating point operation count (Flops) to evaluate the complexity, the frames per second (FPS) to evaluate the speed, the memory footprint (MB) and the number of model parameters (M) to evaluate the memory requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Models for comparison</head><p>We selected a comprehensive set of benchmark methods for quantitative comparison including (i) CNN-based lightweight networks developed for efficient semantic segmentation:</p><p>context aggregation network (CANet) <ref type="bibr" target="#b75">(Yang et al., 2021a)</ref>, bilateral segmentation network (BiSeNet) <ref type="bibr" target="#b79">(Yu et al., 2018)</ref>, ShelfNet <ref type="bibr" target="#b96">(Zhuang et al., 2019)</ref>, SwiftNet <ref type="bibr" target="#b48">(Or?i? and ?egvi?, 2021)</ref>, Fast-SCNN <ref type="bibr" target="#b53">(Poudel et al., 2019)</ref>, DABNet , ERFNet <ref type="bibr" target="#b54">(Romera et al., 2017)</ref> and ABCNet <ref type="bibr" target="#b34">(Li et al., 2021c)</ref>. Transformer-based decoder: SwinUNet , SegFormer <ref type="bibr" target="#b72">(Xie et al., 2021)</ref> and Segmenter <ref type="bibr" target="#b61">(Strudel et al., 2021)</ref>. Baseline: The baseline was constructed by the U-Net with a ResNet18 backbone, which only models the local contextual information in the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Each component of UNetFormer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The global-local Transformer block (GLTB):</head><p>Three global-local Transformer blocks were incorporated into the baseline to build the Baseline+GLTB. Meanwhile, to illustrate the contribution of the cross-shaped window context interaction module in the GLTB, we remove it and apply a direct sum operation on the window context and local context, thereby constructing a simple variant Baseline+GLTB-SUM. As shown in TABLE 1, the deployment of GLTB provides a significant increase of mIoU by 3.4% on the UAVid validation set, where the contribution of the cross-shaped window context interaction module to increase accuracy is 1.0%.</p><p>Meanwhile, Baseline+GLTB achieves an increase of greater than 2.4% in mIoU on the Vaihingen and Potsdam test sets, where the increase provided by the cross-shaped window context interaction module is 1.2% and 1.1%, respectively. To sum up, the results not only demonstrate the effectiveness of GLTB but also indicate the necessity of applying the cross-shaped window context interaction module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The feature refinement module (FRH):</head><p>We inserted the feature refinement head into Baseline+GLTB to generate the entire UNetFormer (indicated as Baseline+GLTB+FRH). As shown in TABLE 1, with the employment of FRH, the mIoU is boosted by 1.0% at least, demonstrating the validity of the proposed feature refinement module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Efficient global-local attention</head><p>To demonstrate the advantages of the proposed efficient global-local attention, we replaced it with other advanced attention mechanisms to reconstruct the variants of UNetformer for ablation studies. Benefiting from the dual-branch structure and the captured global-local context, the deployment of our global-local attention achieves the highest mIoU (70.0%) on the UAVid validation set, as listed in TABLE 2. Besides, the proposed global-local attention also demonstrates superiority in terms of complexity, memory requirement, parameters and inference speed. Especially, our method is more accurate and faster than the efficient attention mechanisms in Transformers, i.e. the shifted window attention and the efficient multi-head self-attention.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Encoder-decoder combination</head><p>To illustrate the superiority of our hybrid structure for efficient semantic segmentation, we selected the UNet, SwinUNet and TransUNet for ablation experiments on the UAVid dataset.</p><p>Since the SwinUNet requires huge GPU memory, the input size was all set as 512?512 for training. The results from Although the TransUNet constructed by a Transformer-based encoder and a CNN-based decoder surpasses ours by 0.5% in mIoU, it is 7 times slower and has much more parameters due to its heavy and complicated Transformer-based encoder. For real-time urban application scenarios, the high execution speed and lightweight model volume are much more important than the slight accuracy reduction. Thus, in comparison with other combinations, the advantage of our hybrid structure, i.e. CNN-based encoder and Transformer-based decoder, is significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison of network efficiency</head><p>Complexity and speed are critical for evaluating a network, especially in real-time urban applications. We compared our UNetFormer with efficient segmentation networks based on the mIoU, GPU memory footprint, complexity, parameters and speed on the official UAVid test set.</p><p>The comparison results are listed in <ref type="table" target="#tab_9">Table 6</ref>. In comparison with the fastest and most shallow model Fast-SCNN, the proposed UNetFormer outperforms it by a large margin of 21.0% in mIoU.</p><p>In comparison with the state-of-the-art CNN-based models of the same volume, our UNetFormer achieves a competitive inference speed of 115.6 FPS, while surpassing other networks by more than 4.0% in mIoU. Notably, our method exceeds the advanced hybrid Transformer network CoaT by 2.0% in mIoU while being 10 times faster. Meanwhile, the proposed method outperforms the pure Transformer network Segmenter by 9.1% in mIoU while being 7 times faster. The outstanding trade-off between accuracy and speed demonstrates the efficiency of our hybrid structure and the effectiveness of the proposed GLTB and FRH. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results on the UAVid dataset</head><p>UAVid is a large-scale urban scene segmentation dataset, where the images are captured by unmanned aerial vehicles in different cities and under different lighting conditions. Thus, it is challenging to obtain high scores on this dataset. We trained several advanced efficient segmentation networks and provide a detailed comparison of results on the official UAVid test set.</p><p>As illustrated in <ref type="table" target="#tab_10">Table 7</ref>, our method yields the best mIoU (67.8%) while maintaining the advantages in the per-class IoU. Specifically, the proposed UNetFormer not only exceeds the efficient CNN-based network ABCNet by 4.0% in mIoU but also outperforms the recent hybrid Transformer-based networks BANet and BoTNet by 3.2% and 4.6%, respectively. Particularly, the "human" class is hard to handle since it is an extremely small object. Nonetheless, the IoU of this class achieved by our UNetFormer is at least 8.6% higher than for other methods. Furthermore, the segmentation results from the UAVid validation set <ref type="figure">(Fig. 6)</ref> and the visualization results from the UAVid test set <ref type="figure">(Fig. 7)</ref> also demonstrate the effectiveness of our UNetFormer.  <ref type="figure">Fig. 7</ref>. Enlarged visualization of results from the UAVid test set. The first column represents the input RGB images. The second column denotes the segmentation results of the baseline. The third column shows the segmentation maps of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Results on the Vaihingen and Potsdam dataset</head><p>The ISPRS Vaihingen and Potsdam are two widely-used datasets for segmentation tasks.</p><p>Numerically high accuracies have been achieved by the specially designed models on these two datasets. In this section, we demonstrate that our UNetFormer can not only surpass lightweight models but also obtain competitive scores in comparison with leading networks.</p><p>As illustrated in <ref type="table" target="#tab_11">Table 8</ref>, the proposed UNetFormer delivers the best F1, OA and mIoU on the Vaihingen test set, outperforming other CNN-based and Transformer-based lightweight networks by a significant margin. It is worth noting that our method yields an 88.5% F1 score on the "car" class, exceeding other networks by more than 1.7%. Moreover, the prediction results of ID 2 and 22 are shown in <ref type="figure">Fig. 8</ref>, while the enlarged visualization of results is illustrated in <ref type="figure">Fig. 9</ref> (Top), which also demonstrates the effectiveness of our method. For a comprehensive evaluation, we further conducted experiments on the Postdam dataset.</p><p>As shown in  <ref type="bibr" target="#b34">(Li et al., 2021c)</ref> but also outperform recent Transformer-based lightweight networks, such as Segmenter <ref type="bibr" target="#b61">(Strudel et al., 2021)</ref> and BANet .</p><p>We also provide segmentation results for ID 3_14 and 2_13 ( <ref type="figure">Fig. 9)</ref> and an enlarged visualization of the results <ref type="figure" target="#fig_0">(Fig. 10 Bottom)</ref> to show the preferential performance of our network.  <ref type="figure" target="#fig_0">Fig. 11</ref>. Visualization comparisons on the LoveDA validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Global-local context</head><p>The advantage of the dual-branch structure of the proposed efficient global-local attention is that it can extract sufficient global contextual information while preserving fine-grained local information. To demonstrate this, we visualise the feature maps from the efficient global-local attention in <ref type="figure" target="#fig_0">Fig. 12.</ref> As can be seen, the local context extracted by the local branch preserves the abundant local features but lacks spatial consistency, while the global context captured by the global branch has a more consistent character but lacks locality. Meanwhile, for the global branch, performing the self-attention operation within a local window also causes jagged edges in the window context. We address this issue by employing a cross-shaped window context interaction module for context aggregation. By this means, the interaction between windows is enhanced, thereby resolving the jaggedness issue. Notably, the extracted global-local context with both locality and spatial consistency is visibly superior to the single global context or local context. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model efficiency</head><p>The proposed UNetFormer adopts a hybrid structure with a CNN-based encoder and a</p><p>Transformer-based decoder to achieve real-time performance. This hybrid design demonstrates superiority compared to other encoder-decoder combinations <ref type="table" target="#tab_7">(TABLE 5)</ref>. Moreover, the efficient global-local attention module utilizes the cross-shaped window context interaction module to replace the shift window attention for capturing cross-window relationships, which further increases the efficiency <ref type="table" target="#tab_4">(TABLE 2)</ref>. The superior trade-off between accuracy and efficiency brings advantages, such as the potential for the proposed UNetFormer to process real-time UAV images for environmental perception and monitoring in urban areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Transformer-based encoder</head><p>As shown in TABLEs 4 and 5, Transformers make strong encoders but greatly reduce the speed. Although Transformer-based encoders are not suitable for real-time applications, demonstrate advantages in pursuing high precision. Thus, we construct a fully Transformer-based network (FT-UNetFormer) to further explore the potential of the proposed Transformer-based decoder. To compare with state-of-the-art models at a similar level, we replace the lightweight ResNet18 encoder with he Swin Transformer (Swin-Base) . As listed in  <ref type="table" target="#tab_3">(TABLE 12)</ref>. These results further demonstrate the effectiveness of the proposed Transformer-based decoder and its potential in a fully Transformer structure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a novel Transformer-based decoder and constructed a UNet-like</p><p>Transformer <ref type="formula" target="#formula_2">(</ref> applications. Furthermore, the proposed Transformer-based decoder also works well in a fully Transformer structure and obtains state-of-the-art performance on the Vaihingen dataset. In future research, we will continue to explore the potential and feasibility of the Transformer for geospatial vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Illustration of the global and local contextual information. The local contextual information is modelled by convolutions (yellow). The global contextual information is modelled by longrange window-wise dependencies (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6Fig. 2</head><label>2</label><figDesc>Illustration of (a) the standard Transformer block and (b) the global-local Transformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>An overview of the UNetFormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of the efficient global-local attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a global average pooling layer to generate a channel-wise attentional map ? ? 1?1? , where c denotes the channel dimension. The reduce &amp; expand operation contains two 1?1 convolutional layers, which first reduces the channel dimension c by a factor of 4 and then expands it to the original. The spatial path utilizes a depth-wise convolution to produce a spatial-wise attentional map ? ? ?? ?1 , where h and w represent the spatial resolution of the feature map. The attentional features generated by the two paths are further fused using a sum operation. Finally, a post-processing 1?1 convolutional layer and an upsampling operation are applied to produce the final segmentation map. Notably, a residual connection is introduced to prevent network degradation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The feature refinement head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>attentional networks: dual attention network (DANet)<ref type="bibr" target="#b13">(Fu et al., 2019)</ref>, fast attention network (FANet)<ref type="bibr" target="#b20">(Hu et al., 2020)</ref>, local attention network (LANet)<ref type="bibr" target="#b11">(Ding et al., 2021)</ref>, criss-cross network (CCNet)<ref type="bibr" target="#b21">(Huang et al., 2020)</ref>, multi-stage attention residual UNet (MAResU-Net)<ref type="bibr" target="#b31">(Li et al., 2021a)</ref> and multi-attention network (MANet) (Li et al., 2021b), (iii) CNN-based networks for semantic segmentation of remote sensing images: DST_5 (Sherrah, 2016), V-FuseNet (Audebert et al., 2018), CASIA2 (Liu et al., 2018), DLR_9 (Marmanis et al., 2018), RoteEqNet (Marcos et al., 2018), UFMG_4 (Nogueira et al., 2019), HUSTW5 (Sun et al., 2019), TreeUNet (Yue et al., 2019), ResUNet-a (Diakogiannis et al., 2020), S-RA-FCN (Mou et al., 2020), DDCM-Net (Liu et al., 2020), EaNet (Zheng et al., 2020a), HMANet (Niu et al., 2021) and AFNet (Yang et al., 2021b), (iv) hybrid Transformer-based networks with a Transformer-based encoder and a CNNbased decoder: TransUNet (Chen et al., 2021b), SwinUperNet (Liu et al., 2021), DC-Swin (Wang et al., 2022), STranFuse (Gao et al., 2021), SwinB-CNN+BD (Zhang et al., 2022), SwinTF-FPN (Panboonyuen et al., 2021), BANet (Wang et al., 2021b),CoaT, BoTNet<ref type="bibr" target="#b60">(Srinivas et al., 2021)</ref> and ResT<ref type="bibr" target="#b85">(Zhang and Yang, 2021)</ref>,(v) fully Transformer-based networks with a Transformer-based encoder and a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Visualization of the local context, window context, global context and global-local context in the proposed efficient global-local attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>UNetFormer) for efficient semantic segmentation of remotely sensed urban scene images. Since global and local contexts are both crucial for urban scene segmentation, we designed a global-local Transformer block (GLTB) to construct the decoder and developed a feature refinement head (FRH) to optimize the extracted global-local context. For efficient segmentation, the proposed Transformer-based decoder was combined with a lightweight CNNbased encoder. A comprehensive set of benchmark experiments and ablation studies on the ISPRS Vaihingen and Potsdam datasets and the UAVid dataset as well as the LoveDA dataset demonstrated the effectiveness and efficiency of the proposed method for real-time urban</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>As a fine-resolution Unmanned Aerial Vehicle (UAV) semantic segmentation dataset, the UAVid dataset focuses on urban street scenes with two spatial resolutions (3840?2160 and 4096?2160) and eight classes<ref type="bibr" target="#b39">(Lyu et al., 2020)</ref>. Segmentation of UAVid is challenging due to the fine spatial resolution of images, heterogeneous spatial variation, vague categories and generally complex scenes. To be specific, there are 42 sequences with a total of 420 images in the dataset, where 200 images are used for training, 70 images for validation and the officially provided 150 images for testing. In our experiments, each image was padded and cropped into eight 1024?1024 px patches. The Vaihingen dataset consists of 33 very fine spatial resolution TOP image tiles at an average size of 2494?2064 pixels. Each TOP image tile has three multispectral bands (near infrared, red, green) as well as a digital surface model (DSM) and normalized digital surface model (NDSM) with a 9 cm ground sampling distance (GSD). The dataset involves five foreground classes (impervious surface, building, low vegetation, tree, car) and one background class (clutter). In our experiments, only the TOP image tiles were used without the DSM and</figDesc><table><row><cell>4. EXPERIMENTS</cell></row><row><cell>4.1 Experimental settings</cell></row><row><cell>4.1.1 Datasets</cell></row><row><cell>UAVid:</cell></row></table><note>Vaihingen:NDSM. And we utilized ID: 2,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>for testing, and the remaining 16 images for training. The image tiles were cropped into 1024?1024 px patches. The Potsdam dataset contains 38 very fine spatial resolution TOP image tiles (GSD 5 cm) at a size of 6000?6000 pixels and involves the same category information as the Vaihingen dataset. Four multispectral bands (red, green, blue, and near infrared), as well as the DSM and NDSM, are provided in the dataset. We utilized ID: 2_13, 2_14, 3_13, 3_14, 4_13, 4_14, 4_15, For the UAVid dataset, random vertical flip, random horizontal flip and random brightness were used to the input in the size of 1024?1024 for data augmentation in the training period, while the training epoch was set as 40 and the batch size was 8. In the test procedure, the test-time augmentation (TTA) strategies like vertical flip and horizontal flip were used.</figDesc><table><row><cell>For the Vaihinge, Potsdam and LoveDA datasets, the images were randomly cropped into</cell></row><row><cell>512?512 patches. For training, the augmentation techniques like random scale ([0.5, 0.75, 1.0,</cell></row></table><note>Potsdam:5_13, 5_14, 5_15, 6_13, 6_14, 6_15, 7_13 for testing, and the remaining 23 images (except image 7_10 with error annotations) for training. Similarly, only three bands (red, green, blue) were utilized and the original image tiles were cropped into 1024?1024 px patches in the experiments.LoveDA: The LoveDA dataset contains 5987 fine-resolution optical remote sensing images (GSD 0.3 m) at a size of 1024?1024 pixels and includes 7 landcover categories, i.e. building, road, water, barren, forest, agriculture and background (Wang et al., 2021a). Specifically, 2522 images are used for training, 1669 images for validation and the officially provided 1796 images for testing. The dataset encompasses two scenes (urban and rural) which are collected from three cities (Nanjing, Changzhou and Wuhan) in China. Therefore, considerable challenges are brought due to the multi-scale objects, complex background and inconsistent class distributions.4.1.2 Implementation Details All models in the experiments were implemented with the PyTorch framework on a single NVIDIA GTX 3090 GPU. For fast convergence, we deployed the AdamW optimizer to train all models in the experiments. The base learning rate was set to 6e-4. The cosine strategy was employed to adjust the learning rate.1.25, 1.5]), random vertical flip, random horizontal flip and random rotate were adopted during the training process, while the training epoch was set as 100 and the batch size was 16. During the test phase, multi-scale and random flip augmentations were used.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 .</head><label>1</label><figDesc>To evaluate the performance of each component of the proposed UNetFormer separately, we Ablation study of each component of the UNetFormer.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>mIoU</cell></row><row><cell></cell><cell>Baseline</cell><cell>65.4</cell></row><row><cell></cell><cell>Baseline+GLTB-SUM</cell><cell>67.8</cell></row><row><cell>UAVid</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Baseline+GLTB</cell><cell>68.8</cell></row><row><cell></cell><cell>Baseline+GLTB+FRH</cell><cell>70.0</cell></row><row><cell></cell><cell>Baseline</cell><cell>77.1</cell></row><row><cell></cell><cell>Baseline+GLTB-SUM</cell><cell>79.4</cell></row><row><cell>Vaihingen</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Baseline+GLTB</cell><cell>80.6</cell></row><row><cell></cell><cell>Baseline+GLTB+FRH</cell><cell>81.6</cell></row><row><cell></cell><cell>Baseline</cell><cell>82.5</cell></row><row><cell></cell><cell>Baseline+GLTB-SUM</cell><cell>83.8</cell></row><row><cell>Potsdam</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Baseline+GLTB</cell><cell>84.9</cell></row><row><cell></cell><cell>Baseline+GLTB+FRH</cell><cell>85.5</cell></row></table><note>conducted a series of ablation experiments on the UAVid, Vaihingen and Potsdam datasets. For a fair comparison, the test time augmentation strategies and auxiliary loss were not used in all ablation studies. The results are illustrated in TABLE 1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 .</head><label>2</label><figDesc>Ablation studies of different attention mechanisms on the UAVid dataset. We report the speed with an input size of 1024?1024 on a single NVIDIA GTX 3090 GPU. The best values in the column are in bold.To evaluate the network stability, we trained the UNetFormer with different input sizes, including square inputs like 512?512, 1024?1024 and 2048?2048 as well as rectangular inputs like 512?1024 and 1024?2048. From the experimental results in TABLE 3, the UNetFormer demonstrates stability when performing different input sizes, while the deviation of the mIoU is less than 0.7%. The middle input size of 1024?1024 obtains the best mIoU on the UAVid validation set. Furthermore, the square inputs yield relatively higher scores than the rectangular inputs, and too large input size like 2048?2048 can reduce the IoU of very small object "human".</figDesc><table><row><cell>Attention mechanism</cell><cell>Complexity(G)</cell><cell cols="2">Memory(MB) Parameters(M)</cell><cell>Speed(FPS)</cell><cell>mIoU</cell></row><row><cell>Dual attention (Fu et al., 2019)</cell><cell>68.9</cell><cell>2416.4</cell><cell>12.6</cell><cell>53.8</cell><cell>67.3</cell></row><row><cell>Criss-cross attention (Huang et al., 2020)</cell><cell>67.2</cell><cell>1318.4</cell><cell>12.4</cell><cell>79.9</cell><cell>68.3</cell></row><row><cell>Linear attention (Li et al., 2021b)</cell><cell>67.8</cell><cell>1339.5</cell><cell>12.5</cell><cell>91.5</cell><cell>69.0</cell></row><row><cell>Patch attention (Ding et al., 2021)</cell><cell>66.8</cell><cell>1320.5</cell><cell>12.3</cell><cell>95.7</cell><cell>68.9</cell></row><row><cell>Efficient multi-head self-attention (Zhang and Yang, 2021)</cell><cell>67.5</cell><cell>2444.2</cell><cell>12.5</cell><cell>63.6</cell><cell>67.9</cell></row><row><cell>Shifted window attention (Liu et al., 2021)</cell><cell>72.7</cell><cell>1652.0</cell><cell>13.1</cell><cell>67.0</cell><cell>68.5</cell></row><row><cell>Efficient global-local attention (Sudre et al.)</cell><cell>46.9</cell><cell>1003.8</cell><cell>11.7</cell><cell>115.6</cell><cell>70.0</cell></row><row><cell>4.2.3 Network stability</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Ablation studies of different input sizes on the UAVid dataset. The results reveal that introducing lightweight Transformers as the encoder provides a limited improvement of accuracy (within 0.6% in mIoU) but reduces the inference speed of the UNetFormer seriously. Thus, for real-time urban scene segmentation, the application of a lightweight CNN-based encoder like ResNet18 is the currently best scheme.</figDesc><table><row><cell>Input size</cell><cell>Clutter</cell><cell>Building</cell><cell>Road</cell><cell>Tree</cell><cell cols="5">Vegetation MovingCar StaticCar Human mIoU</cell></row><row><cell>512?512</cell><cell>63.1</cell><cell>90.7</cell><cell>76.4</cell><cell>77.4</cell><cell>68.1</cell><cell>70.3</cell><cell>65.9</cell><cell>46.2</cell><cell>69.8</cell></row><row><cell>512?1024</cell><cell>61.9</cell><cell>91.0</cell><cell>74.9</cell><cell>76.9</cell><cell>69.1</cell><cell>70.4</cell><cell>65.6</cell><cell>44.3</cell><cell>69.3</cell></row><row><cell>1024?1024</cell><cell>63.6</cell><cell>91.2</cell><cell>76.4</cell><cell>77.7</cell><cell>68.2</cell><cell>71.6</cell><cell>66.1</cell><cell>44.8</cell><cell>70.0</cell></row><row><cell>1024?2048</cell><cell>63.0</cell><cell>91.2</cell><cell>76.2</cell><cell>77.5</cell><cell>68.7</cell><cell>69.8</cell><cell>65.2</cell><cell>44.6</cell><cell>69.5</cell></row><row><cell>2048?2048</cell><cell>63.4</cell><cell>91.2</cell><cell>76.0</cell><cell>77.9</cell><cell>70.1</cell><cell>70.4</cell><cell>65.7</cell><cell>42.5</cell><cell>69.7</cell></row><row><cell cols="2">4.2.4 Encoder choice</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Current Transformer-based segmentation networks commonly apply the Transformer as the</cell></row></table><note>encoder. This choice, although has been justified for accurate semantic information, reduces the execution speed of the network significantly, which is not suitable for real-time applications. To demonstrate it, we replace our ResNet18 encoder with lightweight Transformers, i.e. ViT-Tiny (Dosovitskiy et al., 2020), Swin-Tiny (Liu et al., 2021) and CoaT-Mini (Xu et al., 2021), for ablation studies (TABLE 4).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Ablation studies of different encoders on the UAVid dataset. The complexity and speed are measured by a 1024?1024 input on a single NVIDIA GTX 3090 GPU.</figDesc><table><row><cell>Method</cell><cell>Encoder</cell><cell>Complexity(G)</cell><cell>Parameters(M)</cell><cell>Speed(FPS)</cell><cell>mIoU</cell></row><row><cell></cell><cell>ViT-Tiny (Dosovitskiy et al., 2020)</cell><cell>35.31</cell><cell>8.6</cell><cell>30.2</cell><cell>69.1</cell></row><row><cell></cell><cell>Swin-Tiny (Liu et al., 2021)</cell><cell>104.4</cell><cell>28.0</cell><cell>28.8</cell><cell>70.6</cell></row><row><cell>UNetFormer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CoaT-Mini (Xu et al., 2021)</cell><cell>159.7</cell><cell>10.6</cell><cell>10.6</cell><cell>70.5</cell></row><row><cell></cell><cell>ResNet18</cell><cell>46.9</cell><cell>11.7</cell><cell>115.6</cell><cell>70.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>reveal that the proposed UNetFormer exceeds the compared networks significantly in terms of complexity and speed while providing a competitive accuracy on the UAVid validation set. Specifically, in comparison with the UNet constructed by the pure CNN structure, the UNetFormer achieves an increase of 4.3% in mIoU. Compared to the pure Transformer network SwinUNet, the UNetFormer saves 80% of the computational complexity.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc>Ablation studies of different encoder-decoder combinations on the UAVid dataset.</figDesc><table><row><cell cols="8">The complexity and speed are measured by a 512?512 input on a single NVIDIA GTX 3090</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>GPU.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell>Encoder</cell><cell>Decoder</cell><cell>Complexity(G)</cell><cell>Memory(MB)</cell><cell>Parameters(M)</cell><cell>Speed(FPS)</cell><cell>mIoU</cell></row><row><cell>UNet (Ronneberger et al., 2015)</cell><cell>-</cell><cell>CNN</cell><cell>CNN</cell><cell>184.6</cell><cell>1622.0</cell><cell>31.0</cell><cell>50.9</cell><cell>65.5</cell></row><row><cell>SwinUNet (Cao et al., 2021)</cell><cell>Swin-Tiny</cell><cell>Transformer</cell><cell>Transformer</cell><cell>237.4</cell><cell>2001.5</cell><cell>41.4</cell><cell>46.9</cell><cell>68.3</cell></row><row><cell>TransUNet (Chen et al., 2021b)</cell><cell>ViT-R50</cell><cell>Transformer</cell><cell>CNN</cell><cell>233.7</cell><cell>1245.7</cell><cell>90.7</cell><cell>43.2</cell><cell>70.3</cell></row><row><cell>UNetFormer</cell><cell>ResNet18</cell><cell>CNN</cell><cell>Transformer</cell><cell>11.7</cell><cell>250.9</cell><cell>11.7</cell><cell>322.4</cell><cell>69.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 .</head><label>6</label><figDesc>Quantitative comparison results on the UAVid test set with state-of-the-art lightweight networks. The complexity and speed are measured by a 1024?1024 input on a single NVIDIA GTX 3090 GPU.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Memory(MB) Parameters(M)</cell><cell>Complexity(G)</cell><cell>Speed</cell><cell>mIoU</cell></row><row><cell>Fast-SCNN (Poudel et al., 2019)</cell><cell>-</cell><cell>619.4</cell><cell>1.1</cell><cell>3.4</cell><cell>222.7</cell><cell>45.9</cell></row><row><cell>Segmenter (Strudel et al., 2021)</cell><cell>ViT-Tiny</cell><cell>828.6</cell><cell>6.7</cell><cell>26.8</cell><cell>14.7</cell><cell>58.7</cell></row><row><cell>BiSeNet (Yu et al., 2018)</cell><cell>ResNet18</cell><cell>970.6</cell><cell>12.9</cell><cell>51.8</cell><cell>121.9</cell><cell>61.5</cell></row><row><cell>DANet (Fu et al., 2019)</cell><cell>ResNet18</cell><cell>611.1</cell><cell>12.6</cell><cell>39.6</cell><cell>189.4</cell><cell>60.6</cell></row><row><cell>FANet (Hu et al., 2020)</cell><cell>ResNet18</cell><cell>971.9</cell><cell>13.6</cell><cell>86.8</cell><cell>94.9</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 .</head><label>7</label><figDesc>Quantitative comparison of results on the UAVid test set with state-of-the-art lightweight models. The best values in the column are in bold. Segmentation results from the UAVid validation set. The first column represents the input 31 RGB images. The second column denotes the ground reference. The third column shows the segmentation maps produced by our method.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="9">Clutter Building Road Tree Vegetation MovingCar StaticCar Human mIoU</cell></row><row><cell>MSD (Lyu et al., 2020)</cell><cell>-</cell><cell>57.0</cell><cell>79.8</cell><cell>74.0</cell><cell>74.5</cell><cell>55.9</cell><cell>62.9</cell><cell>32.1</cell><cell>19.7</cell><cell>57.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8 .</head><label>8</label><figDesc>Quantitative comparison results on the Vaihingen test set with state-of-the-art lightweight networks. The best values in the column are in bold.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="6">Imp.surf. Building Lowveg. Tree Car MeanF1 OA</cell><cell>mIoU</cell></row><row><cell>DABNet (Li et al., 2019)</cell><cell>-</cell><cell>87.8</cell><cell>88.8</cell><cell>74.3</cell><cell>84.9 60.2</cell><cell>79.2</cell><cell>84.3</cell><cell>70.2</cell></row><row><cell>ERFNet (Romera et al., 2017)</cell><cell>-</cell><cell>88.5</cell><cell>90.2</cell><cell>76.4</cell><cell>85.8 53.6</cell><cell>78.9</cell><cell>85.8</cell><cell>69.1</cell></row><row><cell>BiSeNet (Yu et al., 2018)</cell><cell>ResNet18</cell><cell>89.1</cell><cell>91.3</cell><cell>80.9</cell><cell>86.9 73.1</cell><cell>84.3</cell><cell>87.1</cell><cell>75.8</cell></row><row><cell>PSPNet (Zhao et al., 2017a)</cell><cell>ResNet18</cell><cell>89.0</cell><cell>93.2</cell><cell>81.5</cell><cell>87.7 43.9</cell><cell>79.0</cell><cell>87.7</cell><cell>68.6</cell></row><row><cell>DANet (Fu et al., 2019)</cell><cell>ResNet18</cell><cell>90.0</cell><cell>93.9</cell><cell>82.2</cell><cell>87.3 44.5</cell><cell>79.6</cell><cell>88.2</cell><cell>69.4</cell></row><row><cell>FANet (Hu et al., 2020)</cell><cell>ResNet18</cell><cell>90.7</cell><cell>93.8</cell><cell>82.6</cell><cell>88.6 71.6</cell><cell>85.4</cell><cell>88.9</cell><cell>75.6</cell></row><row><cell>EaNet (Zheng et al., 2020a)</cell><cell>ResNet18</cell><cell>91.7</cell><cell>94.5</cell><cell>83.1</cell><cell>89.2 80.0</cell><cell>87.7</cell><cell>89.7</cell><cell>78.7</cell></row><row><cell>ShelfNet (Zhuang et al., 2019)</cell><cell>ResNet18</cell><cell>91.8</cell><cell>94.6</cell><cell>83.8</cell><cell>89.3 77.9</cell><cell>87.5</cell><cell>89.8</cell><cell>78.3</cell></row><row><cell>MAResU-Net (Li et al., 2021a)</cell><cell>ResNet18</cell><cell>92.0</cell><cell>95.0</cell><cell>83.7</cell><cell>89.3 78.3</cell><cell>87.7</cell><cell>90.1</cell><cell>78.6</cell></row><row><cell>SwiftNet (Or?i? and ?egvi?, 2021)</cell><cell>ResNet18</cell><cell>92.2</cell><cell>94.8</cell><cell>84.1</cell><cell>89.3 81.2</cell><cell>88.3</cell><cell>90.2</cell><cell>79.6</cell></row><row><cell>ABCNet (Li et al., 2021c)</cell><cell>ResNet18</cell><cell>92.7</cell><cell>95.2</cell><cell>84.5</cell><cell>89.7 85.3</cell><cell>89.5</cell><cell>90.7</cell><cell>81.3</cell></row><row><cell>BoTNet (Srinivas et al., 2021)</cell><cell>ResNet18</cell><cell>89.9</cell><cell>92.1</cell><cell>81.8</cell><cell>88.7 71.3</cell><cell>84.8</cell><cell>88.0</cell><cell>74.3</cell></row><row><cell>BANet (Wang et al., 2021b)</cell><cell>ResT-Lite</cell><cell>92.2</cell><cell>95.2</cell><cell>83.8</cell><cell>89.9 86.8</cell><cell>89.6</cell><cell>90.5</cell><cell>81.4</cell></row><row><cell>Segmenter (Strudel et al., 2021)</cell><cell>ViT-Tiny</cell><cell>89.8</cell><cell>93.0</cell><cell>81.2</cell><cell>88.9 67.6</cell><cell>84.1</cell><cell>88.1</cell><cell>73.6</cell></row><row><cell>UNetFormer</cell><cell>ResNet18</cell><cell>92.7</cell><cell>95.3</cell><cell>84.9</cell><cell>90.6 88.5</cell><cell>90.4</cell><cell>91.0</cell><cell>82.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 ,</head><label>10</label><figDesc>our UNetFormer achieves a 92.8% mean F1 score and an 86.8% mIoU on the Potsdam test set. The results of the UNetFormer not only exceed the excellent convolutional lightweight network ABCNet</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 9 4 Results on the LoveDA datasetTABLE 10 .TABLE 10 .</head><label>91010</label><figDesc>Quantitative comparison results on the Potsdam test set with state-of-the-art lightweight networks. The best values in the column are in bold. Visualization results of ID 2 and 22 from the Vaihingen test set. The first column denotes the input RGB images. The second column represents the ground truth. The third column shows the segmentation results of the proposed UNetFormer. Visualization results of ID 3_14 and 2_13 from the Potsdam test set. The first column denotes the input RGB images. The second column represents the ground truth. The third column shows the segmentation results of the proposed UNetFormer. We undertook experiments on the LoveDA dataset to further evaluate the performance of the UNetFormer. Benefiting from the captured global-local context, the UNetFormer can handle both urban and rural scenes well in the LoveDA dataset. The comparison results are listed in Remarkably, the UNetFormer obtains the highest mIoU (52.4%) with the least complexity and the fastest speed. Visualized comparisons are exhibited in Fig. 11. Quantitative comparison results on the LoveDA test set with other networks. The complexity and speed are measured by a 1024?1024 input on a single NVIDIA GTX 3090 GPU. The best values in the column are in bold.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="6">Imp.surf. Building Lowveg. Tree Car MeanF1 OA</cell><cell>mIoU</cell></row><row><cell>ERFNet (Romera et al., 2017)</cell><cell>-</cell><cell>88.7</cell><cell>93.0</cell><cell>81.1</cell><cell>75.8 90.5</cell><cell>85.8</cell><cell>84.5</cell><cell>76.2</cell></row><row><cell>DABNet (Li et al., 2019)</cell><cell>-</cell><cell>89.9</cell><cell>93.2</cell><cell>83.6</cell><cell>82.3 92.6</cell><cell>88.3</cell><cell>86.7</cell><cell>79.6</cell></row><row><cell>BiSeNet (Yu et al., 2018)</cell><cell>ResNet18</cell><cell>90.2</cell><cell>94.6</cell><cell>85.5</cell><cell>86.2 92.7</cell><cell>89.8</cell><cell>88.2</cell><cell>81.7</cell></row><row><cell>EaNet (Zheng et al., 2020a)</cell><cell>ResNet18</cell><cell>92.0</cell><cell>95.7</cell><cell>84.3</cell><cell>85.7 95.1</cell><cell>90.6</cell><cell>88.7</cell><cell>83.4</cell></row><row><cell>MAResU-Net (Li et al., 2021a)</cell><cell>ResNet18</cell><cell>91.4</cell><cell>95.6</cell><cell>85.8</cell><cell>86.6 93.3</cell><cell>90.5</cell><cell>89.0</cell><cell>83.9</cell></row><row><cell>DANet (Fu et al., 2019)</cell><cell>ResNet18</cell><cell>91.0</cell><cell>95.6</cell><cell>86.1</cell><cell>87.6 84.3</cell><cell>88.9</cell><cell>89.1</cell><cell>80.3</cell></row><row><cell>SwiftNet (Or?i? and ?egvi?, 2021)</cell><cell>ResNet18</cell><cell>91.8</cell><cell>95.9</cell><cell>85.7</cell><cell>86.8 94.5</cell><cell>91.0</cell><cell>89.3</cell><cell>83.8</cell></row><row><cell>FANet (Hu et al., 2020)</cell><cell>ResNet18</cell><cell>92.0</cell><cell>96.1</cell><cell>86.0</cell><cell>87.8 94.5</cell><cell>91.3</cell><cell>89.8</cell><cell>84.2</cell></row><row><cell>ShelfNet (Zhuang et al., 2019)</cell><cell>ResNet18</cell><cell>92.5</cell><cell>95.8</cell><cell>86.6</cell><cell>87.1 94.6</cell><cell>91.3</cell><cell>89.9</cell><cell>84.4</cell></row><row><cell>ABCNet (Li et al., 2021c)</cell><cell>ResNet18</cell><cell>93.5</cell><cell>96.9</cell><cell>87.9</cell><cell>89.1 95.8</cell><cell>92.7</cell><cell>91.3</cell><cell>86.5</cell></row><row><cell>Segmenter (Strudel et al., 2021)</cell><cell>ViT-Tiny</cell><cell>91.5</cell><cell>95.3</cell><cell>85.4</cell><cell>85.0 88.5</cell><cell>89.2</cell><cell>88.7</cell><cell>80.7</cell></row><row><cell>BANet (Wang et al., 2021b)</cell><cell>ResT-Lite</cell><cell>93.3</cell><cell>96.7</cell><cell>87.4</cell><cell>89.1 96.0</cell><cell>92.5</cell><cell>91.0</cell><cell>86.3</cell></row><row><cell>SwinUperNet (Liu et al., 2021)</cell><cell>Swin-Tiny</cell><cell>93.2</cell><cell>96.4</cell><cell>87.6</cell><cell>88.6 95.4</cell><cell>92.2</cell><cell>90.9</cell><cell>85.8</cell></row><row><cell>UNetFormer</cell><cell>ResNet18</cell><cell>93.6</cell><cell>97.2</cell><cell>87.7</cell><cell>88.9 96.5</cell><cell>92.8</cell><cell>91.3</cell><cell>86.8</cell></row></table><note>Fig. 10. Enlarged visualization of results from the Vaihingen (top) and Potsdam (bottom) test set.4.3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE 11 ,</head><label>11</label><figDesc>the FT-UNetFormer yields the state-of-the-art results (91.3% F1 score and 84.1% mIoU) on the Vaihingen test set and outperforms other networks by at least 0.3% in F1 score. For the Potsdam dataset, our method also achieves competitive results</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 11 .</head><label>11</label><figDesc>Quantitative comparison results on the Vaihingen test set with the state-of-the-art networks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE 12</head><label>12</label><figDesc>Quantitative comparison results on the Potsdam test set with state-of-the-art networks.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Imp.surf.</cell><cell>Building</cell><cell>Low. veg.</cell><cell>Tree</cell><cell>Car</cell><cell>MeanF1</cell><cell>OA</cell><cell>mIoU</cell></row><row><cell>DST_5 (Sherrah, 2016)</cell><cell>FCN</cell><cell>92.5</cell><cell>96.4</cell><cell>86.7</cell><cell>88.0</cell><cell>94.7</cell><cell>91.7</cell><cell>90.3</cell><cell>-</cell></row><row><cell>V-FuseNet (Audebert et al., 2018)</cell><cell>FuseNet</cell><cell>92.7</cell><cell>96.3</cell><cell>87.3</cell><cell>88.5</cell><cell>95.4</cell><cell>92.0</cell><cell>90.6</cell><cell>-</cell></row><row><cell>SWJ_2</cell><cell>ResNet101</cell><cell>94.4</cell><cell>97.4</cell><cell>87.8</cell><cell>87.6</cell><cell>94.7</cell><cell>92.4</cell><cell>91.7</cell><cell>-</cell></row><row><cell>AMA_1</cell><cell>-</cell><cell>93.4</cell><cell>96.8</cell><cell>87.7</cell><cell>88.8</cell><cell>96.0</cell><cell>92.5</cell><cell>91.2</cell><cell>-</cell></row><row><cell>UFMG_4 (Nogueira et al., 2019)</cell><cell>-</cell><cell>90.8</cell><cell>95.6</cell><cell>84.4</cell><cell>84.3</cell><cell>92.4</cell><cell>89.5</cell><cell>87.9</cell><cell>-</cell></row><row><cell>S-RA-FCN (Mou et al., 2020)</cell><cell>VGG16</cell><cell>91.3</cell><cell>94.7</cell><cell>86.8</cell><cell>83.5</cell><cell>94.5</cell><cell>90.2</cell><cell>88.6</cell><cell>82.4</cell></row><row><cell>HUSTW4 (Sun et al., 2019)</cell><cell>ResegNets</cell><cell>93.6</cell><cell>97.6</cell><cell>88.5</cell><cell>88.8</cell><cell>94.6</cell><cell>92.6</cell><cell>91.6</cell><cell>-</cell></row><row><cell>TreeUNet (Yue et al., 2019)</cell><cell>-</cell><cell>93.1</cell><cell>97.3</cell><cell>86.8</cell><cell>87.1</cell><cell>95.8</cell><cell>92.0</cell><cell>90.7</cell><cell>-</cell></row><row><cell>ResUNet-a (Diakogiannis et al., 2020)</cell><cell>-</cell><cell>93.5</cell><cell>97.2</cell><cell>88.2</cell><cell>89.2</cell><cell>96.4</cell><cell>92.9</cell><cell>91.5</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Competing Interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lef?vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="20" to="32" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vision transformers for remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bashmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M A</forename><surname>Rahhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Dayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Ajlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">516</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 13</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Swin-unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Remote sensing image change detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">When CNNs meet vision transformer: A joint framework for remote sensing scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Resunet-a: a deep learning framework for semantic segmentation of remotely sensed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Diakogiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Caccetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="94" to="114" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LANet: Local Attention Embedding to Improve the Semantic Segmentation of Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="426" to="435" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">STransFuse: Fusing Swin Transformer and Convolutional Neural Network for Remote Sensing Image Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="10990" to="11003" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving public data for building segmentation from Convolutional Neural Networks (CNNs) for fused airborne lidar and image data using active contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boehm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="70" to="83" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective Sequential Classifier Training for SVM-Based Multitemporal Remote Sensing Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3036" to="3048" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Spatial-spectral transformer for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">498</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 13</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SpectralFormer: Rethinking hyperspectral image classification with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation with fast attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="263" to="270" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CCNet: Criss-Cross Attention for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning. ISPRS journal of photogrammetry and remote sensing 145</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Salvaggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="60" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Remote sensing image segmentation advances: A metaanalysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lazaridou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="309" to="322" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dabnet: Depth-wise asymmetric bottleneck for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11357</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SCAttNet: Semantic segmentation network with spatial and channel attention mechanism for high-resolution remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="905" to="909" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Transformer with Transfer CNN for Remote-Sensing-Image Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">984</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 14</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multistage Attention ResU-Net for Semantic Segmentation of Fine-Resolution Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Land cover classification from remote sensing images based on multi-scale fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geo-spatial Information Science</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiattention network for semantic segmentation of fine-resolution remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ABCNet: Attentive bilateral contextual network for efficient semantic segmentation of Fine-Resolution remotely sensed imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="84" to="98" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dense dilated convolutions&apos; merging network for land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-B</forename><surname>Salberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="6309" to="6320" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Semantic labeling in very high resolution images via a self-cascaded convolutional neural network. ISPRS journal of photogrammetry and remote sensing 145</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="78" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">UAVid: A semantic segmentation dataset for UAV imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vosselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="108" to="119" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Factseg: Foreground activation-driven small object semantic segmentation in large-scale remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing applications: A meta-analysis and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for large-scale remote-sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="645" to="657" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Land cover mapping at very high resolution with rotation equivariant CNNs: Towards small yet accurate models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="96" to="107" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Classification with an edge: Improving semantic image segmentation with boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Relation Matters: Relational Context-Aware Fully Convolutional Network for Semantic Segmentation of High-Resolution Aerial Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="7557" to="7569" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hybrid multiple attention network for semantic segmentation in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic multicontext segmentation of remote sensing images based on convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dalla Mura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="7503" to="7520" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient semantic segmentation with pyramidal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107611</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Random forest classifier for remote sensing classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="217" to="222" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Panboonyuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jitkajornwanich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lawawirojwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Srestasathiern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vateekul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">5100</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 13</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Big earth observation time series analysis for monitoring Brazilian agriculture. ISPRS journal of photogrammetry and remote sensing 145</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C A</forename><surname>Picoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sanches</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sim?es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maciel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Esquerdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antunes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Begotti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Contextnet: Exploring context and detail for semantic segmentation in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bonde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04554</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Fast-scnn: Fast semantic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Poudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04502</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="234" to="241" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Examining the impacts of future land use/land cover changes on climate in Punjab province, Pakistan: implications for environmental sustainability and economic growth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Samie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Azeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental Science and Pollution Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="25415" to="25433" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Road segmentation for remote sensing images using adversarial spatial pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shamsolmoali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zareapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Optimizing multiscale segmentation with local spectral heterogeneity measure for high resolution remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="13" to="25" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02585</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7262" to="7272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations, Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Problems of encoder-decoder frameworks for high-resolution remote sensing image segmentation: Structural stereotype and insufficient learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="297" to="304" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Land-cover classification with high-resolution remote sensing images using transferable deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">111322</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing of Environment 237</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Building detection in very high resolution multispectral data with deep learning features, 2015 IEEE international geoscience and remote sensing symposium (IGARSS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vakalopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1873" to="1876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08733</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A Novel Transformer Based Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Transformer Meets Convolution: A Bilateral Awareness Network for Semantic Segmentation of Very Fine Resolution Urban Scene Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3065</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing 13</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">SegFormer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A scale-invariant change detection method for land use/cover change research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Caelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="252" to="264" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Co-Scale Conv-Attentional Image Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9981" to="9990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Real-time Semantic Segmentation with Context Aggregation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="124" to="134" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">An attentionfused network for semantic segmentation of very-high-resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="238" to="262" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Land use and land cover change in Inner Mongolia-understanding the effects of China&apos;s re-vegetation programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pflugmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hostert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02147</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="325" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI 16</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">TreeUNet: Adaptive Tree convolutional neural networks for subdecimeter aerial image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Identifying and mapping individual plants in a highly diverse high-elevation ecosystem using UAV imagery and deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diazgranados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gerard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="280" to="291" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Scale Sequence Joint Deep Learning (SS-JDL) for land use and land cover classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sargent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">111593</biblScope>
		</imprint>
	</monogr>
	<note>Remote Sensing of Environment 237</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Transformer and CNN Hybrid Deep Neural Network for Semantic Segmentation of Very-high-resolution Remote Sensing Imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">ResT: An Efficient Transformer for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13677</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning multiscale and deep representations for classifying remotely sensed imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="155" to="165" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Contextually guided very-high-resolution imagery classification with semantic segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Emery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="48" to="60" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Parsing very high resolution urban scene images by learning deep ConvNets with edge-aware loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page" from="15" to="28" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Foreground-aware relation network for geospatial object segmentation in high spatial resolution remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4096" to="4105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<title level="m">Deformable DETR: Deformable Transformers for End-to-End Object Detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing: A comprehensive review and list of resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Shelfnet for fast semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
