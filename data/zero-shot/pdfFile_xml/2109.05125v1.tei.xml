<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MURAL: Multimodal, Multitask Retrieval Across Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aashi</forename><surname>Jain</surname></persName>
							<email>aashijain@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
							<email>krishnaps@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
							<email>iamtingchen@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
							<email>snehakudugunta@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
							<email>chaojia@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
							<email>yinfeiy@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
							<email>jasonbaldridge@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MURAL: Multimodal, Multitask Retrieval Across Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Both image-caption pairs and translation pairs provide the means to learn deep representations of and connections between languages. We use both types of pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual encoder that solves two tasks: 1) image-text matching and 2) translation pair matching. By incorporating billions of translation pairs, MURAL extends ALIGN (Jia et al., 2021)-a state-of-the-art dual encoder learned from 1.8 billion noisy image-text pairs. When using the same encoders, MURAL's performance matches or exceeds ALIGN's cross-modal retrieval performance on wellresourced languages across several datasets. More importantly, it considerably improves performance on under-resourced languages, showing that text-text learning can overcome a paucity of image-caption examples for these languages. On the Wikipedia Image-Text dataset, for example, MURAL-BASE improves zero-shot mean recall by 8.1% on average for eight under-resourced languages and by 6.8% on average when fine-tuning. We additionally show that MURAL's text representations cluster not only with respect to genealogical connections but also based on areal linguistics, such as the Balkan Sprachbund.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multilingual captions for images provide indirect but valuable associations between languages <ref type="bibr" target="#b14">(Gella et al., 2017)</ref>. <ref type="bibr" target="#b4">Burns et al. (2020)</ref> exploit this to scale multimodal representations to support more languages with a smaller model than prior studies. More recent work learns cross encoder models with multitask training objectives <ref type="bibr" target="#b33">(Ni et al., 2021;</ref><ref type="bibr" target="#b53">Zhou et al., 2021)</ref>; in these, a single multimodal encoder attends to both inputs and exploits deep associations between images and captions. Unfortunately, such models do not support efficient retrieval <ref type="bibr">(Geigle et al., 2021)</ref>, and they use object <ref type="figure">Figure 1</ref>: MURAL learns encoders for both language and images by combining both image-text matching and text-text matching tasks, using scalable dual encoder models trained with contrastive losses. detection, machine translation, bilingual dictionaries and many losses. In contrast, multimodal dual encoders can be learned directly on noisy, massive image-caption datasets using a simple loss based on in-batch bidirectional retrieval <ref type="bibr">(Jia et al., 2021;</ref><ref type="bibr" target="#b36">Radford et al., 2021)</ref>. These support efficient retrieval via approximate nearest neighbors search  and can predict similarity within and across modalities <ref type="bibr" target="#b34">(Parekh et al., 2021)</ref>.</p><p>With MURAL: MUltimodal, MUltitask Representations Across Languages <ref type="figure">(Fig. 1)</ref>, we explore dual encoder learning from both image-caption and translation pairs at massive scale: 6 billion translation pairs <ref type="bibr" target="#b12">(Feng et al., 2020</ref>) and 1.8 billion imagecaption pairs <ref type="bibr">(Jia et al., 2021)</ref>. We particularly seek to improve performance for under-resourced languages. Addressing this was infeasible until now because existing multilingual image-text datasets-Multi30k <ref type="bibr" target="#b10">(Elliott et al., 2016)</ref>), STAIR <ref type="bibr" target="#b49">(Yoshikawa et al., 2017)</ref>, and XTD <ref type="bibr" target="#b0">(Aggarwal and Kale, 2020</ref>)support only high-resource languages. However, the recent Wikipedia Image-Text (WIT) dataset <ref type="bibr" target="#b41">(Srinivasan et al., 2021)</ref>, which covers 108 languages, addresses this gap.</p><p>Our results, as a whole, demonstrate that ALIGN, a state-of-the-art multimodal dual encoder, is improved by adding a bitext ranking objective <ref type="bibr" target="#b46">(Yang et al., 2019a)</ref>  <ref type="bibr">(=MURAL)</ref>. The latter matches Name Train-I Train-T Dev-I <ref type="table" target="#tab_0">Dev-T Test-I Test-T #Langs   EOBT Pairs  -500m  ----124  MBT Pairs  ?  -6b  ----109  CC12m  12m  12m  ----1  Alt-Text  ?  1.8b  1.8b  ----110  XTD  ----1k  1k  7  Multi30k  29k  145k  1k  5k  1k  5k  4  MS-COCO  82k  410k  5k  25k  5k  25k  1  STAIR  82k  410k  5k  25k  5k  25k  1  WIT</ref> 11.4m 16m 5/3/1k 5/3/1k 5/3/1k 5/3/1k 108 zero-shot image-text retrieval performance on wellresourced languages, and it dramatically improves performance on under-resourced languages. For XTD, MURAL improves recall@10 by 4% on average. On WIT zero-shot, MURAL improves mean recall by 1.7% on average for nine well-resourced languages, and by 8.1% for eight under-resourced ones. After fine-tuning on WIT, MURAL mean recall is 1.8% and 6.8% better than ALIGN, on average, for well-resourced and under-resourced languages, respectively. We also show that the resulting dual encoder model can outperform more complex cross-encoder baseline models by a wide margin, thus obtaining stronger performance from models that support scalable retrieval. Our largest model, MURAL-LARGE, improves mean recall for zero-shot retrieval by 47.7% on average for four languages in Multi30k over M3P <ref type="bibr" target="#b33">(Ni et al., 2021)</ref>. It improves mean recall by 5.9% over UC2 <ref type="bibr" target="#b53">(Zhou et al., 2021)</ref> for the fine-tuning setting of Multi30k. MURAL-LARGE also improves over a strong translate-test baseline on WIT in a zero-shot setting for wellresourced languages by 13.2% and for underresourced ones by 9.6%.</p><p>We report results on Crisscrossed Captions (CxC) <ref type="bibr" target="#b34">(Parekh et al., 2021)</ref>, which additionally provides image-text, text-text, and image-image similarity ratings. MURAL-LARGE obtains the highest scores to date on CxC text?text and image?image retrieval. Our small ALIGN model and MURAL-LARGE model tie for best Semantic Image Similarity, which measures the correlation between model rankings and human rankings over image-image pairs.</p><p>Finally, we show that multilingual representa-tions learned in MURAL form clusters which are influenced from areal linguistics and contact linguistics, in addition to previously shown genealogical relationships <ref type="bibr" target="#b27">(Kudugunta et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>For training, we use both publicly available datasets and internal ones that are much larger. We evaluate on many publicly available image captioning datasets. <ref type="table" target="#tab_0">Table 1</ref> summarizes their statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training datasets</head><p>Conceptual 12M (CC12M) <ref type="bibr" target="#b5">Changpinyo et al. (2021)</ref> is a publicly available image captioning dataset in English with 12 million pairs obtained from web images and their corresponding alt-text descriptions. CC12M loosens the strong quality filters on the earlier Conceptual Captions (CC3M) dataset <ref type="bibr" target="#b39">(Sharma et al., 2018)</ref> to obtain greater scale. The multilingual version of Alt-Text <ref type="bibr">(Jia et al., 2021)</ref> is a noisy dataset with 1.8 billion images and their alt-text descriptions, covering 110 languages. Alt-Text is minimal filtered; this increases the scale and diversity of both images and languages. <ref type="figure" target="#fig_0">Fig.  2</ref>, which gives the distribution over all languages: over half the captions are English, and the top fifth of languages covers 95% of captions, so many languages still have relatively fewer examples.</p><p>We create an Ensemble of Open Bilingual Translation (EOBT) Pairs dataset by combining publicly available datasets, including Europarl <ref type="bibr" target="#b26">(Koehn, 2005)</ref>, Paracrawl <ref type="bibr" target="#b11">(Espl? et al., 2019)</ref>, Wikimatrix <ref type="bibr" target="#b38">(Schwenk et al., 2021)</ref>, and JW300 <ref type="bibr" target="#b1">(Agi? and Vuli?, 2019</ref>)-see Appendix A.2 for a full list. EOBT has ?500 million pairs across all languages. <ref type="bibr" target="#b12">Feng et al. (2020)</ref> mine translations from the web; we call their dataset as Mined Bilingual Translation (MBT) Pairs. It has 6 billion pairs (up to 100 million per language) for 109 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation datasets</head><p>Flickr30K <ref type="bibr" target="#b51">(Young et al., 2014)</ref> has 31k images, with five English captions per image. Multi30K extends Flickr30k with German, French, and Czech captions. <ref type="bibr" target="#b10">Elliott et al. (2016)</ref> introduces German annotations by 1) translating some Flickr30k English captions and 2) crowdsourcing new German captions for Flickr30K images. Following prior work <ref type="bibr" target="#b4">(Burns et al., 2020)</ref>, we report results on the independent 5 captions/image split. <ref type="bibr" target="#b9">Elliott et al. (2017)</ref> and <ref type="bibr" target="#b2">Barrault et al. (2018)</ref> further extend the dataset by collecting human translations of English Flickr30k captions to French and Czech. MS-COCO <ref type="bibr" target="#b29">(Lin et al., 2014)</ref> also has five human generated English captions per image. We report results on both the 1k and 5k splits defined by <ref type="bibr" target="#b25">Karpathy and Li (2015)</ref>. The STAIR dataset <ref type="bibr" target="#b49">(Yoshikawa et al., 2017)</ref>  image-text pairs each for validation and test, but for less well-resourced languages, we use 3k or 1k pairs. See Appendix A.3 for details.</p><p>Crisscrossed Captions (CxC) <ref type="bibr" target="#b34">(Parekh et al., 2021)</ref> extends the English MSCOCO 5k dev and test sets with human similarity annotations for both intra-and inter-modal tasks. As such, CxC supports evaluation for both inter-modal (image-text) and intra-modal (text-text, image-image) retrieval tasks, and correlation measures that compare model rankings with rankings derived from human similarity judgments (again, for image-text, image-image and text-text comparisons).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head><p>ALIGN <ref type="bibr">(Jia et al., 2021)</ref> is a family of multimodal dual encoders that learn to represent images and text in a shared embedding space. ALIGN's encoders are trained from scratch on image-text pairs via an in-batch normalized softmax loss (contrastive learning). This loss encourages the model to encode positive image-text pairs closer to each other while pushing away in-batch negative pairs. ALIGN delivers state-of-the-art results for several datasets; however, the Alt-Text data used to train it is heavily skewed towards well-resourced languages (see <ref type="figure" target="#fig_0">Fig. 2</ref>). This imbalance reduces ALIGN's ability to represent under-resourced languages; we address that here by using more representative text-text translation pairs mined at scale from the web.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MURAL</head><p>MURAL extends ALIGN with a multitask contrastive learning objective that adds text-text contrastive losses to the image-text ones. MURAL is trained simultaneously with two tasks of image-text (i2t) matching and text-text (t2t) matching. The text encoder is shared between these two tasks to allow transfer of multilingual learning from the text-text task to cross-modal representations. The resulting loss function is the sum of losses from both tasks.</p><p>Weighting of i2t and t2t tasks in the loss function <ref type="bibr" target="#b34">(Parekh et al., 2021)</ref> allows the tasks to be balanced. We experiment with different weights for both tasks; our main focus is cross-modal retrieval, so we weigh the image-text task higher than the text-text task. We use the following loss function:</p><formula xml:id="formula_0">L = w i2t * (L i2t + L t2i ) + w t2t * (L r2l + L l2r )</formula><p>Each loss is an in-batch softmax of the form:</p><formula xml:id="formula_1">L i2t = ? 1 N N i log exp(sim(x i , y i )/? ) N j=1 exp(sim(x i , y j )/?</formula><p>) where x i and y j are embeddings of the image in the i-th pair and the text in the j-th pair, respectively. sim(x, y) = x y/ x y denotes the dot product between 2 normalized x and y (cosine similarity). N is the batch size. ? is the temperature to scale the logits. We use a similar construction for L t2i , L r2l , and L l2r , where l is left-text and r is right-text. The softmax temperature is shared between L i2t and L t2i , and is learned with initial value 1.0. In L r2l and L l2r , the temperature is fixed to 0.01. Following <ref type="bibr" target="#b12">Feng et al. (2020)</ref>, we use additive margin 0.3 in L r2l and L l2r .</p><p>Task-specific projection heads that transform encoder representations before computing cosine similarity between inputs can improve contrastive learning . Similar designs have also been used for a traditional multitask setting . In MURAL, we use two singlelayer, task-specific projection heads above the text encoder: one transforms the text embedding for image-text contrastive loss, and the other for texttext contrastive loss (more details in A.1).</p><p>Fine-tuning: single-task vs. multi-task. Our primary goal with MURAL is to improve zero-shot performance by learning with both image-text and text-text pairs. Nevertheless, fine-tuning has a large impact on performance for any given dataset. After initial experiments, we find that single-task finetuning using image-text pairs performed slightly better than multitask finetuning using co-captions. For further discussion on this comparison, see Appendix A.1. For all models, we report results using single-task fine-tuning using any available training image-text pairs for a given dataset. <ref type="bibr">Jia et al. (2021)</ref> trains a very large model, ALIGN-L2, that uses EfficientNet-L2 <ref type="bibr" target="#b43">(Tan and Le, 2019)</ref> as image encoder and BERT-Large <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> as the text encoder. It was trained on Englishonly Alt-Text data. We explore smaller models and fewer training epochs to study various strategies more efficiently. For this, we use directly comparable ALIGN-BASE and MURAL-BASE models: both use EfficientNet-B5 for image encoding and BERT-Base for text. MURAL-BASE also uses texttext learning and an additional projection head for the image-text task (see Sect. 3.1). We also consider MURAL-LARGE, which uses Efficient-B7 and BERT-Large. ALIGN-BASE and MURAL-BASE have ?300M parameters, MURAL-LARGE has ?430M, and ALIGN-L2 has ?840M parameters. Appendix A.1 gives more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model variants</head><p>Following ALIGN <ref type="bibr">(Jia et al., 2021)</ref>, we use LAMB optimizer <ref type="bibr" target="#b50">(You et al., 2020)</ref> with a weight decay ratio of 1e-5. For ALIGN-BASE and MURAL-BASE, we train our models on 128 Cloud TPU V3 cores with a global batch size of 4096. The imagetext task uses a learning rate of 1e-3 and the texttext task uses 1e-4. Both learning rates are linearly warmed up from zero to their final values in 10k steps and then decayed linearly to zero in 600k steps. This corresponds to only around 1.4 epochs of the Alt-Text dataset and 0.4 epochs of the MBT dataset. MURAL-LARGE is trained on 512 TPU cores (4x larger samples used in training).</p><p>We build a 250k word-piece vocabulary from the Alt-Text training data, 2 which is kept the same in all our experiments to control the changing factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline Strategies</head><p>Our main goal is to explore the potential of large, diverse translations pairs for learning better multimodal encoders, including a single multilingual text encoder. We compare this strategy to the wellestablished, effective baselines of translate-train and translate-test using a strong Neural Machine Translation (NMT) system 3 <ref type="bibr" target="#b48">(Yang et al., 2019b)</ref>.</p><p>Translate-train: To reduce the heavy bias toward English and to support other languages for models training only on image-text pairs (e.g. for  ALIGN), we artificially create image-text pairs by using the NMT system to translate English texts to other languages. 4 These additional pairs are then used to train the model -a core strategy used in UC2 <ref type="bibr" target="#b53">(Zhou et al., 2021)</ref>. Translate-test: An alternative strategy is to train a high-performing English model and then translate non-English inputs into English, which are then encoded for cross-modal retrieval at test time.</p><p>Both strategies are highly dependent on the quality of NMT system, the languages it supports, while also incurring additional cost and complexity 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We focus on: We number the rows in our results tables to ease reference in our discussion and across tables. <ref type="bibr">4</ref> Refer to appendix A.4 for more details. <ref type="bibr">5</ref> Translating a text query with 10 tokens adds additional latency of upto 400ms in run on CPU with a batch size of 1, Multi30k and MSCOCO. <ref type="table" target="#tab_3">Table 2</ref> compares MURAL and previous results <ref type="bibr" target="#b4">(Burns et al., 2020;</ref><ref type="bibr" target="#b33">Ni et al., 2021;</ref><ref type="bibr" target="#b53">Zhou et al., 2021;</ref><ref type="bibr">Jia et al., 2021)</ref> in both zero-shot and fine-tuned settings.</p><p>The additional text-text task used by MURAL-BASE improves zero-shot performance on Czech, a relatively lower-resourced language, by a large margin over ALIGN-BASE (4 vs 6), 47.9 ? 64.6, while nearly matching or somewhat exceeding performance on higher-resource languages.</p><p>Large, noisy pre-training greatly reduces the need for fine-tuning. M3P sees huge performance gains by fine-tuning 6 (1 vs 10), sometimes 3x the zero-shot performance. Both ALIGN-BASE and MURAL-BASE see large gains, but their zero-shot performance is already near M3P's finetuned performance for highly resourced languages. MURAL-LARGE's zero-shot <ref type="formula">(7)</ref> actually exceeds M3P's fine-tuned performance (10) and almost matches UC2's fine-tuned performance (11).</p><p>Even with far less data than AT+MBT, MURAL-BASE trained on CC12M+EOBT (5) has much stronger zero-shot performance than M3P (1). With fine-tuning, MURAL-BASE (CC12M+EOBT) improves on both fine-tuned M3P and UC2 (14 vs 10,11), except for Japanese. Though MURAL benefits from four times more image-text pairs than the others (CC12m &gt; CC3M), both M3P and UC2 are more complex cross-encoder models that require  <ref type="table">Table 3</ref>: Mean Recall on WIT for English (en); German (de); French (fr); Czech (cs); Japanese (ja); Chinese (zh); Russian (ru); Polish (pl); Turkish (tr); Tajik (tg); Uzbek (uz); Irish (ga); Belarusian (be); Malagasy (mg); Cebuano (ceb); Haitian (ht); Waray-Waray (war); * : Translation system not available other resources. M3P uses several different losses and it relies on a synthetic code-switched data generation process and a pretrained Faster-RCN model to obtain object bounding boxes and labels. MU-RAL is simpler: it is a dual encoder using just two loss types, and it works directly on raw text and pixels.</p><p>The translate-train strategy works well compared to using only multilingual image-text pairs (2 vs 4; 12 vs 13) and versus text-text training (2 vs 6; 12 vs 15). Given this, using translate-train (2) to increase language diversity in image-text pairs combined with text-text pair training (6) may yield even more gains. As a zero-shot strategy, translate-test also works well . This suggests that SMALR's combination of multilingual encoding and translate-test <ref type="bibr" target="#b4">(Burns et al., 2020)</ref> may improve zero-shot performance further with MURAL (i.e., 3+6+SMALR).</p><p>Like others before, we find that training larger models on data of this scale produces remarkable gains: MURAL-LARGE obtains big improvements even over MURAL-BASE. MURAL-LARGE's results are state-of-the-art for all languages except English (where the larger, English-only ALIGN-L2 is best). MURAL-LARGE does this while-as a dual encoder-also supporting efficient retrieval. This makes a huge difference when retrieving from billions of items rather than the 1k to 5k examples of Multi30k's and MS-COCO's test sets (for which expensive, exhaustive comparisons can be performed with cross-encoders). See <ref type="bibr">Geigle et al. (2021)</ref> for extensive discussion and experiments around the computational cost of cross-encoders versus dual encoders for retrieval.</p><p>Wikipedia Image Text Results. We extracted two subsets of WIT for evaluation: 1) wellresourced languages and 2) under-resourced languages (more details in Appendix A.3). There   <ref type="table">Table 3</ref> shows MURAL-BASE achieves slightly better zero-shot performance compared to ALIGN-BASE on well-resourced languages, and a large boost on the under-represented ones. These results confirm our hypothesis of combining two tasks to address data scarcity in cross modal pairs. For WIT, MURAL-LARGE again shows that increasing model capacity improves zero-shot performance dramatically (row 7).</p><p>With WIT, the translate-test strategy again proves effective (row 3). It is comparable to both MURAL-BASE and ALIGN-BASE in a zeroshot setting-each wins some contests. Nevertheless, translate-test fails for the extremely under-resourced Waray-Waray language because the NMT system lacks support for it. In all, we found that 27 of WIT's 108 languages lacked NMT support. Thus, we cannot fully rely on translation systems for many under-represented languages; this further bolsters exploration into pivoting on images to overcome data scarcity. Furthermore, simple dual-encoder models are fast and simple at test-time, and thus scale better than translate-test.</p><p>Finally, both ALIGN-BASE and MURAL models benefit from fine-tuning on in-domain multilingual image-text training pairs, 7 when available; both obtain very large gains across all languages, and also easily beat the translate-test baseline fine-tuned on   WIT-en (18, 19, 20 vs 21). XTD. As shown in <ref type="table" target="#tab_7">Table 4</ref>, both ALIGN and MURAL obtain massive gains over the best strategy reported by Aggarwal and Kale (2020)-mUSE  with a multimodal metric loss (M3L). MURAL-LARGE shows especially strong performance across all languages. Note that we only obtained these scores after all experimentation was done on other datasets-this is methodologically important as there is neither training data nor development data for XTD.</p><p>Crisscrossed Captions. For CxC image-text retrieval <ref type="table" target="#tab_9">(Table 5)</ref>, ALIGN-L2 scores highest across all metrics; it is the largest model and was trained only on English Alt-Text. ALIGN-BASE also beats MURAL-BASE for image-text retrieval, but the latter comes back with better text-text and imageimage scores. This indicates that MURAL's texttext task balances both encoders better than a loss focused only on image-text pairs. Similarly, MURAL-LARGE beats ALIGN-L2 for both text-text and image-image retrieval, despite the fact that ALIGN-L2 uses a much larger image encoder.</p><p>The correlation results given in <ref type="table" target="#tab_10">Table 6</ref> tell an interesting story. Contrary to intuition and retrieval results, Semantic Image Similarity (SIS) seems connected with multilinguality, as all Alt-Text models (ALIGN-BASE, MURAL-BASE, MURAL-LARGE) perform nearly the same (and better). DE-T2T+I2T scores the highest on Semantic Text Similarity (STS) followed closely by MURAL-LARGE. It is worth noting that DE-T2T+I2T was trained with MSCOCO co-captions which could explain its high correlation. Semantic Image-Text Similarity (SITS) agrees with Image-Text retrieval results the most, with both MURAL-LARGE and ALIGN-L2 performing considerably better than others. However, with the SITS metric, the gap between both these models diminishes, indicating that ALIGN-L2 is probably more focused on getting positive matches while MURAL-LARGE captures non-matches more effectively.</p><p>The combined retrieval and correlation lens of CxC indicates there is much more to evaluating multimodal representations than the predominant cross-modal retrieval tasks. Ranking a set of items in a manner consistent with human similarity judgments is arguably a harder task than getting a single paired item to be more similar than nearly all others. These two perspectives may reveal useful tensions in finer-grained semantic distinctions. In fact, it is with these correlation measures that we expect cross-encoders to shine compared to the retrievaloriented dual encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Embedding Visualization. We visualize multilingual text representations using Singular Value Canonical Correlation Analysis (SVCCA) <ref type="bibr" target="#b37">(Raghu et al., 2017)</ref>, which allows similarity scores to be computed between languages. Using SVCCA scores computed for 100 languages, we plot a 2dimensional visualization using Laplacian Eigenmaps <ref type="bibr" target="#b3">(Belkin and Niyogi, 2003)</ref>. Following <ref type="bibr" target="#b27">Kudugunta et al. (2019)</ref>, we do so for a subset of languages belonging to the Germanic, Romance, Slavic, Uralic, Finnic, Celtic, and Finno-Ugric language families (widely spoken in Europe and Western Asia). For a fair evaluation, we artificially create a multilingual aligned dataset by using Google's Translation system to translate 1K English captions from the Multi30K dataset to 100 languages. <ref type="figure" target="#fig_2">Figure 3</ref> plots the embedding in a 2-dimensional space for two models: 1) LaBSE, a multilingual text-only sentence representation model (Feng  et al., 2020) and 2) MURAL, a multingual multimodal model. It is evident from the visualization of LaBSE representations that embeddings group largely based on genealogical connections between languages, a phenomenon observed previously in <ref type="bibr" target="#b27">Kudugunta et al. (2019)</ref>. In addition to groupings informed by linguistic genealogy, the MURAL visualization interestingly shows some clusters which are in line with areal linguistics and contact linguistics. Notably, Romanian (ro) is closer to the Slavic languages like Bulgarian (bg), Macedonian (mk) in MURAL than it is for LaBSE, which is in line with the Balkan Sprachbund <ref type="bibr" target="#b24">(Joseph, 1999)</ref>. English (en) and French (fr) are also embedded closer to each other, reflecting their extensive contact <ref type="bibr" target="#b20">(Haeberli, 2014)</ref>. Another possible language contact brings Finnic languages, Estonian (et) and Finnish (fi), closer to the Slavic languages cluster.</p><p>The fact that MURAL pivots on images as well as translations thus appears to add an additional view on language relatedness as learned in deep representations, beyond the language family clustering observed in a text-only setting. This suggests potential future work to explore different linguistic phenomena in these representations. It also suggests that it may be worth trying to improve multimodal, multilingual representations for a given lower-resource language by pivoting on a wellresourced language that is linguistically related or which has been in significant contact with itsimilar to previous studies for machine translation <ref type="bibr" target="#b22">(Islam and Hoenen, 2013)</ref>.</p><p>Retrieval Error Analysis. We analyzed zeroshot retrieved examples on WIT for ALIGN-BASE and MURAL-BASE for English (en), Hindi (hi), French (fr), and Portugese (pt). We list some examples here that indicate the value of using translation pair data for learning multilingual multimodal representations. See Appendix A.5 for more examples.</p><p>Across languages, for both Image?Text retrieval and Text?Image, we observed that MU-RAL displays better fidelity to the concepts described in the image and text. For instance, in <ref type="figure">Fig.  4 ALIGN'</ref>s top five results are somewhat scattered, whereas MURAL's results cohere better around boats with people (water taxis) near land (islands).</p><p>For under-resourced languages like Hindi, MU-RAL shows an improvement with respect to re- trieving results that are culturally more suited to the language <ref type="figure">(Fig. 5)</ref>.</p><p>Finally, with both models, retrieval for some examples could greatly benefit from better recognition of words present in the images. <ref type="figure" target="#fig_4">Fig. 6</ref> shows examples where extracting text from the images would make Image?Text almost trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>English provides a strong starting point for learning multilingual representations because it is so widespread and examples of English paired with other languages can be gathered well-beyond that of any other language, currently. We exploit this to train on translation pairs as a means to improve handling of multilingual inputs in cross-modal representations. With simple dual encoder models trained on large-scale datasets via contrastive learning, we obtain consistent, strong retrieval performance across all languages-especially under-resourced ones. Our error analysis also indicates that this helps increasing cultural specificity and diversity of the retrieved examples. The nuanced results we obtained for CxC also indicate that further improvements in such models might come from better calibration of the different tasks during learning. We also expect that more aggressive use of the translate-train strategy will straightforwardly yield further gains.</p><p>Embedding visualizations of MURAL's text representations also illustrates how languages cluster based on multimodal learning. Prior work has shown that English is not the ideal pivot language for many under-resourced languages <ref type="bibr" target="#b30">(Mulcaire et al., 2019;</ref><ref type="bibr" target="#b7">Conneau and Lample, 2019)</ref>. Our improvements for multilingual and multimodal models suggest further investigations into which well-resourced languages can be better pivots for learning representations for under-resourced languages. In addition to reflecting established language groupings, it also opens up possibilities of discovering new clusters. For instance, the proximity of Hungarian and Czech <ref type="figure" target="#fig_2">(Fig 3)</ref> for MURAL might be attributed to the geographical proximity of these languages, and warrants further analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ethics</head><p>Models trained on data collected from the web show strong results, and we are particularly encouraged by the fact that doing so leads to large improvements on under-resourced languages-and does so without requiring large amounts of (or any) image-text training data for those languages. Nevertheless, we should take utmost caution when using large datasets which went through minimal filtering processes. There could be potential biases both in the training data and models trained on them. Conscious research efforts should be made to detect and address such biases prior to releasing and using these models.</p><p>Fortunately, with prior research work in ethical AI research, it is possible to use findings from these areas to make the cross-modal models more accountable for their retrieval and broader use. We believe our findings and models can contribute positively to better understanding issues of and opportunities for addressing ethics, fairness, bias, and responsibility-especially with respect to crosscultural issues-in language and images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Modeling</head><p>Model variants We include further details about the main model variants we explore:</p><p>ALIGN-BASE: We use EfficientNet-B5 for the image encoder and BERT-Base Transformer for the text encoder which uses 12 layers, 12 attention heads resulting in an embedding of 768-dimensions. To match the image representation dimension of 512, we add an additional FC layer on top of the text encoder. The ALIGN-BASE model has 300M parameters in total, including 30M for EfficientNet-B5, 192M for the token embeddings, and 78M for the BERT Transformer. With this setting, we train on both the full multilingual Alt-Text dataset and the English subset, to get ALIGN-BASE and ALIGN-BASE-EN, respectively.</p><p>MURAL-BASE: The same as ALIGN-BASE, but also using text-text learning and the additional projection head for the image-text task (an FC layer that projects the text embedding from 768d to 512d). MURAL-LARGE: We use EfficientNet-B7 for the image encoder and BERT-Large Transformer 8 for the text encoder. To fit this model into memory, we use a 256-dimension token embedding size and project it to 1024 hidden size, which is then used by the large transformer encoder. The model uses 66M parameters for EfficientNet-B7, 64M for the token embeddings, and 300M for the BERT Transformer (=430M parameters total). ALIGN-L2 uses an EfficientNet-L2 (=480M parameters) image encoder with a BERT-Large Transformer (300M parameters) as a text encoder. Along with the 64M parameters for token embeddings, ALIGN-L2 has 840M parameters. Projection Heads For MURAL, we experiment with different layers of projection heads, e.g. 1 Fully Connected (FC) layer and a Multi-Layer Perceptron with non-linearity in between the FC layers. Empirically, we find that MURAL learns better image-text representations when using single layer projection heads on top of the text-encoder, one per task.</p><p>Different Task Weights <ref type="figure" target="#fig_5">Figure 7</ref> shows retrieval performance of models trained using different task weights in the loss function. We report zero-shot results on Multi30K val set for comparison. Weighing both t2i and i2t tasks equally (1:1) shows a consistent drop in cross-modal retrieval performance, which indicates that we need to weigh text-image task higher than the text-text task for optimal performance. From the figure we see that the ratios 0.1:1 and 0.05:1 achieve similar mean recall for t2t and i2t tasks across all Multi30K languages. In all our experiments, we use the ratio 0.1:1 for training MURAL.</p><p>Checkpoint Initialization. For MURAL, we either (1) initialize from a trained ALIGN checkpoint or (2) train both encoders from scratch. Our early experiments showed that the first strategy does not work as well. This is likely because ALIGN discards information about other languages early on because of English dominance in the Alt-Text dataset (2)-and as a result, performance on other languages is worse when training with a multitask objective. Since the model training with checkpoint initialization achieves a higher performance faster than the model trained on scratch, it offers a potential trade-off between performance and time for training. Given the early empirical results, in this paper, we always train MURAL from scratch unless otherwise stated. We stress that in the MU-RAL multitask model, the per-task layers on top of the text-encoders are trained from scratch in both the settings.</p><p>Finetuning Strategies: Single-task vs. Multitask We experimented with the standard singletask fine-tuning using image-text pairs in downstream datasets like Multi30K. However, we also tried constructing text-text aligned pairs from the Multi30K dataset (e.g. by using co-caption pairs as text-text pairs), similar to the multitask strategy of <ref type="bibr" target="#b34">Parekh et al. (2021)</ref>. We found that including text-text fine-tuning slightly decreased cross-modal retrieval performance. This is may be because the large pretrained MURAL model benefits little from seeing text-text pairs at the fine-tuning stage. This is interesting because this indicates that the training strategies at different stages affect the final performance differently. That said, it may just be that we lack the necessary evaluation data, such as multilingual variant of Crisscrossed Captions <ref type="bibr" target="#b34">(Parekh et al., 2021)</ref> with non-English Semantic Textual Similarity scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Ensemble of Open Bilingual Translation (EOBT) Pairs</head><p>The complete list of open-sourced bilingual translation pairs dataset used in the construction of EOBT includes: Europarl <ref type="bibr" target="#b26">(Koehn, 2005)</ref>, Paracrawl (Espl? et al., 2019), TED57, Tanzil <ref type="bibr" target="#b44">(Tiedemann, 2012)</ref>, NewsCommentary, Wikimatrix <ref type="bibr" target="#b38">(Schwenk et al., 2021)</ref>, Wikititles, JW300 (Agi? and Vuli?, 2019), Opus100 <ref type="bibr" target="#b52">(Zhang et al., 2020)</ref>, SETimes (Tyers and Alperen, 2010), UNv1.0, Autshumato (Groenewald and du Plooy, 2010), PMIndia <ref type="bibr" target="#b19">(Haddow and Kirefu, 2020)</ref>, CVIT <ref type="bibr" target="#b42">(Srivastava et al., 2020)</ref>, Inuktitut <ref type="bibr" target="#b21">(Hernandez and Nguyen, 2020)</ref>, NLPC, JESC <ref type="bibr" target="#b35">(Pryzant et al., 2018)</ref>, KFTT <ref type="bibr" target="#b32">(Neubig, 2011)</ref>, ASPEC <ref type="bibr" target="#b31">(Nakazawa et al., 2016)</ref>, Flores <ref type="bibr" target="#b18">(Guzm?n et al., 2019)</ref>. The data was processed in the same way as outlined in <ref type="bibr" target="#b40">Siddhant et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Wikipedia Image-text Dataset</head><p>To maintain high quality text descriptions, all the splits in the WIT dataset uses the reference descriptions paired with the images. This is the text description underneath an image in a Wikipedia page. This also prevents any potential overlap with the Alt-Text training data. Similar to the Alt-Text data distribution across languages, WIT data distribution <ref type="formula">(8)</ref>  reference descriptions in the WIT dataset. We split this into 108 individual language sets based on the language of the Wikipedia page. We observe that sometimes a particular language page might include a caption in an alternate language, especially an under-resourced language using a text in an well-resourced language. For e.g., an image in a Hindi page has a text caption in English. Each language set is further split into train, val and test sets. We maintain 5K image-text pairs for most of the languages but for the under-resourced we cut this down to 3K or 1K. For each language, we make sure that an image is only in one set <ref type="bibr">(train, val, test)</ref>. We also create two evaluation groups from WIT for well-resourced languages and under-resourced ones, ensuring they cover a broad range of language families and geographic areas:</p><p>? well-resourced: English (en), German(de), French (fr), Czech (cs), Japanese (ja), Chinese (zh), Russian (ru), Polish (pl), Turkish (tr) ? under-resourced: Tajik (tg), Uzbek (uz), Irish (ga), Belarusian (be), Malagasy (mg), Cebuano (ceb), Haitian (ht), Waray-Waray (war)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Translate-Train Languages</head><p>For translate-train baseline, we translate the English captions to some other well-resourced languages. For Alt-Text translation we translate English Alt-Text to German, French, Czech, Japanese, Korean, and Chinese. For CC12m dataset, we translate to languages present in the Multi30k and MSCOCO dataset namely, German, French, Czech, and Japanese. We augment the image-text pairs in English with these machine translated captions for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Error Analysis</head><p>We include more examples of retrieved images and text on the WIT dataset comparing ALIGN and MURAL. Some more observations-Using color as pivots is displayed by both ALIGN and MURAL in retrieving examples, but is stronger in MURAL. For instance <ref type="figure">(Figure 11</ref>),   <ref type="figure" target="#fig_0">Figure 12</ref>, ALIGN uses white and blue to retrieve captions mentioning those colors. This kind of backfires for ALIGN, because it retrieves "Blue colored lava lamp" as one of the captions. With MURAL we observe an increased object identification performance. In <ref type="figure" target="#fig_2">Figure 13</ref>, ALIGN fails to identify the sundial in the image, whereas MURAL retrieves the correct caption. We believe additional translation pairs helped MURAL learn the word for sundial in French.</p><p>For a relatively under-resourced language such as Hindi, both ALIGN and MURAL have a tendency to retrieve captions in English, which is comparatively high-resourced ( <ref type="figure">Figure 14</ref>. However, <ref type="figure">Figure 11</ref>: Color identification of the image to retrieve captions describing food that matches the white color represented in the image <ref type="figure" target="#fig_0">Figure 12</ref>: Identifying the noodles by its color and shape to retrieve captions such as "rice". <ref type="figure" target="#fig_2">Figure 13</ref>: MURAL learns to identify the sundial ("cadran solaire" in French) being displayed in the input image <ref type="figure">Figure 14</ref>: For an input image, both ALIGN and MU-RAL tend to retrieve English captions than Hindi captions in comparison to ALIGN, MURAL tends to infer characters and culture from the images and retrieve more Hindi captions.</p><p>Some of these observations hint us that there is definite value in using translation data to improve representations for which data is scarce. We see there are clear benefits of MURAL over ALIGN for languages other than English.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Alt-Text language distribution: (left) linear scale, which clearly conveys the skew toward well-resourced languages; (right) log-scale, which provides a better view of under-represented languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of text representations of LaBSE<ref type="bibr" target="#b12">(Feng et al., 2020)</ref> and MURAL for 35 languages using laplacian eigen values and SVCCA scores. Languages are color coded based on their genealogical association.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Portuguese: retrieval coherence. ("Water taxi in Puerto Ayora in the Galap?gos Islands.") Hindi: Text?Image. ("A bowl containing plain noodles without any spices or vegetables.")</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Image ? Text examples where recognizing text in the input image would greatly help.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Zero-shot performance on Multi30K (val set) for different task weights (format: text-text weight : image-text weight). Overall, a ratio of 0.1:1 works best across all languages. 8 24 layers, 24 attention heads, and 1024 hidden size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>WIT language distribution: (left) linear scale, which clearly conveys the skew toward well-resourced languages; (right) log-scale, which provides a better view of under-represented languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Fidelity to word 'bo?tes' (boxes) in a French caption Figure 10: Fidelity to both words famille and dolfin with MURAL identifying image of flour by its color. Also in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics. Counts are per language, except that Alt-Text and WIT training counts aggregate over all languages. WIT text counts are for reference descriptions. (Key: I=Image, T=Text; ? : indicates internal datasets); see Section 2 for abbreviations and further details on each dataset.)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Mean recall on standard datasets.</figDesc><table /><note>? : Numbers from UC2 paper; these were fine-tuned on MSCOCO-CN (Li et al., 2019), which has a different split than en and ja, resulting in possible train/test infiltration. SMALR MSCOCO 1K results use a different test split. (Key: AT=Alt-Text dataset, DE=Dual Encoder, CE=Cross Encoder, TrTrain=translate-train)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>BASE-EN 46.5 33.9 42.3 32.4 29.9 36.2 40.1 39.2 40.5 30.0 23.4 26.1 27.3 33.6 34.9 41.6 n/a * (4) ALIGN-BASE 46.7 33.5 45.0 26.5 33.6 35.2 30.9 29.9 31.4 21.2 15.6 12.9 8.9 23.9 31.0 33.1 24.0 (6) MURAL-BASE 46.4 33.9 44.8 31.5 34.3 35.6 33.7 33.2 34.7 35.3 24.1 20.8 21.4 33.0 35.7 39.1 26.1 (7) MURAL-LARGE 60.7 46.1 60.0 43.6 48.1 49.9 45.7 45.8 49.8 45.7 33.7 30.8 33.4 45.6 45.6 52.4 37.7 Fine-tuned (21) ALIGN-BASE-EN 66.4 48.8 58.5 44.7 40.2 48.2 55.2 52.0 58.0 47.0 29.6 32.7 37.7 44.2 48.4 53.5 n/a* (18) ALIGN-BASE 75.6 69.2 76.2 65.5 64.4 78.2 68.3 68.3 75.0 53.0 36.3 35.8 50.3 45.0 72.4 62.5 78.1 (19) MURAL-BASE 77.1 70.0 77.2 68.4 64.8 79.6 70.8 70.7 78.2 64.2 44.1 41.9 59.3 55.1 76.4 67.6 79.0 (20) MURAL-LARGE 82.4 76.3 83.3 74.5 71.9 86.7 77.4 77.4 85.7 72.9 53.5 51.4 69.8 62.3 82.3 76.7 84.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Well-resourced</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Under-resourced</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Model</cell><cell>en</cell><cell>de</cell><cell>fr</cell><cell>cs</cell><cell>ja</cell><cell>zh</cell><cell>ru</cell><cell>pl</cell><cell>tr</cell><cell>tg</cell><cell>uz</cell><cell>ga</cell><cell>be</cell><cell>mg</cell><cell>ceb</cell><cell>ht</cell><cell>war</cell></row><row><cell>Zero-shot</cell><cell>(3)</cell><cell>ALIGN-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>XTD zero-shot Text?Image Recall@10. are no prior results; here, we compare MURAL with ALIGN-BASE and ALIGN-BASE-EN using the translate-test baseline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>CxC Image?text (left), Text?Text (middle), and Image?Image (right) retrieval results. DE-T2T+I2T is the strongest model of Parekh et al. (2021). DE-T2T+I2T and ALIGN-L2 are fine-tuned on MSCOCO data, while ALIGN-BASE, MURAL-BASE, and MURAL-LARGE are fine-tuned on both Multi30K and MSCOCO data).</figDesc><table><row><cell>Model</cell><cell>STS avg ? std</cell><cell>SIS avg ? std</cell><cell>SITS avg ? std</cell></row><row><cell>(22) DE-T2T+I2T</cell><cell cols="3">74.5 ? 0.4 74.5 ? 0.9 61.9 ? 1.3</cell></row><row><cell>(13) ALIGN-BASE</cell><cell cols="3">72.7 ? 0.4 80.4 ? 0.7 63.7 ? 1.3</cell></row><row><cell>(15) MURAL-BASE</cell><cell cols="3">73.9 ? 0.4 80.0 ? 0.7 64.0 ? 1.2</cell></row><row><cell cols="4">(16) MURAL-LARGE 74.1 ? 0.4 80.4 ? 0.7 67.1 ? 1.3</cell></row><row><cell>(17) ALIGN-L2</cell><cell cols="3">72.9 ? 0.4 77.2 ? 0.8 67.6 ? 1.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Semantic Simliarity using CxC.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Image-Text data size distribution across languages for WIT and Alt-Text Datasets # Examples Alt-Text # Lang WIT # Lang</figDesc><table><row><cell>&gt; 10 8</cell><cell>4</cell><cell>-</cell></row><row><cell>&gt; 10 7</cell><cell>11</cell><cell>-</cell></row><row><cell>&gt; 10 6</cell><cell>22</cell><cell>2</cell></row><row><cell>&gt; 10 5</cell><cell>37</cell><cell>29</cell></row><row><cell>&gt; 10 4</cell><cell>18</cell><cell>52</cell></row><row><cell>&gt; 10 3</cell><cell>12</cell><cell>25</cell></row><row><cell>&gt; 10 2</cell><cell>4</cell><cell>-</cell></row><row><cell>&gt; 10 1</cell><cell>2</cell><cell>-</cell></row><row><cell>Total</cell><cell>110</cell><cell>108</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The vocabulary is built using the standard wpm library from tensorflow_text.3 https://cloud.google.com/translate</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Fine-tuned on Multi30k and MSCOCO combined, trained for 40k steps and learning rate sweeping of 1e-5, 5e-5, and 1e-4. Other hyperparameters are kept the same.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We fine-tune on WIT training split for 300K steps with initial learning rate 1e-4. Other hyper-parameters are the same as pre-training.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their helpful feedback. We also thank Anosh Raj and Daphne Luong from Google for initial reviews of the manuscript. We thank Zarana Parekh for helping us setup evaluation on the CxC dataset, Orhan Firat for providing guidance on vocabulary coverage, Yuqing Chen and Apu Shah for assisting with latency metrics of the NMT models.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards zero-shot Cross-lingual Image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajinkya</forename><surname>Kale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05107</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">JW300: A widecoverage parallel corpus for low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?eljko</forename><surname>Agi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1310</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3204" to="3210" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the third shared task on multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiraag</forename><surname>Lala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6402</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
		<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="304" to="323" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to scale multilingual representations for visionlanguage tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derry</forename><surname>Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08981</idno>
		<title level="m">Conceptual 12M: Pushing webscale image-text pre-training to recognize long-tail visual concepts</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="7057" to="7067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Findings of the second shared task on multimodal machine translation and multilingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4718</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="215" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi30K: Multilingual English-German image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-3210</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Vision and Language</title>
		<meeting>the 5th Workshop on Vision and Language<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ParaCrawl: Web-scale parallel corpora for the languages of the EU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miquel</forename><surname>Espl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Forcada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gema</forename><surname>Ram?rez-S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks</title>
		<meeting>Machine Translation Summit XVII: Translator, Project and User Tracks<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>European Association for Machine Translation</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="118" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Languageagnostic BERT sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01852</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Nils Reimers, Ivan Vulic, and Iryna Gurevych. 2021. Retrieve fast, rerank smart: Cooperative and joint approaches for improved cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Geigle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<idno>abs/2103.11920</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image pivoting for learning multilingual multimodal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1303</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2839" to="2845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Processing parallel text corpora for three South African language pairs in the Autshumato project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hendrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liza</forename><surname>Groenewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Plooy</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AfLaT</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AutoSeM: Automatic task selection and mixing in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakanth</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1355</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3520" to="3531" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accelerating large-scale inference with anisotropic vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Lindgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Simcha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Chern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="3887" to="3896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Two new evaluation datasets for low-resource machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
		<respStmt>
			<orgName>Nepali-English and Sinhala-English</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">PMIndia-a collection of parallel corpora of languages of India</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faheem</forename><surname>Kirefu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09907</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">When English meets French: A case study of language contact in Middle English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Haeberli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Papers Dedicated to Jacques Moeschler</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The ubiqus English-Inuktitut system for WMT20</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation</title>
		<meeting>the Fifth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Source and translation classification using most frequent words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zahurul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Hoenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1299" to="1305" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">2021. Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Romanian and the Balkans: Some comparative perspectives. The Emergence of the Modern Language Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies on the Transition from Historical-Comparative to Structural Linguistics in Honour of EFK Koerner</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="218" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298932</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Translation Summit X: Papers</title>
		<meeting>Machine Translation Summit X: Papers<address><addrLine>Phuket, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Investigating multilingual NMT representations at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1167</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1565" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">COCO-CN for cross-lingual image tagging, captioning, and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyu</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2347" to="2360" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Polyglot contextual representations improve crosslingual transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phoebe</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1392</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3912" to="3918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ASPEC: Asian scientific paper excerpt corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Yaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)<address><addrLine>Portoro?, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2204" to="2208" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The Kyoto free translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="http://www.phontron.com/kftt" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">M3p: Learning universal representations via multitask multilingual multimodal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minheng</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3977" to="3986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for MS-COCO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Waters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2855" to="2870" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">JESC: Japanese-English subtitle corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoo</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SVCCA: singular vector canonical correlation analysis for deep learning dynamics and interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="6076" to="6085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wiki-Matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1351" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1238</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Leveraging monolingual data with self-supervision for multilingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.252</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2827" to="2835" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">WIT: Wikipedia-based image text dataset for multimodal multilingual machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01913</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">IndicSpeech: Text-tospeech corpus for Indian languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nimisha</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrabha</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K R</forename><surname>Prajwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6417" to="6422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in OPUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
	<note>Istanbul, Turkey. European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">South-East European Times: A parallel corpus of Balkan languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serdar Alperen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC workshop on exploitation of multilingual resources and tools for Central and (South-) Eastern European Languages</title>
		<meeting>the LREC workshop on exploitation of multilingual resources and tools for Central and (South-) Eastern European Languages</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="49" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving multilingual sentence embedding using bi-directional dual encoder with additive margin softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Hern?ndez</forename><surname>?brego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/746</idno>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="5370" to="5378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multilingual universal sentence encoder for semantic retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jax</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Hernandez Abrego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PAWS-X: A cross-lingual adversarial dataset for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1382</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3687" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">STAIR captions: Constructing a large-scale Japanese image caption dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akikazu</forename><surname>Takeuchi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2066</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="417" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training BERT in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00166</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improving massively multilingual neural machine translation and zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.148</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1628" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Uc2: Universal cross-lingual cross-modal vision-and-language pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4155" to="4165" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
