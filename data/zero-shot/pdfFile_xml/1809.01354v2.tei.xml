<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Human Ma ing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiezheng</forename><surname>Ge</surname></persName>
							<email>tiezheng.gtz@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
							<email>zhang.zhiqiang@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Yang</surname></persName>
							<email>xinxin.yxx@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Human Ma ing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ma ing</term>
					<term>Human Ma ing</term>
					<term>Semantic Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human ma ing, high quality extraction of humans from natural images, is crucial for a wide variety of applications. Since the ma ing problem is severely under-constrained, most previous methods require user interactions to take user designated trimaps or scribbles as constraints. is user-in-the-loop nature makes them di cult to be applied to large scale data or time-sensitive scenarios. In this paper, instead of using explicit user input constraints, we employ implicit semantic constraints learned from data and propose an automatic human ma ing algorithm Semantic Human Ma ing (SHM). SHM is the rst algorithm that learns to jointly t both semantic information and high quality details with deep networks. In practice, simultaneously learning both coarse semantics and ne details is challenging. We propose a novel fusion strategy which naturally gives a probabilistic estimation of the alpha ma e. We also construct a very large dataset with high quality annotations consisting of 35,513 unique foregrounds to facilitate the learning and evaluation of human ma ing. Extensive experiments on this dataset and plenty of real images show that SHM achieves comparable results with state-of-the-art interactive ma ing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human ma ing, which aims at extracting humans from natural images with high quality, has a wide variety of applications, such as mixed reality, smart creative composition, live streaming, lm production, etc. For example, in an e-commerce website, smart creative composition provides personalized creative image to customers. is requires extracting fashion models from huge amount of original images and re-compositing them with new creative designes. In such a scenario, due to the huge volume of images to be processed and in pursuit of a be er customer experience, it is critical to have an automatic high quality extraction method. <ref type="figure" target="#fig_0">Fig. 1</ref> gives an example of smart creative composition with automatic human ma ing in a real-world e-commerce website.</p><p>Designing such an automatic method is not a trivial task. One may think of turning to either semantic segmentation or image ma ing techniques. However, neither of them can be used by itself to reach a satisfactory solution. On the one hand, semantic segmentation, which directly identi es the object category of each pixel, usually focuses on the coarse semantics and is prone to blurring structural details. On the other hand, image ma ing, widely adopted for ne detail extractions, usually requires user interactions and therefore is not suitable in data-intensive or time-sensitive scenarios such as smart creative composition. More speci cally, for an input image I , ma ing is formulated as a decomposition into </p><p>where for color images, there are 7 unknown variables but only 3 known variables, and thus, this decomposition is severely underconstrained <ref type="bibr" target="#b16">[16]</ref>. erefore, most ma ing algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b27">27]</ref> need to take user designated trimaps or scribbles as extra constraints.</p><p>In this paper, we propose a uni ed method, Semantic Human Ma ing (SHM), which integrates a semantic segmentation module with a deep learning based ma ing module to automatically extract the alpha ma e of humans. e learned semantic information distinguishing foreground and background is employed as an implicit constraint to a deep ma ing network which complements the capability of detail extraction. A straightforward way to implement such a method is to train these two modules separately and feed the segmentation results as trimaps into the ma ing network. However, this intuitive approach does not work well <ref type="bibr" target="#b24">[24]</ref>. e reason is that semantic segmentation aims at classifying each pixel and is able to roughly distinct humans from background, whereas the goal of ma ing is to assign to each pixel a more ne grained oat opacity value of foreground without determining the semantics. ey are responsible for recovering coarse segmentations and ne details respectively, and therefore they need to be carefully handled in order to cooperate properly towards high quality human ma ing. Shen et al. <ref type="bibr" target="#b24">[24]</ref> use a closed form ma ing <ref type="bibr" target="#b16">[16]</ref> layer through which the semantic information can directly propagate and constitute the nal result. But with deep learning based ma ing, the ma ing module is highly nonlinear and trained to focus on structural patterns of details, thus the semantic information from input hardly retains. To combine the coarse semantics and ne ma ing details exquisitely, we propose a novel fusion strategy which naturally gives a probabilistic estimation of the alpha ma e. It can be viewed as an adaptive ensemble of both high and low level results on each pixel. Further, with this strategy, the whole network automatically amounts the nal training error to the coarse and the ne, and thus can be trained in an end-to-end fashion.</p><p>We also constructed a very large dataset with high quality annotations for the human ma ing task. Since annotating details is di cult and time-consuming, high quality datasets for human matting are valuable and scarce. e most popular alphama ing.com dataset <ref type="bibr" target="#b21">[21]</ref> has made signi cant contributions to the ma ing research. Unfortunately it only consists of 27 training images and 8 testing images. Shen et al. <ref type="bibr" target="#b24">[24]</ref> created a dataset of 2,000 images, but it only contains portrait images. Besides, the groundtruth images of this dataset are generated with closed form ma ing <ref type="bibr" target="#b21">[21]</ref> and KNN ma ing <ref type="bibr" target="#b3">[4]</ref> and therefore can be potentially biased. Recently, Xu et al. <ref type="bibr" target="#b27">[27]</ref> built a large high quality ma ing dataset, with 202 distinct human foregrounds. To increase the volume and diversity of human ma ing data that bene t the learning and evaluation of human ma ing, we collected another 35,311 distinct human images with ne ma e annotations. All human foregrounds are composited with di erent backgrounds and the nal dataset includes 52,511 images for training and 1,400 images for testing. More details of this dataset will be discussed in Section 3.</p><p>Extensive experiments are conducted on this dataset to empirically evaluate the e ectiveness of our method. Under the commonly used metrics of ma ing performance, our method can achieve comparable results with the state-of-the-art interactive ma ing methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b27">27]</ref>. Moreover, we demonstrate that our learned model generalizes to real images with justifying plenty of natural human images crawled from the Internet.</p><p>To summarize, the main contributions of our work are three-fold: 1. To the best of our knowledge, SHM is the rst automatic matting algorithm that learns to jointly t both semantic information and high quality details with deep networks. Empirical studies show that SHM achieves comparable results with the state-of-theart interactive ma ing methods.</p><p>2. A novel fusion strategy, which naturally gives a probabilistic estimation of the alpha ma e, is proposed to make the entire network properly cooperate. It adaptively ensembles coarse semantic and ne detail results on each pixel which is crucial to enable end-to-end training.</p><p>3. A large scale high quality human ma ing dataset is created. It contains 35,513 unique human images with corresponding alpha ma es. e dataset not only enables e ective training of the deep network in SHM but also contributes with its volume and diversity to the human ma ing research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>In this section, we will review semantic segmentation and image ma ing methods that most related to our works.</p><p>Since Long et al. <ref type="bibr" target="#b18">[18]</ref> use Fully Convolutional Network (FCN) to predict pixel level label densely and improve the segmentation accuracy by a large margin, FCN has became the main framework for semantic segmentation and kinds of techniques have been proposed by researchers to improve the performance. Yu et al. <ref type="bibr" target="#b28">[28]</ref> propose dilated convolutions to increase the receptive led of the network without spatial resolution decrease, which is demonstrated e ective for pixel level prediction. Chen et al. <ref type="bibr" target="#b2">[3]</ref> add fully connected CRFs on the top of network as post processing to alleviate the "hole" phenomenon of FCN. In PSPNet <ref type="bibr" target="#b29">[29]</ref>, network-in pyramid pooling module is proposed to acquire global contextual prior. Peng et al. <ref type="bibr" target="#b20">[20]</ref> state that using large convolutional kernels and boundary re nement block can improve the pixel level classi cation accuracy while maintaining precise localization capacity. With the above improvements, FCN based models trained on large scale segmentation datasets, such as VOC <ref type="bibr" target="#b6">[7]</ref> and COCO <ref type="bibr" target="#b17">[17]</ref>, have achieved the top performances in semantic segmentation. However, these models can not be directly applied to semantic human ma ing for the following reasons. 1) e annotations of current segmentation datasets are relative "coarse" and "hard" to ma ing task. Models trained on these datasets do not satisfy the accuracy requirement of pixel level location and oating level alpha values for ma ing. 2) Pixel level classi cation accuracy is the only consideration during network architecture and loss design in semantic segmentation. is leads the model prone to blurring complex structural details which is crucial for ma ing performance.</p><p>In the past decades, researchers have developed variety of general ma ing methods for natural images. Most methods predict the alpha ma es through sampling <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b26">26]</ref> or propagating <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b16">16</ref>, 25] on color or low-level features. With the rise of deep learning in computer vision community, several CNN based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b27">27]</ref> have been proposed for general image ma ing. Cho et al. <ref type="bibr" target="#b4">[5]</ref> design a convolutional neural network to reconstruct the alpha ma e by taking the results of the closed form ma ing <ref type="bibr" target="#b16">[16]</ref> and KNN ma ing <ref type="bibr" target="#b3">[4]</ref> along with the normalized RGB color image as inputs. Xu et al. <ref type="bibr" target="#b27">[27]</ref> directly predict the alpha ma e with a pure encoder decoder network which takes the RGB image and trimap as inputs and achieve the state-of-the-art results. However, all the above general image ma ing methods need scribbles or trimap obtained from user interactions as constraints and so they can not be applied in automatic way.</p><p>Recently, several works <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b30">30]</ref> have been proposed to make an automatic ma ing system. Shen et al. <ref type="bibr" target="#b24">[24]</ref> use closed form ma ing <ref type="bibr" target="#b16">[16]</ref> with CNN to automatically obtain the alpha ma es of portrait images and back propagate the errors to the deep convolutional network. Zhu et al. <ref type="bibr" target="#b30">[30]</ref> follow the similar pipeline while designing a smaller network and a fast lter similar to guided lter <ref type="bibr" target="#b12">[12]</ref> for ma ing to deploy the model on mobile phones. Despite our method and the above two works both use CNNs to learn semantic information instead of manual trimaps to automate the ma ing process, our method is quite di erent from theirs: 1) Both the above methods use the traditional methods as ma ing module, which compute the alpha ma e by solving the ma ing equation (Eq. 1) and may introduce artifacts when the distributions of foreground and background color overlap <ref type="bibr" target="#b27">[27]</ref>. We employ a FCN as ma ing module so as to directly learn complex details in a wide context which have been shown much more robust by <ref type="bibr" target="#b27">[27]</ref>. 2) By  solving the ma ing equation, these method can directly a ect the nal perdition with the input constraints and thus propagate back the errors. However, when the deep ma ing network is adopted, the cooperation of coarse semantics and nd details must be explicitly handled. us a novel fusion strategy is proposed and enables the end-to-end training of the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HUMAN MATTING DATASET</head><p>As a newly de ned task in this paper, the rst challenge is that semantic human ma ing encounters the lack of data. To address it, we create a large scale high quality human ma ing dataset. e foregrounds in this dataset are humans with some accessories(e.g., cellphones, handbags). And each foreground is associated with a carefully annotated alpha ma e. Following Xu et al. <ref type="bibr" target="#b27">[27]</ref>, foregrounds are composited onto new backgrounds to create a human ma ing dataset with 52,511 images in total. Some sample images in our dataset are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>In details, the foregrounds and corresponding alpha ma e images in our dataset comprise:</p><p>? Fashion Model dataset. More than 188k fashion model images are collected from an e-commerce website, whose alpha ma es are annotated by sellers in accordance with commercial quality. Volunteers are recruited to carefully inspect and double-check the ma es to remove those even only with small aws. It takes almost 1,200 hours to select  <ref type="bibr" target="#b27">[27]</ref>. We also select all the images that only contain human from DIM dataset, resulting 202 foregrounds. e background images are from COCO dataset and the Internet. We ensure that background images do not contain humans. e foregrounds are split into train/test set, and the con guration is shown in <ref type="table" target="#tab_0">Table 1</ref>. Following <ref type="bibr" target="#b27">[27]</ref>, each foreground is composited with N backgrounds. For foregrounds from Fashion Model dataset, due to their large number, N is set to 1 for both training and testing dataset. For foregrounds from DIM dataset, N is set to 100 for training dataset and 20 for testing dataset, as in <ref type="bibr" target="#b27">[27]</ref>. All the background images are randomly selected and unique. <ref type="table" target="#tab_1">Table 2</ref> shows the comparisons of basic properties between existing ma ing datasets and ours. Compared with previous ma ing datasets, our dataset di ers in the following aspects: 1) e existing ma ing datasets contain hundreds of foreground objects, while our dataset contains 35,513 di erent foregrounds which is much larger than others; 2) In order to deal with the human ma ing task, foregrounds containing human body are needed. However, DIM <ref type="bibr" target="#b27">[27]</ref> dataset only contains 202 human objects. e dataset proposed by Shen et al. <ref type="bibr" target="#b24">[24]</ref> consists of portraits, which are limited to heads and part of shoulders. In contrast, our dataset has a larger diversity</p><formula xml:id="formula_1">T-Net Concat ! " ! " ! " ! " ! " Deconv5 (512x5x5) Deconv4 (256x5x5) Deconv3 (128x5x5) Deconv2 (64x5x5) Deconv1 (64x5x5) Raw Alpha Pred (1x5x5) M-Net X + Fusion Module Conv+BN+ReLU Conv Max pooling Unpooling + Add</formula><p>Element-wise multiply X <ref type="figure">Figure 3</ref>: Overview of our semantic human matting method. Given an input image, a T-Net, which is implemented as PSPNet-50, is used to predict the 3-channel trimap. e predicted trimap is then concatenated with the original image and fed into the M-Net to predict the raw alpha matte. Finally, both the predicted trimap and raw alpha matte are fed into the Fusion Module to generate the nal alpha matte according to Eq. 4. e entire network is trained in an end-to-end fashion.</p><p>that might cover the whole human body, i.e. head, arms, legs etc. in various poses, which is essential for human ma ing; 3) Unlike the dataset of Shen et al <ref type="bibr" target="#b24">[24]</ref> which is annotated by Closed From <ref type="bibr" target="#b16">[16]</ref>, KNN <ref type="bibr" target="#b3">[4]</ref> and therefore can be potentially biased, all 35,513 foreground objects are manually annotated and carefully inspected, which guarantees the high quality alpha ma es and ensures the semantic integrity and unicity. e dataset not only enables e ective training of the deep network in SHM but also contributes with its volume and diversity to the human ma ing research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OUR METHOD</head><p>Our SHM is targeted to automatically pull the alpha ma e of a speci c semantic pa ern-the humans. <ref type="figure">Fig 3 shows</ref> its pipeline. e SHM takes an image (usually 3 channels representing RGB) as input, and directly outputs a 1-channel alpha ma e image with identical size of input. Note that no auxiliary information (e.g. trimap and scribbles) is required.</p><p>SHM aims to simultaneously capture both coarse semantic classi cation information and ne ma ing details. We design two subnetworks to separately handle these two tasks. e rst one, named T-Net, is responsible to do pixel-wise classi cation among foreground, background and unknown regions; while the second one, named M-Net, takes in the output of T-Net as semantic hint and describes the details by generating the raw alpha ma e image. e outputs of T-Net and M-Net are fused by a novel Fusion Module to generate the nal alpha ma e result. e whole networks are trained jointly in an end-to-end manner. We describe these submodules in detail in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Trimap generation: T-Net</head><p>e T-Net plays the role of semantic segmentation in our task and roughly extract foreground region. Speci cally, we follow the traditional trimap concept and de ne a 3-class segmentationthe foreground, background and unknown region. erefore, the output of T-Net is a 3-channel map indicating the possibility that each pixel belongs to each of the 3 classes. In general, T-Net can be implemented as any of the state-of-the-art semantic segmentation networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29]</ref>. In this paper, we choose PSPNet-50 <ref type="bibr" target="#b29">[29]</ref> for its e cacy and e ciency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Matting network: M-Net</head><p>Similar to general ma ing task <ref type="bibr" target="#b27">[27]</ref>, the M-Net aims to capture detail information and generate alpha ma e. e M-Net takes the concatenation of 3-channel images and the 3-channel segmentation results from T-Net as 6-channel input. Note that it di ers from DIM <ref type="bibr" target="#b27">[27]</ref> which uses 3-channel images plus 1-channel trimap (with 1, 0.5, 0 to indicate foreground, unknown region and background respectively) as 4-channel input. We use 6-channel input since it conveniently ts the output of T-Net and we empirically nd that with 6-channel or 4-channel input have nearly equal performance.</p><p>As shown in <ref type="figure">Fig. 3</ref>, the M-Net is a deep convolutional encoderdecoder network. e encoder network has 13 convolutional layers and 4 max-pooling layers, while the decoder network has 6 convolutional layers and 4 unpooling layers. e hyper-parameters of encoder network are the same as the convolutional layers of VGG16 classi cation network expect for the "conv1" layer in VGG16 that has 3 input channels whereas 6 in our M-Net. e structure of M-Net di ers from DIM <ref type="bibr" target="#b27">[27]</ref> in following aspects: 1) M-Net has 6-channel instead of 4-channel inputs; 2) Batch Normalization is added a er each convolutional layer to accelerate convergence; 3) "conv6" and "deconv6" layers are removed since these layers have large number of parameters and are prone to over ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fusion Module</head><p>e deep ma ing network takes the predicted trimap as input and directly computes the alpha ma e. However, as shown in <ref type="figure">Fig. 3</ref>, it focuses on the unknown regions and recovers structual and textural details only. e semantic information of foreground and background is not retained well. In this section, we describe the fusion strategy in detail.</p><p>We use F , B and U to denote the foreground, background and unknown region channel that predicted by T-Net before so max.</p><p>us the probability map of foreground F s can be wri en as</p><formula xml:id="formula_2">F s = exp(F ) exp(F ) + exp(B) + exp(U )<label>(2)</label></formula><p>We can obtain B s and U s in the similarly way. It is obvious that F s + B s +U s = 1, where 1 denotes an all-1 matrix that has the same width and height of input image. We use ? r to denote the output of M-Net.</p><p>Noting that the predicted trimap gives the probability distribution of each pixel belonging the three categories, foreground, background and unknown region. When a pixel locates in the unknown region, which means that it is near the contour of a human and constitutes the complex structural details like hair, ma ing is required to accurately pull the alpha ma e. At this moment, we would like to use the result of ma ing network, ? r , as an accurate prediction. Otherwise, if a pixel locates outside the unknown region, then the conditional probability of the pixel belonging to the foreground is an appropriate estimation of the ma e, i.e., F s F s +B s . Considering that U s is the probability of each pixel belonging to the unknown region, a probabilistic estimation of alpha ma e for all pixels can be wri en as</p><formula xml:id="formula_3">? p = (1 ? U s ) F s F s + B s + U s ? r<label>(3)</label></formula><p>where ? p denotes the output of Fusion Module. As F s + B s = 1 ? U s , we can rewrite Eq. 3 as</p><formula xml:id="formula_4">? p = F s + U s ? r<label>(4)</label></formula><p>Intuitively, this formulation shows that the coarse semantic segmentation is re ned by the ma ing result with details, and the re nement is controlled explicitly by the unknown region probability. We can see that when U s is close to 1, F s is close to 0, so ? p is approximated by ? r , and when U s is close to 0, ? p is approximated by F s . us it naturally combines the coarse semantics and ne details. Furthermore, training errors can be readily propagated through to corresponding components, enabling the end-to-end training of the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Loss</head><p>Following Xu et al. <ref type="bibr" target="#b27">[27]</ref>, we adopt the alpha prediction loss and compositional loss. e alpha prediction loss is de ned as the absolute di erence between the groundtruth alpha ? and predicted alpha ? p . And the compositional loss is de ned as the absolute di erence between the groundtruth compositional image values c and predicted compositional image values c p . e overall prediction loss for ? p at each pixel is</p><formula xml:id="formula_5">L p = ? ||? p ? ? || 1 + (1 ? ? )||c p ? c || 1<label>(5)</label></formula><p>where ? is set to 0.5 in our experiments. It is worth noting that unlike Xu et al. <ref type="bibr" target="#b27">[27]</ref> which only focus on unknown regions, in our automatic se ings, the prediction loss is summed over the entire image.</p><p>In addition, we need to note that the loss ||F s +U s ? r ?? || forms another decomposition problem of groundtruth ma e, which is again under-constrained. To get a stable solution to this problem, we introduce an extra constraint to keep the trimap meaningful. A classi cation loss L t for the trimap over each pixel is thus involved.</p><p>Finally, we get the total loss</p><formula xml:id="formula_6">L = L p + ?L t<label>(6)</label></formula><p>where we just keep ? to a small value to give a decomposition constraint, e.g. 0.01 throughout our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Implementation Detail</head><p>e pre-train technique <ref type="bibr" target="#b14">[14]</ref> has been widely adopted in deep learning and shown its e ectiveness. We follow this common practice and rst pre-train the two sub-netowrks T-Net and M-Net separately and then netune the entire net in an end-to-end way. Further, when pre-training the subnetwork, extra data with large amount speci c to sub-tasks can also be empolyed to su ciently train the models. Note that the dataset used for pre-training should not overlap with the test set.</p><p>T-Net pre-train. To train T-Net, we follow the common practice to generate the trimap ground truth by dilating the groundtruth alpha ma es. In training phase, square patches are randomly cropped from input images and then uniformly resized to 400?400. To avoid over ing, these samples are also augmented by randomly performing rotation and horizontal ipping. As our T-Net makes use of PSPNet50 which is based on ResNet50 <ref type="bibr" target="#b13">[13]</ref>, we initialize relevant layers with o -the-shelf model trained on ImageNet classi cation task and randomly initialize the rest layers. e cross entropy loss for classi cation(i.e., L t in Eq. 6) is employed.</p><p>M-Net pre-train. We follow deep ma ing network training pipeline as <ref type="bibr" target="#b27">[27]</ref> to pre-train M-Net. Again, the input of M-Net is a 3-channel image with the 3-channel trimap generated by dilating and eroding the groundtruth alpha ma es. Worth noting that, we nd it is crucial for the performance of ma ing to augment the trimap with di erent kernel sizes of dilating and eroding, since it makes the result more robust to the various unknown region widths. For data augmentation, the input images are randomly cropped and resized to 320?320. Entire DIM <ref type="bibr" target="#b27">[27]</ref> dataset is empployed during pre-training M-Net regardless whether it contains humans, since the M-Net focuses on the local pa ern rather than the global semantic meaning. e regression loss same as L p term in Eq. 6 is adopted.</p><p>End-to-end training. End-to-end training is performed on human ma ing dataset and the model is initialized by pre-trained T-Net and M-Net. In training stage, the input image is randomly cropped as 800?800 patches and fed into T-Net to obtain semantic predictions. Considering that M-Net needs to be more focused on details and trained with large diversity, augmentations are performed on the y to randomly crop di erent patches (320?320, 480?480, 640?640 as in <ref type="bibr" target="#b27">[27]</ref>) and resize to 320?320. Horizontal ipping is also randomly adopted with 0.5 chance. e total loss in Eq. 6 is used. For testing, the feed forward is conducted on the entire image without augmentation. More speci cally, when the longer edge of the input image is larger than 1500 pixels, we rst scale it to 1500 for the limitation of GPU memory. We then feed it to the network and nally rescale the predicted alpha ma e to the size of the original input image for performance evaluation. In fact, we can alternatively perform testing on CPU for large images without losing resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experimental Setup</head><p>We implement our method with PyTorch <ref type="bibr" target="#b19">[19]</ref> framework. e T-Net and M-Net are rst pre-trained and then ne-tuned end to end as described in Section 4.5. During end-to-end training phase, we use Adam as the optimizer. e learning rate is set to 10 ?5 and the batch size is 10.</p><p>Dataset. We evaluate our method on the human ma ing dataset, which contains 52,511 training images and 1,400 testing images as described in Section 3.</p><p>Measurement. Four metrics are used to evaluate the quality of predicted alpha ma e <ref type="bibr" target="#b21">[21]</ref>: SAD, MSE, Gradient error and Connectivity error. SAD and MSE are obviously correlated to the training objective, and the Gradient error and Connectivity error are proposed by <ref type="bibr" target="#b21">[21]</ref> to re ect perceptual visual quality by a human observer. To be speci c, we normalize both the predicted alpha ma e and groundtruth to 0 to 1 when calculating all these metrics. Further, all metrics are caculated over entire images instead of only within unknown regions and averaged by the pixel number.</p><p>Baselines. In order to evaluate the e ectiveness of our proposed methods, we compare our method with the following state-of-theart ma ing methods 1 : Closed Form (CF) ma ing <ref type="bibr" target="#b16">[16]</ref> , KNN ma ing <ref type="bibr" target="#b3">[4]</ref>, DCNN ma ing <ref type="bibr" target="#b4">[5]</ref> , Information Flow Ma ing (IFM) <ref type="bibr" target="#b0">[1]</ref> and Deep Image Ma ing (DIM) <ref type="bibr" target="#b27">[27]</ref>. Noting that, all these ma ing methods are interactive and need extra trimaps as input. For a fair comparison, we provide them with predicted trimaps by the well pretrained T-Net. We denote these methods as PSP50 + X , where X represents the above methods.</p><p>To demonstrate the results of applying semantic segmentation to ma ing problem, we also design the following baselines:</p><p>? PSP50 Seg: a PSPNet-50 is used to extract humans via the predicted mask. e groundtruth mask used to train this network is obtained by binarizing the alpha ma e with a threshold of 0. ? PSP50 Reg: a PSPNet-50 is trained to predict the alpha ma e as regression with L1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Comparison</head><p>In this section, we compare our method with the state-of-the-art ma ing methods with generated trimaps and designed baselines  <ref type="table" target="#tab_2">Table 3</ref>. e performances of binary segmentation and regression are poor. Since complex structural details as well as the concepts of human are required in this task, the results show that it is hard to learn them simultaneously with a FCN network. Using the trimaps predicted by the same PSP50 network, DIM outperforms the other methods, such as CF, KNN, DCNN and IFM. It is due to the strong capabilities of deep ma ing network to model complex context of images. We can see that our method performs much be er than all baselines. e key reason is that our method successfully coordinate the coarse semantics and ne details with a probabilistic fusion strategy which enables a be er end-to-end training.</p><p>Several visual examples are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Compared to other methods (from column 2 to column 4), our method can not only obtain more "sharp" details, such as hairs, but also have much li le semantic errors which may bene t from the end-to-end training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Automatic Method vs. Interactive Methods</head><p>We compare our method with state-of-the-art interactive ma ing methods taking the groundtruth trimaps as inputs, which are generated by the same strategy used in T-Net pretraining stage. We denote the baselines as TrimapGT + X , where X represents 5 stateof-the-art ma ing methods including CF <ref type="bibr" target="#b16">[16]</ref> , KNN <ref type="bibr" target="#b3">[4]</ref>, DCNN <ref type="bibr" target="#b4">[5]</ref>, IFM <ref type="bibr" target="#b0">[1]</ref> and DIM <ref type="bibr" target="#b27">[27]</ref>. <ref type="table" target="#tab_3">Table 4</ref> shows the comparisons. We can see that the result of our automatic method trained by endto-end strategy is higher than most interactive ma ing methods, and is slightly inferior to TrimapGT+DIM. Note that our automatic method only takes in the original RGB images, while interactive TrimapGT + X baselines take additional groundtruth trimaps as inputs. Our T-Net could infer the human bodies and estimate coarse predictions which are then complemented with ma ing details by M-Net. Despite slightly higher test loss, our automatic method is visually comparable with DIM, the state-of-the-art interactive ma ing methods, as shown in <ref type="figure" target="#fig_2">Fig. 4</ref> (column "TrimapGT+DIM" vs. "Our method").   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation and Analysis of Di erent Components</head><p>The E ect of End-to-end Training. In order to evaluate the e ectiveness of the end-to-end strategy, we compare our end-to-end trained model with that using only pre-trained parameters (no endto-end). e results are listed in <ref type="table" target="#tab_5">Table 5</ref>. We can see that network trained in end-to-end manner performs be er than no end-to-end, which shows the e ectiveness of the end-to-end training.</p><p>The Evaluation of Fusion Module. To validate the importance of the proposed Fusion Module, we design a simple baseline that directly outputs the result of M-Net, i.e. ? p = ? r . It is trained with the same objective as Eq. 6. We compare the performance between our method with Fusion Module and this baseline without Fusion Module in <ref type="table" target="#tab_5">Table 5</ref>. We can see that our method with Fusion Module achieves be er performance than the baseline. Especially note that although other metrics remain relatively small, the Connectivity error of baseline gets quite large. It can be due to a blurring of the structural details when predicting the whole alpha ma e only with M-Net. us the designed fusion module, which leverages both the coarse estimations from T-Net and the ne predictions from M-Net, is crucial for be er performance.</p><p>The E ect of Constraint L t . In our implementation, we introduce a constraint for the trimap, i.e. L t . We train a network removing it to investigate the e ect of such a constraint. We denote the network trained in this way as no L t . e performance of this network is shown in <ref type="table" target="#tab_5">Table 5</ref>. We can see that the network without L t performs be er than that without end-to-end training, but is worse than the proposed method. is constraint makes the trimap more meaningful and the decomposition in L p more stable.</p><p>Visualization of Intermediate Results. To be er understand the mechanism of SHM, we visualize the intermediate results on a real image shown in <ref type="figure" target="#fig_3">Fig 5.</ref> e rst column (a) shows the original input image, the second column (b) shows the predicted foreground (green), background (red) and unknown region (blue) from T-Net, the third column (c) shows the predicted alpha ma e from M-Net, and the last column (d) shows the fusion result of the second column (b) and the third column (c) according to Eq. 4. We can see that the T-Net could segment the rough estimation of human main body, and automatically distinguish the de nite human edges where predicted unknown region is narrower and structural details where predicted unknown region is wider. In addition, with the help of the coarse prediction provided by T-Net, M-Net could concentrate on the transitional regions between foreground and background and predict more structural details of alpha ma e. Further, we combine the advantages of T-Net and M-Net and obtain a high quality alpha ma e with the aid of Fusion Module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Applying to real images</head><p>Since the images in our dataset are composited with annotated foregrounds and random backgrounds, to investigate the ability of our model to generalize to real-world images, we apply our model and other methods to plenty of real images for a qualitative analysis. Several visual results are shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. We nd that our method performs well on real images even with complicated backgrounds.   Note that the hair details of the woman in the rst image of <ref type="figure" target="#fig_4">Fig. 6</ref> are only recovered nicely by our method. Also, the ngers in the second image are blurred incorrectly by other methods, whereas our method distinguishes them well. Compsition examples of the foregrounds and new backgrounds with the help of automatically predicted alpha ma e are illustrated in the last column of <ref type="figure" target="#fig_4">Fig. 6</ref>. We can see these compositions have high visual quality. More results can be found in supplementary materials.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Semantic Human Matting (SHM) and its applications. SHM takes natural image (top le ) as input and outputs corresponding alpha matte (bottom le ). e predicted alpha matte could be applied to background editting (top right) and smart creative composition (bottom right) foreground F , background B and alpha ma e ? with a linear blend assumption: I = ?F + (1 ? ?)B, ? ? [0, 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Compositional images and corresponding alpha mattes in our dataset. e rst three columns come from the general matting dataset create by Xu et al.<ref type="bibr" target="#b27">[27]</ref> and the last three columns come from model images we collected from an e-commerce website. It's worth notting that images shown here are all resized to the same height.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>e visual comparison results on the semantic human matting testing dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Intermediate results visualization on a real image. (a) an input image, (b) trimap predicted by T-Net, (c) raw alpha matte predicted by M-Net, (d) fusion result according to Eq. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>e visual comparison results on the real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Con guration of our human matting dataset.</figDesc><table><row><cell>Data Source</cell><cell cols="4">Train Set #Foreground #Image #Foreground #Image Test Set</cell></row><row><cell>DIM[27]</cell><cell>182</cell><cell>18,200</cell><cell>20</cell><cell>400</cell></row><row><cell>Model</cell><cell>34,311</cell><cell>34,311</cell><cell>1,000</cell><cell>1,000</cell></row><row><cell>Total</cell><cell>34,493</cell><cell>52,511</cell><cell>1,020</cell><cell>1,400</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>e properties of the existing matting datasets.</figDesc><table><row><cell>Datasets</cell><cell>Foreground</cell><cell>#Image</cell><cell>Annotation</cell></row><row><cell>alpha ma ing [21]</cell><cell>35 Objects</cell><cell>35</cell><cell>Manually</cell></row><row><cell>Shen et al. [24]</cell><cell>2,000 Portraits</cell><cell>2,000</cell><cell>CF [16], KNN [4]</cell></row><row><cell>DIM [27]</cell><cell>493 Objects</cell><cell>49,300</cell><cell>Manually</cell></row><row><cell>Our dataset</cell><cell cols="2">35,513 Humans 52,511</cell><cell>Manually</cell></row><row><cell cols="2">35,311 images out of them.</cell><cell cols="2">e low pass rate (18.88 %)</cell></row><row><cell cols="4">guarantees the high standard of the alpha ma e in our</cell></row><row><cell>dataset.</cell><cell></cell><cell></cell><cell></cell></row></table><note>? Deep Image Matting (DIM) dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>e quantitative results on human matting testing dataset. e best results are emphasized in bold</figDesc><table><row><cell>Methods</cell><cell>SAD (?10 ?3 )</cell><cell>MSE (?10 ?3 )</cell><cell>Gradient (?10 ?5 )</cell><cell>Connectivity (?10 ?5 )</cell></row><row><cell>PSP50 Seg</cell><cell>14.821</cell><cell>11.530</cell><cell>52.336</cell><cell>44.854</cell></row><row><cell>PSP50 Reg</cell><cell>10.098</cell><cell>5.430</cell><cell>15.441</cell><cell>65.217</cell></row><row><cell>PSP50+CF [16]</cell><cell>8.809</cell><cell>5.218</cell><cell>21.819</cell><cell>43.927</cell></row><row><cell>PSP50+KNN [4]</cell><cell>7.806</cell><cell>4.390</cell><cell>20.476</cell><cell>56.328</cell></row><row><cell cols="2">PSP50+DCNN [5] 8.378</cell><cell>4.756</cell><cell>20.801</cell><cell>50.574</cell></row><row><cell>PSP50+IFM [1]</cell><cell>7.576</cell><cell>4.275</cell><cell>19.762</cell><cell>52.470</cell></row><row><cell>PSP50+DIM [27]</cell><cell>6.140</cell><cell>3.834</cell><cell>19.414</cell><cell>41.884</cell></row><row><cell>Our Method</cell><cell>3.833</cell><cell>1.534</cell><cell>5.179</cell><cell>36.513</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>e quantitative results of our method and several state-of-the-art matting methods that need trimap on the semantic human matting testing dataset.</figDesc><table><row><cell>Methods</cell><cell>SAD (?10 ?3 )</cell><cell>MSE (?10 ?3 )</cell><cell>Gradient (?10 ?5 )</cell><cell>Connectivity (?10 ?5 )</cell></row><row><cell>TrimapGT+CF</cell><cell>6.772</cell><cell>2.258</cell><cell>9.0390</cell><cell>34.248</cell></row><row><cell>TrimapGT+KNN</cell><cell>8.379</cell><cell>3.413</cell><cell>16.451</cell><cell>83.458</cell></row><row><cell cols="2">TrimapGT+DCNN 6.760</cell><cell>2.162</cell><cell>9.753</cell><cell>44.392</cell></row><row><cell>TrimapGT+IFM</cell><cell>5.933</cell><cell>1.798</cell><cell>8.290</cell><cell>54.257</cell></row><row><cell>TrimapGT+DIM</cell><cell>2.642</cell><cell>0.589</cell><cell>3.035</cell><cell>25.773</cell></row><row><cell>Our Method</cell><cell>3.833</cell><cell>1.534</cell><cell>5.179</cell><cell>36.513</cell></row><row><cell cols="5">on the human ma ing testing dataset. Trimaps are predicted by the</cell></row><row><cell cols="5">pre-trained T-Net and are provided to interactive ma ing methods.</cell></row><row><cell cols="3">e quantitative results are listed in</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of Di erent Components.</figDesc><table><row><cell>Methods</cell><cell>SAD (?10 ?3 )</cell><cell>MSE (?10 ?3 )</cell><cell>Gradient (?10 ?5 )</cell><cell>Connectivity (?10 ?5 )</cell></row><row><cell cols="2">no end-to-end 7.576</cell><cell>4.275</cell><cell>19.762</cell><cell>52.470</cell></row><row><cell>no Fusion</cell><cell>4.231</cell><cell>2.146</cell><cell>5.230</cell><cell>56.402</cell></row><row><cell>no L t</cell><cell>4.536</cell><cell>2.278</cell><cell>5.424</cell><cell>52.546</cell></row><row><cell>Our Method</cell><cell>3.833</cell><cell>1.534</cell><cell>5.179</cell><cell>36.513</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Implementations provided by their authors are used except for DIM. We implement the DIM network with the same structure as M-Net except with 4 input channels for a fair comparision.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">CONCLUSIONIn this paper, we focus on the human ma ing problem which shows a great importance for a wide variety of applications. In order to simultaneously capture global semantic information and local details, we propose to cascade a trimap network and a ma ing network, as well as a novel fusion module to generate alpha ma e automatically. Furthermore, we create a large high quality human ma ing dataset. Bene ing from the model structure and dataset, our automatic human ma ing achieves comparable results with state-of-the-art interactive ma ing methods.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank Jian Xu for many helpful discussions and valuable suggestions, and Yangjian Chen, Xiaowei Li, Hui Chen, Yuqi Chen for their support on developing the image labeling tool, and Min Zhou for some comments that improved the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing e ective inter-pixel information ow for natural image ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yag?z</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Tun? Ozan Ayd?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Z?rich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pa ern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical ow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">KNN ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingzeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2175" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural image ma ing using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inso</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="626" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on</title>
		<meeting>the 2001 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pa ern Recognition. II</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n. d.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Object Classes Challenge 2012 (VOC2012) Results</title>
		<ptr target="//www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shared Sampling for Real-Time Alpha Ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Eduardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel M</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random walks for interactive alpha-ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiwietz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of VIIP</title>
		<meeting>VIIP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
	<note>Shmuel Aharon, and R?diger Westermann</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pa ern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pa ern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pa ern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guided image ltering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pa ern recognition</title>
		<meeting>the IEEE conference on computer vision and pa ern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geo</forename><surname>Rey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlocal ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pa ern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2193" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pa ern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microso coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pa ern recognition</title>
		<meeting>the IEEE conference on computer vision and pa ern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic di erentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Large Kernel Ma ers-Improve Semantic Segmentation by Global Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02719</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A perceptually motivated online benchmark for image ma ing. In Computer Vision and Pa ern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Ro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1826" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">e SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">e IEEE Conference on Computer Vision and Pa ern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving image ma ing using comprehensive sampling sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Shahrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sco</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pa ern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="636" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep automatic portrait ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="92" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Poisson ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="315" to="321" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael F Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pa ern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep image ma ing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pa ern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pa ern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast Deep Ma ing for Portrait Animation on Mobile Phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinqiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
