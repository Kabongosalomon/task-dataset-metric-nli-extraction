<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Playing 2048 With Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilun</forename><surname>Li</surname></persName>
							<email>shilun@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Playing 2048 With Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The game of 2048 is a highly addictive game. It is easy to learn the game, but hard to master as the created game revealed that only about 1% games out of hundreds million ever played have been won. In this paper, we would like to explore reinforcement learning techniques to win 2048. The approaches we have took include deep Q-learning and beam search, with beam search reaching 2048 28.5% of time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years, reinforcement learning has been widely applied to play various strategy games involving uncertainty including but not limited to backgammon <ref type="bibr" target="#b3">(Tesauro 1995)</ref>, card games (Zha 2019), and go <ref type="bibr" target="#b1">(Silver, 2017)</ref>. As AlphaGo continuously defeated the world champion Jie Ke in the game of Go in 2016, reinforcement learning has been proved to able to achieve superhuman performance without prior human knowledge in strategy games.</p><p>Inspired by previous application of reinforcement learning, we aim to learn the best strategy to play the game 2048. 2048 is a single player name created by Gabriele Cirulli in 2014. The game is played on a 4 ? 4 grid. Each number displayed on the screen is a power of 2, ranging from 2 1 to 2 15 . A player can move all the tiles toward one direction, and two tiles will merge into their sum if they have the same value. Right after the move, a new tile will appear on the grid. The goal of this game is to get the tile with the number 2048 or higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature review</head><p>Previous papers have explored various ways to win 2048 without human knowledge. <ref type="bibr" target="#b0">Amar, et al. 2017</ref>, uses policy network to learn the best strategy to play 2048 without human knowledge. The policy network is a neural network that contains two convolutional layers with ReLU as activation function. The model takes the state of the game as input, and outputs a policy vector of length 4 with each entry representing how likely choosing a policy at the current state will lead to the highest score. To encourage exploration of different strategies, the paper also uses -greedy with as a decreasing function of the number of games played. However, the model doesn't serve to win the game: it reaches 1024 most of the time, and reaches 2048 occasionally. <ref type="bibr" target="#b2">Szubert, et al. 2014</ref>, models the 2048 game as an MDP, and applied temporal difference learning (TDL) on the MDP to learn the best policy. The model dwarfs the human performance by reaching 2048 with a rate of 97%. However, the model still has a hard time reaching larger tiles (never reached 32768 or higher), <ref type="bibr">so Yeh, et al. 2015</ref>, improves the model by applying multi-stage temporal difference learning with 3-ply expectimax search, and the new model reaches 32768 31.75% of the time.</p><p>In this paper, we would like to explore and expand the tools that could be used to win 2048 using what we have learned in class.  We use a numpy of size <ref type="bibr">(16,</ref><ref type="bibr">4,</ref><ref type="bibr">4)</ref> to represent the state. There are thus 16 ? 4 ? 4 = 256 entries in total for the state representation. The second and third dimention (4 ? 4) represents each tile in the game, and the first dimension (16) represents the number on each tile. The first dimension is 16 since each tile can hold a number 0 (which means there is no number in the tile) or 2 i where 0 &lt; i &lt; 16.</p><p>To give an example of the state representation, if the current state only has a tile 2 at position (3, 2), then the entry of numpy at (1, 3, 2) is 1, and all other entries are 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Action</head><p>There are 4 actions potentially available at each state: 1) move all the tiles to the left, 2) move all the tiles up, 3) move all the tiles to the right, and 4) move all the tiles down. However, not all the actions are available at each state. For instance, in the game in <ref type="figure">Figure 1</ref>, a player cannot move all the tiles down or to thee left. In our game, we represent all the available actions at a particular state with an list. The action list for the state in <ref type="figure">Figure 1</ref> would be <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> where 2 represent moving all the tiles up, and 3 represent moving all the tiles to the right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reward</head><p>The reward after performing an action at a state is the sum of numbers on all the merged tiles. For instance, if the we merged two tiles with number 8 and two tiles with number 2, then the reward is 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Source of uncertainty</head><p>The game 2048 is a strategy game with uncertainty. There are two sources of uncertainty, both of which are irreducible uncertainty:</p><p>? The first uncertainty comes from where the new tile will appear on the grid after the move.</p><p>The new tile can appear on a random empty block on the grid. The probability of it appearing on any empty block is the same.</p><p>? The second uncertainty comes from what number will appear on the new tile. The number on the tile is either 2 or 4. The probability for each number to appear is as follows: P(2 appears) = 0.90, P(4 appears) = 0.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline</head><p>Our baseline is to play the game by choosing a valid action at random at each state until we reach the end of the game. The strategy rarely reaches any tile above 128. (See <ref type="table" target="#tab_1">Table 1</ref> below for the performance of the algorithm.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep Q-learning</head><p>We have implemented Deep Q-learning Algorithm with -greedy exploration strategy to win the 2048 game. Deep Q-learning is the same as Q-learning except we use deep learning to evaluate the Q value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Evaluating the Q value</head><p>We tried two deep learning net to evaluate Q value, and both have similar performance.</p><p>The first net contains 4 layers (Model 1):</p><p>1. Layer 1: A linear layer that takes an array of size 256 as an input, and outputs an array of size hidden layer = 128. 256 is an array flattened from the numpy representing the current state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Layer 2:</head><p>A ReLU layer as an activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Layer 3:</head><p>A dropout layer to avoid over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Layer 4:</head><p>A linear layer that takes the output of the former layer and outputs an array of size 4. This array contains the Q-value of the 4 actions.</p><p>The second net contains 4 layers (Model 2):</p><p>1. Layer 1: Two convolutional layers exist in parallel in this layer. Each convolutional layer takes the state as an input and transforms it using a given kernel size. The two convolutional layers use different kernel sizes to stabilize the learning process.</p><p>2. Layer 2: Four convolutional layers exist in parallel in this layer. Each convolutional layer takes the output of the former layer and transforms it using a given kernel size. The four convolutional layers use different kernel sizes to stabilize the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Layer 3:</head><p>A dropout layer to avoid over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Layer 4:</head><p>A linear layer that takes as input the concatenation of former layer' outputs and outputs an array of size 4. This array contains the Q-value of the 4 actions.</p><p>Each model outputs an array of size 4, representing the Q-value of the 4 actions, but not all 4 actions are available for every state. In the case where a certain action is not valid at a particular state, we remove the Q-value of the invalid action from the array before selecting the action with the highest Q value.</p><p>In order to optimize the evaluation of Q value, we minimize the Mean Squared Error Loss</p><formula xml:id="formula_0">r t + max a Q(s t+1 , a) ? Q(s t , a t ) 2</formula><p>We keep track of the experience in a batch and back propagate when the batch size becomes 128 or the game ends. For each model, we trained for 1000 games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">-greedy search</head><p>-greedy search is a search algorithm that encourages exploration. is a number between 0 and 1. When selecting the best action to perform, we select a random number between 0 and 1. If the number is smaller than , we choose a random action regardless of the action's Q value. Otherwise, we choose the action with the highest Q value. We used the epsilon value 0.3 in both of our models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Beam search</head><p>Beam search is a greedy search algorithm that explores a tree by always choosing the k best nodes. In the game of 2048, each node in the tree is a state, and each edge is an action. There are two parameters key to the beam search algorithm:</p><p>? d: how many levels of the tree does the algorithm want to explore. In our algorithm, we have a relatively large d (d = 20) since a wrong action in 2048 usually don't results immediate end of the game, but takes effect later on. We thus want the algorithm to explore the states long after the action is performed to avoid a wrong action.</p><p>? k: The algorithm keeps a list of states with the best scores at each level. k is the upper limit for the number of states we can keep. In our algorithm, we have k = 10 to make the algorithm more likely to get out of local maximum.</p><p>At the beginning of the algorithm, it explores a list of states that can be reached using all valid actions at the current state. A random tile appears after all existing tiles have merged. As the position and the value of random tile varies, the same action at a particular state can reach scores of different states. However, evaluating all possible states takes significant computation time, and in most cases how the existing states merge after an action is much more important than the random tile in deciding whether the game ends. Thus, we randomly select one state that can be reached for each action available at the current state to construct the list of k best states. We follow the same rule when we expand this list later in the algorithm.</p><p>After the initialization of the list, the algorithm repeats the following steps for each depth of the tree. We update the list from last iteration with the random states that could be reached through all valid actions for each state in the list. We then evaluate the 'goodness' of the state using a heuristic function. (A good state means it is not close to an end state of the game.) We then select the k best states from the list according to their 'goodness' score to construct the new list. After iterating for the depth 20, we finally select the state with the highest 'goodness' score, and performs the initial action that leads to this best state.</p><p>Our heuristic function takes 4 factors into account when evaluating the the 'goodness' of the state:</p><p>1. How many empty tiles are there in the game: The end state of the game is when each tiles is occupied by a number. Thus, the more empty tiles there are, the better the state is.</p><p>2. The biggest number on any tile: The higher the number is, the better the state is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Smoothness:</head><p>Smoothness means whether there are any small numbers stuck in the middle of large numbers. It is hard to merge the stuck small numbers with other small numbers to get larger numbers. Thus, the smoother the state is, the better it is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Monotonicity:</head><p>When human play the game, we usually like to stack the highest number in the corner. For the other numbers, the closer the number is to the highest number, the larger it is. In this way, it is easier for humans to merge tiles. Monotonicity measures whether the above relationship holds in the state. Intuitively, the more monotonic the state is, the better it is. In this image, we used k = 2 and d = 2. Each edge represents an action valid at the previous stage, and each node represents a random state reached after performing the action. The number on each node is the heuristic 'goodness' score evaluated for the state. The gray nodes are the 2 nodes with the highest score stored at each depth. In this tree, we would finally perform the action 'up' since it leads to the nodes with the highest score in the last depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Note that the performance of beam search with a heuristic based on human strategy outperforms the deep Q-learning models which does not contain human knowledge. The reason behind the performance difference may be that the human strategy is very close to if not equal to the optimal strategy. We also notice that the strategy of the deep Q-learning models are approaching the human strategy as it trains. So if we can make a heuristic function of the state which incorporates how a human would evaluate the state, then a model with a simple search algorithm would have a rather good performance. On the other hand, since the deep Q-learning models does not have any prior knowledge of the game, it requires a long exploration process, therefore it may not have converged after 1000 games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>Since the deep Q-learning consists of 6 convolutional layers, it is rather complex and requires a decent amount of data to fully train. One of the future work for our project is training the deep Q-learning model with more data. In addition, since the prediction of the best action of a state should be consistent for any rotation of reflection of the grid. By incorporating rotations and reflections of the grid as states to train the model on, we can produce 8 times as much training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure</head><label></label><figDesc>Figure 1: Illustration of the game of 2048</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the beam search algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance of all Algorithms after 1000 runs Max Tile Random Play Deep Q-learning Model 1 Deep Q-learning Model 2 Beam Search</figDesc><table><row><cell>16</cell><cell>0.3 %</cell><cell>0 %</cell><cell>0 %</cell><cell>0 %</cell></row><row><cell>32</cell><cell>7.1 %</cell><cell>0 %</cell><cell>0 %</cell><cell>0 %</cell></row><row><cell>64</cell><cell>36.9 %</cell><cell>7.4 %</cell><cell>5.7 %</cell><cell>0 %</cell></row><row><cell>128</cell><cell>48.5 %</cell><cell>41.8 %</cell><cell>34.2 %</cell><cell>0 %</cell></row><row><cell>256</cell><cell>7.1 %</cell><cell>48.3 %</cell><cell>56.1 %</cell><cell>1.7 %</cell></row><row><cell>512</cell><cell>0.1 %</cell><cell>2.5 %</cell><cell>3.9 %</cell><cell>11.3 %</cell></row><row><cell>1024</cell><cell>0 %</cell><cell>0 %</cell><cell>0.1 %</cell><cell>58.1 %</cell></row><row><cell>2048</cell><cell>0 %</cell><cell>0 %</cell><cell>0 %</cell><cell>28.5 %</cell></row><row><cell>4096</cell><cell>0 %</cell><cell>0 %</cell><cell>0 %</cell><cell>0.4 %</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep Reinforcement Learning for 2048</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Amar</surname></persName>
		</author>
		<ptr target="http://www.mit.edu/?amarj/files/2048.pdf" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mastering the game of Go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature24270</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="page" from="354" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Temporal Difference Learning of N-Tuple Networks for the Game 2048</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Szubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Jaskowski</surname></persName>
		</author>
		<ptr target="http://www.cs.put.poznan.pl/wjaskowski/pub/papers/Szubert2014_2048.pdf" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal Difference Learning and TD-Gammon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<ptr target="https://cling.csd.uwo.ca/cs346a/extra/tdgammon.pdf" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-Stage Temporal Difference Learning for 2048-like Games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kun-Hao</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1606.07374.pdf" />
	</analytic>
	<monogr>
		<title level="j">TCIAIG</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
