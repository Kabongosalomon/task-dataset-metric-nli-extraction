<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duolikun</forename><surname>Danier</surname></persName>
							<email>duolikun.danier@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
							<email>fan.zhang@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bull</surname></persName>
							<email>dave.bull@bristol.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ST-MFNet: A Spatio-Temporal Multi-Flow Network for Frame Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video frame interpolation (VFI) is currently a very active research topic, with applications spanning computer vision, post production and video encoding. VFI can be extremely challenging, particularly in sequences containing large motions, occlusions or dynamic textures, where existing approaches fail to offer perceptually robust interpolation performance. In this context, we present a novel deep learning based VFI method, ST-MFNet, based on a Spatio-Temporal Multi-Flow architecture. ST-MFNet employs a new multi-scale multi-flow predictor to estimate many-to-one intermediate flows, which are combined with conventional one-to-one optical flows to capture both large and complex motions. In order to enhance interpolation performance for various textures, a 3D CNN is also employed to model the content dynamics over an extended temporal window. Moreover, ST-MFNet has been trained within an ST-GAN framework, which was originally developed for texture synthesis, with the aim of further improving perceptual interpolation quality. Our approach has been comprehensively evaluated -compared with fourteen stateof-the-art VFI algorithms -clearly demonstrating that ST-MFNet consistently outperforms these benchmarks on varied and representative test datasets, with significant gains up to 1.09dB in PSNR for cases including large motions and dynamic textures. Our source code is available at https://github.com/danielism97/ST-MFNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video frame interpolation (VFI) has been extensively employed to deliver an improved user experience across a wide range of important applications. VFI increases the temporal resolution (frame rate) of a video through synthesizing intermediate frames between every two consecutive original frames. It can mitigate the need for costly high frame rate acquisition processes <ref type="bibr" target="#b26">[29]</ref>, enhance the rendering of slow-motion content <ref type="bibr" target="#b25">[28]</ref>, support view synthesis <ref type="bibr" target="#b14">[17]</ref> and improve rate-quality trade-offs in video coding <ref type="bibr" target="#b55">[58]</ref>. In recent years, deep learning has empowered a variety of VFI algorithms. These methods can be categorized as flow-based <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b58">61]</ref> or kernel-based <ref type="bibr" target="#b29">[32,</ref><ref type="bibr" target="#b41">44]</ref>. While flowbased methods use the estimated optical flow maps to warp input frames, kernel-based methods learn local or shared convolution kernels for synthesizing the output. To handle challenging scenarios encountered in VFI applications, various techniques have been employed to enhance these methods, including non-linear motion models <ref type="bibr" target="#b43">[46,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b58">61]</ref>, coarse-to-fine architectures <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b63">66]</ref>, attention mechanisms <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b26">29]</ref>, and deformable convolutions <ref type="bibr" target="#b18">[21,</ref><ref type="bibr" target="#b29">32]</ref>.</p><p>Although these methods have significantly improved performance compared with conventional VFI approaches <ref type="bibr" target="#b1">[4]</ref>, their performance can still be inconsistent, especially for content exhibiting large motions, occlusions and dynamic textures. Large motion typically means large pixel displacements, which are difficult to capture using Convolutional Neural Networks (CNNs) with limited receptive fields <ref type="bibr" target="#b40">[43,</ref><ref type="bibr" target="#b41">44]</ref>. In the case of occlusion, pixels relating to occluded objects will not appear in all input frames, thus preventing interpolation algorithms from accurately estimating the intermediate locations of those pixels <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b26">29]</ref>. Finally, dynamic textures (e.g. water, fire, foliage, etc.) exhibit more complex motion characteristics compared to the movements of rigid objects <ref type="bibr" target="#b11">[14,</ref><ref type="bibr" target="#b62">65]</ref>. Typically, they are spatially irregular and temporally stochastic, causing most existing VFI methods to fail, especially those based on optical flow <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b31">34]</ref>.</p><p>To solve these problems, we propose a novel video frame interpolation model, the Spatio-Temporal Multi-Flow Network (ST-MFNet), which decouples the handling of large and complex motions using single-and multi-flows respectively in a multi-branch structure to offer improved interpolation performance across a wide range of content types. Specifically, ST-MFNet employs a two-stage architecture, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In Stage I, the Multi-InterFlow Network (MIFNet) first predicts multi-interflows <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b29">32]</ref> at multiple scales (including an up-sampling scale simulating sub-pixel motion estimation), using a customized CNN architecture, UMSResNext, with variable kernel sizes. The multi-flows here correspond to a many-to-one mapping which enables more flexible transformation, facilitating the modeling of complex motions. To further improve the performance for large motions, a Bi-directional Linear Flow Network (BLFNet) is employed to linearly approximate the intermediate flows based on the bi-directional flows between input frames, which are estimated using a coarseto-fine architecture <ref type="bibr" target="#b51">[54]</ref>. In the second stage, inspired by recent work on texture synthesis <ref type="bibr" target="#b56">[59,</ref><ref type="bibr" target="#b61">64]</ref>, we integrate a 3D CNN, Texture Enhancement Network (TENet) that performs spatial and temporal filtering to capture longer-range dynamics and to predict textural residuals. Finally, we trained our model based on the ST-GAN <ref type="bibr" target="#b61">[64]</ref> methodology, which was originally proposed for texture synthesis. This ensures both spatial consistency and temporal coherence of interpolated content. Extensive quantitative and qualitative studies have been performed which demonstrate the superior performance of ST-MFNet over current state-of-the-art VFI methods on a wide range of test data including large and complex motions and dynamic textures.</p><p>The primary contributions of this work are: </p><formula xml:id="formula_0">? A novel</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we summarize recent advances in video frame interpolation (VFI) and then briefly introduce examples of dynamic texture synthesis, which have inspired the development of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Frame Interpolation</head><p>Most existing VFI methods can be classified as: Flow-based VFI. This class typically involves two steps: optical flow estimation and image warping. Input frames, I 1 and I 2 , are warped to a target temporal location t based on either the intermediate optical flows F t?1 , F t?2 (backward warping <ref type="bibr" target="#b24">[27]</ref>), or F 1?t , F 2?t (forward warping <ref type="bibr" target="#b38">[41]</ref>). These flows can be approximated from bidirectional optical flows (F 1?2 and F 2?1 ) between the input frames <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b3">6,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b58">61]</ref>. Such approximations often assume motion linearity, and hence are prone to errors in non-linear motion scenarios. Various efforts have been made to alleviate this issue, including the use of depth information <ref type="bibr" target="#b2">[5]</ref>, higher order motion models <ref type="bibr" target="#b30">[33,</ref><ref type="bibr" target="#b58">61]</ref>, and adaptive forward warping <ref type="bibr" target="#b39">[42]</ref>. A second group of methods <ref type="bibr" target="#b22">[25,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b43">46,</ref><ref type="bibr" target="#b59">62,</ref><ref type="bibr" target="#b63">66]</ref> have been developed to improve approximation by directly predicting intermediate flows.</p><p>These approaches typically employ a coarse-to-fine architecture, which supports a larger receptive field for capturing large motions. In all of the above methods, the predicted flows correspond to a one-to-one pixel mapping, which inherently limits the ability to capture complex motions. Kernel-based VFI. In these methods, various convolution kernels <ref type="bibr">[10-12, 15, 21, 29, 32, 35, 43, 44, 50]</ref> are learned as a basis for synthesizing interpolated pixels. Earlier approaches <ref type="bibr" target="#b40">[43,</ref><ref type="bibr" target="#b41">44]</ref> predict a fixed-size kernel for each output location, which is then convolved with co-located input pixels. This limits the magnitude of captured motions to the kernel size used, while more memory and computational capacity are required when larger kernel sizes are adopted. To overcome this problem, deformable convolution (DefConv) <ref type="bibr" target="#b10">[13]</ref> was adapted to VFI in AdaCoF <ref type="bibr" target="#b29">[32]</ref>, which allows kernels to be convolved with any input pixels pointed by local offset vectors. This can be considered as multi-interflows, representing a many-to-one mapping. Further improvements to AdaCoF have been achieved by allowing space-time sampling <ref type="bibr" target="#b47">[50]</ref>, feature pyramid warping <ref type="bibr" target="#b12">[15]</ref>, and using a coarse-to-fine architecture <ref type="bibr" target="#b7">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dynamic Texture Synthesis</head><p>Dynamic textures (e.g. water, fire, leaves blowing in the wind etc.) generally exhibit high spatial frequency energy alongside temporal stochasticity, with inter-frame motions irregular in both the spatial and temporal domains. Classic synthesis methods rely on mathematical models such as Markov random fields <ref type="bibr" target="#b54">[57]</ref> and auto-regressive moving av-erage model <ref type="bibr" target="#b13">[16]</ref> to capture underlying motion characteristics. More recently, deep learning techniques, in particular 3D CNNs and GAN-based training <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b53">56,</ref><ref type="bibr" target="#b56">59,</ref><ref type="bibr" target="#b60">63,</ref><ref type="bibr" target="#b61">64]</ref>, have been adopted to achieve more realistic synthesis results. It should be noted that both dynamic texture synthesis and VFI require accurate modeling of spatio-temporal characteristics. However the techniques developed specifically for texture synthesis have not yet been fully exploited in VFI methods. This is a focus of our work. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture of ST-MFNet. While conventionally VFI is formulated as generating the intermediate frame I t (t = 1.5) between two given consecutive frames I 1 , I 2 , we instead employ two more frames I 0 , I 3 to improve the modeling of motion dynamics. Given the frames I 0 , I 1 , I 2 , I 3 , our model first processes I 1 , I 2 in two branches. The Multi-InterFlow Network (MIFNet) branch estimates the multi-scale multi-flows from I t to I 1 , I 2 , where the many-to-one pixel correspondence allows complex transformation, benefiting interpolation of highly complex motion, such as dynamic textures (e.g. water, fire etc.). As the fixed receptive field of MIFNet may lead to limited ability to capture large motion, we also included the Bi-directional Linear Flow Network (BLFNet) branch to approximate one-to-one optical flows from I 1 , I 2 to I t using a coarse-to-fine approach, enhancing large motion capturing. The input frames are warped based on the flows generated by MIFNet and BLFNet, and then fused by the Multi-Scale Fusion module to obtain an intermediate result I t . This multi-branch structure combines the advantages of both single-flow and multi-flow based methods and was found to offer enhanced interpolation performance. In the second stage,? t is combined with all the inputs I 0 , I 1 , I 2 , I 3 in temporal order and fed into the Texture Enhancement Network (TENet), which captures longer-range dynamic and generates residual signals for the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method: ST-MFNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-InterFlow Network</head><p>Multi-InterFlow warping. For self-completeness, we first briefly describe the multi-interflow warping operation <ref type="bibr" target="#b29">[32]</ref>. Given two images I A , I B with size H ? W , conventional optical flow F A?B = (f x , f y ) from I A to I B specifies the x-and y-components of pixel-wise offset vectors, where f x , f y ? R H?W . The pixel value at each location (x, y) of the corresponding backwarped <ref type="bibr" target="#b24">[27]</ref>? A is defined a?</p><formula xml:id="formula_1">I A (x, y) = I B (x + f x (x, y), y + f y (x, y))<label>(1)</label></formula><p>where the values at non-integer grid locations are obtained via bilinear interpolation. The multi-interflow proposed in <ref type="bibr" target="#b29">[32]</ref> can be defined as G A?B = (?, ?, w), but now ?, ? ? R H?W ?N represent a collection of the x-and y-components of N flow vectors respectively and w ? [0, 1] H?W ?N is their weighting kernels ( N i=1 w(x, y, i) = 1). That is, for each location (x, y), G A?B contains N flow vectors and N weights. The corresponding warping is defined as follows.</p><formula xml:id="formula_2">IA(x, y) = N i=1</formula><p>w(x, y, i)?IB(x+?(x, y, i), y+?(x, y, i)) (2) Such multi-flow warping corresponds to a many-to-one mapping, which allows flexible sampling of source pixels, enabling the capture of more complex motions.</p><p>Given input frames I 1 , I 2 , the MIFNet predicts the multiinterflows {G l t?1 , G l t?2 } from the intermediate frame I t to the inputs at three scale levels: l = ?1, 0, 1, where l = i means spatial down-sampling by 2 i (i.e. l = ?1 denotes upsampling), so that re-sampled inputs I l 1 , I l 2 can be warped to time t using Equation (9) to produce? l t1 ,? l t2 respectively. Architecture. <ref type="figure">Figure 2</ref> (a) shows the architecture of the MIFNet. In order to capture pixel movements at multiple scales, we devise a U-Net style feature extractor, U-MultiScaleResNext (UMSResNext), consisting of eight MSResNext blocks (shown in <ref type="figure">Figure 3</ref>). Each MSResNext block employs two ResNext blocks <ref type="bibr" target="#b57">[60]</ref> in parallel with different kernel sizes in the middle layer, 3?3 and 7?7, which further increases the network cardinality <ref type="bibr" target="#b34">[37,</ref><ref type="bibr" target="#b57">60]</ref>. The outputs of these two ResNext blocks are then concatenated and connected to a channel attention module <ref type="bibr" target="#b21">[24]</ref>, which learns adaptive weighting of the feature maps extracted by the two ResNext blocks. Such feature selection mechanism has also been found to enhance motion modeling <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b26">29]</ref>. In UM-SResNext, the up-sampling operation is performed by replacing the k?k grouped convolutions in the middle layer with (k+1)?(k+1) grouped transposed convolutions.</p><p>The features extracted by UMSResNext are then passed to the multi-flow heads for multi-interflow prediction. In contrast to <ref type="bibr" target="#b29">[32]</ref>, multi-flows here are predicted at various scales l = ?1, 0, 1, and occlusion maps are not generated (occlusion is handled by the BLFNet). As shown in <ref type="figure">Figure 2</ref> (b), each multi-flow head contains 6 sub-branches, predicting the x-, y-components (?, ?) and the kernel weights (w) of G l t?1 , G l t?2 . The predicted flows are then used to backwarp the inputs I 1 , I 2 at corresponding scales using Equation <ref type="bibr" target="#b6">(9)</ref>. Here a bilinear filter is used for down-sampling input frames, and an 8-tap filter originally designed for subpixel motion estimation <ref type="bibr" target="#b50">[53]</ref> is employed for up-sampling. The down-sampled scale used here encourages the motion search in a larger region, while the up-sampled scale allows the motion vectors to point to finer sub-pixel locations, increasing the precision of multi-flow warping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bi-directional Linear Flow Network</head><p>To improve large motion interpolation, bi-directional flows F 1?2 , F 2?1 between inputs I 1 , I 2 are also predicted using a pre-trained flow estimator <ref type="bibr" target="#b51">[54]</ref>, which is based on a coarse-to-fine architecture. The intermediate flows are then linearly approximated as follows.</p><formula xml:id="formula_3">F 1?t = 0.5F 1?2 F 2?t = 0.5F 2?1 (3)</formula><p>According to the intermediate flows, the frames I 1 , I 2 are forward warped using the efficient softsplat operator <ref type="bibr" target="#b39">[42]</ref>, which learns occlusion-related softmax-alike weighting of reference pixels in the forward warping process. Another advantage of softsplat is that it is differentiable, allowing the flow estimator to be end-to-end optimized. Finally, BLFNet branch outputs warped frames? soft t1 ,? soft t2 . The employment of the BLFNet branch was found to be essential for handling large motion and occlusion and improving the overall capacity of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-Scale Fusion Module</head><p>The Multi-Scale Fusion Module is employed to produce an intermediate interpolation result using the frames warped at multiple scales in the previous steps. Here we adopt the  GridNet <ref type="bibr" target="#b15">[18]</ref> architecture due to its superior performance on fusing multi-scale information <ref type="bibr" target="#b38">[41,</ref><ref type="bibr" target="#b39">42]</ref>. The GridNet is configured here to have 4 columns and 3 rows, with the first, second and third rows corresponding to scales of l = ?1, 0, 1 respectively. The first and third rows take</p><formula xml:id="formula_4">{? ?1 t1 ,? ?1 t2 } and {? 1 t1 ,? 1 t2 } as inputs, while the second row takes {? 0 t1 ,? 0 t2 ,? soft t1 ,? soft t2 },</formula><p>where {?} denotes channel-wise concatenation. Finally, this module outputs the intermediate result? t at the original spatial resolution (l = 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Texture Enhancement Network</head><p>At the end of the first stage, the output of the Multi-Scale Fusion module,? t , is concatenated with four original inputs to form {I 0 , I 1 ,? t , I 2 , I 3 }, which are then fed into the Texture Enhancement Network (TENet). Including additional frames here allows better modeling of higher-order motions and also provides more information on longer-term spatiotemporal characteristics. Motivated by recent work in dynamic texture synthesis <ref type="bibr" target="#b56">[59,</ref><ref type="bibr" target="#b61">64]</ref>, where spatio-temporal filtering was found to be effective for generating coherent video textures, we integrate a 3D CNN for texture enhancement. This CNN architecture (shown in <ref type="figure" target="#fig_3">Figure 4</ref>) is a modified version of the network developed in <ref type="bibr" target="#b26">[29]</ref>, but with reduced layer widths. This is based on the consideration that the intermediately warped frame? t has already been produced which is relatively close to the target. It is different from the original scenario in <ref type="bibr" target="#b26">[29]</ref>, where the network is expected to directly synthesize the interpolated output using the four original input frames. Finally, the TENet is expected to output a residual signal containing textural difference between? t and the target frame, which contributes to the final output of ST-MFNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Functions</head><p>We trained two versions of ST-MFNet in this work. For the distortion oriented model, a Laplacian pyramid loss <ref type="bibr" target="#b5">[8]</ref> (L lap ) was used as the objective function. This model was further fine-tuned using an ST-GAN based perceptual loss (L p ) to obtain the perceptually optimized version. Laplacian pyramid loss. ST-MFNet was trained end-toend by matching its output I out t with the ground-truth intermediate frame I gt t using the Laplacian pyramid loss <ref type="bibr" target="#b5">[8]</ref>, which has been previously used for VFI in <ref type="bibr" target="#b30">[33,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b39">42]</ref>. The loss function is defined below.</p><formula xml:id="formula_5">L lap = S s=1 2 s?1 L s (I out t ) ? L s (I gt t ) 1<label>(4)</label></formula><p>Here L s (I) denotes the s th level of the Laplacian pyramid of an image I, and S is the maximum level. Spatio-temporal adversarial loss. To further improve the perceptual quality of the ST-MFNet output, we also trained our model using the Spatio-Temporal Generative Adversarial Networks (ST-GAN) training methodology <ref type="bibr" target="#b61">[64]</ref>. Different from the conventional GAN <ref type="bibr" target="#b17">[20]</ref> focusing on a single image, the discriminator D here also processes adjacent video frames which improves temporal consistency. This is key for video frame interpolation. The architecture of the discriminator is provided in Appendix A. This discriminator was trained with the following loss.</p><formula xml:id="formula_6">L D = ? log(1?D(I out t , I 1 , I 2 ))?log(D(I gt t , I 1 , I 2 )) (5)</formula><p>The corresponding adversarial loss for the generator (ST-MFNet) is given below.</p><formula xml:id="formula_7">L adv = ? log(D(I out t , I 1 , I 2 ))<label>(6)</label></formula><p>This is then combined with the Laplacian pyramid loss to form the perceptual loss for ST-MFNet fine-tuning,</p><formula xml:id="formula_8">L p = L lap + ?L adv<label>(7)</label></formula><p>where ? is a weighting hyper-parameter that controls the perception-distortion trade-off <ref type="bibr" target="#b4">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head><p>Implementation details. In our implementation, we set the number of flows N = 25 (the default value in <ref type="bibr" target="#b29">[32]</ref>) for the MIFNet branch. The maximum level S for L lap was set to 5, and the weighting hyper-parameter ? = 100. We used the AdaMax optimizer <ref type="bibr" target="#b27">[30]</ref> with ? 1 = 0.9, ? 2 = 0.999. The learning rate was set to 0.001 and reduced by a factor of 0.5 whenever the validation performance stops improving for 5 epochs. The pre-trained flow estimator <ref type="bibr" target="#b51">[54]</ref> in the BLFNet branch was frozen for the first 60 epochs and then fine-tuned for 10 more epochs to further improve VFI performance.</p><p>The network was trained for a total number of 70 epochs using a batch size of 4. All training and evaluation were executed with NVIDIA P100 [?] and RTX 3090 GPUs. Training datasets. We used the training split of Vimeo-90k (septuplet) dataset <ref type="bibr" target="#b59">[62]</ref> which contains 91,701 frame septuplets (448?256). As Vimeo-90k was produced with constrained motion magnitude and complexity, to further enhance the VFI performance on large motion and dynamic textures, we used an additional dataset, BVI-DVC <ref type="bibr" target="#b33">[36]</ref>, which covers a wide range of texture/motion types, frame rates (24 to 120 FPS) and spatial resolutions (2160p, 1080p, 540p, and 270p). We randomly sampled 12800, 6400, 800, 800 septuplets from videos at each of these resolutions respectively, leaving out a subset of frames for validation. We augmented all septuplets from both datasets by randomly cropping 256?256 patches and performing flipping and temporal order reversing. This resulted in more than 100,000 septuplets of 256?256 patches. In each septuplet, the 1 st , 3 rd , 5 th and 7 th frames were used as inputs and the 4 th as the training target. The test split of Vimeo-90k together with unused subset of BVI-DVC was utilized as the validation set for hyper-parameter tuning and training monitoring. Evaluation dataset. Since our model takes four frames as input, the evaluation dataset should be able to provide frame quintuplets I 0 , I 1 , I gt t , I 2 , I 3 (t = 1.5). In this work, we used the test quintuplets in <ref type="bibr" target="#b58">[61]</ref>, which were extracted from the UCF-101 <ref type="bibr" target="#b49">[52]</ref> (100 quintuplets) and DAVIS <ref type="bibr" target="#b44">[47]</ref> (2847 quintuplets) datasets. We also evaluated on the SNU-FILM dataset <ref type="bibr" target="#b9">[12]</ref>, which specifies a list of 310 triplets at four motion magnitude levels. As original sequences are provided in the SNU-FILM dataset, we extended its pre-defined test triplets into quintuplets for the evaluation here.</p><p>To further test interpolation performance on various texture types, we developed a new test set, VFITex, which contains twenty 100-frame UHD or HD videos at 24, 30 or 50 FPS, collected from the Xiph <ref type="bibr" target="#b36">[39]</ref>, Mitch Martinez Free 4K Stock Footage <ref type="bibr" target="#b0">[1]</ref>, UVG database <ref type="bibr" target="#b35">[38]</ref> and the Pexels website [2]. This dataset covers diverse textured scenes, including crowds, flags, foliage, animals, water, leaves, fire and smoke. Based on the computational capacity available, we center-cropped HD patches from the UHD sequences, preserving the original UHD characteristics. All frames in each sequence were used for evaluation, totaling 940 quintuplets. More details of the training and evaluation datasets and their license information are provided in Appendix I. Evaluation Methods. Two most commonly used quality metrics, PSNR and SSIM <ref type="bibr" target="#b52">[55]</ref>, were employed here for objective assessment of the interpolated content. We note that these metrics do not always correlate well with video quality as perceived by a human observer <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b26">29]</ref>. Therefore, a more perceptually motivated metric, LPIPS <ref type="bibr" target="#b64">[67]</ref>, were used. Furthermore, in order to directly compare the perceptual quality of the video frames interpolated by our method  and the benchmark references, a user study was conducted based on a psychophysical experiment. The details of the user study are described in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Analysis</head><p>In this section, we analyze our proposed model through ablation studies, and compare it with 14 state-of-the-art methods both quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>The key ablation study results are summarized in Table 3, where five versions of ST-MFNet have been evaluated. <ref type="figure" target="#fig_6">Figure 9</ref> provides a visual comparison between the frames generated by each test variant and the full ST-MFNet model. Additional ablation study results, visualizations and analyses are available in Appendix B and C. MIFNet and BLFNet branches. To verify that the MIFNet and BLFNet branches are both effective, two variants of ST-MFNet, Ours-w/o MIFNet and Ours-w/o BLFNet, were created by removing the two branches respectively. Both variants were trained and evaluated using the same configurations described above. It is observed that, firstly, both Oursw/o MIFNet and Ours-w/o BLFNet achieve lower overall performance compared to the full ST-MFNet (Ours). Sec-ondly, compared to Ours, the model performance on large motion (DAVIS) drops more significantly when BLFNet is removed, and that on complex motion (VFITex) degrades more severely when MIFNet is removed. It can also be observed in <ref type="figure" target="#fig_6">Figure 9</ref>, for the case without MIFNet (sub- <ref type="figure">figures  (a-d)</ref>), that the model fails to capture the complex motion of the wave. When BLFNet was removed from the original ST-MFNet (sub-figures (e-h)), the occluded region which is also undergoing a large movement has not been interpolated properly. These observations mean that the contribution of each branch aligns well with our original motivation, hence implying the unique advantages of both multiflow (for complex motion) and single-flow (for large motion) branches have been enabled.</p><p>UMSResNext for multi-flow estimation. To measure the efficacy of the new UMSResNext, we replaced the UM-SResNext described in Section 3.1 with the U-Net used in <ref type="bibr" target="#b29">[32]</ref> to predict similar multi-flows. This is denoted as Ours-unet. As shown in <ref type="table">Table 3</ref>, ST-MFNet with UMSRes-Next achieves enhanced performance on all test sets, and this is also demonstrated by the visual comparison example in <ref type="figure" target="#fig_6">Figure 9</ref> (i-l). Another advantage of UMSResNext is that it has much fewer parameters (?4M) than U-Net (?21M).</p><p>Texture Enhancement. The importance of the TENet was also analyzed by training another variant Ours-w/o TENet, where the TENet is removed. <ref type="table">Table 3</ref> shows that there is a significant performance decrease compared to the full version, especially on DAVIS and VFITex. This demonstrates the contribution of the spatio-temporal filtering on frames over a wider temporal window for content with large and complex motions. <ref type="figure" target="#fig_6">Figure 9</ref> (m-p) also shows an example, where the full ST-MFNet with the TENet produces richer textural detail compared to the version without TENet. ST-GAN. To investigate the effectiveness of the ST-GAN training, we compared the perceptual quality of the interpolated content generated by the fine-tuned network Ours-L p and the distortion-oriented model Ours-L lap . We also replaced the ST-GAN with two existing GANs used for VFI, FIGAN <ref type="bibr" target="#b29">[32]</ref> and TGAN <ref type="bibr" target="#b46">[49]</ref>. Example frames produced by these variants are shown in <ref type="figure" target="#fig_6">Figure 9</ref> (q-v), where the result generated by Ours-L p exhibits sharper edges and clearer structures compared to those produced by other variants. Quantitative evaluation results of these variants based on LPIPS are provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Evaluation</head><p>We compared the proposed ST-MFNet with 14 state-ofthe-art VFI models including DVF <ref type="bibr" target="#b31">[34]</ref>, SuperSloMo <ref type="bibr" target="#b25">[28]</ref>, SepConv <ref type="bibr" target="#b41">[44]</ref>, DAIN <ref type="bibr" target="#b2">[5]</ref>, BMBC <ref type="bibr" target="#b42">[45]</ref>, AdaCoF <ref type="bibr" target="#b29">[32]</ref>, FeFlow <ref type="bibr" target="#b18">[21]</ref>, CDFI <ref type="bibr" target="#b12">[15]</ref>, CAIN <ref type="bibr" target="#b9">[12]</ref>, Softsplat <ref type="bibr" target="#b39">[42]</ref>, EDSC <ref type="bibr" target="#b8">[11]</ref>, XVFI <ref type="bibr" target="#b48">[51]</ref>, QVI <ref type="bibr" target="#b58">[61]</ref> and FLAVR <ref type="bibr" target="#b26">[29]</ref>. For fair comparison, we re-trained all benchmark models with the same training and validation datasets used for ST-MFNet under identical training configurations. The comprehensive evaluation results are summarized in <ref type="table" target="#tab_2">Table 2</ref>. For all these models, we additionally evaluated their pre-trained versions provided in the original literature (where applicable). If the pre-trained results are better than the re-trained counterparts, the former are presented and underlined.</p><p>Two key observations can be made from <ref type="table" target="#tab_2">Table 2</ref>. Firstly, by using our training set (Vimeo-90k+BVI-DVC), the retrained performance of all compared models has been improved over their pre-trained versions on large and complex motions, i.e. on DAVIS, SNU-FILM (medium, hard, ex-treme) and VFITex. For seven models, the pre-trained versions achieved higher PSNR and SSIM values on the UCF-101 dataset. This may be due to the similar characteristics between their pre-training dataset, Vimeo-90k and UCF-101. We also noted that our ST-MFNet offers the best results for DAVIS, SNU-FILM (all subsets) and VFITex, with a significant improvement of 0.36-1.09dB (PSNR) over the runner-up for each test set. It is only outperformed by the pre-trained FLAVR on UCF101 with marginal difference of 0.005dB (PSNR) and 0.001 (SSIM). This demonstrates the excellent generalization ability of the proposed ST-MFNet. Additional evaluation results in terms of LPIPS, and results on 4?/8? interpolation can be found in Appendix E. Complexity. The model complexity was measured on the 480p sequences from DAVIS test set. The average runtime (RT, in seconds) for interpolating one frame is reported in <ref type="table" target="#tab_2">Table 2</ref> for each tested network, alongside its total number of parameters. We noticed that ST-MFNet has a relatively high computational complexity among all tested models. The reduction of model complexity remains one of our future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Qualitative Evaluation</head><p>Visual comparisons. Examples frames interpolated by our model and several best-performing state-of-the-art methods are shown in <ref type="figure">Figure 6</ref>. It can be observed that the results generated by the perceptually trained ST-MFNet (Ours-L p ) are closer to the ground truth, containing fewer visual artifacts and exhibiting better perceptual quality. User Study. As single frames cannot fully reflect the perceptual quality of interpolated content, we conducted a user study where our method was compared against three competitive benchmark approaches, QVI, FLAVR and Softsplat (re-trained using its original perceptual loss <ref type="bibr" target="#b39">[42]</ref>). For this study, 20 videos randomly selected from DAVIS, SNU-FILM and VFITex were used as the test content for the three tested models. In each trial of a test session, participants were shown a pair of videos including one interpolated by perceptually optimized ST-MFNet and the other one by QVI, FLAVR or Softsplat. This results in a total of 60 trials in each test session. The order of video presentation was randomized in each trial (the order of trials was also random), and the subject in each case was asked to choose the sequence with higher perceived quality. Twenty subjects were paid to participate in this study. See more details of the user study in Appendix G.</p><p>The collected user study results are summarized in <ref type="figure">Figure 7</ref>. We observed that approaching 70% of users on average preferred ST-MFNet against QVI, and this figure is statistically significant for 95% confidence based on a t-test experiment (p &lt; .00000003). The average preference difference between our method and FLAVR is smaller, with 56% users in favor of ST-MFNet results. This was also significant at a 95% confidence level (p &lt; .0001). Finally, when comparing against Softsplat, around 60% of subjects favored our method, where the significance holds again at 95% level (p &lt; .000005).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations and Potential Negative Impacts</head><p>Although superior interpolation performance has been observed from the proposed method, we are aware of the relatively low inference speed associated with this model. This is mainly due to its large network capacity. Training such large models can also potentially introduce negative impact on the environment due to the significant power consumption of computational hardware <ref type="bibr" target="#b28">[31]</ref>. This can be mitigated through model complexity reduction based on network compression <ref type="bibr" target="#b6">[9]</ref> and knowledge distillation <ref type="bibr" target="#b19">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose a novel video frame interpolation algorithm, ST-MFNet, which consistently achieved improved interpolation performance (up to a 1.09dB PSNR gain) over state-of-the-art methods on various challenging video content. The proposed method features three main innovative design elements. Firstly, flexible many-to-one multi-flows were combined with conventional one-to-one optical flows in a multi-branch fashion, which enhances the ability of capturing large and complex motions. Secondly, a novel architecture was designed to predict multi-interflows at multiple scales, leading to reduced complexity but enhanced performance. Thirdly, we employed a 3D CNN architecture and the ST-GAN originally proposed for texture synthesis to enhance the visual quality of textures in the interpolated content. Our quantitative and qualitative experiments showed that all of these contribute to the final performance of our model, which consistently outperforms many state-of-the-art methods with significant gains.</p><p>The architecture of the discriminator employed in this work is illustrated in <ref type="figure">Figure 8</ref>; this was originally designed to train ST-GAN <ref type="bibr" target="#b61">[64]</ref> for texture synthesis. It contains a temporal and a spatial branch. The former takes the differences between the interpolated output I out t (where t = 1.5) of ST-MFNet and its two adjacent original frames I 1 , I 2 as input. The differences here represent the high-frequency temporal information within these three frames. The spatial branch in this network processes the ST-MFNet output I out t to generate spatial features. Finally, the temporal and spatial features generated in these two branches are concatenated before fed into the final fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Ablation Study Results</head><p>In the main paper, we presented key ablation study results where the primary contributions in the proposed ST-MFNet are evaluated. Here the effectiveness of the upsampling scale is further investigated, which has been employed during the multi-flow prediction in the MIFNet branch (see Section 3.1 of the main paper). In addition, we present the quantitative ablation study results for the ST-GAN in terms of a perceptually-oriented metric, the Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b64">[67]</ref>. Up-sampling. To evaluate the contribution of the upsampling scale during the multi-flow prediction, the version of ST-MFNet (Ours-w/o US) with only two multi-flow estimation heads (at l = 0, 1 scales) were implemented. It was also trained and evaluated using the same configurations described in the main paper. Its interpolation results are summarized in <ref type="table">Table 3</ref> alongside more comprehensive ablation study results for the other four variants of ST-MFNet (described in the main paper). It can be observed that Oursw/o US was outperformed by the full version of ST-MFNet (Ours) on all test datasets. The performance difference can also be demonstrated through visual comparison as shown in <ref type="figure" target="#fig_6">Figure 9</ref>. All of these confirm the effectiveness of the up-sampling scale in multi-flow estimation. ST-GAN. In the main paper, due to space limitations, we only evaluated the effectiveness of the adopted ST-GAN using visual examples. Here we additionally present the quantitative ablation study results for the adopted ST-GAN. For this purpose, we evaluate the same variants of ST-MFNet as described in Section 5.1 (the ST-GAN sub-section) of the main paper, that is, the distortion-oriented version (Our-L lap ), the version fine-tuned with ST-GAN (Our-L p ), the version fine-tuned with FIGAN <ref type="bibr" target="#b29">[32]</ref> and the version finetuned with TGAN <ref type="bibr" target="#b46">[49]</ref>. <ref type="table" target="#tab_3">Table 4</ref> summarizes the performance of these variants on all four test sets in terms of LPIPS. It can be clearly observed from the table that the ST-GAN adopted in our work provides the best overall LPIPS performance, indicating its effectiveness for enhancing perceptual quality of the interpolated results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization of Motion Fields</head><p>To better understand the effectiveness of the multi-scale multi-flow estimation in the MIFNet branch, the predicted multi-flows are visualized here in the same manner as done in <ref type="bibr" target="#b29">[32]</ref>. That is, the mean flow maps at scale l,? l t?n (where n = 1, 2), are obtained using Equations <ref type="bibr" target="#b5">(8)</ref> and <ref type="bibr" target="#b6">(9)</ref>, and shown in <ref type="figure" target="#fig_0">Figure 10</ref>. Note that for the purpose of visualization, the flows at the down-and up-sampled scales are re-scaled to the original resolution using the nearest neighbor filter.</p><p>g(x, y, i) = (?(x, y, i), ?(x, y, i)) (8)</p><formula xml:id="formula_9">G l t?n (x, y) = N i=1</formula><p>w(x, y, i)g(x, y, i)</p><p>It can be observed from <ref type="figure" target="#fig_0">Figure 10</ref> that compared to the mean flow map at the original scale (l = 0), the flows estimated for the down-sampled scale (l = 1) tend to depict the general motion coarsely in different regions. On the other hand, the flow maps at the up-sampled scale (l = ?1) reflect more detailed motion information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comprehensive Evaluation Results</head><p>In the main paper, we presented our quantitative evaluation results of the proposed ST-MFNet and 14 competing methods in terms of PSNR and SSIM. Here, we additionally evaluate these methods in terms of LPIPS. The full results on the test sets UCF101 <ref type="bibr" target="#b49">[52]</ref>, DAVIS <ref type="bibr" target="#b44">[47]</ref> and VFITex are summarized in <ref type="table">Table 5</ref>, and the results on SNU-FILM <ref type="bibr" target="#b9">[12]</ref> are shown in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results of 4? and 8? Interpolation</head><p>The performance of the proposed ST-MFNet on multiframe interpolation task is also evaluated, and compared to three best-performing benchmark algorithms: QVI <ref type="bibr" target="#b58">[61]</ref>, FLAVR <ref type="bibr" target="#b26">[29]</ref> and Softsplat <ref type="bibr" target="#b39">[42]</ref>. The algorithms were applied recursively to generate all the intermediate frames.</p><p>The 11 test sequences at 240 FPS in the GoPro dataset <ref type="bibr" target="#b37">[40]</ref> were used as the test set for 4? and 8? interpolation. <ref type="table" target="#tab_5">Table 7</ref> summarizes the results, where it can be seen that ST-MFNet shows the best overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Validation of Model Design</head><p>The proposed ST-MFNet combines multi-flow and single-flow based warping methods to enhance the interpolation quality of both complex and large motions. A natural question to ask is whether the performance of the model   comes from the specific model design or simply from ensembling effect. To address this question, we create an ensemble model as a baseline, which simply combines Ada-CoF <ref type="bibr" target="#b29">[32]</ref> and Softsplat <ref type="bibr" target="#b39">[42]</ref> through arithmetic averaging. This baseline model was trained under the same configurations as ST-MFNet and compared to the latter quantitatively.</p><p>The results are summarized in <ref type="table">Table 8</ref>, where it is noted that although ensembling of AdaCoF and Softsplat does provide some benefit, the gain is marginal. This implies that the main source of the performance gain in ST-MFNet is the model design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. User Study</head><p>The user study was conducted in a darkened, lab-based environment. The test sequences were played on a SONY PVM-X550 display, with screen size 124.2?71.8cm. The display resolutions were configured to 1920?1080 (spatial) and 60Hz (temporal), and the viewing distance was 2.15 meters (three times the screen height) <ref type="bibr" target="#b23">[26]</ref>. The presentation of video sequences was controlled by a Windows PC running Matlab Psychtoolbox [3]. In each trial, a pair of videos to be compared were played twice, then the participant was asked to select the video with better perceived quality through an interface developed using the Psychtoolbox. This user study and the use of human data have undergone an internal ethics review and has been approved by the Institutional Review Board. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Video Demo</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Attribution of Assets</head><p>The data and code assets employed in this work and their corresponding license information are summarized in Table 9 and 10 respectively.   <ref type="table">Table 5</ref>. Quantitative comparison results for our model and 14 tested methods on UCF101, DAVIS and VFITex, in terms of PSNR, SSIM and LPIPS. OOM denotes cases where our GPU runs out of memory for the evaluation. For each column, the best result is colored in red and the second best is colored in blue. Underlined scores denote the performance of pre-trained models rather than our re-trained versions.  <ref type="table">Table 8</ref>. Quantitative evaluation results of the proposed ST-MFNet and a simple baseline that combines AdaCoF and Softsplat. For each row, the best result is colored in red and the second best is colored in blue. Note Softsplat here is trained with Charbonnier loss so that AdaCoF, Softsplat and the baseline only differ in model design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNU-FILM</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>High-level architecture of ST-MFNet, which employs a two-stage workflow to interpolate an intermediate frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Illustration of the MIFNet. (a) The overall architecture of MIFNet, with a U-Net style backbone and multi-flow estimation heads at three scales. (b) The convolutional layers inside the multi-flow head at each scale. Illustration of the MSResNext block, which consists of two ResNext branches with different kernel sizes, followed by a channel attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The architecture of the Texture Enhancement Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results interpolated by different variants of our method. Here "Overlay" means the overlaid adjacent frames. Figures (a)-(d): w/ MIFNet vs w/o MIFNet; figures (e)-(h): w/ BLFNet vs w/o BLFNet; figures (i)-(j): UMSResNext vs U-Net; figures (m)-(p): w/ TENet vs w/o TENet; figures (q)-(v): comparison of different GANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Qualitative interpolation examples by different methods. The first column (a) shows the overlaid adjacent frames. Columns (b-f) correspond to some of the best-performing benchmark methods. The results of our distortion-oriented model (g) and perception-oriented model (h) are also included, along with the ground truth frames (i). Video comparison examples can be found in Appendix H. Results of the user study showing preference ratios for the tested interpolation methods. The error bars denote standard deviation over test videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>(a) Overlay (b) GT (c) Ours-w/o US (d) Ours-w/ US Qualitative results interpolated by the ST-MFNet with the up-sampled scale removed (Ours-w/o US) and the full version of ST-MFNet (Ours-w/ US). Here "Overlay" means the overlaid adjacent frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 .</head><label>10</label><figDesc>Visualization of the multi-scale multi-flows predicted by the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>A</head><label></label><figDesc>video containing interpolation examples generated by ST-MFNet and more visual comparisons is available via this link: https://drive.google.com/file/d/ 1zpE3rCQNJi4e8ADNWKbJA5wTvPllKZSj / view ? usp=sharing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>SSIM (?) LPIPS (?) PSNR (?) SSIM (?) LPIPS (?) PSNR (?) SSIM (?) LPIPS (?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>BLFNet 33.218/0.970 27.767/0.881 28.498/0.915 Ours-w/o MIFNet 33.202/0.969 27.886/0.889 28.357/0.911</figDesc><table><row><cell>UCF101</cell><cell>DAVIS</cell><cell>VFITex</cell></row><row><cell>Ours-w/o</cell><cell></cell><cell></cell></row></table><note>Ours-w/o TENet 32.895/0.970 27.484/0.880 28.241/0.910 Ours-unet 33.378/0.970 28.096/0.892 28.898/0.925 Ours 33.384/0.970 28.287/0.895 29.175/0.929 Table 1. Ablation study results (PSNR/SSIM) for ST-MFNet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>.965 20.403/0.673 27.528/0.876 24.091/0.817 21.556/0.760 19.709/0.705 19.946/0.709 0.157 3.82 SuperSloMo [28] 32.547/0.968 26.523/0.866 36.255/0.984 33.802/0.973 29.519/0.930 24.770/0.855 27.914/0.911 0.107 39.61 SepConv [44] 32.524/0.968 26.441/0.853 39.894/0.990 35.264/0.976 29.620/0.926 24.653/0.851 27.635/0.907 0.062 21.68</figDesc><table><row><cell></cell><cell>UCF101</cell><cell>DAVIS</cell><cell>Easy</cell><cell>SNU-FILM Medium Hard</cell><cell>Extreme</cell><cell>VFITex</cell><cell>RT (sec)</cell><cell>#P (M)</cell></row><row><cell cols="9">DVF [34] 32.251/0DAIN [5] 32.524/0.968 27.086/0.873 39.280/0.989 34.993/0.976 29.752/0.929 24.819/0.850 27.314/0.909 0.896 24.03</cell></row><row><cell>BMBC [45]</cell><cell cols="8">32.729/0.969 26.835/0.869 39.809/0.990 35.437/0.978 29.942/0.933 24.715/0.856 27.337/0.904 1.425 11.01</cell></row><row><cell>AdaCoF [32]</cell><cell cols="8">32.610/0.968 26.445/0.854 39.912/0.990 35.269/0.977 29.723/0.928 24.656/0.851 27.639/0.904 0.051 21.84</cell></row><row><cell>FeFlow [21]</cell><cell cols="5">32.520/0.967 26.555/0.856 39.591/0.990 35.014/0.977 29.466/0.928 24.607/0.852</cell><cell>OOM</cell><cell cols="2">1.385 133.63</cell></row><row><cell>CDFI [15]</cell><cell cols="7">32.653/0.968 26.471/0.857 39.881/0.990 35.224/0.977 29.660/0.929 24.645/0.854 27.576/0.906 0.321</cell><cell>4.98</cell></row><row><cell>CAIN [12]</cell><cell cols="8">32.537/0.968 26.477/0.857 39.890/0.990 35.630/0.978 29.998/0.931 25.060/0.857 28.184/0.911 0.071 42.78</cell></row><row><cell>SoftSplat [42]</cell><cell cols="8">32.835/0.969 27.582/0.881 40.165/0.991 36.017/0.979 30.604/0.937 25.436/0.864 28.813/0.924 0.206 12.46</cell></row><row><cell>EDSC [11]</cell><cell cols="7">32.677/0.969 26.689/0.860 39.792/0.990 35.283/0.977 29.815/0.929 24.872/0.854 27.641/0.904 0.067</cell><cell>8.95</cell></row><row><cell>XVFI [51]</cell><cell cols="7">32.224/0.966 26.565/0.863 38.849/0.989 34.497/0.975 29.381/0.929 24.677/0.855 27.759/0.909 0.108</cell><cell>5.61</cell></row><row><cell>QVI [61]</cell><cell cols="8">32.668/0.967 27.483/0.883 36.648/0.985 34.637/0.978 30.614/0.947 25.426/0.866 28.819/0.926 0.257 29.23</cell></row><row><cell>FLAVR [29]</cell><cell cols="8">33.389/0.971 27.450/0.873 40.135/0.990 35.988/0.979 30.541/0.937 25.188/0.860 28.487/0.915 0.695 42.06</cell></row><row><cell cols="9">ST-MFNet (Ours) 33.384/0.970 28.287/0.895 40.775/0.992 37.111/0.985 31.698/0.951 25.810/0.874 29.175/0.929 0.901 21.03</cell></row></table><note>. Quantitative comparison results (PSNR/SSIM) for ST-MFNet and 14 tested methods. In some cases, underlined scores based on the pre-trained models are provided in the table, when they outperform their re-trained counterparts. OOM denotes cases where our GPU runs out of memory for the evaluation. For each column, the best result is colored in red and the second best is colored in blue. The average runtime (RT) for interpolating a 480p frame as well as the number of model parameters (#P) for each method are also reported.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>BLFNet 33.218/0.970 27.767/0.881 40.655/0.990 36.890/0.984 31.205/0.947 25.492/0.869 28.498/0.915 Ours-w/o MIFNet 33.202/0.969 27.886/0.889 40.331/0.991 36.530/0.982 31.321/0.949 25.620/0.871 28.357/0.911 Ours-w/o TENet 32.895/0.970 27.484/0.880 40.275/0.991 35.983/0.980 30.527/0.937 25.374/0.864 28.241/0.910 Ours-unet 33.378/0.970 28.096/0.892 40.616/0.991 36.797/0.984 31.383/0.950 25.680/. 872 28.898/0.925 Ours-w/o US 33.371/0.970 28.155/0.893 40.248/0.990 36.689/0.983 31.384/0.949 25.636/0.873 28.977/0.925 Ours 33.384/0.970 28.287/0.895 40.775/0.992 37.111/0.985 31.698/0.951 25.810/0.874 29.175/0.929Table 3. Comprehensive ablation study results on ST-MFNet. Quantitative ablation study results for ST-GAN, in terms of LPIPS.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="6">[Conv3x3+BN+ReLU] x 8</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Conv 3x3</cell><cell>BatchNorm</cell><cell>ReLU</cell><cell>Conv 3x3</cell><cell>BatchNorm</cell><cell>ReLU</cell><cell>...</cell><cell>Conv 3x3</cell><cell>BatchNorm</cell><cell>ReLU</cell><cell>Flatten</cell><cell>Temporal features</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Concat</cell><cell>Linear</cell><cell>LReLU</cell><cell>Linear</cell><cell>Sigmoid</cell></row><row><cell></cell><cell>Conv 3x3</cell><cell>BatchNorm</cell><cell>ReLU</cell><cell>Conv 3x3</cell><cell>BatchNorm</cell><cell>ReLU</cell><cell>...</cell><cell>Conv 3x3</cell><cell>BatchNorm</cell><cell>ReLU</cell><cell>Flatten</cell><cell>Spatial features</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">[Conv3x3+BN+ReLU] x 8</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="13">Figure 8. Architecture of the discriminator used for training ST-MFNet.</cell></row><row><cell>UCF101</cell><cell></cell><cell cols="2">DAVIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SNU-FILM</cell><cell>VFITex</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Easy</cell><cell></cell><cell></cell><cell cols="2">Medium</cell><cell></cell><cell>Hard</cell><cell>Extreme</cell></row><row><cell cols="5">Ours-w/o UCF101 DAVIS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SNU-FILM</cell><cell>VFITex</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Easy Medium Hard Extreme</cell></row><row><cell>Ours-L lap</cell><cell cols="2">0.036</cell><cell></cell><cell>0.125</cell><cell></cell><cell cols="2">0.019</cell><cell cols="2">0.036</cell><cell cols="2">0.073</cell><cell>0.148</cell><cell>0.216</cell></row><row><cell>TGAN</cell><cell cols="2">0.034</cell><cell></cell><cell>0.117</cell><cell></cell><cell cols="2">0.019</cell><cell cols="2">0.033</cell><cell cols="2">0.068</cell><cell>0.142</cell><cell>0.213</cell></row><row><cell>FIGAN</cell><cell cols="2">0.036</cell><cell></cell><cell>0.119</cell><cell></cell><cell cols="2">0.020</cell><cell cols="2">0.035</cell><cell cols="2">0.070</cell><cell>0.146</cell><cell>0.216</cell></row><row><cell>Ours-L p</cell><cell cols="2">0.033</cell><cell></cell><cell>0.116</cell><cell></cell><cell cols="2">0.017</cell><cell cols="2">0.031</cell><cell cols="2">0.065</cell><cell>0.140</cell><cell>0.210</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Easy Medium Hard Extreme PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS DVF [34] 27.528 0.876 0.109 24.091 0.817 0.166 21.556 0.760 0.231 19.709 0.705 0.303 SuperSloMo [28] 36.255 0.984 0.025 33.802 0.973 0.034 29.519 0.930 0.068 24.770 0.855 0.141 SepConv [44] 39.894 0.990 0.022 35.264 0.976 0.043 29.620 0.926 0.094 24.653 0.851 0.183 DAIN [5] 39.280 0.989 0.020 34.993 0.976 0.033 29.752 0.929 0.082 24.819 0.850 0.142 BMBC [45] 39.809 0.990 0.020 35.437 0.978 0.034 29.942 0.933 0.088 24.715 0.856 0.145 AdaCoF [32] 39.912 0.990 0.021 35.269 0.977 0.039 29.723 0.928 0.080 24.656 0.851 0.152 FeFlow [21] 39.591 0.990 0.022 35.014 0.977 0.041 29.466 0.928 0.090 24.607 0.852 0.182 CDFI [15] 39.881 0.990 0.019 35.224 0.977 0.036 29.660 0.929 0.081 24.645 0.854 0.163 CAIN [12] 39.890 0.990 0.021 35.630 0.978 0.037 29.998 0.931 0.097 25.060 0.857 0.203 Softsplat [42] 40.165 0.991 0.021 36.017 0.979 0.036 30.604 0.937 0.066 25.436 0.864 0.119 EDSC [11] 39.792 0.990 0.023 35.283 0.977 0.040 29.815 0.929 0.080 24.872 0.854 0.153 XVFI [51] 38.849 0.989 0.022 34.497 0.975 0.039 29.381 0.929 0.075 24.677 0.855 0.139 QVI [61] 36.648 0.985 0.019 34.637 0.978 0.032 30.614 0.947 0.066 25.426 0.866 0.140 FLAVR [29] 40.135 0.990 0.021 35.988 0.979 0.049 30.541 0.937 0.112 25.188 0.860 0.218 ST-MFNet (Ours-L lap ) 40.775 0.992 0.019 37.111 0.985 0.036 31.698 0.951 0.073 25.810 0.874 0.148 ST-MFNet (Ours-L p ) 40.542 0.991 0.017 36.964 0.983 0.031 31.580 0.949 0.065 25.764 0.871 0.140 Table 6. Quantitative comparison results for our model and 14 tested methods on SNU-FILM dataset, in terms of PSNR, SSIM and LPIPS.For each column, the best result is colored in red and the second best is colored in blue. Underlined scores denote the performance of pre-trained models rather than our re-trained versions. Quantitative comparison results for 4? and 8? interpolation on GoPro dataset in terms of PSNR, SSIM and LPIPS. For each column, the best result is colored in red and the second best is colored in blue.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GoPro-4?</cell><cell></cell><cell>GoPro-8?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">PSNR (?) SSIM (?) LPIPS (?) PSNR (?) SSIM (?) LPIPS (?)</cell></row><row><cell>QVI [61]</cell><cell></cell><cell></cell><cell>29.324</cell><cell>0.927</cell><cell>0.049</cell><cell>29.280</cell><cell>0.929</cell><cell>0.048</cell></row><row><cell cols="2">FLAVR [29]</cell><cell></cell><cell>28.911</cell><cell>0.914</cell><cell>0.110</cell><cell>29.512</cell><cell>0.922</cell><cell>0.101</cell></row><row><cell cols="2">Softsplat [42]</cell><cell></cell><cell>28.858</cell><cell>0.908</cell><cell>0.072</cell><cell>29.663</cell><cell>0.918</cell><cell>0.067</cell></row><row><cell cols="3">ST-MFNet (Ours-L lap )</cell><cell>29.892</cell><cell>0.926</cell><cell>0.098</cell><cell>30.568</cell><cell>0.934</cell><cell>0.092</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">AdaCoF Softsplat AdaCoF+Softsplat ST-MFNet (ours-L lap )</cell></row><row><cell cols="2">UCF101</cell><cell cols="2">PSNR (?) SSIM (?)</cell><cell>32.488 0.968</cell><cell>32.683 0.969</cell><cell>32.729 0.969</cell><cell>33.384 0.970</cell></row><row><cell cols="2">DAVIS</cell><cell cols="2">PSNR (?) SSIM (?)</cell><cell>26.445 0.854</cell><cell>27.359 0.878</cell><cell>27.361 0.878</cell><cell>28.287 0.895</cell></row><row><cell></cell><cell>Easy</cell><cell cols="2">PSNR (?) SSIM (?)</cell><cell>39.912 0.990</cell><cell>40.021 0.991</cell><cell>40.083 0.991</cell><cell>40.775 0.992</cell></row><row><cell>SNU-FILM</cell><cell>Medium</cell><cell cols="2">PSNR (?) SSIM (?)</cell><cell>35.269 0.977</cell><cell>35.833 0.979</cell><cell>35.841 0.979</cell><cell>37.111 0.985</cell></row><row><cell></cell><cell>Hard</cell><cell cols="2">PSNR (?) SSIM (?)</cell><cell>29.723 0.928</cell><cell>30.412 0.936</cell><cell>30.449 0.937</cell><cell>31.698 0.951</cell></row><row><cell></cell><cell>Extreme</cell><cell cols="2">PSNR (?) SSIM (?)</cell><cell>24.656 0.851</cell><cell>25.242 0.862</cell><cell>25.258 0.864</cell><cell>25.810 0.874</cell></row><row><cell cols="2">VFITex</cell><cell cols="2">PSNR (?) SSIM (?)</cell><cell>27.639 0.904</cell><cell>28.620 0.922</cell><cell>28.629 0.923</cell><cell>29.175 0.929</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was funded by the China Scholarship Council, University of Bristol, and the UKRI MyWorld Strength in Places Programme (SIPF00006/1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>UVG database <ref type="bibr" target="#b35">[38]</ref> http://ultravideo.fi Non-commercial Creative Commons BY-NC license.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pexels [2]</head><p>https://www.pexels.com/videos/ All sequences are available for research use. SepConv <ref type="bibr" target="#b41">[44]</ref> https://github.com/sniklaus/sepconv-slomo Academic purposes only.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://mitchmartinez.com/free-4k-red-epic-stock-footage/" />
		<title level="m">Mitch Martinez free 4k stock footage</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MEMC-Net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6228" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05776</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PDWN: Pyramid deformable warping network for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Open Journal of Signal Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multiple video frame interpolation via enhanced deformable separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Channel attention is all you need for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Texture-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duolikun</forename><surname>Danier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Picture Coding Symposium (PCS)</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CDFI: Compression-driven network design for frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Zharkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianfranco</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Residual conv-deconv grid network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Fourure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Tremeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07958</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Texture synthesis using convolutional neural networks. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial networks. Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Featureflow: Robust video interpolation via structure-totexture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shurui</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image quality metrics: PSNR vs. SSIM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Hore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djemel</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 20th international conference on pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2366" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">RIFE: Real-time intermediate flow estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06294</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">500-11, methodology for the subjective assessment of the quality of television pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itu-R</forename><surname>Recommendation</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>International Telecommunication Union</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Flavr: Flow-agnostic video representations for fast frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarun</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08512</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Quantifying the carbon emissions of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dandres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09700</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adacof: Adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enhanced quadratic video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gucan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="434" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BVI-DVC: A training database for deep video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">CVEGAN: A perceptually-inspired gan for compressed video enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David R</forename><surname>Bull</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09190,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">UVG dataset: 50/120fps 4k sequences for video codec analysis and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Viitanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarno</forename><surname>Vanne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Multimedia Systems Conference</title>
		<meeting>the 11th ACM Multimedia Systems Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">org video test media (derf&apos;s collection), the xiph open source community</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Montgomery</surname></persName>
		</author>
		<ptr target="https://media.xiph.org/video/derf,3.5" />
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3883" to="3891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="670" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">BMBC: Bilateral motion estimation with bilateral cost volume for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keunsoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV 16</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Asymmetric bilateral motion estimation for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junheum</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised video interpolation using cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="892" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangdi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Video interpolation via generalized deformable convolution. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">XVFI: extreme video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjun</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Overview of the high efficiency video coding (HEVC) standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens-Rainer</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woo-Jin</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1649" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Conditional generative convnets for exemplar-based texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Ming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2461" to="2475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fast texture synthesis using tree-structured vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Video compression through image interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="416" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning energy-based spatial-temporal generative convnets for dynamic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Quadratic video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video enhancement with taskoriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Stationary dynamic texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE 13th International Conference on Signal Processing</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1135" to="1139" />
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Spatiotemporal generative adversarial network-based dynamic texture synthesis for surveillance video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A parametric framework for video compression using region-based texture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1378" to="1392" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A flexible recurrent residual pyramid network for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="474" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
