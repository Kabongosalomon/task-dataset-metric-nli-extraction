<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenad</forename><surname>Tomasev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Bica</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Buesing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovana</forename><surname>Mitrovic</surname></persName>
						</author>
						<title level="a" type="main">Pushing the limits of self-supervised ResNets: Can we outperform supervised learning without labels on ImageNet?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite recent progress made by self-supervised methods in representation learning with residual networks, they still underperform supervised learning on the ImageNet classification benchmark, limiting their applicability in performancecritical settings. Building on prior theoretical insights (Mitrovic et al., 2021)  we propose RELICv2 which combines an explicit invariance loss with a contrastive objective over a varied set of appropriately constructed data views. RELICv2 achieves 77.1% top-1 classification accuracy on ImageNet using linear evaluation with a ResNet50 architecture and 80.6% with larger ResNet models, outperforming previous state-ofthe-art self-supervised approaches by a wide margin. Most notably, RELICv2 is the first representation learning method to consistently outperform the supervised baseline in a like-for-like comparison using a range of standard ResNet architectures. Finally we show that despite using ResNet encoders, RELICv2 is comparable to state-of-theart self-supervised vision transformers. * Equal contribution, ? Work done during internship at DeepMind. simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a. Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. ArXiv, abs/2003.04297, 2020b. Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021. Fran?ois Chollet. Xception: Deep learning with depthwise separable convolutions. In : Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large-scale foundation models <ref type="bibr" target="#b5">(Bommasani et al., 2021</ref>)in particular for language <ref type="bibr">(Devlin et al., 2018;</ref><ref type="bibr" target="#b8">Brown et al., 2020)</ref> and multimodal domains <ref type="bibr">(Radford et al., 2021)</ref>-are an important recent development in representation learning. The idea that massive models can be trained without labels in an unsupervised (or self-supervised) manner and be readily adapted, in a few-or zero-shot setting, to perform well on a variety of downstream tasks is important for many problem areas for which labeled data is expensive and impractical to obtain. Multi-view contrastive objectives have emerged as a successful strategy for representation learning <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">He et al., 2019)</ref>. However, downstream utility 1 of these representations has until now never exceeded the performance of supervised learning, limiting their usefulness. In this work we show that it is possible to train a representation without using labels which outperforms an established supervised baseline using the same network architecture in a like-for-like comparison on ImageNet.</p><p>To achieve this we build on the Representation Learning via Invariant Causal Mechanisms (RELIC) framework <ref type="bibr">(Mitrovic et al., 2021)</ref> which learns representations based on the principle of invariant prediction. Unlike other methods, RELIC explicitly enforces invariance over the relationship between similar and dissimilar points in the dataset via an additional term in the loss function. RELIC learns representations that more closely follow the geometry of the underlying data <ref type="bibr">(Mitrovic et al., 2021)</ref>. This property ensures the learned representations transfer well to downstream tasks.</p><p>In this paper, we propose RELICv2 which leverages the theoretical understanding of RELIC with better strategies for selecting similar and dissimilar points and incorporates these into both the contrastive and invariance objectives. As a result RELICv2 achieves a new state-of-the-art performance in self-supervised learning on a wide range of ResNet architectures. Furthermore, RELICv2 is the first self-supervised representation learning method that outperforms the supervised ResNet50 baseline on linear ImageNet evaluation across 1?, 2? and 4? variants (figure 1), as established in <ref type="bibr">(Chen et al., 2020a)</ref>; note that other work outperforms this baseline <ref type="bibr" target="#b0">(Caron et al., 2021)</ref> but do so by using a different network architecture to the baseline, and thus are not a like-for-like comparison of network architectures. We demonstrate how to outperform the supervised baseline by changing the self-supervised training scheme without needing to change the network architecture. On top-1 classification accuracy on ImageNet RELICv2 achieves 77.1%, 78. with a ResNet50, while with a ResNet200 2? it achieves 80.6%.</p><p>Furthermore, RELICv2 also outperforms the supervised baseline on larger ResNet architectures such as ResNet101, ResNet152 and ResNet200. We also demonstrate that RELICv2 exhibits competitive performance in a variety of other tasks including transfer learning, semi-supervised learning, and robustness and out-of-distribution generalization. Finally, although using ResNet architectures RELICv2 demonstrates comparable performance to the latest vision transformer-based methods (figure 6).</p><p>Summary of contributions. We briefly review RELIC and introduce RELICv2, which incorporates our proposed improvements, in section 2. We present our main results in section 3. To accompany our empirical results, we provide further insights and analysis into how RELICv2 learns representations as well as its scaling capabilities in section 4, and place our contributions in the wider context of recent developments in representation learning in section 5.</p><p>Finally, in section 6 we show that RELICv2 performs comparably to the latest vision transformer (ViT) architectures <ref type="bibr">(Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b0">Caron et al., 2021;</ref><ref type="bibr">Li et al., 2021)</ref> and argue that the concepts and results developed in this work could have important implications for wider adoption of self-supervised pre-training in a variety of domains as well as the design of objectives for foundational machine learning systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We consider the large family of multi-view representation learning methods <ref type="bibr" target="#b1">(Bachman et al., 2019;</ref><ref type="bibr">Chen et al., 2020a;</ref><ref type="bibr">He et al., 2019;</ref><ref type="bibr" target="#b9">Caron et al., 2020;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr">Gidaris et al., 2020;</ref><ref type="bibr">Dwibedi et al., 2021;</ref><ref type="bibr">Gidaris et al., 2021)</ref>. In particular, contrastive methods are a subset of methods that leverage an instance classification problem in order to learn representations. Given a batch of data B these methods learn the representation of an anchor point x i sampled from B by comparing it against positives x + i ? P(x i ) and negatives x ? i ? N (x i ). <ref type="bibr">2</ref> The goal of contrastive methods is to classify the positive point as an instance of the anchor point against the negatives.</p><p>In the simplest setting, x i and x + i consist of two differently augmented versions of the same image and N comprises the rest of the images in the batch (augmented using the same procedure) <ref type="bibr">(Chen et al., 2020a)</ref>. The standard contrastive objective can then be concisely described in the following likelihood function</p><formula xml:id="formula_0">log p(x i ; x + i ) (1) = log e ?? (xi;x + i ) e ?? (xi;x + i ) + x ? i ?N (xi) e ?? (xi;x ? i )</formula><p>where ? ? (x i ; x + i ) = h(f (x i )), q(g(x + i )) /? is a similarity function which compares the output of two encoder networks, f and g; h and q are multi-layer perceptrons. We use the target network setting <ref type="bibr">(Grill et al., 2020)</ref> where f and g have the same architecture, but the weights of g are an exponential moving average of the weights of f . Note that p(x i ; x + i ) is also a function of the set of negatives N (x i ) and due to the comparisons in the denominator, it is not symmetric in its arguments. <ref type="bibr">RELIC (Mitrovic et al., 2021)</ref> introduces an invariance loss defined as the Kullback-Leibler divergence between the likelihood of the anchor point x i and one of its positives x + i both of which are computed as in equation 1</p><formula xml:id="formula_1">D KL (p(x i )|p(x + i )) =sg E p(xi;x + i ) log p(x i ; x + i ) (2) ? E p(xi;x + i ) log p(x + i ; x i ).</formula><p>We apply the stop gradient operator, sg <ref type="bibr">[?]</ref>, which does not affect the computation of the KL-divergence but avoids degenerate solutions during optimization <ref type="bibr">(Xie et al., 2019)</ref>. Due to the presence of negatives in equation 1, this function effectively measures the similarity of f (x i ) and f (x + i ) relative to the points in N (x i ).</p><p>RELICv2. Similarly to RELIC, the basic objective of RELICv2 is to minimize the combination of contrastive negative log likelihood and invariance loss. Given a mini-batch B, for sample x i where i ? B the loss function is</p><formula xml:id="formula_2">ReLICv2 (x i ) = x + i ?P(xi) ? ? log p(x i ; x + i ) (3) + ?D KL (p(x i )|p(x + i )),</formula><p>where ? and ? are scalar hyper-parameters which weight the relative importance of the contrastive and invariance losses to the overall objective.</p><p>RELICv2 differs from RELIC in the selection of appropriate sets of positive and negative points and how the resulting views of data are combined in the objective function.</p><p>The set of positives P(x i ) is particularly important in our method due to the addition of the explicit invariance term in the loss (c.f. equation 2). P(x i ) is defined by the set of augmentations we apply to each image. In addition to the standard SimCLR augmentations (Chen et al., 2020a) we apply two further augmentation strategies: multi-crop augmentations and saliency-based background removal.</p><p>Multi-crop augmentations were introduced by <ref type="bibr" target="#b9">(Caron et al., 2020)</ref> and consist of comparing several larger (224 ? 224) and smaller (96?96) sized random crops of the same image. In a notable difference from <ref type="bibr" target="#b9">(Caron et al., 2020)</ref>, we use four larger and two smaller crops.</p><p>We probabilistically apply saliency-based background removal to the larger crops. To achieve this, we use a fully unsupervised version of DeepUSPS <ref type="bibr">(Nguyen et al., 2019)</ref> which we trained using a small number of images from the ImageNet training set. We then randomly apply the saliency mask to the image with probability p m to separate the image foreground from the background; this enables us to learn representations that can localize the objects in the image <ref type="bibr">(Zhao et al., 2021)</ref>. We set p m = 0.1 for most of our experiments with more details provided in the supplementary material.</p><p>We then apply the standard SimCLR augmentations: a random horizontal flip and color distortion consisting of a random sequence of brightness, saturation, contrast and hue changes and an optional grayscale conversion. As a final step, Gaussian blur and solarization are applied.</p><p>There are several valid strategies for defining N (x i ) including importance sampling and hard negative mining <ref type="bibr">(Robinson et al., 2020)</ref>. For simplicity, we settled on sampling N (x i ) uniformly from B. In our experiments we set |N (x i )| = 10. This approach was found by <ref type="bibr">(Mitrovic et al., 2020)</ref> to eliminate the issue of false negatives (sampling a negative from the same class as the anchor) in expectation and leads to good performance.</p><p>Listing 1 provides PyTorch-like pseudo-code detailing how we compute equation 3 for our choices of P(x i ) and N (x i ) and specifically, how the different views of data are com- bined in the target network setting. Similar to previous work <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Grill et al., 2020)</ref> we minimize our objective using the LARS optimizer <ref type="bibr">(You et al., 2017)</ref> with a cosine decay learning rate schedule without restarts. Unless otherwise indicated, we train our models for 1000 epochs with a warm-up period of 10 epochs and a batch size of |B| = 4096. Precise architectural and implementation details are provided in the supplementary material.</p><p>Theory and intuitions. Similarly to several other works <ref type="bibr">(Saunshi et al., 2019;</ref><ref type="bibr">HaoChen et al., 2021;</ref><ref type="bibr">Robinson et al., 2020)</ref>, <ref type="bibr">RELIC (Mitrovic et al., 2021)</ref> was theoretically shown to learn good representations in the sense that they cluster according to the latent class structure of the data and are therefore useful for solving a downstream classification task <ref type="bibr">(Mitrovic et al., 2021)</ref>. RELIC and RELICv2 differ from other works in the use of an explicit invariance loss (eq. 2) in conjunction with a contrastive loss.</p><p>The contrastive part of the loss repels representations of negative points with different strengths corresponding to the distance to the anchor point. These have the effect of separating the mean representation of the classes. This phenomenon is described in Theorem 2 of (Mitrovic et al., 2021) and Theorem 5.1 of <ref type="bibr">(Saunshi et al., 2019)</ref>. This also illustrates the problem of false negatives and why selecting hard negatives is difficult: negative points closest to the anchor contribute most to the contrastive objective <ref type="bibr">(Mitrovic et al., 2020;</ref><ref type="bibr">Chuang et al., 2020;</ref><ref type="bibr">Robinson et al., 2020)</ref>.</p><p>The invariance loss encourages the representation of positive points to be similar to that of the anchor leading to representations with tighter within-class concentration as described in Lemma 1 of <ref type="bibr">(Mitrovic et al., 2021)</ref>. However, since the measure of similarity is the KL divergence between contrastive likelihoods (c.f. eq. 2), this enforces that the predictive distribution of a positive relative to the other negative samples in the batch should be similar to that of the anchor point. In effect, the invariance loss allows us to enforce constraints between all points in the batch, rather than just pairwise constraints between the positives, leading to representations which better respect the underlying structure of the data. We investigate how this additional loss affects the learned representations in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results</head><p>We pretrain representations without using labels on the training set of the ImageNet ILSVRC-2012 dataset <ref type="bibr">(Russakovsky et al., 2015)</ref> and then evaluate the learned representations in a wide variety of downstream settings, datasets and tasks. First we examine the performance under the standard linear evaluation and semi-supervised protocols on the ImageNet validation set. Next we investigate the transfer capabilities of RELICv2 representations on other image classification datasets as well as on semantic segmentation. We also test the robustness and out-of-distribution (OOD) generalization capabilities of RELICv2 on a series of challenging datasets. Finally, to investigate the scalability and generality of our approach, we also pretrain on the much larger and more complex Joint Foto Tree (JFT-300M) dataset <ref type="bibr">(Sun et al., 2017)</ref> and report the results of the linear evaluation protocol on the ImageNet validation set. A complete set of results and details of all experimental settings is provided in the supplementary material.</p><p>The supervised baseline we consider refers to the ResNet50 architecture trained with cross-entropy loss and full access to labels using the same set of basic data augmentations for 1000 epochs as proposed by <ref type="bibr">(Chen et al., 2020a)</ref> and used throughout the representation learning literature (c.f. <ref type="bibr">(Grill et al., 2020;</ref><ref type="bibr" target="#b9">Caron et al., 2020;</ref><ref type="bibr">Dwibedi et al., 2021)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linear evaluation on ImageNet</head><p>We first evaluate RELICv2's representations by training a linear classifier on top of the frozen representation according to the procedure described in <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr" target="#b9">Caron et al., 2020;</ref><ref type="bibr">Dwibedi et al., 2021)</ref> and the supplementary material. We report top-1 and top-5 accuracies on the ImageNet test in table 1. RELICv2 outperforms all previous state-of-the-art self-supervised approaches by a significant margin in terms of both top-1 and top-5 accuracy.</p><p>Remarkably, RELICv2 even outperforms the supervised baseline in terms of top-1 accuracy despite using no label information to pretrain the representation. <ref type="figure" target="#fig_0">Figure 1</ref> compares the performance of RELICv2 against the supervised baseline and other competing methods for both the standard ResNet50 architecture as well as configurations with 2? and 4? wider layers and a 2? wider ResNet200. RELICv2 not only outperforms competing methods but is also the first self-supervised representation learning method which consistently outperforms the supervised baseline for all encoder configurations. Furthermore, RELICv2 also outperforms the supervised baseline for 101, 152 and 200layer ResNet architectures <ref type="bibr">(Grill et al., 2020)</ref> and performs competitively to the latest vision transformer architectures at similar parameter counts. See <ref type="figure" target="#fig_6">figure 6</ref> and the supplementary material for detailed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semi-supervised training on ImageNet</head><p>Next we evaluate the performance of RELICv2 in a semisupervised setting. We pretrain the representation and leverage a small subset of the available labels in the ImageNet training set to refine the learned representation following the protocol as described in <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr" target="#b9">Caron et al., 2020;</ref><ref type="bibr">Dwibedi et al., 2021;</ref><ref type="bibr">Lee et al., 2021)</ref> and the supplementary material. Top-1 and top-5 accuracy on the ImageNet validation set is reported in table 2. RELICv2 outperforms both the supervised baseline and all previous state-of-the-art self-supervised methods when   <ref type="bibr">(Grill et al., 2020)</ref> 53.2 68.8 78.4 89.00 SwAV <ref type="bibr" target="#b9">(Caron et al., 2020)</ref> 53.9 70.2 78.5 89.9 NNCLR <ref type="bibr">(Dwibedi et al., 2021)</ref> 56.4 69.8 80.7 89.3 C-BYOL <ref type="bibr">(Lee et al., 2021)</ref> 60.6 70. using 10% of the data for fine-tuning. When using 1% of the data, only C-BYOL performs better than RELICv2. For further semi-supervised results using larger ResNet models and different dataset splits see the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Transfer to other tasks</head><p>We evaluate the generality of RELICv2 representations by testing whether the learned features are useful across image domains.</p><p>Classification. We perform linear evaluation and finetuning on the same set of classification tasks used in <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr">Dwibedi et al., 2021</ref>) and follow their evaluation protocol detailed in the supplementary material. We report standard metrics for each dataset and report performance on a held-out test set. <ref type="figure" target="#fig_1">Figure 2</ref> compares the transfer performance of representations pre-trained using BYOL <ref type="bibr">(Grill et al., 2020)</ref>, <ref type="bibr">NNCLR (Dwibedi et al., 2021)</ref> and RELICv2 relative to supervised pre-training. Overall, RELICv2 improves upon both the supervised baseline and competing methods, performing best on 7 out of 11 tasks. RELICv2 exhibits an average relative improvement over the supervised baseline of over 5% across all tasks-over double that of NNCLR. Detailed results for both linear and fine-tuned evaluation protocols are in the supplementary material.</p><p>Other vision tasks. To further evaluate the generality of the learned representation, we assess the performance of RELICv2 in other challenging vision tasks via finetuning, more specifically: PASCAL <ref type="bibr">(Everingham et al., 2010)</ref> and Cityscapes semantic segmentation <ref type="bibr">(Cordts et al., 2016)</ref>. In accordance with (He et al., 2019), we use the RELICv2 ImageNet representation to initialise a fully convolutional backbone, which we fine-tune on the PASCAL train aug2012 set for 45 epochs and report the mean intersection over union (mIoU) on the val2012 set. The fine-tuning on Cityscapes is done on the train fine set for 160 epochs and evaluated on the val fine set.  <ref type="bibr" target="#b2">(Barbu et al., 2019)</ref>. On all datasets, we evaluate the representations from a standard ResNet50 encoder under a linear evaluation protocol akin to 3.1, i.e. we train a linear classifier on top of the frozen representation using the labelled ImageNet training set; the test evaluation is performed zero-shot, i.e no training is done on the above datasets. RELICv2 learns more robust representations and outperforms both the supervised baseline and the competing self-supervised methods on ImageNetV2 and ImageNet-C. Also, RELICv2 learns representations that outperform competing self-supervised methods while being on par with supervised performance in terms of OOD generalization. For a detailed explanation of the datasets and a full breakdown of the results see the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Large-scale transfer with JFT-300M</head><p>Next we test how well RELICv2 scales to much larger datasets by pretraining representations using the Joint Foto Tree (JFT-300M) dataset which consists of 300 million images from more than 18k classes <ref type="bibr">(Hinton et al., 2015;</ref><ref type="bibr">Chollet, 2017;</ref><ref type="bibr">Sun et al., 2017)</ref>. We then evaluate the learned representations on the ImageNet validation set under the same linear evaluation protocol as described in section 3.1. We compare RELICv2 against BYOL and Divide and Contrast (DnC) (Tian et al., 2021), a method that was specifically designed to handle large and uncurated datasets and represents the current state-of-art in self-supervised JFT-300M pretraining. <ref type="table">Table 3</ref> reports the top-1 accuracy when training the various methods using the standard ResNet50 architecture as the backbone for different number of ImageNet equivalent epochs on JFT-300M; implementation details can be found in the supplementary material. RELICv2 improves over DnC by more than 2% when training on JFT for 1000 epochs and achieves better overall performance than competing methods while needing a smaller number of training epochs. In the supplementary material we also report robustness and OOD generalization results after JFT pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Latent space analysis.</head><p>In order to understand the effect of the explicit invariance term in the loss function on the representations learned by RELICv2, we look at the distances between learned representations of closely related classes. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the Euclidean distances between nearest-neighbour representations learned by RELICv2 and BYOL on ImageNet using the protocol described in section 3. Here we pick two breeds of dog and two breeds of cat. Each of these four classes has 50 points associated with it from the ImageNet validation set, ordered contiguously. Each row represents an image and each coloured point in a row represents one of the five nearest neighbours of the representation of that image where the colour indicates the distance between the image and the nearest neighbour. Representations which align perfectly with the underlying class structure would exhibit a perfect block-diagonal structure; that is their nearest neighbours all belong to the same underlying class. We see that RELICv2 learns representations whose nearest neighbours are closer and exhibit less confusion between classes and super-classes than BYOL.</p><p>To quantify the overall structure of the learned latent space, we examine the within-and between-class distances of all classes. <ref type="figure" target="#fig_3">Figure 4</ref> compares the distribution of ratios of between-class and within-class 2 -distances of the representations of points in the ImageNet validation set learned by RELICv2 against those learned by the supervised baseline. 3 A larger ratio implies that the representation is better concentrated within the corresponding classes and better separated between classes and therefore more easily linearly separated (c.f. Fisher's linear discriminants <ref type="bibr">(Friedman et al., 2009)</ref>). We see that RELICv2's distribution is shifted to the right (i.e. having a higher ratio) compared to the supervised baseline suggesting that the representations can be better separated using a linear classifier. The empirical results in this section further confirm the theoretical insights of (Mitrovic et al., 2021) and explain the superior performance of RELICv2 reported in section 3.1. <ref type="figure" target="#fig_4">Figure 5</ref> shows the ImageNet linear evaluation accuracy obtained by representations learned using RELICv2 as a function of the number of images seen during pre-training using the ImageNet training set. It can be seen that in order to reach 70% accuracy the ResNet50 model requires approximately twice the number of iterations as the ResNet295 model. The ResNet295 has approximately 3.6? the number of parameters as the ResNet50 (87M vs 24M, respectively). This finding is in accordance with other works which show that larger models are more sample efficient (i.e. they require fewer samples to reach a given accuracy) <ref type="bibr">(Zhai et al., 2021)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Scaling analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations</head><p>The two main distinction between prior work <ref type="bibr">(Mitrovic et al., 2021;</ref> and RELICv2 are the use of multi-crop and saliency masking. Here we ablate the use of these techniques using top-1 ImageNet validation set performance under the linear evaluation protocol on a ResNet50 pretrained for 1000 epochs. In summary, starting from RELIC (74.3%), we gain +2.5% by adding multi-crop and another +0.3% by adding saliency masking on top of that which yields the final RELICv2 performance of 77.1%.  using fewer large crops performs better than the procedure proposed by <ref type="bibr" target="#b9">(Caron et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi</head><formula xml:id="formula_3">R295 (87M) R200 (63M) R152 (58M) R101 (43M) R50 (24M)</formula><p>Saliency masking. We apply saliency masks on top of multi-crop during training enabling us to learn representations that focus on the semantically-relevant parts of the image, i.e. the foreground objects, and that are more robust to background changes. We report the top-1 accuracy under linear evaluation on ImageNet for different probabilities p m of removing the background of the large augmented crops during training. Applying the saliency masks 10% of the time results in best performance and significantly improves over not using masking (p m = 0). Moreover, we also explored using different datasets for training <ref type="bibr">DeepUSPS (Nguyen et al., 2019)</ref> in an unsupervised way to obtain the saliency masks. We found this to have little overall effect on the results. See the supplementary material for full details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>Unsupervised learning of representations by combining multiple views of data is a classical problem in machine learning which builds on earlier foundational work on co-training <ref type="bibr" target="#b4">(Blum and Mitchell, 1998)</ref> and multi-view learning <ref type="bibr">(Kakade and Foster, 2007;</ref><ref type="bibr">Sridharan and Kakade, 2008;</ref><ref type="bibr">Chaudhuri et al., 2009;</ref><ref type="bibr">McWilliams and Montana, 2012;</ref><ref type="bibr">McWilliams et al., 2013)</ref>. More recently, contrastive multi-view approaches to representation learning have become an important area of research owing to their excellent performance in visual recognition tasks <ref type="bibr">(Oord et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr">Chen et al., 2020a)</ref>. However, the underlying mechanisms for why these techniques work is less well understood <ref type="bibr">(Tschannen et al., 2019)</ref>.</p><p>One intuitive avenue borrowing from earlier work <ref type="bibr" target="#b4">(Blum and Mitchell, 1998)</ref> is to analyse contrastive methods from the perspective of an appropriate evaluation metric: a downstream classification task <ref type="bibr">(Saunshi et al., 2019;</ref><ref type="bibr">HaoChen et al., 2021)</ref>. In this way optimizing an unsupervised loss can be connected to performance on a supervised problem. The RELIC family of methods also approaches representation learning through this lens.</p><p>In this review we focus on how important algorithmic choices: namely explicitly enforcing invariance and more considered treatment of positive and negative examples are key factors in improving downstream classification performance of unsupervised representations. A detailed comparison is provided in the supplementary material. Outside of this scope, (Lee et al., 2021) have taken a conditional entropy bottleneck approach which is particularly notable as it has also resulted in better-than-supervised performance albeit only on the ResNet50 (2x) architecture (see <ref type="figure" target="#fig_0">figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negatives.</head><p>A key observation of (Chen et al., 2020a) was that large batches (up to 4096) improve results. This was partly attributed to the effect of more negatives. This motivated the incorporation of queues that function as large reservoirs of negative examples into contrastive learning <ref type="bibr">(He et al., 2019)</ref>. However subsequent work has shown that naively using a large number of negatives can have a detrimental effect on learning <ref type="bibr">(Mitrovic et al., 2020;</ref><ref type="bibr">Saunshi et al., 2019;</ref><ref type="bibr">Chuang et al., 2020;</ref><ref type="bibr">Robinson et al., 2020)</ref>. One reason for this is due to false negatives, that is points in the set of negatives which actually belong to the same latent class as the anchor point. These points are likely to have a high relative similarity to the anchor under ? and therefore contribute disproportionately to the loss. This will have the effect of pushing apart points belonging to the same class in representation space.</p><p>Subsampling-based approaches have been proposed to avoid false negatives via importance sampling to attempt to find true negatives which are close to the latent class boundary Positives and invariance. Learning representations which are invariant to data augmentation is known to be important for self-supervised learning. Invariance is achieved heuristically through comparing two different augmentations of the same anchor point. Incorporating an explicit clustering step is another way of enforcing some notion of invariance <ref type="bibr" target="#b9">(Caron et al., 2020)</ref>. However, neither of these strategies can be directly linked theoretically to learning more compact representations. More rigorously (Mitrovic et al., 2021) approach invariance from a causal perspective. They show that invariance must be explicitly enforced-via an invariance loss in addition to the contrastive loss-in order to obtain guaranteed generalization performance. Most recently <ref type="bibr">(Dwibedi et al., 2021)</ref> and <ref type="bibr" target="#b0">(Assran et al., 2021)</ref> use nearest neighbours to identify other elements from the batch which potentially belong to the same class as the anchor point. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>RELICv2 demonstrates for the first time that representations learned without access to labels can consistently outperform a strong, supervised baseline on ImageNet. In terms of a like-for-like comparison using ResNet50 encoders, RELICv2 represents a substantial improvement over current state-of-art. This is a direct consequence of incorporating better strategies for selecting positive and negative points in the RELIC framework as suggested by the theoretical results of <ref type="bibr">(Mitrovic et al., 2021)</ref>.</p><p>Although several works present similar ideas for negative <ref type="bibr">(Chuang et al., 2020;</ref><ref type="bibr">Robinson et al., 2020)</ref> and positive selection <ref type="bibr">(Dwibedi et al., 2021)</ref> individually, we are the first to a) combine both positive and negative selection and b) incorporate an invariance loss to fully exploit the selection of better positives. Our approach is general and is not dependent on specific positive/negative selection strategies. Indeed, the optimal choice of positives remains an open question and is likely to be highly problem and data dependent.</p><p>Finally, as noted in section 5 vision transformers (ViTs) have emerged as promising architectures for visual representation learning. <ref type="figure" target="#fig_6">Figure 6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Preprocessing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Augmentations</head><p>Following the data augmentations protocols of <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr" target="#b9">Caron et al., 2020)</ref>, RELICv2 uses a set of augmentations to generate different views of the original image which has three channels, red r, green g and blue b with r, g, b ? [0, 1].</p><p>The augmentations used, in particular (corresponding to aug in Listing 1) are the same as in <ref type="bibr">(Grill et al., 2020)</ref> and are generated as follows; for exact augmentations parameters see <ref type="table" target="#tab_11">table 4</ref>). The following sequence of operations is performed in the given order. 6. Randomly solarize: threshold each channel value such that all values less than 0.5 are replaced by 0 and all values above or equal to 0.5 are replaced with 1.</p><p>Apart from the initial step of image cropping, each step is executed with some probability to generate the final augmented image. These probabilities and other parameters are given in table 4, separately for augmenting the original image x i and the positives P(x i ). Note that we use 4 large views of size 224 ? 224 pixels and 2 small views of 96 ? 96 pixels; to get the first and third large views and the first small view we use the parameters listed below for odd views, while for the second and fourth large view and the second small view we use the parameters for even views.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Even views</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Saliency Masking</head><p>Using unsupervised saliency masking enables us to create positives for the anchor image with the background largely removed and thus the learning process will rely less on the background to form representations. This encourages the representation to localize the objects in the image (Zhao et al., 2021).</p><p>We develop a fully unsupervised version of DeepUSPS <ref type="bibr">(Nguyen et al., 2019)</ref> to compute saliency masks for each image in the ImageNet training set. By applying the saliency masks on top of the large views, we obtain masked images with the background removed. To further increase the background variability, instead of using a black background for the images, we apply a homogeneous grayscale to the background with the grayscale level randomly sampled for each image during training. We also use a foreground threshold such that we apply the saliency mask only if it covers at least 5% of the image. The masked images with the grayscaled background are used only during training. Specifically, with a small probability p m we selected the masked image of the large view in place of the large view. <ref type="figure" target="#fig_7">Figure 7</ref> shows how the saliency masks are added on top of the images to obtain the images with grayscale background.  <ref type="bibr">et al., 2013)</ref> to compute the initial saliency masks. Note that these methods do not make use of any supervised label information.</p><p>For training DeepUSPS, we closely follow the approach described by <ref type="bibr">(Nguyen et al., 2019)</ref>. We employ the two-stage mechanism for DeepUSPS. In the first stage, the noisy pseudo-labels from each handcrafted method are iteratively refined.</p><p>In the second stage, these refined labels from each handcrafted saliency method are used to train the final saliency detection network. The saliency detection network is then used to compute the saliency masks for all images in the ImageNet training set. We use the publicly available code for training DeepUSPS: https://tinyurl.com/wtlhgo3.</p><p>Note that the official implementation for DeepUSPS uses as backbone a <ref type="bibr">DRN-network (Yu et al., 2017)</ref> which was pretrained on CityScapes <ref type="bibr">(Cordts et al., 2016)</ref> with supervised labels. To be consistent with our fully-unsupervised setting, we replace this network with a ResNet50 2x model which was pretrained on ImageNet using the self-supervised objective from SWaV <ref type="bibr" target="#b9">(Caron et al., 2020)</ref>. We used the publicly available pretrained SWaV model from: https: //github.com/facebookresearch/swav.</p><p>To account for this change in the architecture, we adjust some of the model hyperparameters of DeepUSPS. In the first stage of DeepUSPS training, the pseudo-generation networks used for refining the noisy pseudo-labels from each of the handcrafted methods are trained for 25 epochs in three self-supervised iterations. We start with a learning rate of 1e ? 5 which is doubled during each iteration. In the second stage, the saliency detection network is trained for 200 epochs using a learning rate of 1e ? 5. We use the Adam optimizer with momentum set to 0.9 and a batch size of 10. The remaining hyperparameters are set in the same way as they are in the original DeepUSPS code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pretraining on ImageNet -implementation details and additional results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Linear evaluation</head><p>Following the approach of <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr" target="#b9">Caron et al., 2020;</ref><ref type="bibr">Dwibedi et al., 2021)</ref>, we follow the standard linear evaluation protocol on ImageNet. We train a linear classifier on top of the frozen representation which has been pretrained, i.e. the encoder parameters as well as the batch statistics are not being updated. For training the linear layer, we preprocess the data by applying standard spatial augmentations, i.e. randomly cropping the image with subsequent resizing to 224 ? 224 and then randomly applying a horizontal flip. At test time, we resize images to 256 pixels along the shorter side with bicubic resampling and apply a 224 ? 224 center crop to it. Both for training and testing, after performing the above processing, we normalize the color channels by substracting the average channel value and dividing by the standard deviation of the channel value (as computed on ImageNet). To train the linear classifier, we optimize the cross-entropy loss with stochastic gradient descent with Nestorov momentum for 100 epochs using a batch size of 1024 and a momentum of 0.9; we do not use any weight decay or other regularization techniques. In the following tables we report the top-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Semi-supervised learning</head><p>We further test RELICv2 representations learned on bigger ResNet models in the semi-supervised setting. For this, we follow the semi-supervised protocol as in <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr" target="#b9">Caron et al., 2020)</ref>. First, we initialize the encoder with the parameters of the pretrained representation and we add on top of this encoder a linear classifier which is randomly initialized. Then we train both the encoder and the linear layer using either 1% or 10% of the ImageNet training data; for this we use the splits introduced in <ref type="figure" target="#fig_1">(Chen et al., 2020a)</ref> which have been used in all the methods we compare to <ref type="bibr">(Grill et al., 2020;</ref><ref type="bibr" target="#b9">Caron et al., 2020;</ref><ref type="bibr">Dwibedi et al., 2021;</ref><ref type="bibr">Lee et al., 2021)</ref>. For training, we randomly crop the image and resize it to 224 ? 224 and then randomly apply a horizontal flip. At test time, we resize images to 256 pixels along the shorter side with bicubic resampling and apply a 224 ? 224 center crop to it. Both for training and testing, after performing the above processing, we normalize the color channels by substracting the average channel value and dividing by the standard deviation of the channel value (as computed on ImageNet). Note that this is the same data preprocessing protocol as in the linear evaluation protocol. To train the model, we use a cross entropy loss with stochastic gradient descent with Nesterov momentum of 0.9. For both 1% and 10% settings, we train for 20 epochs and decay the initial learning rate by a factor 0.2 at 12 and 16 epochs. Following the approach of <ref type="bibr" target="#b9">(Caron et al., 2020)</ref>, we use the optimizer with different learning rates for the encoder and linear classifier parameters. For the 1% setting, we use a batch size of 2048 and base learning rates of 10 and 0.04 for the linear layer and encoder, respectively; we do not use any weight decay or other regularization technique.</p><p>For the 10% setting, we use a batch size of 512 and base learning rates of 0.3 and 0.004 for the linear layer and encoder, respectively; we use a weight decay of 1e ? 5, but do not use any other regularization technique. From table 6, we see that RELICv2 outperforms competing self-supervised methods on ResNet50 2? in both the 1% and 10% setting. For larger ResNets, ResNet50 4? and ResNet200 2?, RELICv2 is state-of-the-art with respect to top-1 accuracy for the low-data regime of 1%. On these networks for the higher data regime of 10% BYOL outperforms RELICv2. Note that BYOL trains their semi-supervised models for 30 or 50 epochs whereas RELICv2 is trained only for 20 epochs. We hypothesize that longer training (e.g. 30 or 50 epochs as BYOL) is needed for RELICv2 representations on larger ResNets as there are more model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top-1 Top-5 1% 10% 1% 10%</p><p>SimCLR <ref type="bibr">(Chen et al., 2020a)</ref> 58.5 71.7 83.0 91.2 BYOL <ref type="bibr">(Grill et al., 2020)</ref> 62.2 73.5 84.1 91.7 RELICv2 (ours) 64.7 73.7 85.4 92.0 (a) ResNet50 2? encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top-1 Top-5 1% 10% 1% 10%</p><p>SimCLR <ref type="bibr">(Chen et al., 2020a)</ref> 63.0 74.4 85.8 92.6 BYOL <ref type="bibr">(Grill et al., 2020)</ref> 69.1 75.7 87.9 92.5 RELICv2 (ours) 69.5 74.6 87.3 91.6 <ref type="bibr">(b)</ref> ResNet50 4? encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Top-1 Top-5 1% 10% 1% 10% BYOL (Grill et al., 2020) 71.2 77.7 89.5 93.7 RELICv2 (ours) 72.1 76.4 89.5 93.0 (c) ResNet200 2? encoder. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Transfer</head><p>We follow the transfer performance evaluation protocol as outlined in <ref type="bibr">(Grill et al., 2020;</ref><ref type="bibr">Chen et al., 2020a)</ref>. We evaluate RELICv2 both in both transfer settings -linear evaluation and fine-tuning. For the linear evaluation protocol we freeze the encoder and train only a randomly initialized linear classifier which is put on top of the encoder. On the other hand, for finetuning in addition to training the randomly initialized linear classifier, we also allow for gradients to propagate to the encoder which has been initialized with the parameters of the pretrained representation. In line with prior work <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr">Dwibedi et al., 2021)</ref>, we test RELICv2 representations on the following datasets: Food101 <ref type="bibr" target="#b6">(Bossard et al., 2014)</ref>, For DTD and SUN397, we only use the first split, of the 10 provided splits in the dataset as per <ref type="bibr">(Chen et al., 2020a;</ref><ref type="bibr">Grill et al., 2020;</ref><ref type="bibr">Dwibedi et al., 2021)</ref>.</p><p>We train on the training sets of the individual datasets and sweep over different values of the models hyperparameters. To select the best hyperparameters, we use the validation sets of the individual datasets. Using the chosen hyperparameters, we train the appropriate using the merged training and validation data and test on the held out test data in order to obtain the numbers reported in As can be seen from table 7, RELICv2 representations yield better performance than both state-of-the-art self-supervised methods as well as the supervised baseline across a wide range of datasets. Specifically, RELICv2 is best on 7 out of 11 datasets and on 8 out of 11 datasets in the linear and fine-tuning settings, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Food101 CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft DTD Pets Caltech101 Flowers</head><p>Linear evaluation:  <ref type="table" target="#tab_15">Table 7</ref>. Accuracy (in %) of transfer performance of a ResNet50 pretrained on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Semantic segmentation</head><p>We evaluate the ability of RELICv2 to facilitate successful transfer of the learned representations to <ref type="bibr">PASCAL (Everingham et al., 2010)</ref> and Cityscapes (Cordts et al., 2016) semantic segmentation tasks.</p><p>In accordance with (He et al., 2019), we use the RELICv2 ImageNet representation to initialise a fully convolutional backbone, which we fine-tune on the PASCAL train aug2012 set for 45 epochs and report the mean intersection over union (mIoU) on the val2012 set. The fine-tuning on Cityscapes is done on the train fine set for 160 epochs and evaluated on the val fine set.</p><p>The results in main text demonstrate that RELICv2 compares favourably in terms of semantic segmentation to both BYOL and DetCon on PASCAL, reaching 77.9 IoU. RELICv2 also outperforms BYOL on Cityscapes, 75.2 vs 74.6 IoU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Robustness and OOD Generalization</head><p>The robustness and out-of-distribution (OOD) generalization abilities of RELICv2 representations are tested on several detasets. We use ImageNetV2 <ref type="bibr">(Recht et al., 2019)</ref> and <ref type="bibr">ImageNet-C (Hendrycks and Dietterich, 2019)</ref> datasets to evaluate robustness. ImageNetV2 <ref type="bibr">(Recht et al., 2019)</ref> has three sets of 10000 images that were collected to have a similar distribution to the original ImageNet validation set, while ImageNet-C (Hendrycks and Dietterich, 2019) consists of 15 synthetically generated corruptions (e.g. blur, noise) that are added to the ImageNet validation set.</p><p>For OOD generalization we examine the performance on ImageNet-R <ref type="bibr">(Hendrycks et al., 2021</ref><ref type="bibr">), ImageNetSketch (Wang et al., 2019</ref> and ObjectNet <ref type="bibr" target="#b2">(Barbu et al., 2019)</ref>. <ref type="bibr">ImageNet-R (Hendrycks et al., 2021)</ref> consists of 30000 different renditions (e.g. paintings, cartoons) of 200 ImageNet classes, while <ref type="bibr">ImageNet-Sketch (Wang et al., 2019)</ref> consists of 50000 images, 50 for each ImageNet class, of object sketches in the black-and-white color scheme. These datasets aim to test robustness to different textures and other naturally occurring style changes and are out-of-distribution to the ImageNet training data.</p><p>ObjectNet <ref type="bibr" target="#b2">(Barbu et al., 2019)</ref> has 18574 images from differing viewpoints and backgrounds compared to ImageNet.</p><p>On all datasets we evaluate the representations of a standard ResNet50 encoder under a linear evaluation protocol akin to Section 3.1, i.e. we freeze the pretrained representations and train a linear classifier using the labelled ImageNet training set; the test evaluation is performed zero-shot, i.e no training is done on the above datasets. As can be seen from table 8a, RELICv2 learns more robust representations and outperforms both the supervised baseline and the competing self-supervised methods on ImageNetV2 and ImageNet-C. We provide a detailed breakdown across the different ImageNet-C corruptions in SimCLR <ref type="bibr">(Chen et al., 2020a)</ref> 53.2 61.7 68.0 31.1 BYOL <ref type="bibr">(Grill et al., 2020)</ref> 62.2 71.6 77.0 42.8 RELIC <ref type="bibr">(Mitrovic et al., 2021)</ref>  SimCLR <ref type="bibr">(Chen et al., 2020a)</ref> 18.3 3.9 14.6 BYOL <ref type="bibr">(Grill et al., 2020)</ref> 23.0 8.0 23.0 RELIC <ref type="bibr">(Mitrovic et al., 2021)</ref> 23.8 9.1 23.8 RELICv2 (ours) 23.9 9.9 25.9 <ref type="bibr">(b)</ref> Datasets testing OOD generalization.  <ref type="table" target="#tab_17">Table 9</ref>. Top-1 accuracies for for Gauss, Shot, Impulse, Blur, Weather, and Digital corruption types on ImageNet-C. use 2.0 as the invariance scale, while for 3000 ImageNet-equivalent epochs, we use invariance scale 1.0. We then follow the linear evaluation protocol on ImageNet described in Appendix B.1. We train a linear classifier on top of the pretrained representations from JFT-300M with stochastic gradient descent with Nesterov momentum for 100 epochs using batch size of 256, learning rate of 0.5 and momentum of 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Transfer</head><p>We evaluate the transfer performance of JFT-300M pretrained representations under the linear evaluation protocol. For this, we freeze the encoder and train only linear classifier on top of the frozen encoder output, i.e. representation. As before in B.3, we follow the transfer performance evaluation protocol as outlined in <ref type="bibr">(Grill et al., 2020;</ref><ref type="bibr">Chen et al., 2020a)</ref>. In line with prior work, for Food101 <ref type="bibr" target="#b6">(Bossard et al., 2014</ref><ref type="bibr">), CIFAR10 (Krizhevsky et al., 2009</ref><ref type="bibr">), CIFAR100 (Krizhevsky et al., 2009</ref>, Birdsnap <ref type="bibr" target="#b3">(Berg et al., 2014)</ref>, SUN397 (split 1) <ref type="bibr">(Xiao et al., 2010)</ref>, DTD (split 1) <ref type="bibr">(Cimpoi et al., 2014), and</ref><ref type="bibr">Cars (Krause et al., 2013)</ref> we report the top-1 accuracy on the test set, and for Aircraft <ref type="bibr">(Maji et al., 2013</ref><ref type="bibr">), Pets (Parkhi et al., 2012</ref>, Caltech101 <ref type="bibr">(Fei-Fei et al., 2004), and</ref><ref type="bibr">Flowers (Nilsback and</ref><ref type="bibr">Zisserman, 2008)</ref> we report the mean per-class accuracy as the relevant metric in the comparisons. For DTD and SUN397, we only use the first split, of the 10 provided splits in the dataset.</p><p>We train on the training sets of the individual datasets and sweep over different values of the models hyperparameters. To select the best hyperparameters, we use the validation sets of the individual datasets. Using the chosen hyperparameters, we train the linear layer from scratch using the merged training and validation data and test on the held out test data in order to obtain the numbers reported in As can be seen from table 10, longer pretraining benefits transfer performance of RELICv2. Although <ref type="bibr">DnC (Tian et al., 2021)</ref> was specifically developed to handle uncurated datasets such as JFT-300M, we see that RELICv2 has comparable performance to DnC in terms of the number of datasets with state-of-the-art performance among self-supervised representation learning methods; this showcases the generality of RELICv2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Food101 CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft DTD Pets Caltech101 Flowers BYOL-5k <ref type="bibr">(Grill et al., 2020)</ref>   <ref type="table" target="#tab_20">Table 10</ref>. Accuracy (in %) of transfer performance of a ResNet50 pretrained on JFT under the linear transfer evaluation protocol. xk refers to the length of pretraining in ImageNet-equivalent epochs, e.g. 1k corresponds to 1000 ImageNet-equivalent epochs of pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Robustness and OOD Generalization</head><p>We also tested the robustness and out-of-distribution (OOD) generalization of RELICv2 representations pretrained on JFT. We use the same set-up described in B.5 where we freeze the pretrained representations on JFT-300M, train a linear classifier using the labelled ImageNet training set and perform zeroshot test evaluation on datasets testing robustness and OOD generalization. As in B.5, we evaluated robustness using the ImageNetV2 <ref type="figure" target="#fig_0">(Recht et al., 2019)</ref> and ImageNet-C (Hendrycks and Dietterich, 2019) datasets and OOD generalization using ImageNet-R (Hendrycks et al., 2021), ImageNetSketch <ref type="bibr" target="#b2">(Wang et al., 2019)</ref> and ObjectNet <ref type="bibr" target="#b2">(Barbu et al., 2019)</ref> datasets. We report the robustness results in table 11a and the OOD generalization results in table 11b. We notice that RELICv2 representations pretrained on JFT-300M for different number of ImageNet-equivalent epochs have worse robustness and OOD generalization performance compared to RELICv2 representations pretrained directly on ImageNet (see tables 8a and 8b for reference). Given that the above datasets have been specifically constructed to measure the robustness and OOD generalization abilities of models pretrained on ImageNet (as they have been constructed in relation to ImageNet), this result is not entirely surprising. We hypothesize that this is due to there being a larger discrepancy between datasets and JFT-300M than these datasets and ImageNet and as such JFT-300M-pretrained representations perform worse than ImageNet-pretrained representations. Additionally, note that pretraining on JFT-300M for longer does not necessarily result in better downstream performance on the robustness and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablations</head><p>In order to determine the sensitivity of RELICv2 to different model hyperparameters, we perform an extensive ablation study. Unless otherwise noted, in this section we report results after 300 epochs of pretraining. As saliency masking is one of the main additions of RELICv2 on top of RELIC and was not covered extensively in the main text, we start our ablation analysis with looking into the effect of different modelling choices for it. We also explored training DeepUSPS on 5000 randomly selected images from the ImageNet dataset and this resulted in the DeepUSPS model overfitting, which degraded the quality of the saliency masks and resulted in a RELICv2 performance of 76.7% top-1 and 93.3% top-5 accuracy on the ImageNet validation set after 1000 epochs of pretraining on ImageNet training set.</p><p>The results for RELICv2 in Section 3.5 are obtained by applying the DeepUSPS saliency detection network trained on MSRA-B to all images in JFT-300M and then applying the saliency masks to the large augmented views during training as described in Section A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Analysis and ablations for saliency masks</head><p>Using saliency masking during RELICv2 training enables us to learn representations that focus on the semantically-relevant parts of the image, i.e. the foreground objects, and as such the learned representations should be more robust to background changes. We investigate the impact of using saliency masks with competing self-supervised benchmarks, the effect of the probability p m of applying the saliency mask to each large augmented view during training as well as the robustness of RELICv2 to random masks and mask corruptions. For the ablation experiments described in this section, we train the models for 300 epochs.</p><p>Using saliency masks with competing self-supervied methods. We evaluate the impact of using saliency masks with competing self-supervised methods such as BYOL <ref type="bibr">(Grill et al., 2020)</ref>. This method only uses two large augmentented views during training and we randomly apply the saliency masks, in a similar way as described in Section A.2, to each large augmented view with probability p m . We report in table 12 the top-1 and top-5 accuracy under linear evaluation on ImageNet for different settings of p m for removing the background of the augmented images. We notice that saliency masking also helps to improve performance of BYOL.</p><p>Mask apply probability. We also investigate the effect of using probabilities ranging from 0 to 1 for applying the saliency mask during training for RELICv2. In addition, we explore further the effect of using different datasets for training the saliency detection network in DeepUSPS that is subsequently used for computing the saliency masks.  <ref type="table" target="#tab_23">Table 13</ref>. Top-1 and top-5 accuracy (in %) under linear evaluation on the ImageNet validation set for a ResNet50 (1x) encoder set for different probabilities pm of using the saliency mask to remove the background of the large augmented views during training and for using different datasets to train the DeepUSPS saliency detection network for computing the saliency masks. Models are trained for 300 epochs.</p><p>Random masks and mask corruptions. To understand how important having accurate saliency masks for the downstream performance of representations is we also investigated using random masks, corrupting the saliency masks obtained from DeepUSPS and using a bounding box around the saliency masks during RELICv2 training.</p><p>We explored using completely random masks, setting the saliency mask to be a random rectangle of the image and also a centered rectangle. As ImageNet images generally consists of images with objects centered in the middle of the image, we expect that using a random rectangle that is centered around the middle will cover a reasonable portion of the object. <ref type="table" target="#tab_11">Table  14</ref> reports the performance under linear evaluation on the ImageNet validation set when varying the size of the random masks to cover different percentage areas a p of the full image. We notice that improving the quality of the masks, by using random rectangle patches instead of completely random points in the image as the mask, results in better performance. However, the performance with random masks is &gt; 1% lower than using saliency masks from DeepUSPS. As expected, using centered rectangles instead of randomly positioned rectangles as masks results in better peformance.</p><p>Moreover, to test the robustness of RELICv2 to corruptions of the saliency masks, we add/remove from the masks a rectangle proportional to the area of the saliency mask. The mask rectangle is added/removed from the image center. <ref type="table" target="#tab_13">Table  15</ref> reports the results when varying the area of the rectangle to be added/removed to cover different percentages m p of the saliency masks obtained from DeepUSPS. We notice that while RELICv2 is robust to small corruptions of the saliency mask its performance drops in line with the quality of the saliency masks degrading.</p><p>Finally, we also explore corrupting the masks using a bounding box around the saliency mask obtained from DeepUSPS which results in 74.5% top-1 and 92.2% top-5 accuracy under linear evaluation on the ImageNet validation set for a ResNet50 (1x) encoder trained for 300 epochs with mask apply probability of 0.1 Note that this performance is comparable to using random rectangles to mask the large augmented views during training (see <ref type="table" target="#tab_11">table 14</ref>) and by is lower than directly using the saliency masks from DeepUSPS.  <ref type="table" target="#tab_13">Table 15</ref>. Top-1 and top-5 accuracy (in %) under linear evaluation on the ImageNet validation set for a ResNet50 (1x) encoder set for corrupting the saliency masks by adding/remove a rectangle from the image center. The rectangle is a percentage (mp) of the saliency mask area (the higher the percentage the higher the corruption). The corrupted saliency masks are applied on top of the large augmented views during training with probability 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Other model hyperparameters</head><p>Now we turn our attention to ablating the effect of other model hyperparameters on the downstream performance of RELICv2 representations. Note that these hyperparameters have been introduced and extensively ablated in prior work <ref type="bibr">(Grill et al., 2020;</ref><ref type="bibr">Mitrovic et al., 2021;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of negatives.</head><p>As mentioned in Section 2 RELICv2 selects negatives by randomly subsampling the minibatch in order to avoid false negatives. We investigate the effect of changing number of negatives in table 16. We can see that the best performance can be achieved with relatively low numbers of negatives, i.e. just 10 negatives. Furthermore, we see that using the whole batch as negatives has one of the lowest performances.</p><p>In further experiments, we observed that for longer pretraining (e.g. 1000 epochs) there is less variation in performance than for pretraining for 300 epoch which itself is also quite low.</p><p>Target EMA. RELICv2 uses a target network whose weights are an exponential moving average (EMA) of the online encoder network which is trained normally using stochastic gradient descent; this is a setup first introduced in (Grill et al., 2020) and subsequently used in <ref type="bibr">(Mitrovic et al., 2021)</ref> among others. The target network weights at iteration t are ? t = ?? t?1 + (1 ? ?)? t where ? is the EMA parameter which controls the stability of the target network (? = 0 sets ? t = ? t ); ? t are the parameters of the online encoder at time t, while ? t are the parameters of the target encoder at time t.</p><p>As can be seen from table 17, all decay rates between 0.9 and 0.996 yield similar performance for top-1 accuracy on the ImageNet validation set after pretraining for 300 epochs indicating that RELICv2 is robust to choice of ? in that range. For values of ? of 0.999 and higher, the performance quickly degrades indicating that the updating of the target network is too slow. Note that contrary to <ref type="bibr">(Grill et al., 2020)</ref> where top-1 accuracy drops below 20% for ? = 1, RELICv2 is significantly more robust to this setting achieving double that accuracy.  <ref type="table" target="#tab_15">Table 17</ref>. Top-1 and top-5 accuracy (in %) under linear evaluation on the ImageNet validation set for a ResNet50 (1x) encoder set for different setting of the target exponentially moving average (EMA). All settings are trained for 300 epochs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Top-1 linear evaluation accuracy on ImageNet using ResNet50 encoders with 1?, 2? and 4? width multipliers and a ResNet200 encoder with a 2? width multiplier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FFigure 2 .</head><label>2</label><figDesc>Transfer performance relative to the supervised baseline (a value of 0 indicates equal performance to supervised).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Distances between nearest-neighbour representations. Each coloured point in a row represents one of the five nearest neighbours of the representation of that image where the colour indicates the distance between the points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Distribution of the linear discriminant ratio: the ratio of between-class distances and within-class distances of embeddings computed on the ImageNet validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>ImageNet accuracy obtained by RELICv2 as a function of number of images seen during pre-training for a variety of ResNet architectures. The number of parameters of each model is in parenthesis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of ImageNet top-1 accuracy between RELICv2 and recent vision transformer-based architectures (Swin (Liu et al., 2021) represents a fully supervised transformer baseline).of the anchor point(Robinson et al., 2020), or uniformly-atrandom sampling a small number of points to avoid false negatives(Mitrovic et al., 2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Illustration of how for each image in the ImageNet training set (left) we use our unsupervised version of DeepUSPS to obtain the saliency mask (middle) which we then apply on top of the image to obtain the image with the background removed (right).A.2.1. USING DEEPUSPS TO OBTAIN SALIENCY MASKSDeepUSPS(Nguyen et al., 2019)  is an unsupervised saliency prediction method that uses self-supervision to refine pseudolabels from a number of handcrafted saliency methods. To train DeepUSPS, we firstly sample a random subset of 2500 ImageNet images; note that the original implementation of DeepUSPS uses 2500 images from the MSRA-B dataset. We instead use a randomly selected subset of the ImageNet training set of the same size to ensure a fair comparison to previous work. As the handcrafted saliency methods we use Robust Background Detection (RBD)(Zhu et al., 2014), Manifold Ranking (MR)(Yang et al., 2013), Dense and Sparse Reconstruction (DSR)(Li et al., 2013)  and Markov Chain (MC) (Jiang</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>CIFAR10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al., 2009), Birdsnap (Berg et al., 2014), SUN397 (split 1) (Xiao et al., 2010), DTD (split 1) (Cimpoi et al., 2014), Cars (Krause et al., 2013) Aircraft (Maji et al., 2013), Pets (Parkhi et al., 2012), Caltech101 (Fei-Fei et al., 2004), and Flowers (Nilsback and Zisserman, 2008).Again in line with previous methods(Chen et al., 2020a; Grill et al., 2020; Dwibedi et al., 2021), for Food101<ref type="bibr" target="#b6">(Bossard et al., 2014)</ref>, CIFAR10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al., 2009), Birdsnap<ref type="bibr" target="#b3">(Berg et al., 2014)</ref>, SUN397 (split 1)(Xiao et al., 2010), DTD (split 1)(Cimpoi et al., 2014), and Cars (Krause et al., 2013)  we report the Top-1 accuracy on the test set, and for Aircraft(Maji et al., 2013), Pets (Parkhi et al., 2012), Caltech101 (Fei-Fei et al., 2004), and  Flowers (Nilsback and Zisserman, 2008 we report the mean per-class accuracy as the relevant metric in the comparisons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>-crop.RELIC (Mitrovic et al., 2021)  constructs views using 2 crops of size 224 ? 224.<ref type="bibr" target="#b9">(Caron et al., 2020)</ref> suggested using 2 crops of size 224?224 and 6 crops of size 96?96. Here we ablate the use of different numbers of large and small crops using only standard SimCLR augmentations (without saliency masks).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60 70 80 ImageNet top-1 accuracy (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30M Images seen during pre-training 60M</cell><cell>90M</cell><cell>120M</cell></row><row><cell cols="6">Crops [2, 0] [2, 2] [2, 6] [4, 2] [6, 2] [8, 2]</cell></row><row><cell>Top-1 74.3</cell><cell>76.2</cell><cell>76.0</cell><cell>76.8</cell><cell>76.5</cell><cell>76.5</cell></row></table><note>We find that using multi-crop improves results significantly over the RELIC baseline ([2,0]). However we observe that</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>compares recent ViT-based methods against RELICv2 using a variety of larger ResNet architectures. Notably, RELICv2 outperforms DINO<ref type="bibr" target="#b0">(Caron et al., 2021)</ref> and MoCo v3(Chen et al., 2021)  and exhibits similar performance toEsViT (Li et al., 2021)  for comparable parameter counts despite these methods using more powerful architectures and more involved training procedures. Our results suggest that combining the insights we have developed with RELICv2 alongside recent architectural innovations could lead to further improvements in representation learning and more powerful foundation models. An incremental bayesian approach tested on 101 object categories.In 2004 conference on computer vision and pattern recognition workshop, pages 178-178. IEEE, 2004. Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 2. Springer series in statistics New York, 2009. Jiang, Lihe Zhang, Huchuan Lu, Chuan Yang, and Ming-Hsuan Yang. Saliency detection via absorbing markov chain. In Proceedings of the IEEE international conference on computer vision, pages 1665-1672, 2013. Sham M Kakade and Dean P Foster. Multi-view regression via canonical correlation analysis. In International Conference on Computational Learning Theory, pages 82-96. Springer, 2007. McWilliams and Giovanni Montana. Multi-view predictive partitioning in high dimensions. Statistical Analysis and Data Mining: The ASA Data Science Journal, 5 (4):304-321, 2012. Brian McWilliams, David Balduzzi, and Joachim M Buhmann. Correlated random features for fast semisupervised learning. In Advances in Neural Information Processing Systems, pages 440-448, 2013. Jovana Mitrovic, Brian McWilliams, and Melanie Rey. Less can be more in contrastive learning. PMLR, 2020. Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with hard negative samples. arXiv preprint arXiv:2010.04592, 2020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211-252, 2015. Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Saliency detection via graph-based manifold ranking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3166-3173, 2013.</figDesc><table><row><cell>Bowen Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint arXiv:2004.11362, 2020. Joshua Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learn-ing. In International Conference on Machine Learning, Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. pages 5628-5637, 2019. Karthik Sridharan and Sham M Kakade. An information theoretic framework for multi-view learning. 2008.</cell><cell>Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P?rez, and Matthieu Cord. Learning representations by predicting bags of visual words. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6928-6938, 2020. Brian Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars Chuan Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv preprint arXiv:1708.03888, 2017. Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Di-lated residual networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 472-480, 2017. Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Ko-modakis, Matthieu Cord, and Patrick Perez. Obow: On-line bag-of-visual-words generation for self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6830-Buesing, and Charles Blundell. Representation learn-Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and ing via invariant causal mechanisms. In International Conference on Learning Representations (ICLR), 2021. Lucas Beyer. Scaling vision transformers. arXiv preprint arXiv:2106.04560, 2021.</cell></row><row><cell>Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-</cell><cell>Nanxuan Zhao, Zhirong Wu, Rynson WH Lau, and Stephen 6840, 2021. Lin. Distilling localization for self-supervised representa-</cell></row><row><cell>nav Gupta. Revisiting unreasonable effectiveness of data</cell><cell>tion learning. Association for the Advancement of Artifi-</cell></row><row><cell>in deep learning era. In Proceedings of the IEEE inter-</cell><cell>cial Intelligence, 2021.</cell></row><row><cell>national conference on computer vision, pages 843-852,</cell><cell></cell></row><row><cell>2017.</cell><cell></cell></row><row><cell>Yonglong Tian, Olivier J Henaff, and Aaron van den Oord.</cell><cell></cell></row><row><cell>Divide and contrast: Self-supervised learning from uncu-</cell><cell></cell></row><row><cell>rated data. arXiv preprint arXiv:2105.08054, 2021.</cell><cell></cell></row><row><cell>Michael Tschannen, Josip Djolonga, Paul K Rubenstein,</cell><cell></cell></row><row><cell>Sylvain Gelly, and Mario Lucic. On mutual information</cell><cell></cell></row><row><cell>maximization for representation learning. arXiv preprint</cell><cell></cell></row><row><cell>arXiv:1907.13625, 2019.</cell><cell></cell></row><row><cell>Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P</cell><cell></cell></row><row><cell>Xing. Learning robust global representations by penaliz-</cell><cell></cell></row><row><cell>ing local predictive power. In Advances in Neural Infor-</cell><cell></cell></row><row><cell>mation Processing Systems, pages 10506-10518, 2019.</cell><cell></cell></row><row><cell>Ross Wightman, Hugo Touvron, and Herv? J?gou. Resnet Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre strikes back: An improved training procedure in timm. Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations. arXiv preprint arXiv:2104.14548, arXiv preprint arXiv:2110.00476, 2021. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, 2021. Zhang, Stephen Lin, and Baining Guo. Swin transformer: and Antonio Torralba. Sun database: Large-scale scene</cell><cell>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya 8340-8349, Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-2021. try, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-Learning transferable visual models from natural lan-</cell></row><row><cell>M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, Hierarchical vision transformer using shifted windows. recognition from abbey to zoo. In 2010 IEEE computer</cell><cell>ing the knowledge in a neural network. arXiv preprint guage supervision. arXiv preprint arXiv:2103.00020,</cell></row><row><cell>and A. Zisserman. The pascal visual object classes (voc) arXiv preprint arXiv:2103.14030, 2021. society conference on computer vision and pattern recog-</cell><cell>arXiv:1503.02531, 2015. 2021.</cell></row><row><cell>challenge. International Journal of Computer Vision, 88 nition, pages 3485-3492. IEEE, 2010.</cell><cell></cell></row><row><cell>(2):303-338, June 2010.</cell><cell></cell></row></table><note>Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gen- erative visual models from few training examples:Jean-Bastien Grill, Florian Strub, Florent Altch?, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020. Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised deep learning with spectral contrastive loss. arXiv preprint arXiv:2106.04156, 2021. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019. Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Ka- davath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pagesOlivier J. H?naff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van den Oord, Oriol Vinyals, and Jo?o Carreira. Efficient visual pretraining with contrastive detection, 2021.In Proceedings of the IEEE international conference on computer vision workshops, pages 554-561, 2013. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. Kuang-Huei Lee, Anurag Arnab, Sergio Guadarrama, John Canny, and Ian Fischer. Compressive visual representa- tions. arXiv preprint arXiv:2109.12909, 2021. Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng Gao. Effi- cient self-supervised vision transformers for representa- tion learning. arXiv preprint arXiv:2106.09785, 2021. Xiaohui Li, Huchuan Lu, Lihe Zhang, Xiang Ruan, and Ming-Hsuan Yang. Saliency detection via dense and sparse reconstruction. In Proceedings of the IEEE interna- tional conference on computer vision, pages 2976-2983, 2013. Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Advances in Neural Information Processing Systems, 32:6665-6675, 2019. Tie Liu, Zejian Yuan, Jian Sun, Jingdong Wang, Nanning Zheng, Xiaoou Tang, and Heung-Yeung Shum. Learning to detect a salient object. IEEE Transactions on Pattern analysis and machine intelligence, 33(2):353-367, 2010.Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual clas- sification of aircraft. arXiv preprint arXiv:1306.5151, 2013.Duc Tam Nguyen, Maximilian Dax, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Zhongyu Lou, and Thomas Brox. Deepusps: Deep robust unsupervised saliency prediction with self- supervision. Advances in Neural Information Processing Systems, 2019. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing, pages 722-729. IEEE, 2008. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Rep- resentation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 3498- 3505. IEEE, 2012. Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta pseudo labels. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pages 11557-11568, 2021.Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389-5400. PMLR, 2019.Yonglong Tian, C. Sun, Ben Poole, Dilip Krishnan, C. Schmid, and Phillip Isola. What makes for good views for contrastive learning. ArXiv, abs/2005.10243, 2020.Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V Le. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848, 2019.Wangjiang Zhu, Shuang Liang, Yichen Wei, and Jian Sun. Saliency optimization from robust background detection. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition, pages 2814-2821, 2014. Wenbin Zou and Nikos Komodakis. Harf: Hierarchy- associated rich features for salient object detection. In Proceedings of the IEEE international conference on com- puter vision, pages 406-414, 2015.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 .</head><label>4</label><figDesc>Parameters of data augmentation scheme. Small/large indicates small or large crop.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>1 and top-5 accuracies of different methods under a varied set of ResNet encoders of different sizes, spanning ResNet50, ResNet101, ResNet152 and ResNet200 and layer widths of 1?, 2? and 4?. ResNet50 with 2? and 4? wider layers has 94 and 375 million parameters, respectively. ResNet101, ResNet152, ResNet200 and ResNet200 2? have 43, 58, 63 and 250 million parameters, respectively.In the following table 5, we present results under linear evaluation on the ImageNet validation set a varied set of ResNet architectures; we compare against different unsupervised representation learning methods and use as the supervised baselines the results reported in(Chen et al., 2020a; Grill et al., 2020). Note that the supervised baselines reported in (Chen et al., 2020a) are extensively used throughout the self-supervised literature in order to compare performance against supervised learning. For architectures for which supervised baselines are not available in (Chen et al., 2020a), we use supervised baselines reported in(Grill et al., 2020)  which use stronger augmentations for training supervised models than(Chen et al.,  2020a)  and as such do not represent a direct like-for-like comparison with self-supervised methods.Across this varied set of ResNet architectures, RELICv2 outperforms supervised baselines in all cases with margins up to 1.2% in absolute terms.</figDesc><table><row><cell>Method</cell><cell cols="2">Top-1 Top-5</cell><cell>Method</cell><cell cols="2">Top-1 Top-5</cell></row><row><cell>Supervised (Chen et al., 2020a) MoCo (He et al., 2019) SimCLR (Chen et al., 2020a) BYOL (Grill et al., 2020) SwAV (Caron et al., 2020) C-BYOL (Lee et al., 2021) RELICv2 (ours)</cell><cell>77.8 65.4 74.2 77.4 77.3 78.8 79.0</cell><cell>--92.0 93.6 -94.5 94.5</cell><cell>Supervised (Chen et al., 2020a) MoCo (He et al., 2019) SimCLR (Chen et al., 2020a) SwAV (Caron et al., 2020) BYOL (Grill et al., 2020) RELICv2 (ours)</cell><cell>78.9 68.6 76.5 77.9 78.6 79.4</cell><cell>--93.2 -94.2 94.3</cell></row><row><cell cols="2">(a) ResNet50 2? encoder.</cell><cell></cell><cell cols="2">(b) ResNet50 4? encoder.</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Top-1 Top-5</cell><cell>Method</cell><cell cols="2">Top-1 Top-5</cell></row><row><cell>Supervised (Grill et al., 2020)</cell><cell>78.0</cell><cell>94.0</cell><cell>Supervised (Grill et al., 2020)</cell><cell>79.1</cell><cell>94.5</cell></row><row><cell>BYOL (Grill et al., 2020)</cell><cell>76.4</cell><cell>9.0</cell><cell>BYOL (Grill et al., 2020)</cell><cell>77.3</cell><cell>93.7</cell></row><row><cell>RELICv2 (ours)</cell><cell>78.7</cell><cell>94.4</cell><cell>RELICv2 (ours)</cell><cell>79.3</cell><cell>94.6</cell></row><row><cell cols="2">(c) ResNet101 encoder.</cell><cell></cell><cell cols="2">(d) ResNet152 encoder.</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Top-1 Top-5</cell><cell>Method</cell><cell cols="2">Top-1 Top-5</cell></row><row><cell>Supervised (Grill et al., 2020)</cell><cell>79.3</cell><cell>94.6</cell><cell>Supervised (Grill et al., 2020)</cell><cell>80.1</cell><cell>95.2</cell></row><row><cell>BYOL (Grill et al., 2020)</cell><cell>77.8</cell><cell>93.9</cell><cell>BYOL (Grill et al., 2020)</cell><cell>79.6</cell><cell>94.8</cell></row><row><cell>RELICv2 (ours)</cell><cell>79.8</cell><cell>95.0</cell><cell>RELICv2 (ours)</cell><cell>80.6</cell><cell>95.2</cell></row><row><cell cols="2">(e) ResNet200 encoder.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(f) ResNet200 2? encoder.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Top-1 and top-5 accuracy (in %) under linear evaluation on the ImageNet validation set for a varied set of ResNet architectures.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 .</head><label>6</label><figDesc>Top-1 and top-5 accuracy (in %) after semi-supervised training with a fraction of ImageNet labels for different ResNet encoders and unsupervised representation learning methods. Results are reported on the ImageNet validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>table 7</head><label>7</label><figDesc>Nesterov {True, False}, and the number of training epochs. For linear transfer we considered setting epochs among {20, 30, 60, 80, 100}, and for fine-tuning, we also considered {150, 200, 250}, for datasets where lower learning rates were preferable. Models were trained with the SGD optimizer with momentum.</figDesc><table><row><cell>. We swept over learning rates {.01, 0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 1., 2.}, batch sizes {128,</cell></row><row><cell>256, 512, 1024}, weight decay between {1e?6, 1e?5, 1e?4, 1e?3, 0.01, 0.1}, warmup epochs {0, 10}, momentum {0.9,</cell></row><row><cell>0.99},</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>table 9 .</head><label>9</label><figDesc>Furthermore, RELICv2 learns representations that outperform competing self-supervised methods while being on par with supervised performance in terms of OOD generalization; see table 8b.</figDesc><table><row><cell>Method</cell><cell>MF T-0.7</cell><cell>Ti</cell><cell>IN-C</cell></row><row><cell>Supervised</cell><cell cols="3">65.1 73.9 78.4 40.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 8 .</head><label>8</label><figDesc>TopFor results reported in table 3, we use the following training and evaluation protocol. To pretrain RELICv2 on the Joint Foto Tree (JFT-300M) dataset, we used a base learning rate of 0.3 for pretraining the representations for 1000 ImageNetequivalent epochs. For longer pretraining of 3000 and 5000 ImageNet-equivalent epochs, we use a lower base learning rate of 0.2. We set the target exponential moving average to 0.996, the contrast scale to 0.3, temperature to 0.2 and the saliency mask apply probability to 0.15 for all lenghts of pretraining. For 1000 and 5000 ImageNet-equivalent epochs we</figDesc><table><row><cell cols="15">-1 Accuracy (in %) under linear evaluation on the ImageNetV2 and ImageNet-C datasets (robustness datasets) and ImageNet-R</cell></row><row><cell cols="15">(IN-R), ImageNet-Sketch (IN-S), ObjectNet (out-of-distribution datasets) for different unsupervised representation learning methods.</cell></row><row><cell cols="15">We evaluate on all three variants on ImageNetv2 -matched frequency (MF), Threshold 0.7 (T-0.7) and Top Images (TI). The results for</cell></row><row><cell cols="7">ImageNet-C (IN-C) are averaged across the 15 different corruptions.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="15">C. Pretraining on Joint Foto Tree (JFT-300M) -implementation details and additional results</cell></row><row><cell cols="2">C.1. Linear evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Blur</cell><cell></cell><cell></cell><cell></cell><cell>Weather</cell><cell></cell><cell></cell><cell>Digital</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="14">Gauss Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG</cell></row><row><cell>Supervised (Lim et al., 2019)</cell><cell>37.1</cell><cell>35.1</cell><cell>30.8</cell><cell>36.8</cell><cell>25.9</cell><cell>34.9</cell><cell>38.1</cell><cell>34.5</cell><cell>40.7 56.9</cell><cell>68.1</cell><cell>40.6</cell><cell>45.6</cell><cell>32.6</cell><cell>56.0</cell></row><row><cell>SimCLR (Chen et al., 2020a)</cell><cell>29.1</cell><cell>26.3</cell><cell>17.3</cell><cell>22.1</cell><cell>14.7</cell><cell>20.0</cell><cell>18.6</cell><cell>27.2</cell><cell>33.3 46.2</cell><cell>59.7</cell><cell>53.9</cell><cell>31.0</cell><cell>24.2</cell><cell>43.9</cell></row><row><cell>BYOL (Grill et al., 2020)</cell><cell>41.5</cell><cell>38.7</cell><cell>31.9</cell><cell>37.8</cell><cell>22.5</cell><cell>31.6</cell><cell>29.6</cell><cell>35.1</cell><cell>42.9 60.1</cell><cell>69.0</cell><cell>58.4</cell><cell>41.5</cell><cell>46.3</cell><cell>55.9</cell></row><row><cell>RELIC (Mitrovic et al., 2021)</cell><cell>43.4</cell><cell>40.7</cell><cell>36.6</cell><cell>40.5</cell><cell>24.5</cell><cell>34.3</cell><cell>30.5</cell><cell>36.6</cell><cell>43.8 61.4</cell><cell>69.5</cell><cell>59.5</cell><cell>42.8</cell><cell>46.8</cell><cell>57.3</cell></row><row><cell>RELICv2 (ours)</cell><cell>41.6</cell><cell>39.0</cell><cell>31.1</cell><cell>39.7</cell><cell>22.6</cell><cell>35.2</cell><cell>34.5</cell><cell>40.1</cell><cell>46.1 64.5</cell><cell>71.0</cell><cell>60.0</cell><cell>44.6</cell><cell>46.6</cell><cell>58.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>table 10</head><label>10</label><figDesc>Nesterov {True, False}, and the number of training epochs {60, 80, 100}. Models were trained with the SGD optimizer with momentum.</figDesc><table><row><cell>. We swept over learning rates {.01, 0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 1., 2.}, batch sizes</cell></row><row><cell>{128, 256, 512, 1024}, weight decay between {1e?6, 1e?5, 1e?4, 1e?3, 0.01, 0.1}, warmup epochs {0, 10}, momentum</cell></row><row><cell>{0.9, 0.99},</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>D.1. Using different datasets for obtaining the saliency masksIn the main text in Sections 3.1, 3.2, 3.3, 3.4 we used a DeepUSPS(Nguyen et al., 2019)  saliency detection network trained only on a randomly selected subset of 2500 ImageNet images. Here we explore whether using additional data could help improve the performance of the saliency estimation and of the overall representations learnt by RELICv2. For this purpose, we use the MSRA-B dataset(Liu et al., 2010), which was originally used by DeepUSPS to train their saliency detection network. MSRA-B consists of 2500 training images for which handcrafted masks computed with the methods RobustWe explored whether using saliency masks obtained from training DeepUSPS on the MSRA-B affects performance of RELICv2 pre-training on ImageNet. We noticed that for RELICv2 representations pretrained on ImageNet for 1000 epochs, we get 77.2% top-1 and 93.3% top-5 accuracy under linear evaluation on the ImageNet validation set for a ResNet50 (1x) encoder. The slight performance gains may due to the larger variety of images in MSRA-B used for training the saliency detection network, as opposed to the random sample of 2500 ImageNet images that we used for training DeepUSPS directly on the ImageNet dataset.</figDesc><table><row><cell>Background Detection (RBD) (Zhu et al., 2014), Hierarchy-associated Rich Features (HS) (Zou and Komodakis, 2015),</cell></row><row><cell>Dense and Sparse Reconstruction (DSR) (Li et al., 2013) and Markov Chain (MC) (Jiang et al., 2013) are already available.</cell></row><row><cell>We use the same hyperparameters as described in Section A.2.1 to train DeepUSPS on MSRA-B.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 13</head><label>13</label><figDesc>reports the top-1 and top-5 accuracy for varying the mask apply probability p m between 0 and 1 and for using the ImageNet vs. the MSRA-B dataset(Liu et al., 2010)  for training DeepUSPS. Note that using the additional images from the MSRA-B dataset to train the DeepUSPS saliency detection network results in better saliency masks which translates to better performanceTable 12. Top-1 and top-5 accuracy (in %) under linear evaluation on the ImageNet validation set for BYOL trained using different probabilities of using the saliency mask to remove the background of the augmented images. Models are trained for 300 epochs. when using the saliency masks during RELICv2 training.</figDesc><table><row><cell cols="2">Mask probability p m</cell><cell>0</cell><cell cols="2">0.1 0.15 0.2 0.25 0.3</cell></row><row><cell>BYOL Top-1</cell><cell></cell><cell cols="3">73.1 73.4 73.2 73.3 72.8 71.8</cell></row><row><cell>Top-5</cell><cell></cell><cell cols="3">91.2 91.3 91.2 91.3 90.8 90.1</cell></row><row><cell cols="5">DeepUSPS trained on ImageNet DeepUSPS trained on MSRA-B</cell></row><row><cell cols="2">Mask probability p m Top-1</cell><cell>Top-5</cell><cell>Top-1</cell><cell>Top-5</cell></row><row><cell>0</cell><cell>75.2</cell><cell>92.4</cell><cell>75.2</cell><cell>92.4</cell></row><row><cell>0.05</cell><cell>75.3</cell><cell>92.6</cell><cell>75.2</cell><cell>92.6</cell></row><row><cell>0.1</cell><cell>75.4</cell><cell>92.5</cell><cell>75.3</cell><cell>92.4</cell></row><row><cell>0.15</cell><cell>75.2</cell><cell>92.5</cell><cell>75.5</cell><cell>92.5</cell></row><row><cell>0.2</cell><cell>75.2</cell><cell>92.5</cell><cell>75.6</cell><cell>92.6</cell></row><row><cell>0.25</cell><cell>75.0</cell><cell>92.3</cell><cell>75.3</cell><cell>92.5</cell></row><row><cell>0.3</cell><cell>75.1</cell><cell>92.3</cell><cell>74.8</cell><cell>92.4</cell></row><row><cell>0.4</cell><cell>75.0</cell><cell>92.3</cell><cell>75.3</cell><cell>92.5</cell></row><row><cell>0.5</cell><cell>74.7</cell><cell>92.2</cell><cell>75.0</cell><cell>92.4</cell></row><row><cell>0.6</cell><cell>75.0</cell><cell>92.3</cell><cell>75.0</cell><cell>92.3</cell></row><row><cell>0.7</cell><cell>74.4</cell><cell>92.3</cell><cell>74.6</cell><cell>92.0</cell></row><row><cell>0.8</cell><cell>73.9</cell><cell>91.7</cell><cell>75.0</cell><cell>92.1</cell></row><row><cell>0.9</cell><cell>74.0</cell><cell>91.7</cell><cell>74.6</cell><cell>92.0</cell></row><row><cell>1.0</cell><cell>73.7</cell><cell>91.7</cell><cell>74.5</cell><cell>92.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc>Table 14. Top-1 and top-5 accuracy (in %) under linear evaluation on the ImageNet validation set for a ResNet50 (1x) encoder set for using different types of random masks that cover various percentage areas (ap) of the full image. These random masks are applied on top of the large augmented views during training with probability 0.1. Models are trained for 300 epochs.</figDesc><table><row><cell></cell><cell cols="2">Random</cell><cell cols="2">Rectangle</cell><cell cols="2">Centered Rectangle</cell></row><row><cell cols="6">Image percentage area a p Top-1 Top-5 Top-1 Top-5 Top-1</cell><cell>Top-5</cell></row><row><cell>10%</cell><cell>70.8</cell><cell>89.9</cell><cell>70.9</cell><cell>90.3</cell><cell>71.3</cell><cell>90.1</cell></row><row><cell>20%</cell><cell>72.2</cell><cell>90.7</cell><cell>73.1</cell><cell>91.3</cell><cell>73.4</cell><cell>91.3</cell></row><row><cell>30%</cell><cell>72.9</cell><cell>91.3</cell><cell>73.8</cell><cell>91.8</cell><cell>73.8</cell><cell>91.9</cell></row><row><cell>40%</cell><cell>73.1</cell><cell>91.4</cell><cell>74.2</cell><cell>91.9</cell><cell>74.1</cell><cell>92.0</cell></row><row><cell>50%</cell><cell>73.3</cell><cell>91.5</cell><cell>74.0</cell><cell>92.0</cell><cell>74.3</cell><cell>92.0</cell></row><row><cell>60%</cell><cell>73.6</cell><cell>91.8</cell><cell>74.2</cell><cell>92.1</cell><cell>74.3</cell><cell>92.2</cell></row><row><cell>70%</cell><cell>73.7</cell><cell>91.9</cell><cell>74.4</cell><cell>92.1</cell><cell>74.4</cell><cell>92.2</cell></row><row><cell>80%</cell><cell>74.1</cell><cell>92.1</cell><cell>74.4</cell><cell>92.2</cell><cell>74.2</cell><cell>92.1</cell></row><row><cell>90%</cell><cell>74.1</cell><cell>92.2</cell><cell>74.4</cell><cell>92.1</cell><cell>74.2</cell><cell>92.2</cell></row><row><cell></cell><cell cols="6">Add rectangle to mask Remove rectangle from mask</cell></row><row><cell cols="2">Mask percentage area m p Top-1</cell><cell cols="2">Top-5</cell><cell>Top-1</cell><cell></cell><cell>Top-5</cell></row><row><cell>10%</cell><cell>75.2</cell><cell>92.5</cell><cell></cell><cell>75.2</cell><cell></cell><cell>92.3</cell></row><row><cell>20%</cell><cell>75.3</cell><cell>92.6</cell><cell></cell><cell>75.1</cell><cell></cell><cell>92.4</cell></row><row><cell>30%</cell><cell>75.1</cell><cell>92.3</cell><cell></cell><cell>74.7</cell><cell></cell><cell>92.2</cell></row><row><cell>40%</cell><cell>74.9</cell><cell>92.2</cell><cell></cell><cell>74.6</cell><cell></cell><cell>92.2</cell></row><row><cell>50%</cell><cell>74.9</cell><cell>92.4</cell><cell></cell><cell>74.5</cell><cell></cell><cell>92.0</cell></row><row><cell>60%</cell><cell>74.9</cell><cell>92.2</cell><cell></cell><cell>74.0</cell><cell></cell><cell>91.7</cell></row><row><cell>70%</cell><cell>74.8</cell><cell>92.2</cell><cell></cell><cell>73.6</cell><cell></cell><cell>91.7</cell></row><row><cell>80%</cell><cell>74.8</cell><cell>92.4</cell><cell></cell><cell>73.4</cell><cell></cell><cell>91.4</cell></row><row><cell>90%</cell><cell>74.7</cell><cell>92.2</cell><cell></cell><cell>73.0</cell><cell></cell><cell>91.3</cell></row><row><cell>100%</cell><cell>74.6</cell><cell>92.3</cell><cell></cell><cell>72.6</cell><cell></cell><cell>90.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head></head><label></label><figDesc>Table 16. Top-1 and top-5 accuracy (in %) under linear evaluation on the ImageNet validation set for a ResNet50 (1x) encoder set for different numbers of randomly selected negatives. All settings are trained for 300 epochs.</figDesc><table><row><cell cols="3">Number of negatives Top-1 Top-5</cell></row><row><cell>1</cell><cell>75.1</cell><cell>92.4</cell></row><row><cell>5</cell><cell>75.2</cell><cell>92.6</cell></row><row><cell>10</cell><cell>75.4</cell><cell>92.5</cell></row><row><cell>20</cell><cell>75.3</cell><cell>92.7</cell></row><row><cell>50</cell><cell>75.5</cell><cell>92.5</cell></row><row><cell>100</cell><cell>75.4</cell><cell>92.5</cell></row><row><cell>500</cell><cell>75.1</cell><cell>92.4</cell></row><row><cell>1000</cell><cell>75.3</cell><cell>92.6</cell></row><row><cell>2000</cell><cell>75.4</cell><cell>92.5</cell></row><row><cell>4096</cell><cell>75.2</cell><cell>92.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Commonly measured by how well a method performs under a standard linear evaluation protocol on ImageNet. See section 3.1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"> P(xi)  and N (xi) may be sampled from B or more generally from some other data structure which accumulates points or statistics from successive batches such as a queue(Dwibedi et al., 2021;  He et al., 2019).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Both RELICv2 and the supervised baseline were trained on the ImageNet training set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Relja Arandjelovic, Yusuf Aytar, Andrew Zisserman, Koray Kavukcuoglu, Daan Wierstra and Karen Simonyan for discussions on this work and feedback on the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As a companion to section 5, table 18 provides a detailed comparison in terms of how prominent representation learning methods utilize positive and negative examples and how they incorporate both explicit contrastive and invariance losses. Here aug(x i ) refers to the standard set of SimCLR augmentations <ref type="bibr">(Chen et al., 2020a)</ref>, nn(x i ) refers to a scheme which selects nearest neighbours of x i , mc(x i ) are multicrop augmentations (c.f. <ref type="bibr" target="#b9">(Caron et al., 2020)</ref>). proto + (x i ) and proto ? (x i ) refer to using prototypes computed via an explicit clustering step c.f. <ref type="bibr" target="#b9">(Caron et al., 2020)</ref>. Finally, sal(x i ) refers to a scheme which computes saliency masks of x i and removes backgrounds as described in section 2. Note that SwAV first computes a clustering of the batch then contrasts the embedding of the point and its nearest cluster centroid (proto + ) against the remaining K ? 1 cluster centroids (proto ? ); invariance is implicitly enforced in the clustering step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Contrastive  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rabbat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13963</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15509" to="15519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung</forename><surname>Woo Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled data with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simran</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Epochs IN-R IN-Sketch ObjectNet</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imagenet-R</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">ImageNet-Sketch and ObjectNet datasets</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">ImageNet-Sketch (IN-S) and ObjectNet out-ofdistribution datasets and on ImageNetV2 dataset for RELICv2 pre-trained on JFT-300M for different numbers of ImageNet-equivalent epochs. We evaluate on all three variants on ImageNetV2 -matched frequency (MF), Threshold 0.7 (T-0.7) and Top Images (TI). The results for ImageNet-C</title>
		<imprint/>
	</monogr>
	<note>Table 11. Top-1 Accuracy (in %) under linear evaluation on the the ImageNet-R (IN-R). IN-C) are averaged across the 15 different corruptions</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
