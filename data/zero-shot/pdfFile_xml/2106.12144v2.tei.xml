<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NODEPIECE: COMPOSITIONAL AND PARAMETER- EFFICIENT REPRESENTATIONS OF LARGE KNOWL- EDGE GRAPHS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
							<email>mikhail.galkin@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<settlement>Mila</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Denis</surname></persName>
							<email>deniseti@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<settlement>Mila</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
							<email>jiapeng.wu@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<settlement>Mila</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
							<email>hamilton@mila.quebec</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University Montreal</orgName>
								<address>
									<settlement>Mila</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NODEPIECE: COMPOSITIONAL AND PARAMETER- EFFICIENT REPRESENTATIONS OF LARGE KNOWL- EDGE GRAPHS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector. Such a shallow lookup results in a linear growth of memory consumption for storing the embedding matrix and incurs high computational costs when working with real-world KGs. Drawing parallels with subword tokenization commonly used in NLP, we explore the landscape of more parameter-efficient node embedding strategies. To this end, we propose NodePiece, an anchor-based approach to learn a fixed-size entity vocabulary. In NodePiece, a vocabulary of subword/sub-entity units is constructed from anchor nodes in a graph with known relation types. Given such a fixed-size vocabulary, it is possible to bootstrap an encoding and embedding for any entity, including those unseen during training. Experiments show that NodePiece performs competitively in node classification, link prediction, and relation prediction tasks while retaining less than 10% of explicit nodes in a graph as anchors and often having 10x fewer parameters. To this end, we show that a NodePiece-enabled model outperforms existing shallow models on a large OGB WikiKG 2 graph having~70x fewer parameters 1 . 1  The code is available on GitHub: https://github.com/migalkin/NodePiece 2 We then concentrate on nodes as usually their size is orders of magnitude larger than that of edge types.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Representation learning tasks on knowledge graphs (KGs) often require a parameterization of each unique atom in the graph with a vector or matrix. Traditionally, in multi-relational KGs such atoms constitute a set of all nodes n ? N (entities) and relations (edge types) r ? R <ref type="bibr" target="#b29">(Nickel et al., 2016)</ref>. Assuming parameterization with vectors, atoms are mapped to d-dimensional vectors through shallow encoders f n : n ? R d and f r : r ? R d which scale linearly to the number of nodes and edge types 2 , i.e., having O(|N |) space complexity of the entity embedding matrix. Albeit efficient on small conventional benchmarking datasets based on Freebase <ref type="bibr" target="#b45">(Toutanova &amp; Chen, 2015</ref>) (~15K nodes) and WordNet <ref type="bibr" target="#b12">(Dettmers et al., 2018</ref>) (~40K nodes), training on larger graphs (e.g., <ref type="bibr">YAGO 3-10 (Mahdisoltani et al., 2015)</ref> of 120K nodes) becomes computationally challenging. Scaling it further up to larger subsets <ref type="bibr" target="#b18">(Hu et al., 2020;</ref><ref type="bibr" target="#b38">Safavi &amp; Koutra, 2020)</ref> of Wikidata <ref type="bibr" target="#b48">(Vrandecic &amp; Kr?tzsch, 2014)</ref> requires a top-level GPU or a CPU cluster as done in, e.g., PyTorch-BigGraph  that maintains a 78M ? 200d embeddings matrix in memory (we list sizes of current best performing models in <ref type="table">Table 1</ref>).</p><p>Taking the perspective from NLP, shallow node encoding in KGs corresponds to shallow word embedding popularized with word2vec <ref type="bibr" target="#b27">(Mikolov et al., 2013)</ref> and GloVe <ref type="bibr" target="#b32">(Pennington et al., 2014)</ref> that learned a vocabulary of 400K-2M most frequent words, treating rarer ones as out-of-vocabulary (OOV). The OOV issue was resolved with the ability to build infinite combinations with a finite vocabulary enabled by subword units. Subword-powered algorithms such as fastText <ref type="bibr" target="#b5">(Bojanowski et al., 2017)</ref>, Byte-Pair Encoding <ref type="bibr" target="#b42">(Sennrich et al., 2016)</ref>, and WordPiece <ref type="bibr" target="#b41">(Schuster &amp; Nakajima, 2012</ref>) became a standard step in preprocessing pipelines of large language models and allowed to construct fixed-size token vocabularies, e.g., BERT <ref type="bibr" target="#b13">(Devlin et al., 2019)</ref> contains~30K tokens and Published as a conference paper at ICLR 2022 <ref type="table">Table 1</ref>: Node embedding sizes of state-of-the-art KG embedding models compared to BERT Large. Parameters of type float32 take 4 bytes each. FB15k-237, WN18RR, and YAGO3-10 models as reported in <ref type="bibr" target="#b43">Sun et al. (2019)</ref>, OGB WikiKG2 as in <ref type="bibr" target="#b56">Zhang et al. (2020c)</ref>, Wikidata 5M as in , PBG Wikidata as in , and BERT Large as in <ref type="bibr" target="#b13">Devlin et al. (2019)</ref>. GPT-2 <ref type="bibr" target="#b34">(Radford et al., 2019)</ref> employs~50K tokens. Importantly, relatively small input embedding matrices enabled investing the parameters budget into more efficient encoders <ref type="bibr" target="#b20">(Kaplan et al., 2020)</ref>.</p><p>Drawing inspiration from subword embeddings in NLP, we explore how similar strategies for tokenizing entities in large graphs can dramatically reduce parameter complexity, increase generalization, and naturally represent new unseen entities as using the same fixed vocabulary. To do so, tokenization has to rely on atoms akin to subword units and not the total set of nodes.</p><p>To this end, we propose NodePiece, an anchor-based approach to learn a fixed-size vocabulary V (|V | |N |) of any connected multi-relational graph. In NodePiece, the set of atoms consists of anchors and all relation types that, together, allow to construct a combinatorial number of sequences from a limited atoms vocabulary. In contrast to shallow approaches, each node n is first tokenized into a unique hash(n) of k closest anchors and m immediate relations. A key element to build a node embedding is a proper encoder function enc(n) : hash(n) ? R d which can be designed leveraging inductive biases of an underlying graph or downstream tasks. Therefore, the overall parameter budget is now defined by a small fixed-size vocabulary of atoms and the complexity of the encoder function.</p><p>Our experimental findings suggest that a fixed-size NodePiece vocabulary paired with a simple encoder still yields competitive results on a variety of tasks including link prediction, node classification, and relation prediction. Furthermore, anchor-based hashing enables conventional embedding models to work in the inductive and out-of-sample scenarios when unseen entities arrive at inference time, which otherwise required tailored learning mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Conventional KG embedding approaches. To the best of our knowledge, all contemporary embedding algorithms <ref type="bibr" target="#b19">(Ji et al., 2020;</ref><ref type="bibr" target="#b1">Ali et al., 2020)</ref> for link prediction on KGs employ shallow embedding lookups mapping each entity to a unique embedding vector thus being linear O(|N |) to the total number of nodes |N | and size of an embedding matrix. This holds for different embedding families, e.g., translational <ref type="bibr" target="#b43">(Sun et al., 2019)</ref>, tensor factorization <ref type="bibr" target="#b21">(Lacroix et al., 2018)</ref>, convolutional <ref type="bibr" target="#b12">(Dettmers et al., 2018)</ref>, and hyperbolic <ref type="bibr" target="#b7">(Chami et al., 2020;</ref><ref type="bibr" target="#b4">Balazevic et al., 2019)</ref>. The same applies to relation-aware graph neural network (GNN) encoders <ref type="bibr" target="#b40">(Schlichtkrull et al., 2018;</ref><ref type="bibr" target="#b46">Vashishth et al., 2020)</ref> who still initialize each node with a learned embedding or feature vector before message passing. Furthermore, shallow encoding is also used in higher-order KG structures such as hypergraphs <ref type="bibr">(Fatemi et al., 2020)</ref> and hyper-relational graphs <ref type="bibr" target="#b35">(Rosso et al., 2020;</ref>. NodePiece can be used as a drop-in replacement of the embedding lookup with any of those models.</p><p>Distillation and compression. Several recent techniques for reducing memory footprint of embedding matrices follow successful applications of distilling large language models in NLP <ref type="bibr" target="#b39">(Sanh et al., 2019)</ref>, i.e., distillation <ref type="bibr" target="#b49">(Wang et al., 2020;</ref><ref type="bibr" target="#b57">Zhu et al., 2020)</ref> into low-dimensional counterparts, and compression of trained matrices into discrete codes <ref type="bibr" target="#b36">(Sachan, 2020)</ref>. However, all of them require a full embedding matrix as input which we aim to avoid designing NodePiece.</p><p>Vocabulary reduction in recommender systems. Commonly, recommender systems operate on thousands of categorical features combined in sparse high-dimensional vectors.Recent approaches <ref type="bibr" target="#b25">(Medini et al., 2021;</ref><ref type="bibr" target="#b23">Liang et al., 2021)</ref> employ anchor-based hashing techniques to factorize sparse feature vectors into dense embeddings. Contrary to those setups, we do not expect availability of feature vectors for arbitrary KGs and rather learn vocabulary embeddings from scratch.  <ref type="figure">Figure 1</ref>: NodePiece tokenization strategy. Given three anchors a 1 , a 2 , a 3 , a target node can be tokenized into a hash of top-k closest anchors, their distances to the target node, and the relational context of outgoing relations from the target node. This hash sequence is passed through an injective encoder to obtain a unique embedding. Inverse relations are added to ensure connectivity.</p><p>Entity descriptions and language models. A recent line of work such as KG-BERT <ref type="bibr" target="#b53">(Yao et al., 2019)</ref>, MLMLM <ref type="bibr" target="#b9">(Clou?tre et al., 2021)</ref>, BLP <ref type="bibr" target="#b10">(Daza et al., 2021)</ref> utilize entity descriptions passed through a language model (LM) encoder as entity embeddings suitable for link prediction. We would like to emphasize that such approaches are rather orthogonal to NodePiece. Textual features are mostly available in Wikipedia-derived KGs like Wikidata but are often missing in domain-specific graphs like social networks and product graphs. We therefore assume textual features are not available and rather learn node representations based on their spatial characteristics. Still, textual features can be easily added by concatenating NodePiece-encoded features with LM-produced features.</p><p>Out-of-sample representation learning. This task focuses on predictions involving previously unseen, or out-of-sample, entities that attach to a known KG with a few edges. These new edges are then utilized as a context to compute its embedding. Previous work <ref type="bibr" target="#b17">Hamaguchi et al., 2017;</ref><ref type="bibr" target="#b0">Albooyeh et al., 2020)</ref> proposed different neighborhood aggregation functions for this process or resorted to meta-learning <ref type="bibr" target="#b8">(Chen et al., 2019;</ref><ref type="bibr" target="#b3">Baek et al., 2020;</ref><ref type="bibr" target="#b54">Zhang et al., 2020a)</ref>. However, all of them follow the shallow embedding paradigm. Instead, NodePiece uses the new edges as a basis for anchor-based tokenization of new nodes in terms of an existing vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NODEPIECE VOCABULARY CONSTRUCTION</head><p>Given a directed KG G = (N, E, R) consisting of |N | nodes, |E| edges, and |R| relation types, our task is to reduce the original vocabulary size of |N | nodes to a smaller, fixed-size vocabulary of node pieces akin to subword units. In this work, we represent node pieces through anchor nodes a ? A, A ? N , a pre-selected set of nodes in a graph following a deterministic or stochastic strategy. A full NodePiece vocabulary is then constructed from anchor nodes and relation types, i.e, V = A + R. Note that in order to maintain reachability of each node and balance in-and out-degrees we enrich G with inverse edges with inverse relation types, such that |R| inverse = |R| direct and |R| = |R| direct + |R| inverse . Using elements of the constructed vocabulary each node n can be tokenized into hash(n) as a sequence of k closest anchors, discrete anchor distances, and a relational context of m immediate relations. Then, any encoder function enc(n) : hash(n) ? R d can be applied to embed the hash into a d-dimensional vector. An intuition of the approach is presented in <ref type="figure">Fig. 1</ref> with each step explained in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ANCHOR SELECTION</head><p>Subword tokenization algorithms such as BPE <ref type="bibr" target="#b42">(Sennrich et al., 2016</ref>) employ deterministic strategies to create tokens and construct a vocabulary, e.g., based on frequencies of co-occurring n-grams, such that more frequent words are tokenized with fewer subword units. On graphs, such strategies might employ centrality measures like degree centrality or Personalized PageRank <ref type="bibr" target="#b30">(Page et al., 1999)</ref>. However, in our preliminary experiments, we found random anchor selection to be as effective as centrality-based strategies. A choice for deterministic strategies might be justified when optimizing for certain task-specific topological characteristics, e.g., degree and PPR strategies indeed skew the distribution of shortest anchor distances towards smaller values thus increasing chances to find anchors in 2-or 3-hop neighborhood of any node (we provide more evidence for that in Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NODE TOKENIZATION</head><p>Once the vocabulary V = A + R is constructed, each node n can be hashed (or tokenized) into a hash(n) using 1) k nearest anchors and their discrete distances; 2) m immediate outgoing relations from the relational context of n. Since anchor nodes are concrete nodes in G, they get hashed in the same way as other non-anchor nodes.</p><p>Anchors per node. Given |A| anchor nodes, it is impractical to use all of them for encoding each node. Instead, we select k anchors per node and describe two possible strategies for that, i.e., random and deterministic. The basic random strategy uniformly samples an unordered set of k anchors from A yielding |A| k possible combinations. To avoid collisions when hashing the nodes, |A| and k are to be chosen according to the lower bound on possible combinations that is defined by the total number of nodes, e.g., |A| k ? |N |. Note that running depth-first search (DFS) to random anchors at inference time is inefficient and, therefore, hash(n) of the random strategy has to be pre-computed.</p><p>On the other hand, the deterministic strategy selects an ordered sequence of k nearest anchors. Hence, the anchors can be obtained via breadth-first search (BFS) in the l-hop neighborhood of n at inference time (or pre-computed for speed reasons). However, the combinatorial bound is not applicable in this strategy and we need more discriminative signals to avoid hash collisions since nearby nodes will have similar anchors (we elaborate on the uniqueness issue in Appendix K). Such signals have to better ground anchors to the underlying graph structure, and we accomplish that using anchor distances 3 and relational context described below.</p><p>A node residing in a disconnected component is assigned with an auxiliary [DISCONNECTED] token or can be turned into an anchor. However, the majority of existing KGs are graphs with one large connected component with very few disconnected nodes, such that this effect is negligible.</p><p>Anchor Distances. Given a target node n and an anchor a i , we define anchor distance z ai ? [0; diameter(G)] as an integer denoting the shortest path distance between a i and n in the original graph G. Note that when tokenizing an anchor a j with the deterministic strategy, the nearest anchor among top-k is always a j itself with distance 0. We then map each integer to a learnable d-dimensional vector f z : z ai ? R d akin to relative distance encoding scheme.</p><p>Relational Context. We also leverage the multi-relational nature of an underlying KG. Commonly 4 , the amount of unique edge types in G is orders of magnitude smaller than the total number of nodes, i.e., |R| |N |. This fact allows to include the entire |R| in the NodePiece vocabulary V NP and further featurize each node with a unique relational context. We construct a relational context of a node n by randomly sampling a set of m immediate unique outgoing relations starting from n, i.e., rcon n = {r j } m ? N r (n) where N r (n) denotes all outgoing relation types. Due to a non-uniform degree distribution, if |N r (n)| &lt; m, we add auxiliary [PAD] tokens to complete rcon n to size m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ENCODING</head><p>At this step, a node n is tokenized into a sequence of k anchors, their k respective distances, and relational context of size m:</p><formula xml:id="formula_0">hash(n) = {a i } k , {z ai } k , {r j } m<label>(1)</label></formula><p>Taking anchors vectors a n and relation vectors r n from the learnable NodePiece vocabulary V ? R |V |?d , and anchor distances z an from Z ? R (diameter (G)+1)?d , we obtain a vectorized hash:</p><p>hash(n) = a n + z an , r n = ? n , r n ? R (k+m)?d (2) 3 A full relational path can be mined as well but it has proven to be not scalable as each path needs to be encoded separately through a sequence encoder, e.g., <ref type="bibr">GRU. 4</ref> As of 2021, one of the largest open KGs Wikidata contains about 100M nodes and 6K edge types Although other operations are certainly possible, in this work, we use anchor distances as positional encodings of corresponding anchors and sum up their representations that helps to maintain the overall hash dimension of (k + m) ? d.</p><p>Finally, an encoder function enc : R (k+m)?d ? R d is applied to the vectorized hash to bootstrap an embedding of n. In our experiments, we probe two basic encoders: 1) MLP that takes as input a concatenated hash vector R 1?(k+m)d projecting it down to R d ; 2) Transformer encoder <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> with average pooling that takes as input an original sequence R (k+m)?d . While MLP is faster and better scales to graphs with more edges, Transformer is slower but requires less trainable parameters. As the two encoders were chosen to illustrate the general applicability of the whole approach, we leave a study of even more efficient and effective encoders for future work.</p><p>While the nearest-neighbor hashing function has a greater number of collisions, its non-arbitrary mapping means that it is effectively permutation invariant. We show this in Proposition 1 through the framework of Janossy pooling and permutation sampling based SGD, ?-SGD <ref type="bibr" target="#b28">(Murphy et al., 2019)</ref>. A proof is provided in Appendix H.</p><p>Proposition 1. The nearest-anchor encoder with |A| k anchors and |m| subsampled relations, can be considered a ?-SGD approximation of (k + |m|)-ary Janossy pooling with a canonical ordering induced by the anchor distances.</p><p>Janossy pooling with ?-SGD can be used to learn a permutation-invariant function from a broad class of permutation-sensitve functions such as MLPs <ref type="bibr" target="#b28">(Murphy et al., 2019)</ref>. The permutation-invariant nature of the nearest-neighbor encoding scheme combined with the lack of transductive features such as node-specific embeddings mean that NodePiece can be used for inductive learning tasks as well.</p><p>With a fixed-size vocabulary V NP , the overall complexity and parameter budget of downstream models are largely defined by the complexity of the encoder and its inductive biases. By design, the NodePiece smaller vocabulary -larger encoder framework is similar to various Transformer-based language models <ref type="bibr" target="#b33">(Qiu et al., 2020</ref>) whose vocabulary size remains rather stable with the encoder being the most important part responsible for the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We design the experimental program not seeking to outperform the best existing approaches but to show the versatility of NodePiece on a variety of KG-related tasks: transductive, inductive, out-ofsample link prediction, and node classification (with relation prediction results in Appendix I). With this desiderata, we formulate the following research questions: RQ 1) Is it necessary to map each node to a unique vector for an acceptable performance on KG tasks?; RQ 2) What is the effect of hashing features?; RQ 3) Is there an optimal number of anchors per node, after which diminishing returns hit the performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">TRANSDUCTIVE LINK PREDICTION</head><p>Setup. We run experiments on five KGs of different sizes (Appendix A.1) varying the total number of nodes from~15K to~2.5M. As a baseline, we compare to RotatE <ref type="bibr" target="#b43">(Sun et al., 2019)</ref> that remains one of state-of-the-art shallow embedding models for transductive link prediction tasks. To balance with NodePiece, RotatE operates on a graph with added inverse edges as well. We report MRR with Hits@10 in the filtered <ref type="bibr" target="#b6">(Bordes et al., 2013)</ref> setting as evaluation metrics, and count parameters for all models. On larger KGs, we also compare to a smaller RotatE with a similar parameter budget.</p><p>In this task, NodePiece is equipped with a 2-layer MLP encoder. For a fair comparison, we also adopt the RotatE scoring function as a link prediction decoder. As to the NodePiece configuration, we generally keep the number of anchors below 10% of total nodes in respective graphs. We select 1k/20 for FB15k-237 (i.e., total 1000 anchors and 20 anchors per tokenized node) with 15 unique outgoing relations in the relational context; 500/50 with 4 relations for WN18RR; 7k/20 with 6 relations for CoDEx-L; 10k/20 with 5 relations for YAGO 3-10. Other hyperparameters are listed in Appendix A.</p><p>Discussion. Generally, the results suggest that a fixed-size NodePiece vocabulary of &lt;10% of nodes sustains 80-90% of Hits@10 compared to 10x larger best shallow models. Some performance loss is expected due to the compositional and compressive nature of entity tokenization. On smaller graphs  <ref type="bibr" target="#b43">(Sun et al., 2019)</ref>. |V | denotes vocabulary size (anchors + relations), #P is a total parameter count (millions). % denotes the Hits@10 ratio based on the strongest model.   <ref type="table" target="#tab_1">(Table 2)</ref>, parameter saving might not be well pronounced due to the overall small number of nodes to embed. Still, taking even as few as 500 nodes as anchors on WN18RR retains 90% of the best model performance. On bigger graphs <ref type="table" target="#tab_3">(Table 3)</ref>, parameter efficiency is more pronounced, i.e., on YAGO 3-10, a RotatE model of comparable size is 20 Hits@10 points worse than a NodePiece-based one. This observation can be attributed to the fact the shrinking shallow models results in shrinking the embedding dimension of each node (20d for RotatE) which is inefficient on small parameter budgets. In contrast, a small fixed-size vocabulary allows for larger anchor embedding dimensions (100d for NodePiece with RotatE) since most of the parameter budget is defined by the encoder.</p><p>We further study the effect of different anchor selection combinations <ref type="figure" target="#fig_1">(Fig. 2)</ref>. On WN18RR, fewer anchors with fewer anchors per node (|A|/k) yield relatively low accuracy but starting from 50/20 (~0.1% of 40k nodes in the graph) the Hits@10 performance starts to saturate. On FB15k-237, as few as 25 anchors already exhibit the signs of saturation where a further increase to 500 or 1000 anchors only marginally improves the performance. We hypothesize such a difference can be explained by graph density, e.g., WN18RR is a sparse graph with a diameter of 23 and average anchor distance of 6 hops; while FB15k-237 is a denser graph with an average anchor distance of 2-3. Hence, on a sparse graph with longer distances, it takes more anchors to properly encode a node.</p><p>However, more precise predictions (e.g., Hits@1) reflected in the MRR metric (see Appendix E) still remain a challenging task for small vocabulary NodePiece setups, and bigger |A|/k combinations alleviate this issue. We also observe that diminishing returns, which make further vocabulary increase less rewarding, start to appear from anchor set sizes of~1% of total nodes.</p><p>Ablations. In the ablation study, we measure the impact of relational context and anchor distances on link prediction <ref type="table" target="#tab_1">(Table 2)</ref>. Removing relational context and anchor distances does not tangibly affect the denser FB15k-237 data but does impair the accuracy on the sparser WN18RR. Pushing vocabulary sizes to the limit, we also investigate NodePiece behavior in the absence of anchors at all, i.e., when hashes are defined only by the relational context of size m. Interestingly, this still yields fair performance on FB15k-237 with just 7 points Hits@10 drop, but drops to zero the WN18RR performance. The fact that node embeddings might not be at all necessary but relations are more important supports the recent findings of Teru et al. (2020) that relies only on relations seen in a small subgraph around a target node. However, at this point, it seems to be a virtue of graphs with a diverse set of unique relations. That is, FB15k-237 has 20x more unique relations than WN18RR and resulting hashes have more diverse combinations of relations which lead to more discriminative node representations. Additionally, we visualize anchor embedding projections in Appendix D.    <ref type="bibr" target="#b51">Zhu et al. (2021)</ref>. <ref type="table" target="#tab_1">FB15k-237  WN18RR  NELL-995   V1  V2  V3  V4  V1  V2  V3  V4  V1  V2  V3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NODE CLASSIFICATION</head><p>Setup. Due to the lack of established node classification datasets on multi-relational KGs, we design a multi-class multi-label task based on a triple version of a recent WD50K  extracted from Wikidata. The pre-processing steps are described in Appendix F, and the final graph consists of 46K nodes and 222K edges. The task belongs to the family of transductive (the whole graph is seen during training) semi-supervised (only a fraction of nodes are labeled) problems, where labels are 465 classes as seen in Wikidata. In a semi-supervised mode, we test the models on a graph with 5% and 10% of labeled nodes. Node features have to be learned as node embeddings.</p><p>As baselines, we compare to a 2-layer MLP and CompGCN <ref type="bibr" target="#b46">(Vashishth et al., 2020)</ref> in a full-batch mode which is one of the strongest GNN encoders for multi-relational KGs. Both baselines learn a full entity vocabulary. We report ROC-AUC, PRC-AUC, and Hard Accuracy metrics as commonly done in standard graph benchmarks like OGB <ref type="bibr" target="#b18">(Hu et al., 2020)</ref>. For PRC-AUC and Hard Accuracy, we binarize predicted logits using a threshold of 0.5. Hard Accuracy corresponds to the exact match of a predicted sparse 465-dimensional vector to a sparse 465-dimensional labels vector.</p><p>NodePiece is configured to have only 50 anchors and use 10 nearest anchors per node with 5 unique relations in the relational context. The dimensionality of anchors and relations is the same as in the baseline CompGCN. Each epoch, we first materialize all entity embeddings through the NodePiece encoder and then send the materialized matrix to CompGCN with the class predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>. Surprisingly,~1000x vocabulary reduction ratio (50 anchors against 46k for shallow models) greatly outperforms the baselines <ref type="table" target="#tab_6">(Table 6)</ref>. MLP, as expected, is not able to cope with the task producing random predictions. CompGCN, in turn, outperforms MLP demonstrating nonrandom outputs as seen by the ROC-AUC score of 0.836 and higher PRC-AUC and Hard Accuracy metrics. Still, a NodePiece-equipped CompGCN with 50 anchors reaches even higher ROC-AUC of 0.98 with considerable improvements along other metrics, i.e., +16-19 PRC-AUC points and 3x boost along the hardest accuracy metric. We attribute such a noticeable performance difference to better generalization capabilities of the NodePiece model. That is, a generalization gap between training and validation metrics of the NodePiece + CompGCN is much smaller compared to the baselines who overfit rather heavily ( cf. the training curves in the Appendix G). The effect remains after increasing the number of labeled nodes to 10%. Even with 50 anchors, the overall performance is saturated as the further increase of the vocabulary size did not bring any improvements.</p><p>Ablations. We probe setups where NodePiece hashes use only anchors or only relational context, and find they both deliver a similar performance. Following the previous experiments on dense graphs with lots of unique relations, it appears that node classification can be performed rather accurately based only on the node relational context which is captured by NodePiece hashes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">OUT-OF-SAMPLE LINK PREDICTION</head><p>Setup. In the out-of-sample setup, validation and test splits contain unseen entities that arrive with a few edges connected to the seen nodes. For this experiment, we use the out-of-sample FB15k-237 split (oFB15k-237) as designed in <ref type="bibr" target="#b0">Albooyeh et al. (2020)</ref>. We do not employ their version of WN18RR as the split contains too many disconnected entities and components in the train graph. Instead, using the authors script, we sample a much bigger out-of-sample version of YAGO 3-10.</p><p>As a baseline, we compare to oDistMult <ref type="bibr" target="#b0">(Albooyeh et al., 2020)</ref> which aggregates embeddings of all seen neighboring nodes around the unseen one (akin to 1-layer message passing with mean aggregator). We adopt the same evaluation protocol -given an unseen node with its connecting edges, we mask one of the edges and predict its tail or head using the rest of the edges, repeating this procedure for each edge. We report filtered MRR and Hits@10 as main metrics.</p><p>NodePiece enables traditional transductive-only models to perform inductive inference as both seen and unseen nodes are tokenized using the same vocabulary. For a smaller oFB15k-237 the NodePiece vocabulary has 1k/20 configuration with 15 relations, while in a bigger oYAGO 3-10 we use 10k/20 with 5 relations. For this task, we apply a transformer encoder instead of MLP. For a fair comparison, we use DistMult as a scoring function as well.</p><p>Discussion. The results in <ref type="table" target="#tab_7">Table 7</ref> show that a simple NodePiece-based model retains~90% of the baseline performance on oFB15k-237, but achieved faster and computationally inexpensive compared to oDistMult. Moreover, while oDistMult is tailored specifically for the out-of-sample task, we did not do any task-specific modifications to the NodePiece-enabled model as it is inductive by design. Furthermore, oDistMult is not able to scale to a bigger oYAGO 3-10 on a 256 GB RAM machine due to the out of memory crash. Conversely, a NodePiece-equipped model has the same computational requirements as in other tasks and converges rather quickly (40 epochs). Performed ablations underline the importance of having both anchors and relational context for tokenizing unseen entities. We elaborate more on possible inference strategies for transductive and inductive tasks in Appendix A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have introduced NodePiece, a compositional approach for representing nodes in multi-relational graphs with a fixed-size vocabulary. Similar to subword units, NodePiece allows to tokenize every node as a combination of anchors and relations where the number of anchors can be 10-100? smaller than the total number of nodes. We show that in some tasks, node embeddings are not even necessary for getting an acceptable accuracy thanks to a rich set of relation types. Moreover, NodePiece is inductive by design and is able to tokenize unseen entities and perform downstream prediction tasks in the same fashion as on seen ones.</p><p>Reproducibility Statement. The source code is openly available on GitHub. All hyperparameters and implementation details are presented in Appendix A. Information on the used datasets is presented in <ref type="table" target="#tab_8">Table 8</ref> and we provide more details on dataset construction for node classification and out-ofsample link prediction tasks in Appendix F. The proof for Proposition 1 is given in the Appendix H.</p><p>Ethics Statement. As NodePiece is a general graph representation learning method, we do not foresee immediate ethical consequences pertaining to the method itself.  For all downstream tasks and datasets we employ the deterministic anchor selection strategy where 40% of the total number of anchors |A| are nodes with top PPR scores, 40% are top degree nodes, and remaining 20% are selected randomly. All anchor sets are non-overlapping and disjoint, i.e., if some top degree nodes have already been selected with the PPR policy, they will be skipped in favor of next nodes in the sorted list. The choice for this strategy is motivated in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A IMPLEMENTATION &amp; HYPERPARAMETERS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DATASETS</head><p>Details on the datasets for transductive link prediction, out-of-sample link prediction, relation prediction and node classification are collected in <ref type="table" target="#tab_8">Table 8</ref>. The inductive link prediction benchmark introduced by Teru et al. (2020) includes 3 graphs, FB15k-237, WN18RR, and NELL-995, each has 4 different splits that vary in the number of unique relations, number of nodes and triples at training and inference time. Full dataset statistics is provided in <ref type="table" target="#tab_9">Table 9</ref>. FB15k-237 and most splits of NELL-995 can be considered as relation-rich graphs while WN18RR is a sparse graph with few relation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 TRANSDUCTIVE LINK PREDICTION</head><p>The optimizer is Adam for all experiments. As RotatE is a scoring function in the complex space, the reported embedding dimensions are a sum of real and imaginary dimensions, e.g., 1000d means that both real and imaginary vectors are 500d.   <ref type="table" target="#tab_1">(Table 12)</ref> for the compared models are almost identical to those of the transductive link prediction experiment. We mostly reduce the number of epochs and negative samples as models converge faster on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 NODE CLASSIFICATION</head><p>In this experiment <ref type="table" target="#tab_3">(Table 13)</ref>, NodePiece is used at the initial step to bootstrap a node embeddings matrix which is then sent to the CompGCN graph encoder. In contrast, CompGCN and MLP baselines use directly a trained node embedding matrix as their initial input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 OUT-OF-SAMPLE LINK PREDICTION</head><p>The set of NodePiece hyperparameters <ref type="table" target="#tab_4">(Table 14)</ref> is similar to the set of the transductive experiments except the scoring function (DistMult), encoder function (Transformer), and number of epochs as the model converges faster. We do not provide a setup for the baseline oDistMult on oYAGO 3-10 as the model was not able to pre-process the dataset on a machine with 256 GB RAM. Reported training  times for NodePiece models exclude evaluation. Training times of the baseline oDistMult were not reported by its authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 DEPLOYMENT IN REAL-WORLD DYNAMIC KNOWLEDGE GRAPHS</head><p>NodePiece, on account of its compositional representation, can be applied to dynamic real-world knowledge graphs where nodes are added and removed over time. That is, training on a graph snapshot we can obtain the embeddings of new nodes without re-computing and updating representations of every other node in the graph. This is valuable in settings where there are latency requirements such as many online services. For example, if a user creates an account on a social media service and begins liking content (represented as a "like" edge in the social network graph between the user and the content), it would be desirable to make future content recommendations rapidly reflect this new data without waiting for the next batched retraining to update the users embedding. In order to reduce latency the entire embedding matrix can be materialized and cached ahead of time, updating embeddings as new nodes and edges are added. The parameter efficiency and compositionality of NodePeice means that for large real-world graphs NodePiece subsumes what would before have been a complex system of a large-scale embedding framework like Pytorch-BigGraph , an OOV embedding method (e.g,. ERAvg) and a shallow embedding method (e.g., RotatE). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B LIMITATIONS AND FUTURE WORK</head><p>Our experimental results demonstrate the promise of using NodePeice to significantly reduce the parameter complexity of node embeddings. While it is difficult to prove, we also hypothesize that the parameters required by NodePeice to maintain the same level of performance (as the graph scales) increase sublinearly according to the size of the graph. The intuition for this is twofold. First, the number of unique anchor combinations of size k that can be encoded increases according to |A| k (i.e. O(|A| k )) if randomly sampled -if the sampling is done via nearest neighbor anchor selection then the number of unique permutations is expected to increase polynomially. Second, increasing the size of the graph will only require sublinear increase in the number of anchors in order to maintain the same average node-anchor distance. Although proving causality is difficult, we believe that maintaining hashing uniqueness and node-anchor distances stable will be sufficient to maintain equivalent performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ANCHOR SELECTION STRATEGIES</head><p>Here, we provide more details as to anchor configurations (k nearest from total A anchors) and anchor distances. Recall that there exist several ways to select the total set of anchors A as stated in Section 3.1, i.e., random or centrality-measure based. Then, k anchors per node can be chosen either as k nearest (default NodePiece mode) or k random anchors. <ref type="figure">Figure 3</ref> depicts the effect of those strategies on the distribution of anchor distances (number of hops between a target node and its anchors). We use the configurations used in the main experiments, i.e., 1000 anchors and 20 anchors per node for FB15k-237, and 500 anchors with 50 anchors per node on WN18RR.</p><p>First, we observe that PPR, degree, and mixed (40% PPR, 40% degree, 20% random) strategies generally skew the distribution towards smaller anchor distances compared to random strategies. This fact supports the hypothesis that deterministic strategies improve the chances to find an anchor in a closer l-hop neighborhood of a target node. Second, varying the way of selecting k anchors per node between nearest (left column) and random (right column), we also observe the skew of a distribution of anchor distances.</p><p>Next, we fix the anchor selection strategy to the mix, fix the number of anchors per node (50 for WN18RR and 20 for FB15k-237), and vary a total number of anchors A (50 to 1000 for WN18RR and 20 to 1000 for FB15k-237) along with the method of sampling k anchors per node, i.e., nearest and random. <ref type="figure">Figure 4</ref> shows that increasing the total number of anchors together with k nearest Finally, we fix the anchor selection strategy as mix, obtain nearest anchors per node, and under this setup study average anchor distances varying k -the number of anchors per node in various combinations of total anchors A. The results presented on <ref type="figure">Figure 5</ref> suggest that sparser graphs (like WN18RR) benefit more from increasing the number of anchors A, i.e., the delta between distances is much larger than that of dense FB15k-237. The difference in distances on <ref type="figure">Figure 5</ref> might also explain the performance on <ref type="figure" target="#fig_6">Figure 8</ref>, i.e., generally, smaller A/k configurations like 25/5 are inferior on sparser graphs but perform competitively on denser ones.  <ref type="figure">Figure 5</ref>: Average node-anchor distances when varying the total number of anchors A from 25 to 1000 and k nearest anchors per node from 5 to 50. Note that on a sparser WN18RR the gap between min and max values is much wider than of denser FB15k-237. Signs of saturation suggest that further increasing A is not beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EMBEDDING VISUALIZATIONS</head><p>To further study learned representations of anchors and capabilities of the encoder, we build tSNE and UMAP projections from subsamples of FB15k-237 and WN18RR based on trained models from the transductive link prediction experiments (hyperparameters listed in <ref type="table">Table A</ref>).</p><p>For FB15k-237, we randomly sample 1000 entities (out of total 15K) and find their top-100 most common anchors. The anchor embeddings are extracted from the learned tensor while 1000 entity embeddings are obtained through the NodePiece encoder. Similarly for WN18RR, we sample 4000 entities (out of total 40K) keeping their top-100 most common anchors. As we use the RotatE decoder that assumes entities and anchors are modeled in a complex space, we visualize their real parts (e.g., first 100 dimensions out of 200).</p><p>Recalling that link prediction performance of NodePiece + RotatE retains 80-90% of the state of the art models performance, the results on <ref type="figure" target="#fig_4">Fig. 6</ref> and <ref type="figure" target="#fig_5">Fig. 7</ref> demonstrate that (1) NodePiece encoder is able to reconstruct clusters of similar entities; (2) anchors are well-scattered among communities. Albeit entity embeddings are built as a composition of k anchors, it can be seen that all communities have "specialized" nearby anchors. On a higher level, common anchors tend to be well-scattered in the space. Less common anchors, as seen on FB15k-237 and <ref type="figure" target="#fig_4">Fig. 6</ref>, tend to group together. However, thanks to the non-linear nature of the NodePiece encoder, resulting entity embeddings still form different clusters and communities not concentrated around one point. We believe this is the effect of a compositional encoder and plan to investigate this phenomenon further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E TRANSDUCTIVE LINK PREDICTION RESULTS: MRR</head><p>In addition to <ref type="figure" target="#fig_1">Figure 2</ref> that presents Hits@10, we report variations of mean reciprocal rank (MRR) depending on combinations of A and k on <ref type="figure" target="#fig_6">Figure 8</ref> from the same set of experiments. On sparser WN18RR, smaller A/k combinations like 25/5 or 50/10 struggle with more precise predictions like  Hits@1 which is captured by low values of MRR. Starting from 500/10, the WN18RR performance starts to saturate. On the other hand, on denser FB15k-237, the difference between minimum and maximum MRR is less than 4 points, and performance exhibits signs of saturation already at 50/10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F DATASETS CONSTRUCTION F.1 NODE CLASSIFICATION: WD50K NC</head><p>The original WD50K  contains a triple-only KG version on which we base a new dataset for semi-supervised multi-class multi-label node classification. First, we remove all triples containing Wikidata properties P31 (instance of ) and P279 (subclass of ) as they already contain class information. We then remove nodes that became disconnected after removing those edges. Third, using SPARQL queries, for each remaining node in a graph, we extract a 3-hop class hierarchy of Wikidata classes and their superclasses. We only keep class labels that occur at least 50 times in the training set. Then, we sample 10% of nodes with labels for validation and 10% for test, and of remaining 80% we sample a set of nodes for the semi-supervised setup, i.e., we keep only 5% and Note that performance gap on FB15k-237 is very small indicating that saturation has occurred already with small anchor configurations.</p><p>10% of those nodes. The resulting graph has 46k nodes, 526 distinct relation types, and 465 class labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 OUF-OF-SAMPLE LINK PREDICTION: OYAGO 3-10</head><p>For sampling the out-of-sample version of a bigger YAGO 3-10 we largely follow the same original procedure described in Section 4 of <ref type="bibr" target="#b0">(Albooyeh et al., 2020)</ref>. We first merge the train, validation and test triples from the original dataset for transductive link prediction. Then, from all entities appearing in at least two triples, we randomly sample 5% of nodes to be the out-of-sample entities for validation and 5% for test. All triples containing the out-of-sample entities on subject or object positions are put into validation or test, respectively, as edges that connect an unseen entity with the seen graph. G NODE CLASSIFICATION: TRAINING CURVES <ref type="figure" target="#fig_7">Figure 9</ref> depicts train and validation values of Hard Accuracy and PRC-AUC metrics for all the compared models on WD50K NC with 5% of labeled nodes. The NodePiece model has only 50 total anchors with 10 nearest anchors per node, and 5 unique relation types in the relational context. The performance on the dataset with 10% of nodes is almost the same, so we report the charts only on 5% dataset. By the generalization gap we understand the delta between training and validation values.</p><p>The MLP baseline quickly overfits but fails to generalize on the validation. The generalization gap of CompGCN is smaller compared to MLP but is still significant, i.e., validation performance is 2-3? smaller than train. Finally, the NodePiece-enabled model has the smallest generalization gaps, especially along the Hard Accuracy metric where the validation performance is very close to that of train. Similarly, the gap on PRC-AUC is smaller than 10 points.</p><p>As shown in the ablation study in <ref type="table" target="#tab_6">Table 6</ref>, it appears that explicit node embeddings do not contribute to the classification performance. Hence, the baseline models tend to be overparameterized where learnable node embeddings add noise, while the NodePiece model has only a few anchors (or no anchors at all when using only the relational context), much fewer parameters, and therefore generalizes better. This hypothesis also explains the observation that the node classification performance does not improve when increasing A/k anchor configurations.</p><p>H PROOFS Proposition 2. The nearest-anchor encoder with |A| k anchors and |m| subsampled relations, can be considered a ?-SGD approximation of (k + |m|)-ary Janossy pooling with a canonical ordering induced by the anchor distances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof.</head><p>We begin by providing the definition of Janossy pooling as it was presented in the original paper <ref type="bibr" target="#b28">Murphy et al. (2019)</ref>.</p><p>Definition 1 (Janossy pooling). Let H ? be the union of all anchors and relations. Consider a function f :</p><formula xml:id="formula_1">N ? H ? ? R d ? F on variable-length but finite sequences h, parameterized by ? (f ) ? R d , d &gt; 0. A permutation-invariant function f : N ? H ? ? R d ? F is the Janossy function associated with f if f (|h|, h; ? (f ) ) = 1 |h|! ??? |h| f (|h|, h ? ; ? (f ) ),<label>(3)</label></formula><p>where ? |h| is the set of all permutations of the integers 1 to |h|, and h ? represents a particular reordering of the elements of sequence h according to ? ? ? |h| . We refer the operation used to construct f from f as Janossy pooling.</p><p>While Janossy pooling provides a simple approach to construct permutation-invariant functions from arbitrary permutation sensitive functions, it is computationally intractable due to the need to sum over all computations. Three general strategies proposed under this framework to overcome this combinatorial challenge: canonical orderings, k-ary Janossy pooling, and ?-SGD approximations.</p><p>A very effective way of reducing the complexity is to constrain the permutations to a canonical ordering that is independent of a specific adjacency matrix ordering over a given graph. More precisely, one defines as a function CANONICAL : H ? ? H ? such that CANONICAL(h) = CANONICAL(h ? )?? ? ? |h| and only considers functions f based on the composition f = CANONICAL ? f <ref type="bibr" target="#b28">(Murphy et al., 2019)</ref>. In the case of NodePiece we are able to define this ordering for the anchors according to their distance to the target node. Assuming that the number of relations is fixed or grows at slow rate throughout the life-cycle of a graph we can define an arbitrary ordering for relations as a canonical ordering for the relational context. However, since anchors can be equidistant such a canonical ordering does fully satisfy permutation invariance. We propose a trivial relaxation of the original definition of canonical orderings simply requiring that an ordering greatly reduce the number of unique permutations since in practice an exact canonical ordering is rarely feasible. Specifically, |{CANONICAL(h ? )?? ? ? |h| }| |{(h ? )?? ? ? |h| }|.</p><p>To further reduce the number of permutations we can truncate our ordered sequence h. This is known as k-ary Janossy pooling pooling (Definition 2) and is implicitly performed by the NodePeice algorithm by varying the anchor per node parameter, k, and the size of the relational context, |m|.</p><p>Definition 2 (k-ary Janossy pooling). Fix k ? N. For any sequence h, define ? k (h) as its projection to a length k sequence; in particular, if |h| ? k, we keep the first k elements. Then, a k-ary permutation-invariant Janossy function f is given by f (|h|, h; ? (f ) ) = 1 |h|! ??? |h| f (|h|, ? k (h ? ); ? (f ) ).</p><p>(4)</p><p>Since an imperfect truncated canonical ordering may still result in a potentially intractable number of permutations, we use permutation sampling also known as ?-SGD to learn arbitrary functions that approximate (k + |m|)-ary Janossy pooling. This is done by randomly ordering anchors that are equidistant resulting in a uniform sampling of possible permutations during training and evaluation. For more details on the formal definition of ?-SGD we point the reader to the original paper <ref type="bibr" target="#b28">(Murphy et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I RELATION PREDICTION</head><p>Setup. We conduct the relation prediction experiment on the same FB15k-237, WN18RR, and YAGO 3-10 datasets. While link prediction deals with entities, the relation prediction model has to rank a correct relation given a (head, ?, tail) query. We report MRR and Hits@10 in the filtered setting as evaluation metrics. Similar to the link prediction configuration, we use NodePiece + 2-layer MLP and compare against RotatE of the same total parameter count.</p><p>Discussion. The reported results <ref type="table" target="#tab_5">(Table 15</ref>) demonstrate a competitive performance of NodePiecebased models with reduced vocabulary sizes bringing more than 97% Hits@10 across graphs of different sizes. In the case of WN18RR and YAGO 3-10, NodePiece models with fewer anchors even slightly improve the accuracy upon the shallow embedding baseline. The ablation study suggests that on dense graphs with a reasonable amount of unique relations having explicit learnable node embeddings might not be needed at all for this task. That is, we see that on FB15k-237 and YAGO 3-10 the NodePiece hashes comprised only of the relational context deliver the same performance without any performance drop confirming the findings from the previous experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J INDUCTIVE LINK PREDICTION</head><p>Setup. Nodes in the inference graphs do not have any associated feature vectors which makes this benchmark very relevant for graph representation learning. Importantly, the set of relation types in the inference graphs is a subset of those seen in the training set. Since the relation embedding matrix can be learned on the training graph, we therefore have a uniform method for constructing node representations both on seen and unseen graphs. As an encoder we try MLP and Transformer.</p><p>Evaluation Protocol. Following the original work (Teru et al., 2020), we employ a filtered setting and rank each triple against 50 random negative triples reporting the Hits@10 metric. This setup is motivated by computational complexity of GraIL at inference time, while NodePiece + CompGCN is as fast in the inductive inference as in the transductive regime reported in other experiments.</p><p>Discussion. For each KG, there are 4 splits of increasing size of train and inference nodes and edges. The empirical results on this spectrum of various sizes demonstrates interesting scalability properties of NodePiece in inductive settings. Without anchor nodes, the NodePiece vocabulary size is independent of the number of nodes and edges, depending only on the number of relation types. On dense relation-rich graphs such a vocabulary is enough to yield very competitive performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Combinations of total anchors A and anchors per node. Denser FB15k-237 saturates faster on smaller A while sparse WN18RR saturates at around 500 anchors. 4.1.1 OGB WIKIKG 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>0.873 0.939 0.944 0.949 0.830 0.886 0.785 0.807 0.890 0.901 0.936 0.893</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>tSNE (left) and UMAP (right) projections of 1000 encoded entities sampled randomly from FB15k-237 and their top 100 most common anchors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>tSNE (left) and UMAP (right) projections of 4000 encoded entities sampled randomly from WN18RR and their top 100 most common anchors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Combinations of total anchors A and anchors per node. Denser FB15k-237 saturates faster on smaller A while sparse WN18RR saturates at around 500 anchors. MRR metric captures all ranks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Generalization gap on WD50K (5% labeled nodes). NodePiece-based model has observably smaller generalization gaps compared to the baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Transductive link prediction on smaller KGs. ? results taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Transductive link prediction on bigger KGs. The same denotation as inTable 2. Second RotatE has a similar parameter budget as a NodePiece-based model.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">CoDEx-L</cell><cell></cell><cell></cell><cell cols="2">YAGO 3-10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>|V |</cell><cell cols="2">#P (M) MRR H@10</cell><cell>%</cell><cell>|V |</cell><cell cols="3">#P (M) MRR H@10</cell><cell>%</cell></row><row><cell>RotatE (500d)</cell><cell>77k + 138</cell><cell>77</cell><cell cols="3">0.258 0.387 100 123k + 74</cell><cell>123</cell><cell cols="3">0.495  ? 0.670  ? 100</cell></row><row><cell>RotatE (20d)</cell><cell>77k + 138</cell><cell>3.8</cell><cell>0.196 0.322</cell><cell cols="2">83 123k + 74</cell><cell>4.8</cell><cell>0.121</cell><cell>0.262</cell><cell>39</cell></row><row><cell>NodePiece + RotatE</cell><cell>7k + 138</cell><cell>3.6</cell><cell>0.190 0.313</cell><cell>81</cell><cell>10k + 74</cell><cell>4.1</cell><cell>0.247</cell><cell>0.488</cell><cell>73</cell></row><row><cell cols="2">-no rel. context 7k + 138</cell><cell>3.1</cell><cell>0.201 0.332</cell><cell>86</cell><cell>10k + 74</cell><cell>3.7</cell><cell>0.249</cell><cell>0.482</cell><cell>72</cell></row><row><cell cols="2">-no distances 7k + 138</cell><cell>3.6</cell><cell>0.179 0.302</cell><cell>78</cell><cell>10k + 74</cell><cell>4.1</cell><cell>0.250</cell><cell>0.491</cell><cell>73</cell></row><row><cell>-no anchors, rels only</cell><cell>0 + 138</cell><cell>0.6</cell><cell>0.063 0.121</cell><cell>31</cell><cell>0 + 74</cell><cell>0.5</cell><cell>0.025</cell><cell>0.041</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test MRR and parameter budget on OGB WikiKG 2. 1% of total nodes) where each node is represented with k = 20 nearest anchors and a relational context of size m = 12, and we use a 2-layer MLP as a hash encoder (other hyperparameters as in Appendix A). Generally, such a NodePiece configuration can be paired with any link prediction decoder and we chose a non-parametric Au-toSF<ref type="bibr" target="#b55">(Zhang et al., 2020b)</ref> as one of the strongest decoders on this graph. Overall, the NodePiece + AutoSF model has only 6.9M parameters, about 70? smaller than top shallow models. Compared to the best reported shallow approaches, the NodePiece-enabled model exhibits (cf.Table 4, averaged over 10 seeds) even better performance achieved with a orders of magnitude smaller parameter budget. We believe this result shows the effectiveness of NodePiece on large KGs with a dramatic parameter size reduction without significant performance losses. Ablations report the duality of a relational context, i.e., removing it from hashes leads to even higher MRR scores. On the other hand, the relational context alone with 0 learnable anchors still yields considerably better results than 1000? larger shallow models RotatE and TransE.4.2 INDUCTIVE LINK PREDICTIONWe conduct a set of experiments on the inductive link prediction benchmark introduced by Teru et al.(2020) to measure the performance of NodePiece features in the extreme case when anchor nodes are not available and only relational context can be used to compose entity representations.Setup. The unique feature of this benchmark compared to other evaluated tasks is that training and inference graphs are disjoint, i.e., inference at validation and test time is performed on a completely new graph comprised of new entities, and link prediction involves only entities unseen during training. As inference graphs are disconnected from training ones, learning anchors from the training graph is useless, so node hashes are built only using the m-sized relational context. On top of the obtained NodePiece features we then employ a relational message passing GNN, CompGCN<ref type="bibr" target="#b46">(Vashishth et al., 2020)</ref>, with RotatE<ref type="bibr" target="#b43">(Sun et al., 2019)</ref> as a scoring function for triples. More details on the setup and best hyperparameters for NodePiece are presented in Appendix J.Baselines. We compare NodePiece + CompGCN with two families of models applicable in the inductive setting, i.e., rule-based methods, Neural LP<ref type="bibr" target="#b52">(Yang et al., 2017)</ref>, DRUM(Sadeghian et al.</figDesc><table><row><cell>Model</cell><cell>#Params</cell><cell>MRR</cell></row><row><cell>NP + AutoSF</cell><cell>6.9M</cell><cell>0.570 ?0.003</cell></row><row><cell>-rel. context</cell><cell>5.9M</cell><cell>0.592 ?0.003</cell></row><row><cell>-anc. dists</cell><cell>6.9M</cell><cell>0.570 ?0.004</cell></row><row><cell>-no anchors</cell><cell>1.3M</cell><cell>0.476 ?0.001</cell></row><row><cell>AutoSF</cell><cell>500M</cell><cell>0.546 ?0.005</cell></row><row><cell>PairRE</cell><cell>500M</cell><cell>0.521 ?0.003</cell></row><row><cell>RotatE</cell><cell cols="2">1250M 0.433 ?0.002</cell></row><row><cell>TransE</cell><cell cols="2">1250M 0.426 ?0.003</cell></row></table><note>, 2019), RuleN (Meilicke et al., 2018), and GNNs: GraIL (Teru et al., 2020) and recently proposed Neural Bellman-Ford Nets (NBFNet) (Zhu et al., 2021). Discussion. Generally, the results confirm the trend identified previously: relation-only features are strong performers in dense relation-rich graphs. NodePiece features paired with CompGCN significantly improve over path-based methods where performance gap might reach 37 absolute Hits@10 points, e.g., in FB15k-237 V1 and NELL-995 V1. Comparing to GNNs, NodePiece +</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Inductive link prediction results, Hits@10. Best results are in bold, second best are underlined. ? results taken from<ref type="bibr" target="#b44">Teru et al. (2020)</ref>. NBFNet results taken from</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Node classification results. |V | denotes vocabulary size (anchors + relations), #P is a total parameter count (millions).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">WD50K (5% labeled)</cell><cell></cell><cell cols="2">WD50K (10% labeled)</cell><cell></cell></row><row><cell></cell><cell>|V |</cell><cell cols="7">#P (M) ROC-AUC PRC-AUC Hard Acc ROC-AUC PRC-AUC Hard Acc</cell></row><row><cell>MLP</cell><cell>46k + 1k</cell><cell>4.1</cell><cell>0.503</cell><cell>0.016</cell><cell>0.001</cell><cell>0.510</cell><cell>0.017</cell><cell>0.002</cell></row><row><cell>CompGCN</cell><cell>46k + 1k</cell><cell>4.4</cell><cell>0.836</cell><cell>0.280</cell><cell>0.176</cell><cell>0.834</cell><cell>0.265</cell><cell>0.161</cell></row><row><cell>NodePiece + GNN</cell><cell>50 + 1k</cell><cell>0.75</cell><cell>0.981</cell><cell>0.443</cell><cell>0.513</cell><cell>0.981</cell><cell>0.450</cell><cell>0.516</cell></row><row><cell cols="2">-no rel. context 50 + 1k</cell><cell>0.64</cell><cell>0.982</cell><cell>0.446</cell><cell>0.534</cell><cell>0.982</cell><cell>0.449</cell><cell>0.530</cell></row><row><cell cols="2">-no distances 50 + 1k</cell><cell>0.74</cell><cell>0.981</cell><cell>0.448</cell><cell>0.516</cell><cell>0.981</cell><cell>0.448</cell><cell>0.513</cell></row><row><cell>-no anchors, rels only</cell><cell>0 + 1k</cell><cell>0.54</cell><cell>0.984</cell><cell>0.453</cell><cell>0.532</cell><cell>0.984</cell><cell>0.456</cell><cell>0.533</cell></row></table><note>CompGCN outperforms GraIL by a large margin in all (except one) experiments and is competitive to NBFNet on relation-rich FB15k-237 splits. As expected, NodePiece features are less efficient on sparse graphs (like WN18RR with few unique relations) but still outperform topology-based GraIL.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Out-of-sample link prediction. ? results are taken from<ref type="bibr" target="#b0">(Albooyeh et al., 2020)</ref>. |V | denotes vocabulary size (anchors + relations), #P is a total parameter count (millions).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">oFB15k-237</cell><cell></cell><cell></cell><cell></cell><cell cols="2">oYAGO 3-10 (117k)</cell><cell></cell></row><row><cell></cell><cell>|V |</cell><cell cols="3">#P (M) MRR H@10</cell><cell>%</cell><cell>|V |</cell><cell cols="2">#P (M) MRR H@10</cell><cell>%</cell></row><row><cell>oDistMult-ERAvg</cell><cell>11k + 0.5k</cell><cell>2.4</cell><cell cols="4">0.256  ? 0.420  ? 100 117k + 74</cell><cell>23.4</cell><cell>OOM OOM</cell><cell>-</cell></row><row><cell>NodePiece + DistMult</cell><cell>1k + 0.5k</cell><cell>1</cell><cell>0.206</cell><cell>0.372</cell><cell>88</cell><cell>10k + 74</cell><cell>2.7</cell><cell cols="2">0.133 0.261 100</cell></row><row><cell cols="2">-no rel. context 1k + 0.5k</cell><cell>1</cell><cell>0.173</cell><cell>0.329</cell><cell>78</cell><cell>10k + 74</cell><cell>2.7</cell><cell>0.125 0.245</cell><cell>94</cell></row><row><cell cols="2">-no distances 1k + 0.5k</cell><cell>1</cell><cell>0.208</cell><cell>0.372</cell><cell>88</cell><cell>10k + 74</cell><cell>2.7</cell><cell>0.133 0.260</cell><cell>99</cell></row><row><cell>-no anchors, rels only</cell><cell>0 + 0.5k</cell><cell>0.8</cell><cell>0.069</cell><cell>0.127</cell><cell>30</cell><cell>0 + 74</cell><cell>0.7</cell><cell>0.015 0.017</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Dataset statistics. LP -link prediction, RP -relation prediction, NC -node classification, OOS -out-of-sample. In OOS-LP, Nodes also shows the amount of unseen nodes in validation/test.</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell>Nodes</cell><cell>Relations</cell><cell>Edges</cell><cell>Train</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>FB15k-237 (Toutanova &amp; Chen, 2015)</cell><cell>LP, RP</cell><cell>14,505</cell><cell>237</cell><cell>310,079</cell><cell>272,115</cell><cell>17,526</cell><cell>20,438</cell></row><row><cell>WN18RR (Dettmers et al., 2018)</cell><cell>LP, RP</cell><cell>40,559</cell><cell>11</cell><cell>92,583</cell><cell>86,835</cell><cell>2824</cell><cell>2924</cell></row><row><cell>CoDEx-Large (Safavi &amp; Koutra, 2020)</cell><cell>LP</cell><cell>77,951</cell><cell>69</cell><cell>612,437</cell><cell>551,193</cell><cell>30,622</cell><cell>30,622</cell></row><row><cell>YAGO 3-10 (Mahdisoltani et al., 2015)</cell><cell>LP, RP</cell><cell>123,143</cell><cell>37</cell><cell>1,089,000</cell><cell>1,079,040</cell><cell>4978</cell><cell>4982</cell></row><row><cell>OGB WikiKG 2 (Hu et al., 2020)</cell><cell>LP</cell><cell>2,500,604</cell><cell>535</cell><cell cols="2">17,137,181 16,109,182</cell><cell>429,456</cell><cell>598,543</cell></row><row><cell>WD50K</cell><cell>NC</cell><cell>46,164</cell><cell>526</cell><cell>222,563</cell><cell>4600 (N)</cell><cell cols="2">4600 (N) 4600 (N)</cell></row><row><cell>oFB15k-237 (Albooyeh et al., 2020)</cell><cell cols="2">OOS-LP 11k/1395/1395</cell><cell>234</cell><cell>292,173</cell><cell>193,490</cell><cell>44,601</cell><cell>54,082</cell></row><row><cell>oYAGO 3-10</cell><cell cols="2">OOS-LP 117k/2960/2959</cell><cell>37</cell><cell>1,086,416</cell><cell>988,124</cell><cell>47,112</cell><cell>51,180</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Inductive relation prediction dataset statistics. Facts denote the size of the input graph while queries denote the triples to be predicted. Training sets contain all queries as facts. Note that in validation and test we receive a new graph disjoint from the training one, and queries are sent against this new inference graph (hence the number of entities and facts for validation and test is the same).</figDesc><table><row><cell>Dataset</cell><cell cols="2">Relations</cell><cell cols="2">Train Entity Query</cell><cell>Facts</cell><cell cols="2">Validation Entity Query</cell><cell>Fact</cell><cell cols="2">Test Entity Query</cell><cell>Facts</cell></row><row><cell></cell><cell>v1</cell><cell>183</cell><cell>2,000</cell><cell>4,245</cell><cell>4,245</cell><cell>1,500</cell><cell>206</cell><cell>1,993</cell><cell>1,500</cell><cell>205</cell><cell>1,993</cell></row><row><cell>FB15k-237</cell><cell>v2 v3</cell><cell>203 218</cell><cell cols="4">3,000 4,000 17,986 17,986 3,000 9,739 9,739 2,000</cell><cell>469 866</cell><cell>4,145 7,406</cell><cell>2,000 3,000</cell><cell>478 865</cell><cell>4,145 7,406</cell></row><row><cell></cell><cell>v4</cell><cell>222</cell><cell cols="9">5,000 27,203 27,203 3,500 1,416 11,714 3,500 1,424 11,714</cell></row><row><cell></cell><cell>v1</cell><cell>9</cell><cell>2,746</cell><cell>5,410</cell><cell>5,410</cell><cell>922</cell><cell>185</cell><cell>1,618</cell><cell>922</cell><cell>188</cell><cell>1,618</cell></row><row><cell>WN18RR</cell><cell>v2 v3</cell><cell>10 11</cell><cell cols="4">6,954 15,262 15,262 2,923 12,078 25,901 25,901 5,084</cell><cell>411 538</cell><cell>4,011 6,327</cell><cell>2,923 5,084</cell><cell>441 605</cell><cell>4,011 6,327</cell></row><row><cell></cell><cell>v4</cell><cell>9</cell><cell>3,861</cell><cell>7,940</cell><cell>7,940</cell><cell cols="6">7,208 1,394 12,334 7,208 1,429 12,334</cell></row><row><cell></cell><cell>v1</cell><cell>14</cell><cell>3,103</cell><cell>4,687</cell><cell>4,687</cell><cell>225</cell><cell>101</cell><cell>833</cell><cell>225</cell><cell>100</cell><cell>833</cell></row><row><cell>NELL-995</cell><cell>v2 v3</cell><cell>88 142</cell><cell cols="4">2,564 4,647 16,393 16,393 4,921 8,219 8,219 4,937</cell><cell>459 811</cell><cell>4,586 8,048</cell><cell>4,937 4,921</cell><cell>476 809</cell><cell>4,586 8,048</cell></row><row><cell></cell><cell>v4</cell><cell>77</cell><cell>2,092</cell><cell>7,546</cell><cell>7,546</cell><cell>3,294</cell><cell>716</cell><cell>7,073</cell><cell>3,294</cell><cell>731</cell><cell>7,073</cell></row><row><cell cols="12">NodePiece is implemented in Python using igraph library (licensed under GNU GPL 2) for com-</cell></row><row><cell cols="12">puting centrality measures and perform basic tokenization. Downstream tasks employ NodePiece</cell></row><row><cell cols="12">in conjunction with PyTorch (Paszke et al., 2019) (BSD-style license), PyKEEN (Ali et al., 2021)</cell></row><row><cell cols="12">(MIT License), and PyTorch-Geometric (Fey &amp; Lenssen, 2019) (MIT License). We ran experiments</cell></row><row><cell cols="12">on a machine with one RTX 8000 GPU and 64 GB RAM. The OGB WikiKG 2 experiments were</cell></row><row><cell cols="12">executed on a single Tesla V100 16 GB VRAM and 64 GB RAM. All used datasets are available</cell></row><row><cell cols="2">under open licenses.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>NodePiece hyperparameters for transductive link prediction experiments</figDesc><table><row><cell>Parameter</cell><cell cols="5">FB15k-237 WN18RR CoDEx-L YAGO 3-10 OGB WikiKG 2</cell></row><row><cell># Anchors, |A|</cell><cell>1000</cell><cell>500</cell><cell>7000</cell><cell>10000</cell><cell>20000</cell></row><row><cell># Anchors per node, k</cell><cell>20</cell><cell>50</cell><cell>20</cell><cell>20</cell><cell>20</cell></row><row><cell>Relational context, m</cell><cell>15</cell><cell>4</cell><cell>6</cell><cell>5</cell><cell>12</cell></row><row><cell>Vocabulary dim, d</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>512</cell><cell>256</cell><cell>512</cell><cell>512</cell></row><row><cell>Learning rate</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.00025</cell><cell>0.0001</cell></row><row><cell>Epochs</cell><cell>400</cell><cell>600</cell><cell>120</cell><cell>600</cell><cell>300k (steps)</cell></row><row><cell>Encoder type</cell><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell><cell>MLP</cell></row><row><cell>Encoder dim</cell><cell>400</cell><cell>400</cell><cell>400</cell><cell>400</cell><cell>400</cell></row><row><cell>Encoder layers</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>Encoder dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Loss function</cell><cell>BCE</cell><cell>NSSAL</cell><cell>BCE</cell><cell>NSSAL</cell><cell>NSSAL</cell></row><row><cell>Margin</cell><cell>-</cell><cell>15</cell><cell>-</cell><cell>50</cell><cell>50</cell></row><row><cell># Negative samples</cell><cell>-</cell><cell>20</cell><cell>-</cell><cell>10</cell><cell>128</cell></row><row><cell>Label smoothing</cell><cell>0.4</cell><cell>-</cell><cell>0.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Training time, hours</cell><cell>7</cell><cell>5.5</cell><cell>26</cell><cell>23</cell><cell>11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>RotatE hyperparameters for transductive link prediction experiments.</figDesc><table><row><cell>CoDEx-L and YAGO</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Hyperparameters for relation prediction experiments. The content is largely identical toTable 10, only changed parameters are listed</figDesc><table><row><cell></cell><cell cols="3">NodePiece + RotatE</cell><cell></cell><cell>RotatE</cell><cell></cell></row><row><cell>Parameter</cell><cell cols="6">FB15k-237 WN18RR YAGO 3-10 FB15k-237 WN18RR YAGO 3-10</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell>Epochs</cell><cell>20</cell><cell>150</cell><cell>7</cell><cell>150</cell><cell>150</cell><cell>150</cell></row><row><cell>Loss function</cell><cell>NSSAL</cell><cell>NSSAL</cell><cell>NSSAL</cell><cell>NSSAL</cell><cell>NSSAL</cell><cell>NSSAL</cell></row><row><cell>Margin</cell><cell>15</cell><cell>12</cell><cell>25</cell><cell>9</cell><cell>3</cell><cell>5</cell></row><row><cell># Negative samples</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell><cell>20</cell></row><row><cell>Training time, min</cell><cell>25</cell><cell>30</cell><cell>25</cell><cell>28</cell><cell>10</cell><cell>57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell cols="3">: Hyperparameters for node classification experiments</cell><cell></cell></row><row><cell>Parameter</cell><cell cols="3">NodePiece + CompGCN CompGCN MLP</cell></row><row><cell># Anchors, |A|</cell><cell>50</cell><cell>-</cell><cell>-</cell></row><row><cell># Anchors per node, k</cell><cell>10</cell><cell>-</cell><cell>-</cell></row><row><cell>Relational context, m</cell><cell>5</cell><cell>-</cell><cell>-</cell></row><row><cell>Vocabulary dim, d</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell>Learning rate</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>Epochs</cell><cell>4000</cell><cell>4000</cell><cell>4000</cell></row><row><cell>NodePiece encoder</cell><cell>MLP</cell><cell>-</cell><cell>-</cell></row><row><cell>NodePiece encoder dim</cell><cell>200</cell><cell>-</cell><cell>-</cell></row><row><cell>NodePiece encoder layers</cell><cell>2</cell><cell>-</cell><cell>-</cell></row><row><cell>NodePiece encoder dropout</cell><cell>0.1</cell><cell>-</cell><cell>-</cell></row><row><cell>GNN (MLP) layers</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell>GNN (MLP) dropout</cell><cell>0.5</cell><cell>0.5</cell><cell>0.5</cell></row><row><cell>Loss function</cell><cell>BCE</cell><cell>BCE</cell><cell>BCE</cell></row><row><cell>Label smoothing</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Training time, hours</cell><cell>14</cell><cell>22</cell><cell>6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Hyperparameters for out-of-sample link prediction experiments. The content is largely identical toTable 10, only changed parameters are listed</figDesc><table><row><cell></cell><cell cols="2">NodePiece + DistMult</cell><cell>oDistMult</cell></row><row><cell>Parameter</cell><cell cols="3">oFB15k-237 oYAGO 3-10 oFB15k-237</cell></row><row><cell># Anchors, |A|</cell><cell>1000</cell><cell>10000</cell><cell>-</cell></row><row><cell># Anchors per node, k</cell><cell>20</cell><cell>20</cell><cell>-</cell></row><row><cell>Relational context, m</cell><cell>15</cell><cell>5</cell><cell>-</cell></row><row><cell>Vocabulary dim, d</cell><cell>200</cell><cell>200</cell><cell>200</cell></row><row><cell>Batch size</cell><cell>256</cell><cell>256</cell><cell>1000</cell></row><row><cell>Learning rate</cell><cell>0.0005</cell><cell>0.0005</cell><cell>0.01</cell></row><row><cell>Epochs</cell><cell>40</cell><cell>40</cell><cell>1000</cell></row><row><cell>NodePiece encoder</cell><cell cols="2">Transformer Transformer</cell><cell>-</cell></row><row><cell>NodePiece encoder dim</cell><cell>512</cell><cell>512</cell><cell>-</cell></row><row><cell>NodePiece encoder layers</cell><cell>2</cell><cell>2</cell><cell>-</cell></row><row><cell>NodePiece encoder dropout</cell><cell>0.1</cell><cell>0.1</cell><cell>-</cell></row><row><cell>Loss function</cell><cell>Softplus</cell><cell>Softplus</cell><cell>Softplus</cell></row><row><cell># Negative samples</cell><cell>5</cell><cell>5</cell><cell>1</cell></row><row><cell>Training time, hours</cell><cell>2</cell><cell>8</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Relation prediction results. |V | denotes vocabulary size (anchors + relations).</figDesc><table><row><cell></cell><cell cols="2">FB15k-237</cell><cell></cell><cell>WN18RR</cell><cell cols="2">YAGO 3-10</cell></row><row><cell></cell><cell>|V |</cell><cell>MRR H@10</cell><cell>|V |</cell><cell>MRR H@10</cell><cell>|V |</cell><cell>MRR H@10</cell></row><row><cell>RotatE</cell><cell cols="6">15k + 0.5k 0.905 0.979 40k + 22 0.774 0.897 123k + 74 0.909 0.992</cell></row><row><cell>NodePiece + RotatE</cell><cell cols="4">1k + 0.5k 0.874 0.971 500 + 22 0.761 0.985</cell><cell cols="2">10k + 74 0.951 0.997</cell></row><row><cell cols="5">-no rel. context 1k + 0.5k 0.876 0.968 500 + 22 0.541 0.958</cell><cell cols="2">10k + 74 0.898 0.993</cell></row><row><cell cols="5">-no distances 1k + 0.5k 0.877 0.970 500 + 22 0.746 0.975</cell><cell cols="2">10k + 74 0.943 0.997</cell></row><row><cell>-no anchors, rels only</cell><cell>0 + 0.5k</cell><cell>0.873 0.971</cell><cell>0 + 22</cell><cell>0.545 0.947</cell><cell>0 + 74</cell><cell>0.951 0.998</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors would like to thank Koustuv Sinha, Gaurav Maheshwari, and Priyansh Trivedi for insightful and valuable discussions at earlier stages of this work. We also thank anonymous reviewers for the helpful comments. This work is partially supported by the Canada CIFAR AI Chair Program and Samsung AI grant (held at Mila). We thank Mila and Compute Canada for access to computational resources.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>anchors again skews the distribution of anchor distances towards smaller values and, hence, to higher probabilities of finding anchors in a closer neighborhood of a target node.</p><p>We would recommend using centrality-based strategies to select A with k nearest anchors per node if anchor distances and probability of finding anchors in a closer neighborhood are of higher importance.</p><p>Published as a conference paper at ICLR 2022 Assuming the input graph is a single connected component, when sampling |A| total anchors and selecting randomly k anchors per node, the number of possible hash combinations is bounded by |A| k . In this scenario, uniqueness of hashes is achieved by having this number bigger than the number N of nodes in a graph, |A| k &gt; N , and this happens with high probability for any reasonably large |A|, eg, 50 20 encodes about 4.7 ? 10 13 combinations which covers all existing public KGs combined. In the deterministic selection of nearest anchors per node we do not have such guarantees. Nevertheless, additional sequences of relational contexts and anchor distances help to obtain more unique hashes. Collisions are possible in highly regular graphs like Wordnet, but real-world KGs like Wikidata and DBpedia do not exhibit such a regular structure. Similar to homonyms whose meaning depends on the surrounding context in a sentence, we hypothesize that adding message passing layers (that can be seen as encoding of a neighboring context for a given node) on top of NodePiece hashes might further improve the diversity of node representations. That said, the future work research agenda might include proving tighter theoretical bounds on hashes uniqueness and developing new anchor sampling and tokenization strategies.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Out-of-sample representation learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Albooyeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishab</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed Mehran</forename><surname>Kazemi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.241</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.findings-emnlp.241" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="2657" to="2666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bringing light into the dark: A largescale evaluation of knowledge graph embedding models under a unified framework. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehmann</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v22/20-825.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">82</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to extrapolate knowledge: Transductive few-shot out-of-graph link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Bok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-relational poincar? graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="4465" to="4475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
		<ptr target="https://www.aclweb.org/anthology/Q17-1010" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<editor>Christopher J. C. Burges, L?on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lowdimensional hyperbolic knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6901" to="6914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meta relational learning for few-shot link prediction in knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4208" to="4217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MLMLM: link prediction with mean likelihood masked language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Clou?tre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Trempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><surname>Zouaq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021</title>
		<editor>Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli</editor>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4321" to="4331" />
		</imprint>
	</monogr>
	<note>ACL/IJCNLP 2021 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive entity representations from text via link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Groth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;21: The Web Conference 2021, Virtual Event / Ljubljana</title>
		<editor>Jure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia</editor>
		<meeting><address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="798" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">ACM / IW3C2</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>Jill Burstein, Christy Doran, and Thamar Solorio</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge hypergraphs: Prediction beyond binary relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perouz</forename><surname>Taslakian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<editor>Christian Bessiere</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Message passing for hyper-relational knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyansh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7346" to="7359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge transfer for out-of-knowledge-base entities : A graph neural network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuo</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidekazu</forename><surname>Oiwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<editor>Carles Sierra</editor>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="1802" to="1808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/2002.00388</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer G. Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PyTorch-BigGraph: A Large-scale Graph Embedding System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference</title>
		<meeting>the 2nd SysML Conference<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anchor &amp; transform: Learning sparse embeddings for large vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Paul Pu Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahmed</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Vd7lCMvtLqg" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">YAGO3: A knowledge base from multilingual wikipedias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzaneh</forename><surname>Mahdisoltani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Biega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Biennial Conference on Innovative Data Systems Research, CIDR 2015</title>
		<meeting><address><addrLine>Asilomar, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Online Proceedings. www.cidrdb.org</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">{SOLAR}: Sparse orthogonal learned and random embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tharun</forename><surname>Medini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beidi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshumali</forename><surname>Shrivastava</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=fw-BHZ1KjxJ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-grained evaluation of rule-and embedding-based systems for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2018 -17th International Semantic Web Conference</title>
		<meeting><address><addrLine>Monterey, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11136</biblScope>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held</title>
		<editor>Christopher J. C. Burges, L?on Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger</editor>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="11" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<idno>number = SIDL-WP-1999-0120</idno>
		<imprint>
			<date type="published" when="1999-11" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Alessandro Moschitti, Bo Pang, and Walter Daelemans</editor>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Technological Sciences</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Beyond triplets: Hyper-relational knowledge graph embedding for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Cudr?-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1885" to="1896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.238</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.238" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="2681" to="2691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DRUM: end-to-end differentiable rule mining on knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Armandpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="15321" to="15331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CoDEx: A Comprehensive Knowledge Graph Completion Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.669</idno>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-main.669" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="8328" to="8350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1910.01108" />
		<imprint>
			<date type="published" when="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael Sejr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018-06-03" />
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2012.6289079</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2012.6289079" />
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-03-25" />
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Inductive relation prediction by subgraph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Komal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9448" to="9457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4007</idno>
		<ptr target="https://www.aclweb.org/anthology/W15-4007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Composition-based multirelational graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BylA_C4tPr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<editor>Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Mulde: Multi-teacher knowledge distillation for low-dimensional knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07152</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Logic attention based neighborhood aggregation for inductive knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7152" to="7159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">KEPLER: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="2319" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">KG-BERT: BERT for knowledge graph completion. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1909.03193" />
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Few-shot knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3041" to="3048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Autosf: Searching scoring functions for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th IEEE International Conference on Data Engineering</title>
		<meeting><address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Autosf: Searching scoring functions for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05912</idno>
		<title level="m">Distile: Distiling knowledge graph embeddings for faster and cheaper reasoning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Neural bellman-ford networks: A general graph neural network framework for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuobai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
