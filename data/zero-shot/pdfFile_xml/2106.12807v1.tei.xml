<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple Truncated SVD based Model for Node Classification on Heterophilic Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Lingam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research India Bengaluru</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Ragesh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research India Bengaluru</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Iyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research India Bengaluru</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundararajan</forename><surname>Sellamanickam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research India Bengaluru</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simple Truncated SVD based Model for Node Classification on Heterophilic Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have shown excellent performance on graphs that exhibit strong homophily with respect to the node labels i.e. connected nodes have same labels. However, they perform poorly on heterophilic graphs. Recent approaches have typically modified aggregation schemes, designed adaptive graph filters, etc. to address this limitation. In spite of this, the performance on heterophilic graphs can still be poor. We propose a simple alternative method that exploits Truncated Singular Value Decomposition (TSVD) of topological structure and node features. Our approach achieves up to ?30% improvement in performance over state-of-theart methods on heterophilic graphs. This work is an early investigation into methods that differ from aggregation based approaches. Our experimental results suggest that it might be important to explore other alternatives to aggregation methods for heterophilic setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Homophily <ref type="bibr" target="#b9">[10]</ref> is a principle in sociology that suggests that connections in real life are bred through similarity. In the context of semi-supervised classification, this implies that nodes with similar labels are likely to be connected. Several real world networks exhibit homophily, for example, people on a social network connect to each other based on similar interests. There are also several real world networks that exhibit the opposite behaviour. For example, the Wikipedia page on Homophily is not only connected to other pages from sociology, but also connected to various pages from mathematics, graph theory and statistics. Since Wikipedia is a large body of collective knowledge, its pages often have connections between several different areas.</p><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14]</ref> leverage network information along with node features to improve their semi-supervised classification performance. GNNs are largely dependent on network homophily to be able to give improved performance. For heterophilic networks, their performance can degrade significantly. Several approaches have been proposed in the literature to mitigate this degradation in performance in presence of heterophily. These approaches can be organized into three groups. The first of these approaches involve modifying the aggregation mechanism in graph neural networks in an effort to mitigate issues caused by heterophily. Pei et al. <ref type="bibr" target="#b10">[11]</ref> proposed message passing both over the graph neighborhood and the neighbors in the latent space. Zhu et al. <ref type="bibr" target="#b16">[17]</ref> proposed to keep the self embedding separate from the neighbor embeddings during aggregation, while also incorporating higher order neighbor embeddings in a similar fashion. Kim and Oh <ref type="bibr" target="#b5">[6]</ref> proposed several simple attention models trained on an additional auxiliary task and finally present an analysis of which attention model is well suited for homophily and heterophily. The second way to address heterophily is to explicity model a label-label compatibility matrix learnt that can be used as a prior to update the posterior belief in the label predictions. Zhu et al. <ref type="bibr" target="#b15">[16]</ref> proposes to model the label compatibility matrix that reflects the heterophily in the graph and utilizes the model in a GNN. The third group of approaches involve designing graph filters that can directly adapt to low frequency as well as high frequency parts of the graph as needed by the model. Bo et al. <ref type="bibr" target="#b2">[3]</ref> proposes to learn an attention mechanism that captures the proportion of low-frequency and highfrequency signals per edge. Chien et al. <ref type="bibr" target="#b3">[4]</ref> proposes an adaptive polynomial filter to pick up which low-frequency high-frequency signals are helpful for the task.</p><p>For homophilic networks, existing models Chien et al. <ref type="bibr" target="#b3">[4]</ref>, Klicpera et al. <ref type="bibr" target="#b8">[9]</ref> already prove to be excellent. Our focus lies in heterophilic networks. We are interested in class of methods that aim at modifying or adapting the graph to obtain better performance in heterophilic graphs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>. The more recent approaches among these class of methods adjust the eigenvalues of the graph to learn improved representations. Another interpretation for this adaptation is that it is selecting eigenvectors by learning coefficients. We replace this complex adaptation by a TSVD and propose simple yet effective methods to improve task performance. Our contributions can be summarized as follows.</p><p>(1) We present simple TSVD based methods based on our insights that outperform state-of-the-art approaches on heterophilic networks with performance gains of up to ? 30%. (2) Deviating from the popular message-passing frameworks, we propose a simple and efficient concatenation-based model that is competitive to neighborhood aggregation-based models and outperforms several baselines on benchmark datasets.</p><p>In the following sections, we discuss related works, motivate our proposed approach, and finally present our experimental results along with ablative studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Most of the development in the GNNs were for homophilic graphs, and they performed poorly in heterophily setting. One of the early works to address heterophily in GNNs was Geom-GCN <ref type="bibr" target="#b10">[11]</ref>. They identified two key weaknesses in GNNs in the context of heterophily. First, since the aggregation over the neighbourhood is permutationinvariant, it is difficult to identify which neighbours contribute positively and negatively to the final performance. Second, longrange information is difficult to aggregate. To mitigate these issues, they proposed aggregating over two sets of the neighbourhood -one from the graph and the other inferred in the latent space. <ref type="bibr" target="#b16">[17]</ref> proposed to separate the self-embeddings from neighbour embeddings. To avoid mixing of information, they concatenate self-embeddings and neighbour embeddings instead of aggregating them. Higher-order neighbourhood embeddings are similarly combined to capture long-range information.</p><formula xml:id="formula_0">H 2 GCN</formula><p>Recent approaches address these shortcomings by adapting the graph. SuperGAT <ref type="bibr" target="#b5">[6]</ref> gave several simple attention models trained on the classification and an additional auxiliary task. They suggest that these attention models can improve model performance across several graphs with varying homophily scores. FAGCN <ref type="bibr" target="#b2">[3]</ref> uses the attention mechanism and learns the weight of an edge as the difference in the proportion of low-frequency and high-frequency signals. They empirically show that negative edge-weights identify edges that connect nodes with different labels. GPR-GNN <ref type="bibr" target="#b3">[4]</ref> takes the idea proposed in APPNP and generalizes the Pagerank model that works well for graphs with varying homophily scores. Our proposed approach greatly simplifies adaptation methods while delivering significant performance improvements for the task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH 3.1 Preliminaries</head><p>We focus on the problem of semi-supervised node classification on a simple graph G = (V, E), where V is the set of vertices and E is the set of edges. Let A ? {0, 1} ? be the adjacency matrix associated with G, where = |V | is the number of nodes. Let Y be the set of all possible class labels. Let X ? R ? be the -dimensional feature matrix for all the nodes in the graph. Given a training set of nodes ? V whose labels are known, along with A and X, our goal is to predict the labels of the remaining nodes. The proportion of edges that connect two nodes with the same labels in a graph is called the homophily score of the graph. In our problem, we are particularly concerned with graphs that exhibit low homophily scores. In the next sub-section, we provide background material on the GPR-GNN modelling method and its approach to graph adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GPR-GNN Model</head><p>The GPR-GNN [4] model consists of two core components: (a) a non-linear network that transforms raw feature input X: Z (0) = (X; W) and (b) a generalized page ranking (GPR) component, G, that essentially aggregates the transformed output Z recursively as: Z ( ) = AZ ( ?1) , = 1, . . . , . Notice that there is no nonlinear operation involved after each aggregation step over . Therefore, the functionality of the GPR component can be written using an operator G defined as: G = =0 A and we obtain aggregated node embedding by applying G on the nonlinear network output: S = GZ (0) . Using singular value decomposition, A = U?U , Chien et al. <ref type="bibr" target="#b3">[4]</ref> presented an interpretation that the GPR component essentially performs a graph filtering operation:</p><formula xml:id="formula_1">G = U? (?)U where ? (?) is a polynomial graph filter applied element-wise and ? ( ) = =0</formula><p>where is the ? eigen value. As explained in Chien et al. <ref type="bibr" target="#b3">[4]</ref>, learning filter coefficients (i.e., ) help to get improved performance. Since the coefficients can take negative values the GPR-GNN model is able to capture high-frequency components of the graph signals, enabling the model to achieve improved performance on heterophilic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Proposed Approach</head><p>In this section, we present an alternative interpretation of the GPR-GNN model and suggest a simple Truncated Singular Value Decomposition (TSVD) based method. We present two approaches, each motivated by considering different aspects of the problem.</p><p>We start by closely observing the GPR component output given by:</p><formula xml:id="formula_2">S = U? (?)U Z (0) .<label>(1)</label></formula><p>Our first observation is that learning filter coefficients is equivalent to learning a new graph,?(U, ?; ) which is dependent on the fixed set of singular vectors and singular values, but, parameterised using . Therefore, the GPR-GNN model may be interpreted as adapting the original adjacency matrix A. Next, as noted in the previous section, using the structures present in singular value decomposition and polynomial function, we can expand (1) by unrolling over eigenvalues and interchanging the summation as:</p><formula xml:id="formula_3">S = ?? =1 ? ( ; )u u Z (0) .<label>(2)</label></formula><p>There are several choices available for selection functions that one can choose from. However, we simplify ? ( ; ) by replacing it with a simple selection function as follows:</p><formula xml:id="formula_4">S = ?? =1 1 ? 1 u u Z (0) .<label>(3)</label></formula><p>Our selection function is equivalent to a TSVD, where 1 largest singular values are used for reconstructing the new graph. Restricting to 1 singular values naturally induces negative edges. It is worth noting that the necessity of negative edges has been highlighted in Bo et al. <ref type="bibr" target="#b2">[3]</ref> and Chien et al. <ref type="bibr" target="#b3">[4]</ref> using the graph filtering concept. Bo et al. <ref type="bibr" target="#b2">[3]</ref> use attention mechanism to learn negative edges. On the other hand, Chien et al. <ref type="bibr" target="#b3">[4]</ref> uses a polynomial function with negative weights to obtain negative edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Hard Low Pass (HLP) Aggregation</head><p>Model. It is often useful to reduce dimension of raw features using principal component analysis. Let Q? Q be the TSVD of XX . upon substituting node embedding with TSVD of raw features (3), we get:</p><formula xml:id="formula_5">S = U(? 1 )U?(? 2 )<label>(4)</label></formula><p>where U?= U Q. We refer (4) as HLP Aggregation model as it involves truncated singular vectors of both A and X. We treat 1 and 2 as hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Hard Low Pass (HLP) Concat</head><p>Model. We suggest a simple alternative modeling approach that works quite well for heterophilic graphs. With neighborhood aggregation, difficulties arise when A and X are incompatible in the sense that it degrades the performance due to a violation of assumptions made. Though graph adaptation methods try to mitigate the effect of any violation, they still operate within the field of improving neighborhood aggregation. Therefore, it may be difficult to improve beyond some limits with the neighborhood aggregation restriction. Also, it may only  add more computational burden. In this context, we explored the approach of concatenating truncated node features (X) and truncated eigenvectors of A, and learning a classifier model. Since the features are decoupled now, this model is less affected by the incompatibility between A and X. Additionally, there is a significant reduction in computational cost. Therefore, the HLP Concat model is faster to train. We found this simple approach to outperform state-of-the-art methods on several heterophilic benchmark datasets. In the following section, we discuss our experimental section and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We validate our proposed models by comparing against several baselines and state-of-the-art heterophily graph networks on node classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate on seven heterophilic datasets to show the effectiveness of our models. Detailed statistics of the datasets used are provided in <ref type="table" target="#tab_1">Table 1</ref>. We borrowed Texas, Cornell, Wisconsin from WebKB 1 , where nodes represent web pages and edges denote hyperlinks between them. Actor is a co-occurence network borrowed from <ref type="bibr" target="#b12">[13]</ref>, where nodes correspond to an actor, and and edge represents the co-occurrence on the same Wikipedia page. Chameleon, Squirrel, and Crocodile are borrowed from <ref type="bibr" target="#b11">[12]</ref>. Nodes correspond to web pages and edges capture mutual links between pages. For all benchmark datasets, we use feature vectors, class labels from <ref type="bibr" target="#b5">[6]</ref>. For datasets in (Texas, Wisconsin, Cornell, Chameleon, Squirrel, Actor), we use 10 random splits (48%/32%/20% of nodes for train/validation/test set) from <ref type="bibr" target="#b10">[11]</ref>. For Crocodile, we create 10 random splits following <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Methods of Comparison</head><p>We provide the methods in comparison along with the hyperparameters ranges for each model. For all the models, we sweep the common hyper-parameters in same ranges. Learning rate is swept over [0.001, 0.003, 0.005, 0.008, 0.01], dropout over [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8], weight decay over [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1], and hidden dimensions over <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">32,</ref><ref type="bibr">64]</ref>. For model specific hyper-parameters, we tune over author prescribed ranges. We use undirected graphs with symmetric normalization for all graph networks in comparison. For all models, test accuracy is reported for 1 http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb the configuration that achieves the highest validation accuracy. We report standard deviation wherever applicable.</p><p>LR and MLP: We trained Logistic Regression classifier and Multi Layer Perceptron on the given node features. For MLP, we limit the number of hidden layers to one. SGCN: SGCN <ref type="bibr" target="#b14">[15]</ref> is a spectral method that models a low pass filter and uses a linear classifier. The number of layers in SGCN is treated as a hyper-parameter and swept over <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>SuperGAT: SuperGAT <ref type="bibr" target="#b5">[6]</ref> is an improved graph attention model designed to also work with noisy graphs. SuperGAT employs a link-prediction based self-supervised task to learn attention on edges. As suggested by the authors, on datasets with homophily levels lower than 0.2 we use SuperGAT SD . For other datasets, we use SuperGAT MX . We rely on authors code 2 for our experiments.</p><p>Geom-GCN: Geom-GCN <ref type="bibr" target="#b10">[11]</ref> proposes a geometric aggregation scheme that can capture structural information of nodes in neighborhoods and also capture long range dependencies. We quote author reported numbers for Geom-GCN. We could not run Geom-GCN on other benchmark datasets because of the unavailability of a pre-processing function that is not publicly available. H 2 GCN: H 2 GCN <ref type="bibr" target="#b16">[17]</ref> proposes an architecture, specially for heterophilic settings, that incorporates three design choices: i) ego and neighbor-embedding separation, higher-order neighborhoods, and combining intermediate representations. We quote author reported numbers where available, and sweep over author prescribed hyper-parameters for reporting results on the rest datasets. We rely on author's code 3 for our experiments. FAGCN: FAGCN [3] adaptively aggregates different low-frequency and high-frequency signals from neighbors belonging to same and different classes to learn better node representations. We rely on author's code 4 for our experiments.</p><p>APPNP: APPNP <ref type="bibr" target="#b8">[9]</ref> is an improved message propagation scheme derived from personalized PageRank. APPNP's addition of probability of teleporting back to root node permits it to use more propagation steps without oversmoothing. We use GPR-GNN's implementation of APPNP for our experiments.</p><p>GPR-GNN: GPR-GNN <ref type="bibr" target="#b3">[4]</ref> adaptively learns weights to jointly optimize node representations and the level of information to be extracted from graph topology. We rely on author's code 5 for our experiments.</p><p>HLP Models: We sweep 1 in <ref type="bibr">[1, min(#nodes, 2048)</ref>] and 2 in <ref type="bibr">[1, min(#features, 2048)</ref>]. HLP models use a one layer MLP as the classifier. We restrict to 2048 dimensions to reduce the computation burden. Unlike aggregation models, HLP concat. model need not be restricted to symmetric normalization. Hence, for HLP Concat. model, we sweep the graph type in [directed-graph, undirectedgraph], and graph norm type in [no-norm, row-norm, sym-norm].</p><p>All models use the Adam optimizer <ref type="bibr" target="#b6">[7]</ref>. For our proposed models that involve learning, we set early stopping to 30 and maximum number of epochs to 300. We utilize learning rate with decay, with decay factor set to 0.99 and decay frequency set to 50. All our experiments were performed on a machine with Intel Xeon 2.60Ghz processor, 112GB Ram, Nvidia Tesla P-100 GPU with 16GB of memory, python 3.6, and Tensorflow 1.15 <ref type="bibr" target="#b0">[1]</ref>. We used Optuna <ref type="bibr" target="#b1">[2]</ref> to optimize the hyperparameter search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>We propose two models HLP Agg. and HLP Concat. Our simple aggregation-based model outperforms popular message-passing approaches like GCN and SGCN. As opposed to modified aggregation schemes proposed in Kim and Oh <ref type="bibr" target="#b5">[6]</ref>, Pei et al. <ref type="bibr" target="#b10">[11]</ref>, our simple aggregation based on truncation tends to be more effective for heterophilic settings.</p><p>The disparity between node features and graphs can affect the performance of aggregation schemes. To decouple this effect, we propose concatenation-based methods that effectively leverage signals from the given topology and features. We observe from Table 2 that our proposed concatenation-based method outperforms state-of-the-art approaches on benchmark datasets. We see massive performance gains of up to ? 30%. FAGCN, GPR-GNN have identified that high-frequency components are beneficial for improving task performance on heterophilic graphs. However, our results suggest that carefully selecting lowfrequency components can lead to significant performance gains. We believe that the negative edges induced by TSVD are capturing high-frequency components and leading to improvements in performance, similar to how high-frequency coefficients were inducing negative edge-weights in FAGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Effect of varying Truncated SVD dimensions.</head><p>We observe in <ref type="table" target="#tab_4">Table 3</ref> that varying dimensions as opposed to fixing dimensions for node features and graphs in general lead to significant improvements in performance. For instance, performance on the Squirrel dataset jumps from 56.59% to 74.17%. We believe that, by varying 1 , the optimal set of negative edges induced can be found that improves performance. <ref type="table">Table 4</ref>, we observe for the HLP Agg. model that varying graph normalization over [no-norm., row-norm., symmetric-norm.] set, we see significant gains in performance. We conjecture that the usual symmetric normalization might be limiting aggregation-based methods' performance. However, moving to other normalization schemes will require rethinking the theoretical properties. This is beyond the scope of this work.   <ref type="figure" target="#fig_1">Figure 1</ref> shows the TSNE plots of learned embeddings for GPR-GNN and HLP Concat model on the Squirrel dataset. We can observe discernible clusters in our learned embedding plot. The plot qualitatively depicts the superiority of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Effect of varying graph normalization. In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we presented a TSVD based approach inspired by the GPR-GNN [4] model, which we show can be interpreted as selecting/weighing the singular vectors by scaling the corresponding singular values. We propose a TSVD based regularization model, that enables our model to avoid overfitting and acts as a hard lowpass filter. We show that our models outperform baselines across all heterophilic datasets. This model is simple and computationally cheaper. It begs the question of whether there are alternative ways to model Graph Neural Networks that work across varying homophily scores. We leave it as future work.    <ref type="table">Table 4</ref>: Effect of Graph Normalization. When treated as a hyperparameter, we vary the normalization in [symmetricnormalization, row-normalization, no-normalization], tried both the original directed graphs as well as the undirected variant. We pick the best model based on Validation Accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>TSNE plots of output layer embedding for Squirrel Dataset 4.4.3 TSNE Plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Datasets Statistics</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison With Baselines. The results marked with "*" are obtained from the corresponding paper. Variable # of Dims. 87.57 (5.44) 86.67 (4.22) 34.59 (1.32) 74.17 (1.83) 77.48 (0.80) 55.87 (1.25) 84.05 (4.67)</figDesc><table><row><cell>HLP Concat</cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Actor</cell><cell>Squirrel</cell><cell cols="2">Chameleon Crocodile</cell><cell>Cornell</cell></row><row><cell>Fixed # of Dims.</cell><cell>82.43 (6.97)</cell><cell>80.59 (4.42)</cell><cell>32.84 (1.34)</cell><cell>62.29 (1.50)</cell><cell>69.52 (0.87)</cell><cell>54.24 (2.67)</cell><cell>78.65 (6.67)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effect of Varying dimensions for Truncated SVD based features .94) 27.78 (0.98) 56.59 (1.36) 66.36 (2.07) 54.58 (1.88) 66.76 (6.84) Treated as hyperparameter 68.65 (7.94) 65.69 (3.07) 27.51 (0.83) 73.62 (1.94) 76.51 (1.81) 53.48 (2.30) 69.46 (4.02)</figDesc><table><row><cell>HLP Aggregation</cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Actor</cell><cell>Squirrel</cell><cell>Chameleon Crocodile</cell><cell>Cornell</cell></row><row><cell>Symmetric Norm</cell><cell>67.57 (4.68)</cell><cell>65.49 (3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/dongkwan-kim/SuperGAT 3 https://github.com/GemsLab/H2GCN 4 https://github.com/bdy9527/FAGCN 5 https://github.com/jianhao2016/GPRGNN</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/Softwareavailablefromtensorflow.org" />
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<editor>Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi?gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<meeting><address><addrLine>Dandelion Man?, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Optuna: A Next-generation Hyperparameter Optimization Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shotaro</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<idno>ArXiv abs/1907.10902</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond Low-frequency Information in Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua-Wei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive Universal Generalized PageRank Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining Neural Networks with Personalized PageRank for Classification on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Birds of a Feather: Homophily in Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miller</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-Scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Social Influence Analysis in Large-Scale Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph Neural Networks with Heterophily</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<editor>Tung Mai, Nedim Lipka, Nesreen K. Ahmed, and Danai Koutra. 2021</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
