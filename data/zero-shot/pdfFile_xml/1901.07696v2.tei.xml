<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Product-Aware Answer Generation in E-Commerce Question-Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 11-15, 2019. 2019. February 11-15, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Gao</surname></persName>
							<email>shengao@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
							<email>renzhaochun@jd.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Zhao</surname></persName>
							<email>zhaody@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
							<email>yindawei@acm.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<email>ruiyan@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICST</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ICST</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">ICST</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Product-Aware Answer Generation in E-Commerce Question-Answering</title>
					</analytic>
					<monogr>
						<title level="m">The Twelfth ACM International Conference on Web Search and Data Mining (WSDM &apos;19)</title>
						<meeting> <address><addrLine>Melbourne, VIC, Australia; Melbourne, VIC, Australia</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">February 11-15, 2019. 2019. February 11-15, 2019</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3289600.3290992</idno>
					<note>ACM ISBN 978-1-4503-5940-5/19/02. . . $15.00 ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Information systems ? Question answering; KEYWORDS Question answering</term>
					<term>e-commerce</term>
					<term>product-aware answer genera- tion * Work performed during</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In e-commerce portals, generating answers for product-related questions has become a crucial task. In this paper, we propose the task of product-aware answer generation, which tends to generate an accurate and complete answer from large-scale unlabeled e-commerce reviews and product attributes. Unlike existing question-answering problems, answer generation in e-commerce confronts three main challenges: (1) Reviews are informal and noisy; (2) joint modeling of reviews and key-value product attributes is challenging; (3) traditional methods easily generate meaningless answers. To tackle above challenges, we propose an adversarial learning based model, named PAAG, which is composed of three components: a questionaware review representation module, a key-value memory network encoding attributes, and a recurrent neural network as a sequence generator. Specifically, we employ a convolutional discriminator to distinguish whether our generated answer matches the facts. To extract the salience part of reviews, an attention-based review reader is proposed to capture the most relevant words given the question.</p><p>Conducted on a large-scale real-world e-commerce dataset, our extensive experiments verify the effectiveness of each module in our proposed model. Moreover, our experiments show that our model achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, the explosive popularity of question-answering (QA) is revitalizing the task of reading comprehension with promising results <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b33">34]</ref>. Unlike traditional knowledge-based QA methods that require a structured knowledge graph as the input and output resource description framework (RDF) triples <ref type="bibr" target="#b9">[10]</ref>, most of reading comprehension approaches read context passages and extract text spans from input text as answers <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">34]</ref>. E-commerce are playing an increasingly important role in our daily life. As a convenience of users, more and more e-commerce portals provide community question-answering services that allow users to pose product-aware questions to other consumers who purchased the same product before. Unfortunately, many productaware questions lack of proposer answers. Under the circumstances, users have to read the product's reviews to find the answer by themselves. Given product attributes and reviews, an answer is manually generated following a cascade procedure: (1) a user skims reviews and finds relevant sentences; (2) she/he extracts useful semantic units; <ref type="bibr" target="#b2">(3)</ref> and the user jointly combines these semantic units with attributes, and writes a proper answer. However, the information overload phenomenon makes this procedure an energy-draining process to pursue an answer from a rapidly increasing number of reviews. Consequently, automatic product-aware question-answering become more and more helpful in this scenario. The task on which we focus is the product-aware answer generation given reviews and product attributes. Our goal is to respond product-aware questions automatically given a large amount of reviews and attributes of a specific product. Unlike either a "yes/no" binary classification task <ref type="bibr" target="#b12">[13]</ref> or a review ranking task <ref type="bibr" target="#b16">[17]</ref>, product-aware answer generation provides a natural-sounding sentence as an answer. The definition of our task is similar as the reading comprehension. However, most of existing reading comprehension solutions only extract text spans from contextual passages <ref type="bibr" target="#b36">[37]</ref>. Since the target of product-aware answer generation is to generate a naturalsounding answer instead of text spans, most of reading comprehension methods and datasets (e.g., SQuAD <ref type="bibr" target="#b19">[20]</ref>) are not applicable. As far as we know, only few of reading comprehension approaches aim to generate a natural-sounding answers from extraction results <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>. With a promising performance on MS-MARCO <ref type="bibr" target="#b17">[18]</ref>, S-Net framework proposed by Tan et al. <ref type="bibr" target="#b28">[29]</ref> focuses on synthesizing answers from extraction results. However, S-Net requires a large amount of labeling data for extracting text spans, which is still unrealistic given a huge number of reviews. Moreover, product reviews from e-commerce website are informal and noisy, whereas in reading comprehension the given context passages are usually in a formal style. Generally, existing reading comprehension approaches confront three challenges when addressing product-aware question answering: (1) Review text is irrelevant and noisy. <ref type="bibr">(</ref>2) It's extremely expensive to label large amounts of explicit text spans from real-world e-commerce platforms. (3) Traditional loss function calculation in reading comprehension tends to generate meaningless answers such as "I don't know".</p><p>In this paper, we propose the product-aware answer generator (PAAG), a product related question answering model which incorporates customer reviews with product attributes. Specifically, at the beginning we employ an attention mechanism to model interactions between a question and reviews. Simultaneously, we employ a key-value memory network to store the product attributes and extract the relevance values according to the question. Eventually, we propose a recurrent neural network (RNN) based decoder, which combines product-aware review representation and attributes to generate the answer. More importantly, to tackle the problem of meaningless answers, we propose an adversarial learning mechanism in the loss calculation for optimizing parameters. Conducted on a large-scale real-world e-commerce dataset, we evaluate the performance of PAAG using extensive experiments. Experimental results demonstrate that the PAAG model achieves significant improvement over other baselines, including the state-of-the-art reading comprehension model. Furthermore, we also examine the effectiveness of each module in PAAG. Our experiments verify that adversarial learning is capable to significantly improve the denoising and facts extracting capacity of PAAG.</p><p>To sum up, our contributions can be summarized as follows:</p><p>? We propose a product-aware answer generation task. ? To tackle this task, we propose an end-to-end learning method to extract fact that is helpful for answering questions from reviews and attributes and then generate answer text.</p><p>? Due to the review is in an informal style with noise, we propose an attention based review reader and use the Wasserstein distance based adversarial learning method to learn to denoise the review text. The discriminator can also give an additional training signal for generating more consistence answer.</p><p>? Experiments conducted on a large-scale real-world dataset show that our PAAG method outperforms all baselines, include the state-of-the-art model in terms of all metrics. The effectiveness of each module in PAAG is also demonstrated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We detail related work on product-aware question-answering, reading comprehension, and sequence-to-sequence architecture. Product-aware question answering. In recent years, productaware question answering has received several attention. Most of existing strategies aim at extracting relevant sentences from input text to answer the given question. Yu et al. <ref type="bibr" target="#b38">[39]</ref> propose a framework for opinion QA, which first organizes reviews into a hierarchy structure and retrieves review sentence as the answer. Yu et al. <ref type="bibr" target="#b39">[40]</ref> propose an answer prediction model by incorporating an aspect analytic model to learn latent aspect-specific review representation for predicting the answer. External knowledge has been considered with the development of knowledge graphs. McAuley et al. <ref type="bibr" target="#b12">[13]</ref> propose a method using reviews as knowledge to predict the answer, where they classify answers into two types, binary answers (i.e. "yes" or "no") and open-ended answers. Incorporating review information, recent studies employ ranking strategies to optimize an answer from candidate answers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b37">38]</ref>. Meanwhile, productaware question retrieval and ranking has also been studied. Cui et al. <ref type="bibr" target="#b3">[4]</ref> propose a system which combines questions with RDF triples. Yu et al. <ref type="bibr" target="#b37">[38]</ref> propose a model which retrieves the most similar queries from candidate QA pairs, and uses corresponding answer as the final result. However, all above task settings differ from our task. Unlike above approaches, our method is aimed to generate an answer from scratch, based on both reviews and product attributes. Reading Comprehension. Given a question and relevant passages, reading comprehension extracts a text span from passages as an answer <ref type="bibr" target="#b19">[20]</ref>. Recently, based on a widely applied dataset, i.e., SQuAD <ref type="bibr" target="#b19">[20]</ref>, many appraoches have been proposed. Seo et al. <ref type="bibr" target="#b22">[23]</ref> use bi-directional attention flow mechanism to obtain a queryaware passage representation. Wang et al. <ref type="bibr" target="#b33">[34]</ref> propose a model to match the question with passage using gated attention-based recurrent networks to obtain the question-aware passage representation. Consisting exclusively of convolution and self-attention, QANet <ref type="bibr" target="#b36">[37]</ref> achieves the state-of-the-art performance in reading comprehension. As mentioned above, most of the effective methods contain question-aware passage representation for generating a better answer. This mechanism make the models focus on the important part of passage according to the question. Following these previous work, our method models the reviews of product with a question aware mechanism. Sequence-to-sequence architecture. In recent years, sequenceto-sequence (seq2seq) based neural networks have been proved effective in generating a fluent sentence. The seq2seq model <ref type="bibr" target="#b27">[28]</ref> is originally proposed for machine translation and later adapted to various natural language generation tasks, such as text summarization and dialogue generation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref>. Rush et al. <ref type="bibr" target="#b20">[21]</ref> apply the seq2seq mechanism with attention model to text summarization field. Then See et al. <ref type="bibr" target="#b21">[22]</ref> add copy mechanism and coverage loss to generate summarization without out-of-vocabulary and redundancy words. The seq2seq architecture has also been broadly used in dialogue system. Tao et al. <ref type="bibr" target="#b29">[30]</ref> propose a multi-head attention mechanism to capture multiple semantic aspects of the query and generate a more informative response. Different from seq2seq models, our model utilizes not only the information in input sequence but also many external knowledge from user reviews and product attributes to generate the answer that matches the facts. Unlike traditional seq2seq model, there are several tasks which input data is in keyvalue structure instead of a sequence. In order to utilize these data when generating text, key-value memory network (KVMN) is purposed to store this type of data. He et al. <ref type="bibr" target="#b9">[10]</ref> incorporate copying and retrieving knowledge from knowledge base stored in KVMN to generate natural answers within an encoder-decoder framework. Tu et al. <ref type="bibr" target="#b31">[32]</ref> use a KVMN to store the translate history which gives model the opportunity to take advantage of document-level information instead of translate sentences in an isolation way. We will use the KVMN architecture in our model to store and retrieve the product attributes data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>Before introducing our answer generation task for product-aware question, we introduce our notation and key concepts.</p><p>At the beginning, for a product, we assume there is a question</p><formula xml:id="formula_0">X q = {x q 1 , x q 2 , . . . , x q T q }, T r reviews X r = {x r 1 , x r 2 , . . . , x r T r } and T a key-value pairs of attributes A = {(a k 1 , a v 1 ), (a k 2 , a v 2 ), . . . , (a k T a , a v T a )},</formula><p>where a k i is the name of i-th attribute and a v i is the attribute content. In our task, we assume that each attribute, both key a k i and value a v i are represented as a single word. Given a question X q , an answer generator reads the reviews X r and attributes A, then generates an answer? = {? 1 ,? 2 , . . . ,? T y }. The goal is to generate an answer? that is not only grammatically correct but also consistent with product attributes and opinions in the reviews. Essentially, the generator tries to optimize the parameters to maximize the probability</p><formula xml:id="formula_1">P(Y |X q , X r , A) = T y t =1 P(y t |X q , X r , A) where Y = {y 1 , y 2 , . . . , y T y } is the ground truth answer.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PAAG MODEL 4.1 Overview</head><p>In this section, we propose our product-aware answer generator model, abbreviated as PAAG. The overview of PAAG is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. PAAG can be split into two main parts: answer generator and consistency discriminator. We start by detailing the answer generator which generates an answer according to the reviews and attributes. We then describe the consistency discriminator which distinguishes whether the generated answer matches the facts given by reviews and attributes.</p><p>? Answer generator. (1) Review reader: (See Section 4.2) In this part, we encode the review text into vector representations. By matching the relevance of the given question, we signify important semantic units of reviews. (2) Attributes encoder: (See Section 4.3) Our model stores the product attributes information into a keyvalue memory network. For each key-value pair, a correlation score between a key and the question is aggregated into the value. (3) Facts decoder: <ref type="bibr">(See Section 4.4)</ref> To generate the answer, we use the RNN-based decoder which fuses the facts extracted from reviews and attributes when generating words.</p><p>? Consistency discriminator. (See Section 4.5) Existing approaches easily generate a grammatically correct answer but conflicts to the facts. In order to produce factual answer, we use a discriminator to determine whether the generated sentence matches the facts. By employing the earth-mover distance to optimize our network, we use the result of discriminator as a training signal to encourage our model to produce a better answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Review reader</head><p>At the beginning, we use an embedding matrix e to map one-hot representation of each word in the question X q , reviews X r , and attributes A to a high-dimensional vector space. We denote e(x) as the embedding representation of word x. From these embedding representations, we employ a bi-directional recurrent neural network (Bi-RNN) to model the temporal interactions between words:</p><formula xml:id="formula_2">h q t = Bi-RNN q (e(x q t ), h q t ?1 ),<label>(1)</label></formula><formula xml:id="formula_3">h r i,t = Bi-RNN r (e(x r i,t ), h r i,t ?1 ).<label>(2)</label></formula><p>where h q t and h r i,t denote the hidden state of t-th step in Bi-RNN for question X q and i-th review in X r respectively. We denote the final hidden state h q T q of Bi-RNN q as the vector representation of the question X q . Following <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>, we choose the long short-term memory (LSTM) as a cell of Bi-RNN.</p><p>For producing a fixed size vector representation of reviews, an intuitive method is to conduct an average-pooling strategy on all the hidden states of each review, which neglects the question-oriented salient part of the review. Accordingly, we propose a gated attentionbased method to incorporate the focus point of question into the review representation. Furthermore, we add an additional gate which learns the relevance between question X q and review X r via a soft-alignment, so we have:</p><formula xml:id="formula_4">s k i, j = v ? tanh(W q h q k + W r h r i, j ),<label>(3)</label></formula><formula xml:id="formula_5">s i, j = max(s 1 i, j , s 2 i, j , . . . , s T q i, j ),<label>(4)</label></formula><formula xml:id="formula_6">? i, j = exp(s i, j )/ T i r t =1 exp(s i,t ),<label>(5)</label></formula><p>where W q ,W r , v are all trainable parameters. ? i, j ? R refers to the importance score of the j-th word in the i-th review given X q . Thereafter, we apply the attention-pooling operation on each review hidden state h r i, ? to produce the question-aware review representation c r i , shown in Equation <ref type="bibr" target="#b5">6</ref>:</p><formula xml:id="formula_7">c r i = T i r t =1 ? i,t h r i,t ,<label>(6)</label></formula><p>Given an answer generation procedure, not all the reviews are useful to answer the question due to the informal style problem. Thus if we directly calculate the arithmetic mean vector of all reviews' representations, we can not capture salient passages. To tackle this problem, a gated fusion method is utilized here to sum up all the review representations. We first calculate the relevance between each review representation c r i and question representation h q T q via a bi-linear layer shown in Equation <ref type="bibr" target="#b6">7</ref>:</p><formula xml:id="formula_8">u i = c r i W f h q T q ,<label>(7)</label></formula><formula xml:id="formula_9">u ? i = exp(u i )/ T r t =1 exp(u t ),<label>(8)</label></formula><p>where W f is a trainable parameter. Afterwards, we use a softmax function to simulate the relevance score u i , i.e., u ? i , shown in Equation 8. Then we use u ? i as the weight of the i-th review to do weighted-average on review representation c r i over all reviews, so we have: ates the final answer according to the facts learned by the two modules introduced before. (4) Consistency discriminator distinguishes whether the generated answer matches the extracted facts, and we also use the result of discriminator as another training signal.</p><formula xml:id="formula_10">c r = T r i=1 u ? i c r i .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attributes encoder</head><p>The attributes of a product can be seen as structured knowledge data in our task. As key-value memory network (KVMN) is shown effective in structured data utilization <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>, in our work we employ KVMN to store product attributes for generating answers. Correspondingly, we store the word embedding of each attribute's key and value in the KVMN. The read operation in our KVMN is divided into two steps: key matching and value combination. Key matching The goal of key matching is to calculate the relevance between each attribute and the given question. Given question X q , for the i-th attribute a i = (a k i , a v i ) ? A, we calculate the probability of a i over X q , i.e., P(a i |X q ), as the matching score. To this end, we exploit the question representation h q T q to calculate the probability distribution:</p><formula xml:id="formula_11">P(a i |X q ) = exp(h q T q W a e(a k i )) T a t =1 exp(h q T q W a e(a k t )) ,<label>(10)</label></formula><p>Since question representation h q T q and attribute key representation e(a k i ) are not in the same semantic space, following <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>, we use a trainable key matching parameter W a to transform these representations into a same space.</p><p>Value combination As the relevance between question X q and attribute a i , the matching score P(a i |X q ) can help to capture the most relevant attribute for generating a correct answer. Therefore, as shown in <ref type="bibr">Equation 11</ref>, the attribute encoder reads the information m from KVMN via summing over the stored attribute values, and guide the follow-up answer generation, so we have:</p><formula xml:id="formula_12">m = T a i=1 P(a i |X q )e(a v i ).<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Facts decoder</head><p>PAAG generates an answer based on a set of facts extracted from reviews and attributes. Same as our encoder settings, we set LSTM as a cell in our RNN-based facts decoder. We concatenate the question, review and attribute representations and apply a linear transform, then use this vector as the initial state d 0 ; at every decoding step, we feed a context vector ? t into RNN cell. At t-th decoding step, context vector ? t summarizes the input question and review, and we will show the detail of producing ? t at follows. The procedure of t-th decoding step is shown in <ref type="figure" target="#fig_0">Equation 13</ref>. We use the notion [?; ?] as the concatenation of two vectors.</p><formula xml:id="formula_13">d 0 = W e m; h q T q ; c r + b e ,<label>(12)</label></formula><formula xml:id="formula_14">d t = LSTM (d t ?1 , [? t ?1 ; e(y t ?1 )]) ,<label>(13)</label></formula><p>where W e , b e are the trainable parameters, d t is the hidden state of t-th decoding step. Similar with the seq2seq with attention mechanism, we use the hidden state of previous step d t ?1 to attend the question hidden states and review hidden states to get the context vector ? t of current decoding step. The algorithm of attending reviews hidden states is same as attending question hidden states, so we use h * i to represent the hidden state where * can be r or q.</p><formula xml:id="formula_15">? ? i,t = z ? tanh W s h * i + W d d t ,<label>(14)</label></formula><formula xml:id="formula_16">? i,t = exp ? ? i,t / T q j=1 exp ? ? j,t ,<label>(15)</label></formula><formula xml:id="formula_17">? * t = T i=1 ? i,t h * i ,<label>(16)</label></formula><p>where W s ,W d are all trainable parameters. After two attention procedures of question and review finished, we concatenate context vector ? r t and ? q t with a balanced gate ? which is determined by decoder state d t :</p><formula xml:id="formula_18">? = ? W ? d t + b ? , ? t = ?? r t ; (1 ? ? ) ? q t ,<label>(17)</label></formula><p>The context vector ? t , which can be seen as a representation of reading from the question and reviews, is concatenated with the decoder state d t and then fed into a linear transformation layer to conduct the generated word distribution P v over the vocabulary.</p><formula xml:id="formula_19">d o t = (W o [d t ; ? t ] + b o ) ,<label>(18)</label></formula><formula xml:id="formula_20">P v = softmax W v d o t + b v ,<label>(19)</label></formula><p>At the t-th decoding step, we set the loss as the negative log likelihood of the target word y t :</p><formula xml:id="formula_21">loss ? = ?1/T y T y t =1 log P v (y t ).<label>(20)</label></formula><p>In order to handle the out-of-vocabulary (OOV) problem, we equip the pointer network <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref> with our decoder, which makes our decoder capable to copy words from question. The procedure of pointer network is the same as the model proposed by See et al. <ref type="bibr" target="#b21">[22]</ref>, and we omit this procedure in our paper due to the limited space.</p><p>Up to now, we can use loss ? to compute gradients for all the parameters in answer generator and use gradient descent method to update these parameters. But the correctness constraint given by cross entropy loss loss ? is not enough. So we need a classifier to judge whether the generated answer is consistent with the facts. In this way, we use this classification result to guide the answer generator to produce more consistent answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Consistency discriminator</head><p>To generate sentences which are more consistent with the facts, we add a discriminator to provide additional training signals for the answer generator. We propose a convolutional neural network (CNN) based classifier as discriminator. The goal of this classifier is to distinguish whether a sentence is consistent with the given facts. So we can use the confidence of classifying a sentence as a training signal to encourage the answer generator to produce a better answer. We use the answer generated by the facts decoder presented in Section 4.4 as the negative sample for classification, and use the representation of ground truth answer as the positive sample.</p><p>As for giving a positive sample for discriminator, we use an RNN to encode the ground truth answer into a vector representation.</p><formula xml:id="formula_22">d ? t = LST M(y t ,d ? t )<label>(21)</label></formula><p>Since the ground truth is encoded by another RNN which is different from the decoder RNN of d o t , we use a linear transformation to transform the high-dimensional space ofd ? t to the same space as d o t in.</p><formula xml:id="formula_23">d ? t = W z * d ? t + b z<label>(22)</label></formula><p>where W z and b z are all trainable parameters. For training the discriminator ability of capturing whether an answer is consistent with the facts, we construct another negative samples to train the discriminator. We present an answer decoder (shown at the bottom of <ref type="figure" target="#fig_0">Figure 1)</ref>, which employs the same decoding mechanism as the facts decoder but no fact is attached during decoding. Specifically, we use the hidden state d o t shown in Equation 18 as the representation of each word in the generated sentence by the facts decoder. Similarly, the decoder RNN without feeding facts representation ? t generates hidden states d f t . We use d f t as the representation of generated answer without facts support.</p><p>Then a two-dimensional convolutional layer convolves the hidden states d * t with multiple convolutional kernels of different widths. Each kernel corresponds a linguistic feature detector which extracts a specific pattern of multi-grained n-grams <ref type="bibr" target="#b11">[12]</ref>. A convolutional filter W c maps hidden states in the receptive field to a single feature. As we slide the filter across the whole sentence, we obtain a sequence of new features n = [n 1 , n 2 , . . . , n l ], shown in Equation <ref type="bibr" target="#b22">23</ref>:</p><formula xml:id="formula_24">n * t = relu d * t ? W c + b c ,<label>(23)</label></formula><p>where W c , b c are all trainable parameters and ? denotes the convolution operation. For each convolutional filter, the max-pooling layer takes the maximal value among the generated convolutional features n f , n o and n ? respectively, resulting in a fixed-size vector N f , N o and N ? . Then we obtain the classification result D(d * t ) ? R through an interaction between N * and the facts, i.e., attribute representation m and review representation c r . So we have:</p><formula xml:id="formula_25">D(d * t ) = W h relu N * + m + c r + b h ,<label>(24)</label></formula><p>whereW h , b h are all trainable parameters. Here we apply the Vanilla generative adversarial network (GAN) with a sigmoid function on the D(d * t ) to produce the classification probability and tries to minimize the Jensen-Shannon divergence between real and generated data distribution.</p><p>However, as vanilla GAN often leads to gradient vanishing as the discriminator saturates <ref type="bibr" target="#b7">[8]</ref>, which makes the discriminator can not give the correct training signal. Inspired by previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>, we tackle this problem by minimize the earth-mover (also called Wasserstein-1) distance W (P r , P ? ) instead of Jensen-Shannon divergence. Informally, given a distribution P r of ground truth answer and a distribution P ? of facts-based answer or answer without facts. Then we minimize the cost of transporting mass from P r to P ? . The discriminator D ? D is a 1-Lipschitz function, where D is the set of 1-Lipschitz functions <ref type="bibr" target="#b7">[8]</ref>.</p><p>In order to meet the Lipschitz constraint of discriminator D, we use an alternative way to enforce the Lipschitz constraint. We add a gradient norm of the output of D with respect to its input, which is simply sampled uniformly along a straight line between points sampled from the ground truth representation d</p><formula xml:id="formula_26">d ? t = ?d o t + (1 ? ?)d ? t ,<label>(25)</label></formula><formula xml:id="formula_27">loss d = 1 T y T y t D(d f t ) + D(d o t ) ? D(d ? t ) + ? ? d ? t D(d ? t ) 2 ? 1 2 .<label>(26)</label></formula><p>where ? ? U [0, 1] is a random number and ? is a coefficient of gradient penalty term. Then we can use the optimization methods to update the parameters of discriminator use the loss function loss d . Meanwhile, we add the ?D(d o t ) to the previous defined loss ? in <ref type="bibr">Equation 20</ref> to encourage the answer generator produce better result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL SETUP 5.1 Research questions</head><p>We list four research questions that guide the remainder of the paper: RQ1: What is the overall performance of PAAG? Does it outperform state-of-the-art baselines? RQ2: What is the effect of each module in PAAG? Does the discriminator give a useful training signal to the answer generation module? RQ3: Is PAAG capable to extract useful information from noisy reviews? RQ4: What is the performance of PAAG at different data domain?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset</head><p>We collect a large-scale dataset from a real-world e-commerce website, including question-answering pairs, reviews, and product attributes. This dataset is available at https://github.com/gsh199449/ productqa. On this website, users can post a question about the product. Most questions are asking for an experience of user who has already bought the product. In the collected data, each QA pair is associated with the reviews and attributes of the corresponding product. We remove all QA pairs without any relevant review and split the whole dataset into training and testing set. In total, our dataset contains cover 469,953 products and 38 product categories. The average length of question is 9.03 words and ground truth answer is 10.3 words. The average number of attribute is 9.0 keyvalue pairs. There are 78.74% of training samples have more than 10 relevant reviews and 75.33% of training samples have more than 5 attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation metrics</head><p>To evaluate our proposed method, we employ BLEU <ref type="bibr" target="#b18">[19]</ref> to measure the quality of generated sentence by computing overlapping lexical units (e.g., unigram, bigram) with the reference sentence. We also consider three embedding-based metrics <ref type="bibr" target="#b5">[6]</ref> (including Embedding Average, Embedding Greedy and Embedding Extreme) to evaluate our model, following several recent studies on text generation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>. These three metrics compute the semantic similarity between the generated and reference answer according to the word embedding.</p><p>Since automatic evaluation metrics may not always consistent with human perception <ref type="bibr" target="#b25">[26]</ref>, we use human evaluation in our experiment. Three annotators are invited to judge the quality of 100 randomly sampled answer generated by different models. These annotators are all well-educated Ph.D. students and they are all native speakers. Two of them have the background of NLP/summarization and another annotator does not major in computer science. We show human annotators a question, several reviews and attributes of the product along with answers generated from each model.</p><p>Statistical significance of observed differences between the performance of two runs are tested using a two-tailed paired t-test and is denoted using ? (or ? ) for strong significance for ? = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparisons</head><p>In order to prove the effectiveness of each module in PAAG, we conduct some ablation models shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>To evaluate the performance of our dataset and the proposed framework, we compare our model with the following baselines: (1) S2SA: Sequence-to-sequence framework <ref type="bibr" target="#b27">[28]</ref> has been proposed for language generation task. We use seq2seq framework which is equipped with attention mechanism <ref type="bibr" target="#b2">[3]</ref> and copy mechanism <ref type="bibr" target="#b6">[7]</ref> as baseline method. The input sequence is question and ground truth output sequence is the answer. (2) S2SAR: We implement a simple method which can incorporate the review information when generating the answer. Different from the S2SA, we use an RNN to read all the reviews and concatenate the final state of this RNN with encoder final state as the initial state of decoder RNN. (3) SNet: S-Net <ref type="bibr" target="#b28">[29]</ref> is a two-stage state-of-the-art model which extracts some text spans from multiple documents context and synthesis the answer from those spans. Due to the difference between our dataset and MS-MARCO <ref type="bibr" target="#b17">[18]</ref>, our dataset does not have text span label ground truth for training the evidence extraction module. So we use the predicted extraction probability to do weighted sum the original review word embeddings, and use this representation as extracted evidence to feed into the answer generation module. (4) QS: We implement the query-based summarization model proposed by Hasselqvist et al. <ref type="bibr" target="#b8">[9]</ref>. Accordingly, we use product reviews as original passage and answer as a summary. (5) BM25: BM25 is a bag-of-words retrieval function that ranks a set of reviews based on the question terms appearing in each review. We use the top review of ranking list as the answer. (6) TF-IDF: Term Frequency-Inverse Document Frequency is a numerical statistic that is intended to reflect how important a question word is to a review. We use this statistic to model the relevance between review and question and select the most similar review as the answer of question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Implementation details</head><p>Without using pre-trained embeddings, we randomly initialize the network parameters at the beginning of our experiments. All the RNN networks have 512 hidden units and the dimension of word embedding is 256. To produce better answers, we use beam search with beam size 4. Adagrad <ref type="bibr" target="#b4">[5]</ref> with learning rate 0.1 is used to optimize the parameters and batch size is 64. We implement our model using TensorFlow <ref type="bibr" target="#b0">[1]</ref> framework and train our model and all baseline models on NVIDIA Tesla P40 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULT 6.1 Overall performance</head><p>For research question RQ1, to demonstrate the effectiveness of PAAG, we examine the overall performance in term of BLEU, embedding metrics and human evaluation. <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref> list performances of all comparisons in terms of two automatic evaluation metrics. Significant differences are with respect to SNet (row with shaded background). In these experimental results, we see that PAAG achieves a 111%, 8% and 62.73% increment over the stateof-the-art baseline SNet in terms of BLEU, embedding greedy and consistency score, respectively. In <ref type="table" target="#tab_2">Table 3</ref>, we see that our PAAG outperforms all the baseline significantly in semantic distance with respect to the ground truth.</p><p>For human evaluation, we ask annotators to rate each generated answer according to two aspects: consistency and fluency. The rating score ranges from 1 to 3, and 3 is the best. We finally take the average across answers and annotators, as shown in <ref type="table" target="#tab_3">Table 4</ref>. In <ref type="table" target="#tab_3">Table 4</ref>, we can see that PAAG outperforms other baseline models in both sentence fluency and consistency with the facts. We calculate the variance score in <ref type="table" target="#tab_3">Table 4</ref>, which shows that annotators agree with each other's judgments in most cases. Although the BLEU score of S2SAR is lower than the S2SA, the embedding score and human score for S2SAR are higher than S2SA. Regardless of few word overlapping between generated and ground answer, the human evaluation and results in terms of embedding metrics verify S2SAR outperforms S2SA. This observation demonstrates the effectiveness of incorporating review in answer generation.</p><p>To explore the difficulty of this task, we use a very intuitive method by adding the review information into decoder shown in S2SAR. Although there is a small increment of S2SAR with respect to S2SA in all metrics, we still find a noticeable gap between S2SAR and PAAG. This observation demonstrates that PAAG makes better use of review and attribute information than the simple method S2SAR. In view of the facts extracted from the review and attributes, we examine directly using the most similar review to question as the answer. More specifically, we evaluate the performance of the top of review ranking list which is ranked by text similarity algorithm such as BM25 and TF-IDF. From the result of three metrics, the performance of extractive methods is worth than all the generative methods. It is worth noting that since the answer generated by extractive methods is written by human, it have very high fluency scores. But these answers may not match the question, so the consistency score is very low. Consequently, using the most similar review to question as answer is not a better method than generating answers from scratch.</p><p>As our task definition and query based text summarization have some similarities in some way, we can see the reviews as original passage and answer as a query based summary. We also use the query-based text summarization algorithm <ref type="bibr" target="#b8">[9]</ref> to generate answer. Similarly, we also employ a reading comprehension method SNet to tackle this task. Since query-based text summarization and reading comprehension models are not defined to tackle QA task in    e-commerce scenario, it can not fully utilize the interactions between question, review, and attributes. These methods also lack of ability of denoising the reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation studies</head><p>Next we turn to research question RQ2. We conduct ablation tests on the usage of adversarial learning method. The BLEU score of each ablation model is shown in <ref type="table" target="#tab_4">Table 5</ref>. In the method RAGFD, we use the vanilla GAN architecture which minimize the divergence. There is a slight increment from RAGF to RAGFD, which demonstrates the effectiveness of discriminator. From <ref type="table" target="#tab_4">Table 5</ref>, we find that RAGFWD achieves a 4.3% improvement over RAGFD in terms of BLEU, and PAAG outperforms RAGFWD 4.1% in terms of BLEU. Accordingly, we conclude that the performance of PAAG benefits from using Wasserstein distance based adversarial learning with gradient penalty. This approach can help our model to achieve a better performance than the model using the vanilla GAN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Denoising ability</head><p>To address RQ3, in this section we provide an analysis of the denoising ability of our model. According to <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_4">Table 5</ref>, we observe RAGF achieves 2.1% improvement over SASAR, in terms of BLEU. Such observation demonstrates that question-aware review generation module gives the denoising ability to the model. To further investigate the effectiveness of extracting facts from reviews, we visualize two question-review attention maps, shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Question of the left figure in <ref type="figure" target="#fig_1">Figure 2</ref> is "Will the color fade when cleaning?" and the right is "Is it convenient to clean". The review of the left figure is "Good shopping experience. The pants were washed without discoloration and no color difference compared to the picture. It looks good, comfortable and cheap. " and the right is "The color looks good and the texture is great. I haven't started it yet, but it's very easy to clean". In this figure, we can see that there is a very strong interaction between question word ?? (cleaning) and phrase in review ?????? (very easy to clean). Concretely, these figures show that the question-review attention module can capture the salience semantic part in review according to the question.</p><p>In the most cases, the higher word overlap between question and review, the more useful the review is. To prove the ability of review gated fusion module shown in Equation 8, we use the BM25 algorithm to calculate the similarity between question and each review. Then we calculate the cosine distance between the salience score produced by review gated fusion module calculated and BM25 similarity score, shown in <ref type="figure" target="#fig_2">Figure 3</ref>. In order to demonstrate the denoising ability of adversarial learning method, we compare our full model PAAG with the baseline model RAGF, this experiment proves that the usage of WGAN can encourage our model to capture the salience review better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Discussions</head><p>Finally, we address RQ4. <ref type="table" target="#tab_5">Table 6</ref> shows an example and its corresponding generated answers by different methods. We observe   </p><formula xml:id="formula_28">? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ??(</formula><p>The clothes are beautiful in color, comfortable to wear, and the thread is a bit more. This dress has a pocket and it is convenient and practical!) ?????????????????????????????? ????????????????(This chiffon dress feels heavy when worn. I think summer and spring are suitable for wearing. Putting on this dress will bring out my skin white. But this dress will be transparent on the chest, but it will not affect the wear.) attributes ??: ??||??:  that S2SA only generates the answer which is fluent, but generated answers are contradictory to the facts. Due to there is no fact consistency constraint in RAGF, it will also face this problem when generating answers However, PAAG overcomes this shortcoming by using consistency constraint given by discriminator at training, and then produce the answer which is not only fluent but also consistent with the facts. We evaluate performances of PAAG on different categories. Shown in <ref type="table" target="#tab_7">Table 7</ref>, we see that our proposed model beats the other two baselines (S2SA and S2SAR), on majority of product categories in terms of BLEU score. To prove the significance of the above results, we also do the paired student t-test between our model and baseline methods, the p-value of S2SA is 0.0086 and S2SAR is 0.0100. From the t-test, we can see that the performance of our model is significantly higher than other baselines.</p><p>To investigate the robustness of parameter, we train our model in different parameter size and evaluate them by embedding metric shown in <ref type="figure" target="#fig_3">Figure 4</ref>. As the training progresses, the performance of each model is rising. However, the model with a large number of parameters does not have a great advantage in the final performance of the model with a smaller parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we have proposed the task of product-aware answer generation, which aims to generate an answer for a product-aware question from product reviews and attributes. To address this task, we have proposed product-aware answer generator (PAAG): An attention-based question aware review reader is used to extract semantic units from reviews, and key-value memory network based attribute encoder is employed to fuse relevant attributes. In order to encourage the model to produce answers that match facts, we have employed an adversarial learning mechanism to give additional training signals for the answer generation. To tackle the shortcomings of vanilla GAN, we have applied the Wasserstein distance as value function in the training of consistency discriminator. In our experiments, we have demonstrated the effectiveness of PAAG and have found significant improvements over state-of-the-art baselines in terms of metric-based evaluations and human evaluations. Moreover, we have verified the effectiveness of each module in PAAG for improving product-aware answer generation.</p><p>Future work involves extending our model to multiple hop of memory network used as attribute encoder.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of PAAG. We divide PAAG into four parts: (1) Review reader reads the review to extract relevant semantic parts. (2) Attribute encoder encodes the attribute key-value pairs using key-value memory network. (3) Facts decoder gener-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualizations of question-aware review attention map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Similarity between review gates and BM25 score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Greedy embedding metric with training epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation models for comparison.</figDesc><table><row><cell cols="2">Acronym Gloss</cell></row><row><cell>RAGF</cell><cell>Review reader + Attributes encoder + Gated Fusion</cell></row><row><cell>RAGFD</cell><cell>RAGF + consistency Discriminator</cell></row><row><cell cols="2">RAGFWD RAGF + Wasserstein consistency Discriminator</cell></row><row><cell>PAAG</cell><cell>RAGFWD + Gradient Penalty</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>BLEU scores comparison between baselines.</figDesc><table><row><cell></cell><cell>BLEU</cell><cell>BLEU1</cell><cell>BLEU2</cell><cell>BLEU3</cell><cell>BLEU4</cell></row><row><cell cols="3">Text generation methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>S2SA</cell><cell>1.6186</cell><cell>15.4754</cell><cell>3.1437</cell><cell>0.8267</cell><cell>0.1706</cell></row><row><cell>S2SAR</cell><cell>1.7549</cell><cell>15.1708</cell><cell>3.2156</cell><cell>0.9078</cell><cell>0.2142</cell></row><row><cell>SNet</cell><cell>0.9550</cell><cell>13.7029</cell><cell>2.5374</cell><cell>0.4007</cell><cell>0.0597</cell></row><row><cell>QS</cell><cell>1.6848</cell><cell>15.4961</cell><cell>2.9508</cell><cell>0.8315</cell><cell>0.2119</cell></row><row><cell>PAAG</cell><cell cols="4">2.0189 ? 16.2232 ? 3.5711 ? 1.0290 ?</cell><cell>0.2787 ?</cell></row><row><cell cols="3">Sentence extraction methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell>0.4125</cell><cell>6.9630</cell><cell>0.7097</cell><cell>0.1333</cell><cell>0.0439</cell></row><row><cell>TF-IDF</cell><cell>0.2548</cell><cell>5.5480</cell><cell>0.5127</cell><cell>0.0779</cell><cell>0.0190</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Embedding scores comparison between baselines.</figDesc><table><row><cell></cell><cell>Average</cell><cell>Greedy</cell><cell>Extrema</cell></row><row><cell cols="2">Text generation methods</cell><cell></cell><cell></cell></row><row><cell>S2SA</cell><cell>0.410013</cell><cell>98.653415</cell><cell>0.269461</cell></row><row><cell>S2SAR</cell><cell>0.419979</cell><cell>99.742679</cell><cell>0.278666</cell></row><row><cell>SNet</cell><cell>0.397162</cell><cell>95.791356</cell><cell>0.277781</cell></row><row><cell>QS</cell><cell>0.400291</cell><cell>93.255031</cell><cell>0.252164</cell></row><row><cell>PAAG</cell><cell cols="3">0.424218 ? 103.912364 ? 0.288321 ?</cell></row><row><cell cols="3">Sentence extraction methods</cell><cell></cell></row><row><cell>BM25</cell><cell>0.325946</cell><cell>76.814465</cell><cell>0.172976</cell></row><row><cell>TF-IDF</cell><cell>0.308293</cell><cell>85.020442</cell><cell>0.155390</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Consistency and fluency comparison by human evaluation.</figDesc><table><row><cell></cell><cell cols="2">Fluency</cell><cell cols="2">Consistency</cell></row><row><cell></cell><cell>mean</cell><cell>variance</cell><cell>mean</cell><cell>variance</cell></row><row><cell cols="3">Text generation methods</cell><cell></cell><cell></cell></row><row><cell>S2SA</cell><cell>2.22</cell><cell>0.3</cell><cell>1.62</cell><cell>0.29</cell></row><row><cell>S2SAR</cell><cell>2.405</cell><cell>0.365</cell><cell>1.82</cell><cell>0.39</cell></row><row><cell>SNet</cell><cell>1.93</cell><cell>0.36</cell><cell>1.355</cell><cell>0.225</cell></row><row><cell>QS</cell><cell>2.335</cell><cell>0.285</cell><cell>1.725</cell><cell>0.355</cell></row><row><cell>PAAG</cell><cell>2.865 ?</cell><cell>0.105</cell><cell>2.205 ?</cell><cell>0.445</cell></row><row><cell cols="3">Sentence extraction methods</cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell>2.70</cell><cell>0.24</cell><cell>1.45</cell><cell>0.29</cell></row><row><cell>TF-IDF</cell><cell>2.48</cell><cell>0.38</cell><cell>1.14</cell><cell>0.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>BLEU scores of different ablation models.</figDesc><table><row><cell></cell><cell>BLEU</cell><cell>BLEU1</cell><cell cols="3">BLEU2 BLEU3 BLEU4</cell></row><row><cell>RAGF</cell><cell>1.7931</cell><cell>15.7213</cell><cell>3.3705</cell><cell>0.9385</cell><cell>0.2079</cell></row><row><cell>RAGFD</cell><cell>1.8597</cell><cell>15.9021</cell><cell>3.4160</cell><cell>0.9409</cell><cell>0.2340</cell></row><row><cell>RAGFWD</cell><cell>1.9389</cell><cell>16.1755</cell><cell>3.5986</cell><cell>0.9865</cell><cell>0.2461</cell></row><row><cell>PAAG</cell><cell cols="2">2.0189 16.2232</cell><cell>3.5711</cell><cell cols="2">1.0290 0.2787</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Examples of the generated natural answers by PAAG and other models. The quality of the clothes is very good. Because I am thin, the S size is still quite fat for me. It is suitable for pregnant women to wear.)</figDesc><table><row><cell>????????????????S ??????????????</cell></row><row><cell>?????(</cell></row><row><cell>reviews</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>A ??||??: ??||??: ???||??: ??||??: ? ?||????: 2018 ??||??: A ? question ?????????(I have been pregnant for more than five months, can I wear it?) reference ?????????(You can wear it until your child is born) S2SA ?????????????(I can wear it, my son wore it when he was three months old.) RAGF ???????????(I can wear it, my pregnancy is five months.) PAAG ???????(I can wear it, I am pregnant.)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison of BLEU scores between different product categories.</figDesc><table><row><cell></cell><cell cols="2">PAAG</cell><cell cols="2">S2SA</cell><cell cols="2">S2SAR</cell></row><row><cell></cell><cell>BLEU1</cell><cell>BLEU2</cell><cell>BLEU1</cell><cell>BLEU2</cell><cell>BLEU1</cell><cell>BLEU2</cell></row><row><cell>Jewelry</cell><cell>19.53</cell><cell>6.35</cell><cell>17.65</cell><cell>4.26</cell><cell>18.74</cell><cell>4.69</cell></row><row><cell>Mattress</cell><cell>18.89</cell><cell>4.14</cell><cell>16.35</cell><cell>3.00</cell><cell>17.52</cell><cell>5.57</cell></row><row><cell>Clothing</cell><cell>18.18</cell><cell>5.17</cell><cell>18.39</cell><cell>4.98</cell><cell>18.36</cell><cell>4.68</cell></row><row><cell>Kitchenware</cell><cell>18.00</cell><cell>4.31</cell><cell>15.23</cell><cell>3.19</cell><cell>17.15</cell><cell>4.09</cell></row><row><cell>Power and Handtools</cell><cell>16.34</cell><cell>3.98</cell><cell>13.73</cell><cell>3.20</cell><cell>15.60</cell><cell>3.22</cell></row><row><cell>Skin Care</cell><cell>18.01</cell><cell>4.57</cell><cell>15.39</cell><cell>3.55</cell><cell>18.33</cell><cell>4.40</cell></row><row><cell>Gardening</cell><cell>13.67</cell><cell>2.30</cell><cell>11.86</cell><cell>1.52</cell><cell>15.74</cell><cell>2.30</cell></row><row><cell>Baby</cell><cell>18.22</cell><cell>4.51</cell><cell>16.95</cell><cell>3.71</cell><cell>17.27</cell><cell>3.75</cell></row><row><cell>Automotive Accessories</cell><cell>17.46</cell><cell>3.43</cell><cell>15.49</cell><cell>3.14</cell><cell>17.86</cell><cell>3.00</cell></row><row><cell>Gift</cell><cell>19.25</cell><cell>3.93</cell><cell>17.23</cell><cell>3.06</cell><cell>18.39</cell><cell>4.24</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? t and the facts based output d o t . Then our objective function is shown in Equation 26.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their constructive comments. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno>abs/1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SuperAgent: A Customer Service Chatbot for E-commerce Websites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqun</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bootstrapping dialog systems with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Forgues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marie</forename><surname>Larchev?que</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?al</forename><surname>Tremblay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nips, modern machine learning and natural language processing workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Query-Based Abstractive Summarization Using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hasselqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Helmertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>K?geb?ck</surname></persName>
		</author>
		<idno>abs/1712.06100</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating Natural Answers by Incorporating Copying and Retrieving Mechanisms in Sequence-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwang</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Chen Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Convolutional Neural Network for Modelling Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">International World Wide Web Conferences Steering Committee, Republic and Canton of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872427.2883044</idno>
		<ptr target="https://doi.org/10.1145/2872427.2883044" />
	</analytic>
	<monogr>
		<title level="m">WWW (WWW &apos;16)</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="625" to="635" />
		</imprint>
	</monogr>
	<note>Addressing Complex and Subjective Product-Related Queries with Customer Reviews</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Key-Value Memory Networks for Directly Reading Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Key-Value Memory Networks for Directly Reading Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshee</forename><surname>Mitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06238</idno>
		<title level="m">An Abstractive approach to Question Answering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">AQA: Aspect-based Opinion Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SQuAD: 100, 000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bi-directional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hananneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Summarizing Answers in Non-Factoid Community Question-Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongya</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangsong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="DOI">10.1145/3018661.3018704</idno>
		<ptr target="https://doi.org/10.1145/3018661.3018704" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM &apos;17)</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining (WSDM &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="405" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Evaluating Evaluation Methods for Generation in the Presence of Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Marge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Singhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>In CICLing</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-To-End Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Get The Point of My Utterance! Learning Towards Effective Responses with Multi-Head Attention Mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyue</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4418" to="4424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to Remember Translation History with a Continuous Cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural Response Generation via GAN with an Approximate Embedding Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards Implicit Content-Introducing for Generative Short-Text Conversation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast and Accurate Reading Comprehension by Combining Self-Attention and Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangyong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Answering opinion questions on products by exploiting hierarchical organization of consumer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="391" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aware Answer Prediction for Product-Related Questions Incorporating Aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM. ACM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="691" to="699" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
