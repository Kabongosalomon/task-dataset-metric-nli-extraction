<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjin</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatio-temporal convolution often fails to learn motion dynamics in videos and thus an effective motion representation is required for video understanding in the wild. In this paper, we propose a rich and robust motion representation based on spatio-temporal self-similarity (STSS). Given a sequence of frames, STSS represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, it enables the learner to better recognize structural patterns in space and time. We leverage the whole volume of STSS and let our model learn to extract an effective motion representation from it. The proposed neural block, dubbed SELFY, can be easily inserted into neural architectures and trained end-to-end without additional supervision. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition. Our experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolution. On the standard action recognition benchmarks, Something-Something-V1 &amp; V2, Diving-48, and FineGym, the proposed method achieves the state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning spatio-temporal dynamics is the key to video understanding. While extending standard convolution in space and time has been actively investigated for the purpose in recent years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref>, the empirical results so far indicate that spatio-temporal convolution alone is not sufficient for grasping the whole picture; it often learns irrelevant context bias rather than motion information <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> and thus the additional use of optical flow turns out to boost the performance in most cases <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">32]</ref>. Motivated by this, re-* Equal contribution. <ref type="figure">Figure 1</ref>: Spatio-temporal self-similarity (STSS) representation learning. STSS describes each position (query) by its similarities (STSS tensor) with its neighbors in space and time (neighborhood). It allows to take a generalized, far-sighted view on motion, i.e., both short-term and longterm, both forward and backward, as well as spatial selfmotion. Our method learns to extract a rich motion representation from STSS without additional supervision. cent action recognition methods learn to extract explicit motion, i.e., flow or correspondence, between feature maps of adjacent frames to improve the performance <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>. But, is it essential to extract such an explicit form of flows or correspondences? How can we learn a richer and more robust form of motion information for videos in the wild?</p><p>In this paper, we propose to learn spatio-temporal selfsimilarity (STSS) representation for video understanding. Self-similarity is a relational descriptor for an image that effectively captures intra-structures by representing each local region as similarities to its spatial neighbors <ref type="bibr" target="#b42">[43]</ref>. As illustrated in <ref type="figure">Fig. 1</ref>, given a sequence of frames, i.e., a video, it extends along time and thus represents each local region as similarities to its neighbors in space and time. By converting appearance features into relational values, STSS enables a learner to better recognize structural patterns in space and time. For neighbors at the same frame it computes a spatial self-similarity map, while for neighbors at a different frame it extracts a motion likelihood map. Note that if we fix our attention to the similarity map to the very next frame within STSS and attempt to extract a single displacement vector to the most likely position at the frame, the problem reduces to optical flow, which is a limited type of motion information. In contrast, we leverage the whole volume of STSS and let our model learn to extract a generalized motion representation from it in an end-to-end manner. With a sufficient volume of the neighborhood in space and time, it effectively captures long-term interaction and fast motion in the video, leading to robust action recognition.</p><p>We introduce a neural block for STSS representation, dubbed SELFY, that can be easily inserted into neural architectures and learned end-to-end without additional supervision. Our experimental analysis demonstrates its superiority over previous methods for motion modeling as well as its complementarity to spatio-temporal features from direct convolutions. On the standard benchmarks for action recognition, Something-Something V1&amp;V2 <ref type="bibr" target="#b12">[13]</ref>, Diving-48 <ref type="bibr" target="#b30">[31]</ref>, and FineGym <ref type="bibr" target="#b41">[42]</ref>, the proposed method achieves the stateof-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video action recognition. Video action recognition aims to categorize videos into pre-defined action classes and one of the main issues in action recognition is to capture temporal dynamics in videos. For modern neural networks, previous methods attempt to learn temporal dynamics in different ways: two-stream networks with external optical flows <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b54">55]</ref>, recurrent networks <ref type="bibr" target="#b2">[3]</ref>, temporal pooling methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>, and 3D CNNs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b49">50]</ref>. Recent methods have introduced the advanced 3D CNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> and showed the effectiveness of capturing spatio-temporal features, so that 3D CNNs now become a de facto approach to learn temporal dynamics in the video. However, spatiotemporal convolution is vulnerable unless relevant features are well aligned across frames within the fixed-sized kernel. To address this issue, a few methods adaptively translate the kernel offsets with deformable convolutions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">62]</ref>, while several methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29]</ref> modulate the other hyperparameters, e.g., higher frame rate or larger spatial receptive fields. Unlike these methods, we address the problem of the spatio-temporal convolution by a sufficient volume of STSS, capturing far-sighted spatio-temporal relations. Learning motion features. Since using the external optical flow benefits 3D CNNs to improve the action recognition accuracy <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b64">64]</ref>, several methods propose to learn frame-by-frame motion features from RGB sequences inside neural architectures. Some methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b38">39]</ref> internalize TV-L1 <ref type="bibr" target="#b60">[60]</ref> optical flows into the CNN. Frame-wise feature differences <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b47">48]</ref> are also utilized as the motion features. Recent correlation-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b53">54]</ref> adopt a correlation operator <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b59">59]</ref> to learn motion features between adjacent frames. However, these methods compute frame-by-frame motion features between two adjacent frames and then rely on stacked spatio-temporal convolutions for capturing long-range motion dynamics. In contrast, we propose to learn STSS features, as generalized motion features, that enable to capture both short-term and long-term interactions in the video. Self-similarity. Self-similarity describes a relational structure of individual image features by computing similarities between them <ref type="bibr" target="#b42">[43]</ref>. Several methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49]</ref> use the self-similarity as a shallow relational descriptor, which is robust to photometric variations, in fields of template matching <ref type="bibr" target="#b42">[43]</ref>, capturing view-invariant geometric patterns <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, or finding semantic correspondences <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49]</ref>. In video understanding, there are a few approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b56">56]</ref> that use the self-similarity of a video as a form of STSS. These methods, however, use STSS for a subsequent feature aggregation step rather than learn representation from it; non-local operation <ref type="bibr" target="#b56">[56]</ref> uses STSS as attention weights in aggregating features <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b52">53]</ref> and CPNet <ref type="bibr" target="#b32">[33]</ref> uses STSS in selecting pairs of appearance features. All these methods lose rich motion information of STSS during aggregation, being not suitable for capturing motion content of videos. In contrast, we advocate using STSS directly for motion representation learning. Our method leverages the full STSS as generalized motion information and learns an effective representation for action recognition within a video-processing architecture. To the best of our knowledge, our work is the first in learning STSS representation using modern neural networks.</p><p>The contribution of our paper can be summarized as follows. First, we revisit the notion of self-similarity and propose to learn a generalized, far-sighted motion representation from STSS. Second, we implement STSS representation learning as a neural block, dubbed SELFY, that can be integrated into existing neural architectures. Third, we provide comprehensive evaluations on SELFY, achieving the state-of-the-art on benchmarks: Something-Something V1&amp;V2 <ref type="bibr" target="#b12">[13]</ref>, Diving-48 <ref type="bibr" target="#b30">[31]</ref>, and FineGym <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our approach</head><p>In this section, we first revisit the notions of selfsimilarity and discuss its relation to motion. We then introduce our method for learning effective spatio-temporal selfsimilarity representation, which can be easily integrated into video-processing architectures and learned end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Overview of our self-similarity representation block (SELFY). SELFY block takes as input a video feature tensor V, transforms it to a STSS tensor S, and extracts a feature tensor F from S. It then produces the final STSS representation Z via the feature integration, which is the same size as the input V. The resultant representation Z is fused into the input feature V by element-wise addition, thus making SELFY act as a residual block. See text for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Self-Similarity Transformation</head><p>Self-similarity is a relational descriptor that suppresses variations in appearance and reveals structural patterns <ref type="bibr" target="#b42">[43]</ref>.</p><p>Given an image feature map I ? R X?Y ?C , selfsimilarity transformation of I results in a 4D tensor S ? R X?Y ?U ?V , whose elements are defined as</p><formula xml:id="formula_0">S x,y,u,v = sim(I x,y , I x+u,y+v ),</formula><p>where sim(?, ?) is a similarity function, e.g., cosine similarity. Here, (x, y) is a query coordinate while (u, v) is a spatial offset from it. To impose a locality, the offset is restricted to its neighborhood:</p><formula xml:id="formula_1">(u, v) ? [?d U , d U ] ? [?d V , d V ]</formula><p>, so that U = 2d U + 1 and V = 2d V + 1, respectively. By converting C-dimensional appearance feature I x,y into U V -dimensional relational feature S x,y , it suppresses variations in appearance and reveals spatial structures in the image. Note that the self-similarity transformation closely relates to conventional cross-similarity (or correlation) across two different feature maps (I, I ? R X?Y ?C ), which can be defined as S x,y,u,v = sim(I x,y , I x+u,y+v ).</p><p>Given a moving object of two images, the cross-similarity transformation effectively captures motion information and thus is commonly used in optical flow and correspondence estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b59">59]</ref>.</p><p>For a sequence of frames, i.e., a video, one can naturally extend the spatial self-similarity along the temporal axis. Let V ? R T ?X?Y ?C denote a feature map of the video with T frames. Spatio-temporal self-similarity (STSS) transformation of V results in a 6D tensor S ? R T ?X?Y ?L?U ?V , whose elements are defined as</p><formula xml:id="formula_2">S t,x,y,l,u,v = sim(V t,x,y , V t+l,x+u,y+v ),<label>(1)</label></formula><p>where (t, x, y) is a query coordinate and (l, u, v) is a spatiotemporal offset from the query. In addition to the locality of spatial offsets above, the temporal offset l is also restricted to its temporal neighborhood:</p><formula xml:id="formula_3">l ? [?d L , d L ], so that L = 2d L + 1.</formula><p>What types of information does STSS describe? Interestingly, for each time t, the STSS tensor S can be decomposed along temporal offset l into a single spatial self-similarity tensor (when l = 0) and 2d L spatial cross-similarity tensors (when l = 0); the partial tensors with a small offset (e.g., l = ?1 or +1) collect motion information from adjacent frames and those with larger offsets capture it from further frames both forward and backward in time. Unlike previous approaches to learn motion <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b53">54]</ref>, which rely on cross-similarity between adjacent frames, STSS allows to take a generalized, far-sighted view on motion, i.e., both short-term and long-term, both forward and backward, as well as spatial self-motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatio-temporal self-similarity representation learning</head><p>By leveraging the rich information in STSS, we propose to learn a generalized motion representation for video understanding. To achieve this goal without additional supervision, we design a neural block, dubbed SELFY, which can be inserted into video-processing architectures and learned end-to-end. <ref type="figure">Figure 2</ref> illustrates the overall structure. It consists of three steps: self-similarity transformation, feature extraction, and feature integration.</p><p>Given the input video feature tensor V, the selfsimilarity transformation step converts it to the STSS tensor S as in Eq. 1. In the following, we describe feature extraction and integration steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Feature extraction</head><p>From the STSS tensor S ? R T ?X?Y ?L?U ?V , we extract a C F -dimensional feature for each spatio-temporal position (t, x, y) and temporal offset l so that the resultant tensor is F ? R T ?X?Y ?L?C F , which is equivariant to translation in space, time, and temporal offset. The dimension of L is preserved to extract motion information across different temporal offsets in a consistent manner. While there exist many design choices, we introduce three methods for feature extraction in this work. Soft-argmax. The first method is to compute explicit displacement fields using S, which previous motion learning methods adopt using spatial cross-similarity <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b59">59]</ref>. One may extract the displacement field by indexing the positions with the highest similarity value via arg max (u,v) , but it is not differentiable. We instead use soft-argmax <ref type="bibr" target="#b1">[2]</ref>, which aggregates displacement vectors with softmax weighting <ref type="figure" target="#fig_0">(Fig. 3a)</ref>. The soft-argmax feature extraction can be formulated as</p><formula xml:id="formula_4">F t,x,y,l = u,v exp(S t,x,y,l,u,v /? ) u ,v exp(S t,x,y,l,u ,v /? ) [u; v],<label>(2)</label></formula><p>which results in a feature tensor F ? R T ?X?Y ?L?2 . The temperature factor ? adjusts the softmax distribution, and we set ? = 0.01 in our experiments.</p><p>Multi-layer perceptron (MLP). The second method is to learn an MLP that converts self-similarity values into a feature. For this, we flatten the (U, V ) volume into U Vdimensional vectors, and apply an MLP to them ( <ref type="figure" target="#fig_0">Fig. 3b</ref>). For the reshaped tensor S ? R T ?X?Y ?L?U V , a perceptron f (?) can be expressed as</p><formula xml:id="formula_5">f (S) = ReLU(S ? 5 W ? ),<label>(3)</label></formula><p>where ? n denotes the n-mode tensor product, W ? ? R C ?U V is the perceptron parameters, and the output is f (S) ? R T ?X?Y ?L?C . The MLP feature extraction can thus be formulated as</p><formula xml:id="formula_6">F = (f n ? f n?1 ? ? ? ? ? f 1 )(S),<label>(4)</label></formula><p>which produces a feature tensor F ? R T ?X?Y ?L?C F . This method is more flexible and may also be more effective than the soft-argmax because not only can it encode displacement information but also it can directly access the similarity values, which may be helpful for learning motion distribution.</p><p>Convolution. The third method is to learn convolution kernels over (L, U, V ) volume of S ( <ref type="figure" target="#fig_0">Fig. 3c</ref>). When we regard S as a 7D tensor S ? R T ?X?Y ?L?U ?V ?C with C = 1, the convolution layer g(?) can be expressed as</p><formula xml:id="formula_7">g(S) = ReLU(Conv(S, K e )),<label>(5)</label></formula><p>where K e ? R 1?1?1?L??U??V??C?C is a multi-channel convolution kernel. Starting from R T ?X?Y ?L?U ?V ?1 , we gradually downsample (U,V) and expand channels via multiple convolutions with strides, finally resulting in R T ?X?Y ?L?1?1?C F ; we preserve the L dimension, since maintaining fine temporal resolution is shown to be effective for capturing detailed motion information <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32]</ref>. In practice, we reshape S and then apply a regular 3D convolution along (l, u, v) dimension of S. The convolutional feature extraction with n layers can thus be formulated as</p><formula xml:id="formula_8">F = (g n ? g n?1 ? ? ? ? ? g 1 )(S),<label>(6)</label></formula><p>which results in a feature tensor F ? R T ?X?Y ?L?C F . This method is effective in learning structural patterns with their convolution kernels, thus outperforming the former methods as will be seen in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feature integration</head><p>In this step, we integrate the extracted STSS features F ? R T ?X?Y ?L?C F to feed them back to the original input stream with (T, X, Y, C) volume. We first use spatio-temporal convolution kernels along (t, x, y) dimension of F. The convolution layer h(?) can be expressed as</p><formula xml:id="formula_9">h(F) = ReLU(Conv(F, K i )),<label>(7)</label></formula><p>where K i ? R T??X??Y??1?C F ?C F is a multi-channel convolution kernel. This type of convolution integrates the extracted STSS features by extending receptive fields along (t, x, y) dimension. In practice, we reshape F and then apply a regular 3D convolution along (t, x, y) dimension of F.</p><formula xml:id="formula_10">The resultant features F ? R T ?X?Y ?L?C F is defined as F = (h n ? h n?1 ? ? ? ? ? h 1 )(F).<label>(8)</label></formula><p>We then flatten the (L, C F ) volume into LC F -dimensional vectors to obtain F ? R T ?X?Y ?LC F , and apply an 1 ? 1 ? 1 convolution layer to obtain the final output. This convolution layer integrates features from different temporal offsets and also adjusts its channel dimension to fit that of the original input V. The final output tensor Z is expressed as</p><formula xml:id="formula_11">Z = ReLU(F ? 4 W ? ),<label>(9)</label></formula><p>where ? n is the n-mode tensor product and W ? ? R C?LC F is the weights of the convolution layer. Finally, we combine the resultant STSS representation Z into the input feature V by element-wise addition, thus making SELFY act as a residual block <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>Action recognition architecture.</p><p>We employ TSN ResNets <ref type="bibr" target="#b54">[55]</ref> as 2D CNN backbones and TSM ResNets <ref type="bibr" target="#b31">[32]</ref> as 3D CNN backbones. TSM enables to obtain the effect of spatio-temporal convolutions using spatial convolutions by shifting a part of input channels along the temporal axis before the convolution operation. TSM is inserted into each residual block of the ResNet. We adopt ImageNet pretrained weights for our backbones. To transform the backbones to the self-similarity network (SELFYNet), we insert a single SELFY block after the third stage in the backbone with additive fusion. For the feature extraction and integration in SELFY block, we use four 1 ? 3 ? 3 convolution layers along (l, u, v) dimensions and four 1 ? 3 ? 3 convolution layers along (t, x, y) dimensions, respectively. For more details, please refer to supplementary material A.</p><p>Training &amp; testing. For training, we sample a clip of 8 or 16 frames from each video using segment-based sampling <ref type="bibr" target="#b54">[55]</ref>. The spatio-temporal matching region (L, U, V ) of SELFY block is set as <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> or <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> when using 8 or 16 frames, respectively. For testing, we sample one or two clips from a video, crop their center, and evaluate the averaged prediction of the sampled clips. For more details, please refer to supplementary material A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>For evaluation, we use benchmarks that contain finegrained spatio-temporal dynamics in videos. Something-Something V1 &amp; V2 (SS-V1 &amp; V2) <ref type="bibr" target="#b12">[13]</ref>, which are both large-scale action recognition datasets, contain ?108k and ?220k video clips, respectively. Both datasets share the same 174 action classes that are labeled, e.g., 'pretending to put something next to something.' Diving-48 <ref type="bibr" target="#b30">[31]</ref>, which contains ?18k videos with 48 different diving action classes, is an action recognition dataset that minimizes contextual biases, i.e., scenes or objects. FineGym <ref type="bibr" target="#b41">[42]</ref> is a fine-grained action dataset built on top of gymnastic videos. We adopt the Gym288 and Gym99 sets that contain 288 and 99 classes, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the state-of-the-art methods</head><p>For a fair comparison, we compare our model with other models that are not pre-trained on additional large-scale video datasets, e.g., Kinetics <ref type="bibr" target="#b22">[23]</ref> or Sports1M <ref type="bibr" target="#b21">[22]</ref>, in the following experiments. <ref type="table" target="#tab_1">Table 1</ref> summarizes the results on SS-V1&amp;V2. The first and second compartment of the table shows the results of other 2D CNN and (pseudo-) 3D CNN models, respectively. The last part of each compartment shows the results of SELFYNet. SELFYNet with TSN-ResNet (SELFYNet-TSN-R50) achieves 50.7% and 62.7% at top-1 accuracy, respectively, which outperforms other 2D models using 8 frames only. When we adopt TSM ResNet (TSM-R50) as our backbone and use 16 frames, our method (SELFYNet-TSM-R50) achieves 54.3% and 65.7% at top-1 accuracy, respectively, which is the best among the single models. Compared to TSM-R50, a single SELFY block obtains the significant gains of 7.0%p and 4.5%p at top-1 accuracy, respectively; our method is more accurate than TSM-R50 two-stream on both datasets. Finally, our ensemble model (SELFYNet-TSM-R50 EN ) with 2-clip evaluation sets a new state-of-the-art on both datasets by achieving 56.6% and 67.7% at top-1 accuracy, respectively. <ref type="table" target="#tab_2">Tables 2 and 3</ref> summarize the results on Diving-48 and FineGym. For Diving-48, TSM-R50 using 16 frames shows 38.8% at top-1 accuracy in our implementation. SELFYNet-TSM-R50 outperforms TSM-R50 by 2.8%p at top-1 accuracy so that it sets a new state-of-the-art top-1 accuracy as 41.6% on Diving-48. For FineGym, SELFYNet-TSM-R50 achieves 49.5% and 87.7% at given 288 and 99 classes, respectively, surpassing all the other models reported in <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>We conduct ablation experiments to demonstrate the effectiveness of the proposed method. All experiments are performed on SS-V1 using 8 frames. Unless specified otherwise, we set ImageNet pre-trained TSM ResNet-18 (TSM-R18) with the single SELFY block of which (L, U, V ) = <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref>, as our default SELFYNet. Types of similarity. In <ref type="table" target="#tab_5">Table 4a</ref>, we investigate the effect of different types of similarity by varying the set of temporal offset l on both TSN-ResNet-18 (TSN-R18) and TSM-R18. Interestingly, learning spatial self-similarity ({0}) improves accuracy on both backbones, which implies that selfsimilarity features help capture structural patterns of visual    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Relation with self-attention mechanisms</head><p>Note that self-similarity is also used in self-attention mechanisms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">56]</ref>, but both the purpose and the scheme are very different. Self-attention mechanisms aim to perform dynamic feature transformation based on the image context and thus use the self-similarity as attention weights in aggregating individual features. In contrast, our method focuses on learning relational representation from the self-similarity tensor itself. We directly transform the tensor into a relational representation with learnable convolution kernels, where the relational representation of video is interpreted as generalized motion representation.</p><p>For an apple-to-apple empirical validation, we compare our method with popular self-attention methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b56">56]</ref>. We re-implement the local self-attention <ref type="bibr" target="#b39">[40]</ref> and Transformer <ref type="bibr" target="#b44">[45]</ref> blocks, and extend them to a temporal dimension. For a fair comparison, we insert a single block after res 3 of ResNet-18. All other experimental details are the same as those in supplementary material A. <ref type="table">Table 5</ref> summarizes the results. Our method outperforms the selfattention methods at both top-1 and top-5 accuracies with large margins. These results demonstrate that learning the STSS representation effectively leverages motion features, which play a crucial role in action recognition. For more experiments, please refer to supplementary material C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Complementarity of STSS features</head><p>We conduct experiments for analyzing different meanings of spatio-temporal features and STSS features. We organize two basic blocks for representing two different features: spatio-temporal convolution block (STCB) that consists of several spatial-temporal convolutions <ref type="figure" target="#fig_1">(Fig. 4a</ref>) and SELFY-s block, light-weighted version of the SELFY block by removing spatial convolution layers <ref type="figure" target="#fig_1">(Fig. 4b</ref>). Both blocks have the same receptive fields and a similar number of parameters for a fair comparison. Different combinations of the basic blocks are inserted after the third stage of TSN-ResNet-18.   <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> 48.4 77.6 <ref type="table">Table 5</ref>: Performance comparison with self-attention methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b56">56]</ref>. LSA, NL, and MHSA denote a local self-attention block <ref type="bibr" target="#b39">[40]</ref>, non-local block <ref type="bibr" target="#b56">[56]</ref>, and multihead self-attention block <ref type="bibr" target="#b44">[45]</ref>, respectively.</p><p>other. We conjecture that this complementarity comes from different characteristics of the two features; while spatiotemporal features are obtained by directly encoding appearance features, STSS features are obtained by suppressing variations in appearance and focusing on the relational features in space and time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Improving robustness with STSS</head><p>In this experiment, we demonstrate that STSS representation helps video-processing models to be more robust to   The basic blocks and their different combinations in <ref type="figure" target="#fig_1">Fig. 4</ref> are compared on SS-V1. video corruptions. We test two types of corruption that are likely to occur in real-world videos: occlusion and motion blur. To induce the corruptions, we either cut out a rectangle patch of a particular frame or generate a motion blur <ref type="bibr" target="#b14">[15]</ref>. We corrupt a single center-frame for every clip of SS-V1 at the testing phase and gradually increase the severity of corruption. We compare the results of TSM-R18 and SELFYNet variants of <ref type="table" target="#tab_5">Table 4a</ref>. <ref type="figure" target="#fig_2">Figures 5a  and 5b</ref> summarize the results of two corruptions, respectively. Top-1 accuracy of TSM-R18 and SELFYNets with the short temporal range ({0}, {1}, and {?1, 0, 1}) significantly drops as the severity of corruption becomes harder. We conjecture that features of the corrupted frame propagate through the stacked TSMs, confusing the entire network. However, the SELFYNets with the long temporal range ({?2, ? ? ? , 2} and {?3, ? ? ? , 3}) show more robust performance than the other models. As shown in Figs. 5a and 5b, the accuracy gap between SELFYNets with the long temporal range and the others increases as the severity of corruptions becomes higher, indicating that the larger size of STSS features can improve the robustness on action recognition. We also present some qualitative results <ref type="figure" target="#fig_2">(Fig. 5c)</ref> where two SELFYNets with different temporal ranges, {1} and {?3, ? ? ? , 3}, both answer correctly without corruption, while the SELFYNet with {1} fails for the corrupted input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed to learn a generalized, far-sighted motion representation from STSS for video understanding. The comprehensive analyses on the STSS demonstrate that STSS features effectively capture both short-term and long-term interactions, complement spatio-temporal features, and improve the robustness of video-processing models. Our method outperforms other state-of-the-art methods on the three benchmarks for video action recognition.</p><p>We present more experimental results that could not be included in the main manuscript due to the lack of space.</p><p>A. Implementation details Architecture details. We use TSN-ResNet and TSM-ResNet as our backbone (see <ref type="table" target="#tab_9">Table 7</ref>) and initialize them with ImageNet pre-trained weights. We insert a single SELFY block right after res 3 and use the convolution method as a default feature extraction method. We set the spatio-temporal matching region of SELFY block, (L, U, V ), as <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> or <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> when using 8 or 16 input frames, respectively. We stack four 1 ? 3 ? 3 convolution layers along (l, u, v) dimension for the feature extraction method, and use four 3 ? 3 convolution layers along (x, y) dimension for the feature integration. We reduce a spatial resolution of video feature tensor, V, as 14?14 for computation efficiency before the self-similarity transformation. After the feature integration, we upsample the integrated feature tensor, G , as 28?28 for the residual connection. Training. We sample a clip of 8 or 16 frames from each video by using segment-based sampling <ref type="bibr" target="#b54">[55]</ref>. We resize the sampled clips into 240 ? 320 images and apply random scaling and horizontal flipping for data augmentation. When applying the horizontal flipping on SS-V1&amp;V2 <ref type="bibr" target="#b12">[13]</ref>, we do not flip clips of which class labels include 'left' or 'right' words; the action labels, e.g., 'pushing something from left to right.' We fit the augmented clips into a spatial resolution of 224 ? 224. We adopt the SGD optimizer with a momentum of 0.9. For SS-V1&amp;V2, we set the initial learning rate to 0.01 and the training epochs to 50; the learning rate is decayed by 1/10 after 30 th and 40 th epochs. The training time of SELFYNet-TSM-R50 using 16 frames on SS-V1&amp;V2 is about 2?3 days with 8 Titan RTX GPUs. For Diving-48 <ref type="bibr" target="#b30">[31]</ref> and FineGym <ref type="bibr" target="#b41">[42]</ref>, we use a cosine learning rate schedule <ref type="bibr" target="#b34">[35]</ref> with the first 10 epochs for gradual warm-up <ref type="bibr" target="#b11">[12]</ref>. We set the initial learning rate to 0.01 and the training epochs to 30 and 40, respectively. Testing. Given a video, we sample 1 or 2 clips, resize them into 240 ? 320 images, and crop their centers as 224 ? 224. We evaluate an average prediction of the sampled clips. We report top-1 and top-5 accuracy for SS-V1&amp;V2 and Diving-48, and mean-class accuracy for FineGym. Frame corruption details. We adopt two corruptions, occlusion and motion blur, to test the robustness of SELF-YNet. We only corrupt a single center-frame for every validation clip of SS-V1; we corrupt the 4 th frame amongst 8 input frames. For the occlusion, we cut out a rectangle region from the center of the frame. For the motion blur, we   <ref type="table">Table 8</ref>: Performance comparison on Kinetics-400 <ref type="bibr" target="#b22">[23]</ref>.</p><p>adopt ImageNet-C implementation, which is available online 1 . We set 6 levels of severity for each corruption. We set the side length of the occluded region as 40px, 80px, 120px, 160px, 200px and 224px from the level 1 to 6. For the motion blur, we set (radius, sigma) tuple arguments as <ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b4">5)</ref>, <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b7">8)</ref>, <ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b11">12)</ref>, <ref type="bibr" target="#b19">(20,</ref><ref type="bibr" target="#b14">15)</ref>, <ref type="bibr" target="#b24">(25,</ref><ref type="bibr" target="#b19">20)</ref>, and <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b24">25)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance comparison on Kinetics-400</head><p>We also conduct experiments on Kinetics-400 <ref type="bibr" target="#b22">[23]</ref>, which is the most popular appearance-centric benchmark. <ref type="table">Table 8</ref> summarizes the results on Kinetics-400. The first and second compartment of the table shows the results of different models with Res-50 using 16 frames and the results of the state-of-the-art models, respectively. The last row shows our result. The results demonstrate that SELF-YNet still shows a clear improvement on the appearancecentric benchmark. SELFYNet obtains the improvement of 2.4%p at top-1 accuracy over the TSM baseline <ref type="bibr" target="#b31">[32]</ref>, achieving the best accuracy among the models with Res-50 using 16 frames. Although the accuracy of SELFYNet is inferior to that of SlowFast <ref type="bibr" target="#b9">[10]</ref> or TimeSformer-L <ref type="bibr" target="#b4">[5]</ref>, we expect that SELFYNet can achieve the state-of-the-art when using larger backbones (3D Res-101, ViT-L) or a bigger input.</p><p>In the following, we provide implementation details for Kinetics-400 experiments. We adopt the dense frame sampling method <ref type="bibr" target="#b56">[56]</ref> and sample a clip of 16 frames. For training, we use a cosine learning rate schedule with the first 10 epochs for warm-up. We set the initial learning rate to 0.01 and total epochs to 65. For testing, we sample 10 uniform clips per video and average the softmax scores for the final prediction. We follow the strategy of non-local networks <ref type="bibr" target="#b56">[56]</ref> to pre-process the frames and take 3 crops as input. Other experimental details are the same as those in the supplementary material A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional experiments</head><p>We conduct additional experiments to identify the behaviors of the proposed method. All experiments are performed on SS-V1 by using 8 frames. Unless otherwise specified, we set ImageNet pre-trained TSM ResNet-18 (TSM-R18) with a single SELFY block of which (L, U, V ) = <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref>, as our default SELFYNet. Spatial matching region. In <ref type="table">Table 9a</ref>, we compare a single SELFY block with different spatial matching regions, (U, V ). As a result, indeed, the larger spatial matching region leads the better accuracy. Considering the accuracycomputation trade-off, we set our spatial matching region, (U, V ), as <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b8">9)</ref> as a default. Block position. From the 2 nd to the 6 th row of <ref type="table">Table 9b</ref>, we identify the effect of different positions of SELFY block in the backbone. We resize the spatial resolution of the video tensor, (X, Y ), into 14?14, and fix the matching region, (L, U, V ), as <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b8">9)</ref> for all the cases maintaining the similar computational cost. SELFY after the res 3 shows the best trade-off by achieving the highest accuracy among the cases; early-stage features (pool 1 ,res 2 ) lack enough semantics for robust matching while late-stage ones (res 4 ,res 5 ) lose appearance details for accurate matching. The last row in <ref type="table">Table 9b</ref> shows that the multiple SELFY blocks improve accuracy compared to the single block. Fusing STSS features with visual features. We evaluate SELFYNet purely based on STSS features to see how much the ordinary visual feature V contributes to the final prediction. That is, we pass the STSS features, Z = ReLU(F ? 5 W ? ), into the downstream layers without additive fusion. <ref type="table">Table 9c</ref> compares the results of using different cases of the output tensor (V, Z, and Z + V) on SS-V1. Interestingly, SELFYNet using only Z achieves 45.5% at top-1 accuracy, which is higher as 2.5%p than the baseline. As we add V to Z, we obtain an additional gain of 2.9%p. It indicates that the STSS features and the visual features are complementary to each other. Multi-channel 3?3?3 kernel for feature extraction. We investigate the effect of the convolution method for STSS feature extraction when we use multi-channel 3 ? 3 ? 3 kernels. For the experiment, we stack four 3 ? 3 ? 3 convolution layers followed by the feature integration step, which are the same as in Section 3.2.2 in our main manuscript. <ref type="table">Table 9d</ref> summarizes the results. Note that we do not report models of which temporal window L = 1, e.g., {0} and {1}. As shown in the table, indeed, the long temporal range gives a higher accuracy. However, the effect of the 3 ? 3 ? 3 kernel is comparable to that of the 1 ? 3 ? 3 kernel in <ref type="table" target="#tab_5">Table 4a</ref> in our main manuscript. Considering the accuracy-computation trade-off, we choose to fix the kernel size, L ? ? U ? ? V ? , as 1 ? 3 ? 3 for the STSS feature extraction. Relation with local self-attention mechanisms. The local self-attention <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b61">61]</ref> and our method have a common denominator of using the self-similarity tensor but use it in a very different way and purpose. The local selfattention mechanism aims to aggregate the local context features using the self-similarity tensor, and it thus uses the self-similarity values as attention weights for feature aggregation. However, our method aims to learn a generalized motion representation from the local STSS, so the final STSS representation is directly fed into the neural network instead of multiplying it to local context features.</p><p>For an empirical comparison, we conduct an ablation experiment as follows. We extend the local self attention layer <ref type="bibr" target="#b39">[40]</ref> to the temporal dimension and then add the spatio-temporal local self-attention layer, which is followed by feature integration layers, after res 3 . All experimental details are the same as those in supplementary material A, except that we reduce the channel dimension C of appearance feature V to 32. <ref type="table">Table 9e</ref> summarizes the results on SS-V1. The spatio-temporal local self-attention layer is accurate as 43.8% at top-1 accuracy, and both of SELFY blocks using the embedded Gaussian and the cosine similarity outperform the local self-attention by achieving top-1 accuracy as 47.6% and 47.8%, respectively. These results are in alignment with the prior work <ref type="bibr" target="#b32">[33]</ref>, which reveals that the self-attention mechanism hardly captures motion in the video. Comparison with correlation-based methods.</p><p>We also investigate the difference between our method and correlation-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b53">54]</ref>. While correlationbased methods extract motion features only from the spatial cross-similarity tensor between two adjacent frames, and are thus limited to short-term motion, our method effectively captures bi-directional and long-term motion information via learning with the sufficient volume of STSS. Our method can also exploit richer information from the selfsimilarity values than other methods. MS module <ref type="bibr" target="#b24">[25]</ref> only focuses on the maximal similarity value of the (u, v) dimensions to extract flow information, and Correlation block <ref type="bibr" target="#b53">[54]</ref> uses an 1 ? 1 convolution layer for extracting motion fea-  <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b56">56]</ref>. We insert a single block after res3 in TSM-R50. We use 8 clips per GPU and measure the runtime by following protocols in <ref type="bibr" target="#b24">[25]</ref>. <ref type="table">Table 9</ref>: Additional experiments on SS-V1. Top-1 &amp; 5 accuracy (%) are shown.</p><p>tures from the similarity values. In contrast to the two methods, we introduce a generalized motion learning framework using the self-similarity tensor in Section 3.2 in our main manuscript.</p><p>We also conduct experiments to compare our method with MSNet <ref type="bibr" target="#b24">[25]</ref>, one of the correlation-based methods. For an apple-to-apple comparison, we apply kernel soft-argmax and max pooling operation (KS + CM in <ref type="bibr" target="#b24">[25]</ref>) to our fea-ture extraction method by following their official codes 2 . Please note that, when we restrict the temporal offset l to {1}, the SELFY block using KS + CM is equivalent to the MS module of which feature transformation layers are the standard 2D convolution layers. <ref type="table">Table 9f</ref> summarizes the results. KS+CM method achieves 46.1% at top-1 accuracy. As we enlarge the temporal window L to 5, we obtain the additional gain as 1.3%p. The learnable convolution layers improve the top-1 accuracy by 1.0%p in both cases. The results demonstrate the effectiveness of learning geometric patterns within the sufficient volume of STSS tensors for learning motion features. Efficiency. In <ref type="table">Table 9g</ref>, we compare the efficiency of SELFYNet with that of other self-attention methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b56">56]</ref> in terms of FLOPs, memory footprint, runtime, and accuracy. Compared to TSM-R50 using 16 frames, SELFYNet using 8 frames consumes less memory by 6.1 GB and runs faster by 9.0 ms while improving top-1 accuracy by 5.2 %p. Compared to the self-attention methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b56">56]</ref>, SELFYNet also achieves the best accuracy with less memory footprint and faster inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualizations</head><p>In <ref type="figure">Fig. 6</ref>, we visualize some qualitative results of two different SELFYNet-TSM-R18 ({1} and {?3, ? ? ? , 3}) on SS-V1. We show the different predictions of the two models with 8 input frames. We also overlay Grad-CAMs <ref type="bibr" target="#b40">[41]</ref> on the input frames to see whether a larger volume of STSS benefits to capture long-term interactions in videos. We take Grad-CAMs of features which is right before a global average pooling layer. As shown in the figure, the STSS with the sufficient volume helps to learn the more enriched context of temporal dynamics in the video; in <ref type="figure">Fig. 6a</ref>, for example, SELFYNet with the range of ({?3, ? ? ? , 3}) focuses on not only regions on which an action occurs but also focuses on the white-stain after the action to verify whether the stain is wiped off or not.  <ref type="figure">Figure 6</ref>: Qualitative results of two SELFYNets on SS-V1. Each subfigure visualizes prediction results of the two models with Grad-CAM-overlaid RGB frames. The correct and wrong predictions are colorized as green and red, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Feature extraction from STSS. For a spatio-temporal position (t, x, y), each method transforms (L, U, V ) volume of STSS tensor S into (L, C F ). See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Basic blocks and their combinations. (a) spatiotemporal convolution block (STCB), (b) SELFY-s block, and (cf) their different combinations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>(a) corruption: occlusion (b) corruption: motion blur (c) qualitative results on corrupted videos Robustness experiments. (a) and (b) show top-1 accuracy of SELFYNet variants (Table 4a) when different degrees of occlusion and motion blur, respectively, are added to input. (c) shows qualitative examples where SELFYNet ({?3, ? ? ? , 3}) succeeds while SELFYNet ({1}) fails.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>model</cell><cell>#frame</cell><cell>FLOPs</cell><cell>Top-1</cell></row><row><cell></cell><cell></cell><cell>?clips</cell><cell></cell></row><row><cell>TSN from [31]</cell><cell>-</cell><cell>-</cell><cell>16.8</cell></row><row><cell>TRN from [21]</cell><cell>-</cell><cell>-</cell><cell>22.8</cell></row><row><cell>Att-LSTM [21]</cell><cell>64</cell><cell>N/A?1</cell><cell>35.6</cell></row><row><cell>P3D from [36]</cell><cell>16</cell><cell>N/A?1</cell><cell>32.4</cell></row><row><cell>C3D from [36]</cell><cell>16</cell><cell>N/A?1</cell><cell>34.5</cell></row><row><cell>GST-R50 [36]</cell><cell>16</cell><cell>59 G?1</cell><cell>38.8</cell></row><row><cell>CorrNet-R101 [54]</cell><cell>32</cell><cell>187 G?10</cell><cell>38.2</cell></row><row><cell>GSM-IncV3 [46]</cell><cell>16</cell><cell>54 G?2</cell><cell>40.3</cell></row><row><cell>TSM-R50 (our impl.)</cell><cell>16</cell><cell>65 G?2</cell><cell>38.8</cell></row><row><cell>SELFYNet-TSM-R50 (ours)</cell><cell>16</cell><cell>77 G?2</cell><cell>41.6</cell></row></table><note>Performance comparison on SS-V1&amp;V2. Top-1, 5 accuracy (%) and FLOPs (G) are shown.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on Diving-48. Top-1 accuracy (%) and FLOPs (G) are shown. features. Learning cross-similarity with a short temporal range ({1}) shows a noticeable gain at accuracy on both backbones, indicating the significance of motion features.Learning STSS outperforms other types of similarity, and the accuracy of SELFYNet increases as the temporal range becomes longer. When STSS takes a far-sighted view on motion, STSS learns both short-term and long-term interactions in videos, as well as spatial self-similarity.</figDesc><table><row><cell>model</cell><cell cols="3">#frame Gym288 Gym99</cell></row><row><cell></cell><cell></cell><cell>Mean</cell><cell>Mean</cell></row><row><cell>TSN [55]</cell><cell>3</cell><cell>26.5</cell><cell>61.4</cell></row><row><cell>TRN [63]</cell><cell>3</cell><cell>33.1</cell><cell>68.7</cell></row><row><cell>I3D [1]</cell><cell>8</cell><cell>27.9</cell><cell>63.2</cell></row><row><cell>NL I3D [56]</cell><cell>8</cell><cell>27.1</cell><cell>62.1</cell></row><row><cell>TSM [32]</cell><cell>3</cell><cell>34.8</cell><cell>70.6</cell></row><row><cell>TSM Two-Stream [32]</cell><cell>N/A</cell><cell>46.5</cell><cell>81.2</cell></row><row><cell>TSM-R50 (our impl.)</cell><cell>3</cell><cell>35.3</cell><cell>73.7</cell></row><row><cell>TSM-R50 (our impl.)</cell><cell>8</cell><cell>47.9</cell><cell>86.6</cell></row><row><cell>SELFYNet-TSM-R50 (ours)</cell><cell>8</cell><cell>49.5</cell><cell>87.7</cell></row></table><note>Feature extraction and integration methods. In Table 4b, we compare the performance of different combinations of feature extraction and integration methods. From the 2 nd to the 4 th rows, different feature extraction methods are</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Compared to the baseline, the use of soft-argmax, which extracts spatial displacement features, improves the top-1 accuracy by 1.0%p. Replacing soft-argmax with MLP provides the additional gain of 1.9%p at top-1 accuracy, showing the effectiveness of directly using similarity values. When using the convolution method for feature extraction, we achieve 46.7% at top-1 accuracy; the multi-channel convolution kernel is more ef-Replacing the single FC layer with MLP improves the top-1 accuracy by 0.6%p. Replacing MLP with convolutional layers further improves and achieves 48.4% at top-1 accuracy. These results demonstrate that our design choice of using convolutions along (u, v) and (h, w) dimensions is the most effective in learning the geometry-aware STSS representation. For more experiments, please refer to supplementary material C.</figDesc><table /><note>Performance comparison on FineGym. The av- eraged per-class accuracy (%) is shown. All results in the upper part are from FineGym paper [42].compared, fixing the feature integration methods to a sin- gle fully-connected (FC) layer.fective in capturing structural patterns along (u, v) dimen- sions than MLP. From the 4 th to the 6 th rows, different fea- ture integration methods are compared, fixing the feature extraction method to convolution.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>summarizes the results</cell></row><row><cell>on SS-V1. STSS features (Figs. 4b and 4d) are more effec-</cell></row><row><cell>tive than spatio-temporal features (Figs. 4a and 4c) at top-1</cell></row><row><cell>and top-5 accuracy when the same number of blocks are in-</cell></row><row><cell>serted. Interestingly, the combination of two different fea-</cell></row><row><cell>tures (Figs. 4e and 4f) shows better results at top-1 and top-5</cell></row><row><cell>accuracy compared to the single feature cases (Figs. 4c and</cell></row><row><cell>4d), which demonstrate that both features complement each</cell></row></table><note>(b) Feature extraction and integration methods. Smax denotes the soft-argmax operation. MLP consist of four FC layers. The 1 ? 1 ? 1 layer in the feature integration stage is omitted.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablations on SS-V1.</figDesc><table><row><cell>Top-1 &amp; 5 accuracy (%) are</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Spatio-temporal features v.s. STSS features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>TSN &amp; TSM ResNet-50 backbone.</figDesc><table><row><cell>model</cell><cell>backbone</cell><cell cols="3">#frames FLOPs?clips top-1</cell></row><row><cell>STM [17]</cell><cell>Res-50</cell><cell>16</cell><cell>67 G?30</cell><cell>73.7</cell></row><row><cell>TSM [32]</cell><cell>Res-50</cell><cell>16</cell><cell>65 G?30</cell><cell>74.7</cell></row><row><cell>TEINet [34]</cell><cell>Res-50</cell><cell>16</cell><cell>66 G ?30</cell><cell>76.2</cell></row><row><cell>TEA [30]</cell><cell>Res-50</cell><cell>16</cell><cell>70 G?30</cell><cell>76.1</cell></row><row><cell>MSNet-TSM [25]</cell><cell>Res-50</cell><cell>16</cell><cell>67 G?10</cell><cell>76.4</cell></row><row><cell cols="3">SlowFast 16 ? 8+NL [10] 3D Res-101 16+128</cell><cell>234 G?30</cell><cell>79.8</cell></row><row><cell>TimeSformer-L [5]</cell><cell>ViT-L [6]</cell><cell>96</cell><cell>2380 G?3</cell><cell>80.7</cell></row><row><cell>SELFYNet-TSM (ours)</cell><cell>Res-50</cell><cell>16</cell><cell>77 G?30</cell><cell>77.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Multi-channel 3 ? 3 ? 3 kernel for feature extraction. Four convolution layers are used for extracting STSS features.{?} denotes a set of temporal offsets l. Performance comparison with the local self-attention mechanisms<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>. We implemented the local selfattention by following Ramachandran et al.<ref type="bibr" target="#b39">[40]</ref>. Performance comparison with MSNet<ref type="bibr" target="#b24">[25]</ref>. KS and CM denote the kernel soft-argmax and confidence map, respectively.</figDesc><table><row><cell>model</cell><cell>U ? V</cell><cell cols="3">FLOPs top-1 top-5</cell><cell></cell><cell>model</cell><cell></cell><cell cols="2">position</cell><cell>top-1 top-5</cell></row><row><cell>TSM-R18</cell><cell>-</cell><cell>14.6 G</cell><cell>43.0</cell><cell>72.3</cell><cell></cell><cell>TSM-R18</cell><cell></cell><cell>-</cell><cell>43.0</cell><cell>72.3</cell></row><row><cell></cell><cell>5 ? 5</cell><cell>17.1 G</cell><cell>47.8</cell><cell>77.1</cell><cell></cell><cell></cell><cell></cell><cell>pool 1</cell><cell>45.7</cell><cell>77.6</cell></row><row><cell>SELFYNet</cell><cell>9 ? 9 13 ? 13 17 ? 17</cell><cell>17.3 G 18.4 G 19.8 G</cell><cell>48.4 48.4 48.6</cell><cell>77.6 77.8 78.3</cell><cell></cell><cell>SELFYNet</cell><cell></cell><cell>res2 res3 res4</cell><cell>47.2 48.4 46.6</cell><cell>76.6 77.6 76.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>res5</cell><cell>42.8</cell><cell>72.6</cell></row><row><cell cols="6">(a) Spatial matching region. Performance comparison with</cell><cell></cell><cell></cell><cell cols="2">res2,3,4</cell><cell>48.6</cell><cell>77.9</cell></row><row><cell cols="3">different spatial matching-regions, (U ? V ).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) Position. Performance comparison with different posi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">tions of SELFY block. For the last row, 3 SELFY blocks are</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>used in total.</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell cols="2">features</cell><cell cols="2">top-1 top-5</cell><cell></cell><cell>model</cell><cell></cell><cell cols="2">range of l</cell><cell>top-1 top-5</cell></row><row><cell>TSM-R18</cell><cell>V</cell><cell></cell><cell>43.0</cell><cell>72.3</cell><cell></cell><cell>TSM-R18</cell><cell></cell><cell>-</cell><cell>43.0</cell><cell>72.3</cell></row><row><cell>SELFYNet</cell><cell cols="2">Z Z + V</cell><cell>45.5 48.4</cell><cell>75.9 77.6</cell><cell></cell><cell>SELFYNet</cell><cell></cell><cell cols="2">{?1, 0, 1} {?2, ? ? ? , 2}</cell><cell>47.4 48.3</cell><cell>77.0 77.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">{?3, ? ? ? , 3}</cell><cell>48.5</cell><cell>77.4</cell></row><row><cell cols="6">(c) STSS features with visual features. V, Z denotes the</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">visual features and STSS features, respectively. (d) model similarity extraction top-1 top-5 TSM-R18 --43.0 72.3 SELFYNet embed. G mult. w/ V 43.8 72.3 embed. G Conv 47.6 76.8 cosine Conv 47.8 77.1 TSM-R18 SELFYNet (e) model</cell><cell cols="3">extraction (L, U, V ) --KS + CM (1, 9, 9) KS + CM (5, 9, 9) Conv (1, 9, 9) Conv (5, 9, 9)</cell><cell>top-1 top-5 43.0 72.3 46.1 75.3 47.4 76.8 47.1 76.3 48.4 77.6</cell></row><row><cell></cell><cell cols="9">(f) model frames FLOPs memory runtime top-1 top-5</cell></row><row><cell></cell><cell cols="2">TSM-R50 [32]</cell><cell></cell><cell>8</cell><cell>33.1 G</cell><cell>8.2 GB</cell><cell>15.6 ms</cell><cell>45.6</cell><cell>74.2</cell></row><row><cell></cell><cell cols="2">TSM-R50 [32]</cell><cell></cell><cell>16</cell><cell cols="3">66.3 G 15.7 GB 30.1 ms</cell><cell>47.3</cell><cell>77.1</cell></row><row><cell></cell><cell cols="2">TSM-R50 + NL [56]</cell><cell></cell><cell>8</cell><cell cols="3">46.5 G 10.3 GB 24.0 ms</cell><cell>49.1</cell><cell>77.2</cell></row><row><cell></cell><cell cols="3">TSM-R50 + MHSA [45]</cell><cell>8</cell><cell cols="3">50.6 G 15.9 GB 26.3 ms</cell><cell>49.2</cell><cell>77.9</cell></row><row><cell></cell><cell cols="2">TSM-R50 + SELFY</cell><cell></cell><cell>8</cell><cell>36.6 G</cell><cell>9.6 GB</cell><cell>21.1 ms</cell><cell>52.5</cell><cell>80.8</cell></row></table><note>(g) Efficiency. Performance comparison with other attention mechanisms</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/hendrycks/robustness</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/arunos728/MotionSqueeze</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is supported by Samsung Advanced Institute of Technology (SAIT), the NRF grant (NRF-2021R1A2C3012728), and the IITP grant (No.2019-0-01906, AI Graduate School Program -POSTECH) funded by Ministry of Science and ICT, Korea.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material of "Learning Self-Similarity in Space and Time as Generalized Motion for Video Action</head><p>Recognition"</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Gradient descent optimization of smoothed information retrieval metrics. Information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingrui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bertasius</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dosovitskiy</surname></persName>
		</author>
		<idno>2021. 12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rubiksnet: Learnable 3d-shift for efficient video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanzhi</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Attentional pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01467</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Local relation networks for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">View-independent action recognition from temporal self-similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-view action recognition from temporal selfsimilarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relational embedding for few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahyun</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attentive spatio-temporal representation learning for diving classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhakar</forename><surname>Gagan Kanojia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanmuganathan</forename><surname>Kumawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fcss: Fully convolutional self-similarity for dense semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangryul</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Motionsqueeze: Neural motion feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.09933</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">First person action recognition via two-stream convnet with long-term fusion pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeonho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="167" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Motion feature network: Fixed motion filter for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunggi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyutae</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatio-temporal deformable 3d convnets with attention for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">107037</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning video representations from correspondence proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<idno>2020. 12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grouped spatial-temporal aggregation for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Something-else: Compositional action recognition with spatial-temporal interaction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Integralaction: Pose-driven feature integration for robust human action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeseung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Kyoung Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<idno>2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Standalone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05909</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno>2017. 15</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Finegym: A hierarchical video dataset for fine-grained action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Matching local selfsimilarities across images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gate-shift networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Local self-similarity-based registration of human rois in pairs of stereo thermal-visible videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume-Alexandre</forename><surname>Bilodeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporal pyramid network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="223" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10076" to="10085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Trajectory convolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
