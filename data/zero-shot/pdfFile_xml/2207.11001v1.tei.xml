<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POP: Mining POtential Performance of new fashion products via webly cross-modal query expansion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Humatics s.r.l</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Verona</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">POP: Mining POtential Performance of new fashion products via webly cross-modal query expansion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer Vision for Fashion</term>
					<term>Data-centric Artificial Intel- ligence</term>
					<term>Time Series Forecasting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Christian Joppi ?1[0000?0003?4495?9515] , Geri Skenderi ?2[0000?0001?9968?7727] , and Marco Cristani 2,1[0000?0002?0523?6042]</p><p>Abstract. We propose a data-centric pipeline able to generate exogenous observation data for the New Fashion Product Performance Forecasting (NFPPF) problem, i.e., predicting the performance of a brandnew clothing probe with no available past observations. Our pipeline manufactures the missing past starting from a single, available image of the clothing probe. It starts by expanding textual tags associated with the image, querying related fashionable or unfashionable images uploaded on the web at a specific time in the past. A binary classifier is robustly trained on these web images by confident learning, to learn what was fashionable in the past and how much the probe image conforms to this notion of fashionability. This compliance produces the POtential Performance (POP) time series, indicating how performing the probe could have been if it were available earlier. POP proves to be highly predictive for the probe's future performance, ameliorating the sales forecasts of all state-of-the-art models on the recent VISUELLE fast-fashion dataset. We also show that POP reflects the ground-truth popularity of new styles (ensembles of clothing items) on the Fashion Forward benchmark, demonstrating that our webly-learned signal is a truthful expression of popularity, accessible by everyone and generalizable to any time of analysis. Forecasting code, data and the POP time series are available at: https://github.com/HumaticsLAB/ POP-Mining-POtential-Performance</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>: a) A standard forecasting setup, where an evergreen item has past observations to exploit, e.g., # sales; b) New Fashion Product Performance Forecasting (NFPPF) problem, where no past observations are available and exogenous data must be considered. Here we propose POP, the POtential Performance series, which is webly learned. The signals in b) appear on the same scale purely for visualization purposes, in reality this might not be the case.</p><p>multiple levels. Unfortunately, standard forecasting approaches require observations from the past to provide a forecast for the same product in the future <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b0">1]</ref> and this information is typically available for evergreen products only <ref type="figure">(Fig. 1a)</ref>. In other cases, judgemental forecasts <ref type="bibr" target="#b15">[14]</ref> from fashion professionals 3 are the only ones that can help. Starting from photos or realistic renderings, which we call probe images, they perform comparisons with trends as they surface and then infer the probe's success <ref type="bibr" target="#b32">[31]</ref>. In this paper, we try to model this line of reasoning and create a data-centric <ref type="bibr" target="#b27">[26]</ref> pipeline that is able to extract a highly predictive signal from the web, dubbed "POtential Performance" (POP), which can be fed into any NFFPF forecasting model as an additional variable and lead to more accurate forecasts. (see <ref type="figure">Fig. 1b</ref>). Our cross-modal, query expansion based pipeline is sketched in <ref type="figure">Fig. 2</ref>. The input is a single probe image of the product to be analyzed, or a photorealistic rendering <ref type="bibr" target="#b3">4</ref> . The pipeline first extracts textual tags from the probe automatically or by directly considering the associated technical sheet. The tag set is expanded with positive and negative tags that are used to perform a time-dependent query online, i.e., collecting images of "fashionable" and "unfashionable" items related to the tags, which have been uploaded during some specified K past intervals in the past. These images are used to confidently learn <ref type="bibr" target="#b29">[28]</ref> a binary classifier that captures what is fashionable VS unfashionable in that interval. This learning procedure prunes noisy images from both the positive and negative classes, resulting in a robust model. Subsequently, pruned positive images are projected into an embedding space by the learned model and compared with the (also projected) initial probe image, providing the K past -long POP signal. The POP signal indicates how popular the probe could have been over time if it were available earlier in the past.</p><p>Our approach should be cast in the field of data-centric artificial intelligence (DCAI) <ref type="bibr" target="#b27">[26]</ref>, since it automates the creation of high quality training data that can be used to improve any forecasting model which accommodates multivariate time series forecasting. POP has been tested on diverse state-of-the-art NFPPF algorithms that predict sales curves of new products on the recent VI-SUELLE fast-fashion dataset <ref type="bibr" target="#b36">[35]</ref>, providing superior performance when compared to other types of training signals. It has also been customized to deal with fashion styles (i.e., ensembles of clothing items) on the Fashion Forward benchmark <ref type="bibr" target="#b0">[1]</ref>. Fashion Forward (FF) calculates a popularity time series for an automatically extracted style based on the dataset properties and then applies standard forecasting algorithms. We substitute their popularity series with POP, reaching similar predictions despite relying only on an exogenous input. Surprisingly, on the Dresses partition of FF, we reach the absolute best, suggesting that POP can foresee the success of a potentially new fashion style. Summarizing, the contributions of this work are threefold:</p><p>1. The first data-centric strategy tailored to forecasting, used to create an exogenous observation signal which improves forecasts of the performance (number of sold items, popularity) of brand new clothing items with non-existent pasts. 2. A webly-learned method to freely collect information about fashion trends without relying on private or costly repositories. 3. Best overall results on all the tested NFPPF tasks.</p><p>The rest of the paper is organized as follows: related literature is analyzed in Sec. 2; the proposed approach is detailed in Sec. 3; experiments are reported in Sec. 4, and finally; concluding remarks are drawn in Sec. 5.</p><p>2 Related literature NFPPF problem. The NFPPF problem has been deeply investigated in the fields of quantitative fashion design <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">16]</ref>, marketing and social sciences <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b13">12]</ref>, but is relatively new in the computer vision community. In both <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b35">34]</ref>, the main idea is that new products will sell comparably to similar, older products; this similarity is exploited in <ref type="bibr" target="#b35">[34]</ref> via textual tags only, while in <ref type="bibr" target="#b9">[9]</ref> an autoregressive RNN model takes past sales, textual product attributes, and the product image as input, to forecast the item sales. The work in <ref type="bibr" target="#b36">[35]</ref> focuses on the additional direction of checking the past to look for predictive exogenous signals. In particular, the authors exploit Google Trends, querying textual attributes related to the probe and embed the resulting trend into a Transformer-based <ref type="bibr" target="#b39">[38]</ref> architecture, which considers images, text and other metadata. The authors also rendered accessible the first publicly available dataset for NFPPF, VISUELLE. In our paper we follow the idea of looking back to web data, but use images as the main representation of online fashionability, obtaining a richer exogenous  <ref type="figure">Fig. 2</ref>: Schematic pipeline of our approach; we start with a probe image and obtain the POtential Performance (POP) signal at the end. Along this pipeline, we sequentially process information in different modalities, thereby creating a cross-modal signal. signal. Predicting the success of new fashion styles has never been taken into account, with past works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b23">22]</ref> focusing on the standard forecasting setup. Data-centric AI. Data-Centric AI <ref type="bibr" target="#b27">[26]</ref> (DCAI) shifts the attention from the models to the data used to train and evaluate them. It is a topic whose importance is constantly growing in many AI communities <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b28">27</ref>] 5 , with important effects on CV &amp; ML. In general, DCAI investigates methodologies for accelerating open-source dataset creation from lower-quality resources. Consequently, it is tightly coupled with learning on noisy data, which aims at producing consistent and low noise data samples, or removing labeling noise and inconsistencies from existing data <ref type="bibr" target="#b29">[28,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b40">39]</ref>. Our methodology is data-centric, since it automates the creation of training data from a large amount of web resources, while removing labeling noise. Notably, it represents a novelty in the DCAI panorama, since it creates temporally-dependent training data, i.e., time series, as it is required by NFPPF and in general by forecasting tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The goal of our approach is to produce an exogenous variable that can aid a forecasting model in predicting the future performance of a product (sales, popularity). The input to our approach is the probe image z (t) , where z represents the new clothing item and t the observation time, which is the date from when we begin to look into the past. The output is the POP signal S ? R. In this paper, we describe the observation times in terms of weeks and set K past = 52. This translates to looking one year prior to the observation time t, as typically done in fashion market analysis <ref type="bibr" target="#b38">[37]</ref>. The next sections will sequentially detail the general pipeline of our approach, depicted in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Image Tagging</head><p>The first operation is the extraction of textual tags {a (j) z } j=1,...,J associated to z. These tags should represent the clothing item with sufficient generality, a z :{"yellow", "long sleeve"} z (t) (+) a z ={"fashionable","yellow","long sleeve"} at time interval [t-k-W, t-k], k?1,?,K past (-) a z ={"unfashionable","yellow","long sleeve" capturing at least categorical information (e.g "long sleeve") and a dominant color (e.g "yellow"). Empirically, we found these tags to work well while being easily obtainable. Category and color can be automatically extracted with high accuracy <ref type="bibr" target="#b20">[19]</ref> or are usually contained in the technical data sheet accompanying the product, which is what we exploit in this work, as shown in Sec. 4.</p><formula xml:id="formula_0">} at time interval [t-k-W, t-k], k?1,?,K past [t-Kpast -W, t-Kpast] [t-W, t] [t-Kpast -W, t-Kpast] [t-W, t] (+) x 1 (+) x 2 (+) x 3 (+) x 4 (+) x 5 (+) x M ? ? ? ? POSITIVE QUERY EXPANSION NEGATIVE QUERY EXPANSION IMAGE WEB SEARCH IMAGE WEB SEARCH (-) x 1 (-) x 2 (-) x 3 (-) x 4 (-) x 5 (-) x M</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Time-dependent Query Expansion</head><p>The second operation (detailed in <ref type="figure" target="#fig_1">Fig. 3</ref> on a real example) performs two different textual query expansions, generating positive expansions, {a (j) z } j=1,...,J ? J (+) where the additional J (+) tags indicate attractive clothing items, and conversely for negative expansions. In this paper, we found the tags J (+) = "fashionable" and J (?) = "unfashionable" to be the most effective for positive and negative expansions, respectively. Alternatives as "best seller" and "unattractive" were considered, returning similar results.</p><p>Each expansion, either positive or negative, is associated to a particular k = 1, ..., K past for the time interval [t?k?W, t?k], where W is a temporal window we wish to consider for the image search, also expressed in weeks. In our experiments we set W = 4, which translates to having a sliding window of size 4 and stride of 1 over the temporal axis. This allows the pool of downloaded images to disclose what are newly indexed items in relation to previous time steps, developing a temporal locality in the data pool. The precise value of W was chosen after an empirical evaluation over the range 1, ..., 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image Web Search</head><p>A given expanded textual query along with a time interval is fed into a web API request to gather M representative fashionable and unfashionable im-</p><formula xml:id="formula_1">ages (+) {x i } (t?k) i=1,...,M ; (?) {x i } (t?k) i=1,.</formula><p>..,M that have been uploaded in the interval [t ? k ? W, t ? k], for k = 1, . . . , K past . In particular, we adopt Google Image search, selecting the first M = 25 images returned, assuming the ordering of Google Images perfectly mirrors a genuine image relevance <ref type="bibr" target="#b18">[17]</ref>. After the image web search phase, M ? K past fashionable and unfashionable images are collected respectively (as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>). These images are then used to train a binary classifier ?, aimed at distinguishing fashionable from unfashionable images. Webly learning and supervision based on Google Images has been considered before in computer vision, especially for image classification and object detection <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">18]</ref>. POP goes one step further, merging visual and textual search while adding a time-dependent query expansion to create more discriminative image sets. Nevertheless, the labels assigned to the images from the query expansions might be noisy, therefore we apply a confident learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning From Noisy Labels</head><p>In the following, we adapt the confident learning (CL) methodology specifically for our binary problem. For a broader overview, readers may refer to <ref type="bibr" target="#b29">[28]</ref>. Let X = {x i ,? i } 1...N be our set of N = 2(M ? K past ) images with associated observed noisy binary labels? i ? {"fashionable", "unfashionable"}. CL assumes that a true, latent label y * i ? {"fashionable", "unfashionable"} exists for every sample. CL requires two inputs: 1) the out-of-sample N ?2 matrixP of predicted probabilities whereP i,h =p(? i = h; x i , ?) with ? a generic (binary) classifier initially trained on X; 2) the set of noisy labels {? i }. Subsequently, a robust 2 ? 2 confusion matrix, called the confident joint matrix C? ,y * , is computed 6 :</p><formula xml:id="formula_2">C? ,y * (h, l) = |X? =h,y * =l |, wit? X? =h,y * =l = x ? X? =h :p(? = l; x, ?) ? t l<label>(1)</label></formula><p>where t l is a threshold that represents the expected self confidence value for each class:</p><formula xml:id="formula_3">t l = 1 |X? =l | x?X? =lp (? = l; x, ?)<label>(2)</label></formula><p>In practice, C? ,y * counts only those elements which have been confidently classified in a particular class, where the term "confident" means with a probability that is higher than the average probability of an element belonging to that class. In simpler words, if samples labeled as belonging to class h tend to have higher probabilities because the model is over-confident about class h, then t h will be proportionally larger. It also worth noting that Eq. 1 corresponds to a simplified version of the general building procedure of the confident joint matrix C? ,y * of <ref type="bibr" target="#b29">[28]</ref>, which nonetheless in our case is perfectly acceptable since we deal with binary classification and no label collision may happen, i.e., the fact that a noisy label can correspond to a more than a single alternative class.</p><p>On this robust confusion matrix, we estimate label errors from the off diagonal elements of C? ,y * (h, l). Wrongly labeled images are therefore pruned (indicated by the red boxes in <ref type="figure" target="#fig_1">Fig. 3</ref>), obtaining the cleaned fashionable and unfashionable images (</p><formula xml:id="formula_4">+) {x ? i } (t?k) i=1,...,M ?(t?k) ; (?) {x ? i } (t?k) i=1,...,M ??(t?k) , where M ?(t?k) and M ??(t?k)</formula><p>indicate that we can have a different number of positive and negative images, respectively, related to each t ? k time step, due to the noisy sample elimination. The classifier is retrained on the cleaned data, obtaining a robust trained model ? ? . This procedure is data-centric and model agnostic; the specific ? used in this work is described in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Signal Forming</head><p>The POP signal S</p><formula xml:id="formula_5">(t) z = s (t?Kpast) z , . . . , s (t?k) z , . . . , s (t?1) z , is computed by consid- ering the cleaned fashionable images (+) {x ? i } (t?k) i=1,...,M ?(t?k)</formula><p>, the robust model ? ? , and the image z, as follows:</p><formula xml:id="formula_6">s (t?k) z = 1 M ?(t?k) M ?(t?k) i=1 ?? ? (+) x ? i (t?k) ? ? ? (z)? ? ? ? (+) x ? i (t?k) ?? ? ? (z) ? (3)</formula><p>where ? ? (z) indicates the extracted features of z from ? ? , and ??? indicates the dot product. In other words, the signal value s (t?k) z is the average cosine similarity between the embedding of the probe image z and each fashionable image x ? i (t?k) from the M ?(t?k) downloaded images. An assessment of alternative signal forming options is shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In line with the general requirements of DCAI <ref type="bibr" target="#b27">[26]</ref>, we show how our automatically manufactured time series helps a forecasting model ? achieve better results on a given task ?. The main idea behind our approach is that by knowing POP, the forecasting model can gain a context on the past which otherwise would be missing and therefore improve. To demonstrate this, we perform extensive evaluation on two tasks (and different forecasting models): new fashion product sales curve prediction <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b36">35]</ref>, and style popularity forecasting <ref type="bibr" target="#b0">[1]</ref>. We show ablative studies on the first task and an impressive outcome on the second.</p><p>The binary classifier ? for learning on noisy data (Sec. 3.4) is based on a ResNet50 <ref type="bibr" target="#b14">[13]</ref>, pre-trained on ImageNet <ref type="bibr" target="#b8">[8]</ref>, with two additional fully connected layers. During the confident learning procedure, we fine-tune its last convolutional block and fully connected layers for 50 epochs with a batch size of 64, using CE loss, following a 5-fold cross validation protocol. AdamW <ref type="bibr" target="#b22">[21]</ref> is used as optimizer, with a learning rate of 1e ? 4. The forecasting neural network models are all trained for 200 epochs with a batch size of 128 and L2 loss, using the AdaFactor <ref type="bibr" target="#b33">[32]</ref> optimizer. The experiments are performed on two NVIDIA 3090 RTX GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task 1: New Fashion Product Sales Curve Prediction</head><p>The output of a sales curve forecasting model for a probe clothing item z is a time series O</p><formula xml:id="formula_7">(st) z = o (st+1) z , . . . , o (st+k) z , . . . , o (st+K f ut ) z</formula><p>that indicates how many pieces of z will be sold starting at a particular time step st (typically the start of the season), for the next K f ut time steps.</p><p>We run our first set of experiments on the VISUELLE dataset <ref type="bibr" target="#b36">[35]</ref>. For each available product, multi-modal information is provided: i) images, ii) text tags, iii) Google Trends, iv) sales curves. The evaluation protocol follows that of VISUELLE, simulating how a fast-fashion company deals with new products on two particular moments: the first order setup and the release setup. The former takes place when the company decides which products and how many pieces to order by looking at probe images. The latter is right before the season, and is useful to obtain an accurate forecast in order to plan stock replenishment. These two setups use 28 and 52 week long exogenous signals (originally Google Trends <ref type="bibr" target="#b36">[35]</ref>), respectively.</p><p>Note that for the sake of fairness, we do not alter the training setup or models from <ref type="bibr" target="#b36">[35]</ref>, keeping the cardinality and the type of the training data fixed and substituting only the Google Trends with our POP signal. All the models are trained considering the 12-week long sales signals, whilst the evaluation is done on a 6-week horizon. This is shown to give the best predictions while simulating politics of real fashion companies <ref type="bibr" target="#b36">[35]</ref>. We consider 5 algorithms (from oldest to newest): Gradient Boosting for forecasting <ref type="bibr" target="#b16">[15]</ref>, Concat Multi-Modal RNN <ref type="bibr" target="#b9">[9]</ref> (Concat MM RNN in the tables), Residual Multi-Modal RNN <ref type="bibr" target="#b9">[9]</ref> (Residual MM RNN ), Cross-Attention RNN <ref type="bibr" target="#b9">[9]</ref> (X-Attention RNN ) and GTM Transformer <ref type="bibr" target="#b36">[35]</ref> (GTM Transf.) We consider the Weighted Absolute Percentage Error (WAPE) as primary evaluation metric and   <ref type="bibr" target="#b36">[35]</ref>: Note that the WAPE is not bounded by 100. Finally, we measure the similarity of the slope of the predicted curve with the ground truth using the Edit distance with Real Penalty (ERP) <ref type="bibr" target="#b4">[5]</ref>. This metric counts the number of edit operations (insert, delete, replace) that are necessary to transform one series into the other. Because we are dealing with continuous values, a threshold ?=0.03 is used to decide if values are considered different and have to be edited.</p><p>The results are shown in <ref type="table" target="#tab_2">Table 1</ref> for the first order setup and in <ref type="table" target="#tab_3">Table 2</ref> for the release setup. As reference, we also report results without any exogenous series, to show the net value of these indicators. For all the algorithms and both setups, adding POP to the model boosts the performances over all the metrics, reaching the absolute best when coupled with GTM Transformer. On average, in the first order setup, we improve the WAPE by 3.42% over the Google Trends and by 3.21% over not using any exogenous signals. In the release setup we improve by 2.85% over the Google Trends and by 4.23% over not using exogenous signals. These results demonstrate how our data-centric approach can provide optimal forecasts by creating a highly-predictive signal of past popularity that is imagebased, unlike Google Trends. The forecasts are performed on 497 products over different stores, meaning that these improvements can provide a large impact on the supply chain operations.</p><p>In <ref type="figure">Fig. 4</ref> we show the WAPE per clothing category. We mostly perform better than the other training alternatives, yet some particular categories display limitations of our approach. These limitations arise due to the fact that the Image Tagging phase is assumed as flawless, since we rely on the technical sheet accompanying the probe image to extract the tags. The results per category <ref type="figure">(Fig. 4</ref>) display how possibly mislabeled categories, or categories labeled in a general manner ("solid colours","doll dress") may lead to misleading web images. As visible in <ref type="figure">Fig. 5</ref>, the related images from the web, both fashionable and not, are completely useless, since the tag of the category itself is misleading. In such cases, a robust automated category extraction could potentially lead to better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fashionable</head><p>Green Shorts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unfashionable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Green Shorts</head><p>Fashionable</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blue Long Dress</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unfashionable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blue Long Dress</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fashionable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Red Solid Colours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unfashionable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Red Solid Colours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fashionable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grey Doll Dress</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unfashionable</head><p>Grey Doll Dress <ref type="figure">Fig. 5</ref>: Examples of VISUELLE items (seasons SS17 and SS18 on the left, SS19 and AI19 on the right, respectively) and the correspondent fashionable/unfashionable images from the web. Some web images can be misleading, due to the questionable category names of the VISUELLE dataset ("solid colours", "doll dress").</p><p>Ablation studies. In the following, we focus on alternative versions of our proposed pipeline, ablating the specific modules illustrated in <ref type="figure">Fig. 2</ref>. <ref type="table">Table 3</ref> contains all the results. Time dependent query expansion -No expansion: Images are queried with the original tags collected in the Image Tagging phase, without generating positive or negative expansions. This is equivalent to querying only with "color + category". The learning step is impacted directly, since no positive or negative classes are available for learning, therefore we use our backbone model to extract image features. For each image z (t) , the web images</p><formula xml:id="formula_8">{x i } (t?k) i=1,.</formula><p>..,M that have been uploaded in the interval [t ? k ? W, t ? k], for k = 1, . . . , K past are collected. The signal forming Eq. 3 changes accordingly, using all the M downloaded images; -Misaligned past: We modify the query expansions by looking one year earlier than the "correct" past. Given the observation time t of the probe z (t) , instead of looking backwards from t ? 1 weeks to t ? K past , we go from t ? 1 ? K past to t ? 2 ? K past .</p><p>With respect to all the alternative versions in this study, the No expansion ablation gives the worst result. POP provides an improvement of 0.73% and 1.06% WAPE for the first order setup and release setup, respectively. The Misaligned <ref type="table">Table 3</ref>: Alternative versions of our pipeline <ref type="figure">(Fig. 2</ref>) on both the release and first order setups; "W" stands for WAPE, "M" for MAE. Lower is better for all metrics. past yields slightly better results, but still performs worse than POP by 0.63% and 0.22% WAPE for the first order setup and release setup, respectively. This confirms that fashion has an evolution that changes year after year that we have to take into account.</p><p>Learning from noisy data -No learning: A predefined image classification network is used to compute the distance among embeddings of the probe image with the positive, downloaded images. This is equivalent to ablating the "Learning from Noisy Data" phase of <ref type="figure">Fig. 2</ref>. It will highlight the importance of dealing with distances among embeddings which are specifically learned against distances coming from a general purpose network. We utilise the backbone of our binary classifier, specified in the introduction of Sec. 4; -No robust learning: All of the downloaded positive and negative images are used to learn our binary classifier without pruning noisy data by confident learning; -Symmetric cross entropy <ref type="bibr" target="#b40">[39]</ref>: SCE is a robust classification loss; it adds to the standard cross entropy loss a reverse cross entropy term which assumes the predicted labels as ground truth, and the original labels as possibly faulty. In practice, it penalizes noisy labels, without removing any associated training data; -SELFIE <ref type="bibr" target="#b37">[36]</ref>: the key idea is to correct the label of noisy refurnishable samples with high precision, with the help of clean data which is defined as those samples within a mini-batch creating a small loss. Repeated training runs (dubbed "restarts") allow to use more training data, i.e., noisy samples which have been corrected in their labels. In particular, we use 3 restarts, after which 1.1% of both fashionable and unfashionable items have been removed from the training data.</p><p>The results in <ref type="table">Table 3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task 2: Popularity Prediction Of Fashion Styles</head><p>The style popularity prediction task <ref type="bibr" target="#b0">[1]</ref> is different from product sales forecasting in that it considers a popularity signal y based on multiple clothing items. In the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">23]</ref>, style is defined as a latent property of a set of clothing images that share some common visual features. Concretely, in Fashion Forward (FF) <ref type="bibr" target="#b0">[1]</ref>, Non-negative Matrix factorization is applied to extract K styles from the attribute extraction features <ref type="bibr" target="#b20">[19]</ref> of all the product images. Formally, let A ? R M ?N indicate the confidence that each of the M visual attributes is contained in each of the N images. A can be factorized into two matrices with non-negative entries:</p><formula xml:id="formula_9">A ? WH, W ? R M ?K and H ? R K?N<label>(4)</label></formula><p>where W represents the confidence that each attribute is part of a style and H represents the confidence that each style is associated to an image. The popularity signal y for a style k is built by considering the interactions in the Amazon Reviews dataset <ref type="bibr" target="#b25">[24]</ref> of all the items {z} ? A at time t, weighted by their style membership H(k, z). For a detailed explanation, we refer to <ref type="bibr" target="#b0">[1]</ref>. To extend this problem to a NFPPF setup, we have to imagine we are evaluating the performance of a brand new style that does not have a past. The purpose of POP becomes replacing the original style popularity series. This means that POP has to be modified to deal with a style and not with a single clothing item, where two challenges are presented: 1) To verify how similar POP is to the ground truth popularity signal and; 2) To check if POP is highly predictive of the future popularity.</p><p>To deal with the first challenge, we consider for each style k the 2 textual attributes <ref type="bibr" target="#b20">[19]</ref> w 1 , w 2 (extracted from W) with the highest confidence scores and use them for the time dependent query expansion. FF provides the only dataset for style forecasting where both images and product metadata are available. The task is to predict a popularity score on a yearly basis. The data ranges from <ref type="bibr">[2008 ? 2013]</ref>, but since Google Images returns little to no images for queries before 2010, we use the range [2010 ? 2013] in our experiments. We set K past = 208, meaning that we investigate 4 years back. In this way we can create a weekly series for each year and use the average as the value representing the popularity for that year. As probe image to create our POP signal, we consider the top 10 images {z} that represent a style (based on their membership weight H(k, z)). Each image will lead to one POP signal, which we average together to obtain the POP style signal. This process is repeated for all the dataset partitions presented in FF. To deal with the second challenge, we adopt the best performing statistical forecasting techniques from Fashion Forward and feed them the style POP signal described above. For more details on the forecasting techniques, we refer the reader to <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b3">4]</ref>:</p><p>1. Naive methods. These methods infer by utilizing general information from the training data. Mean forecasts the future as the mean of past observations, while Last as the last observed value. Drift is the same as Last, but the forecasts change over time based on the global trend of the series; 2. Auto Regressive and Moving Average methods. These methods forecast using a linear combination of some past observations in a regression Style attributes: "abstract geo print", "a-line" <ref type="bibr">(Oracle)</ref> Style attributes: "audrey", "asymmetric" <ref type="figure">Fig. 7</ref>: Qualitative results on the forecasting of two different styles from FF, represented by their respective "style-defining" images and top (automatically extracted) attributes. In both cases, POP and the ground-truth (GT) style popularity from Fashion Forward are substantially similar. The plot on the right shows a forecasting failure case, which holds for both POP and the GT).</p><p>framework. The most famous and representative method of this class is the AutoRegressive Integrated Moving Average(ARIMA) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Simple Exponential Smoothing. Stands for simple exponential smooth-</head><p>ing, is a weighted average of previous observations where the weights decrease exponentially as we go further in the past.</p><p>Following the protocol of <ref type="bibr" target="#b0">[1]</ref>, all models are trained on all but the last timestep, which is used for testing. We utilise the mean absolute percentage error (MAPE) and the mean absolute error (MAE) to evaluate the forecasting accuracy on the last timestep of the signal stemming from FF. To provide an additional comparison, we show additional results using Google Trends as the substitute popularity time series <ref type="bibr" target="#b36">[35]</ref>. Note that to obtain fair and comparable results, all the signals are rescaled in the range [0,1] using min-max normalization. The results are shown in <ref type="table" target="#tab_6">Table 4</ref>, where Oracle refers to the original ground-truth style popularity series given as input to the forecasting models. POP proves to be a natural substitute to the GT style popularity time series and it allows for optimal forecasts, providing better results than the GT signal itself for the Dresses partition. On the other hand, Google Trends are not able to convey such similarities, partially because searching only for the popularity of textual tags might not provide a series that is as predictive as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Metaphorically, our approach performs a kind of "time travel": it sends a fashion probe image in the past, before its launch in the market. It then models the popularity from that past point forward by relying on highly ranked web images, queried by using general textual tags related to the probe. The probe similarity with the past is then shown to be a good exogenous indicator for future performance. This pipeline provides a new, effective and data-centric scheme for NFPPF problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Appendix</head><p>The supplementary material discusses the following topics, in order:</p><p>1. Additional ablation study on the signal forming step of POP (Sec. 6.1); 2. Details on the training data and forecasting horizon on the VISUELLE dataset (Sec. 6.2); 3. Complete results on all Fashion Foward (FF) dataset partititions for the new style popularity forecasting task (Sec. 6.3); 4. Qualitative demonstrations of the downloaded images from our temporal cross-modal query expansion, along with interesting insights discovered on the fashionability of different categories and colors (Sec. 6.4); 5. Inspired by latest, established guidelines in CV &amp; ML research, a discussion on ethical, social and economical implications of our work (Sec. 6.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Additional POP ablations</head><p>This section provides an additional ablation related to the ones shown in Sec. 4.1 and Tab. 3 in the main paper, concerning the signal forming step of the POP signal. This is done by varying the embeddings of the cleaned images by CL that are used as input to Equation 3 (Sec. 3.5 in the main paper), based on the following strategies:</p><p>-Negative: it indicates the average distance of z with the pruned unfashionable</p><formula xml:id="formula_10">images (?) {x ? i } (t?k) i=1,...,M ??(t?k)</formula><p>, substituting the positive ones from Equation 3; -Positive and Negative: here we fed into the forecasting approach two signals, the original POP and the Negative one, making POP a multivariate (2D) time series.</p><p>We discover that the Negative approach gives some boost, probably accounting for how much the probe has to be dissimilar to unfashionable items. On the other hand, Positive and Negative shows a decrease in comparison to using only the pruned fashionable images, probably because the two signals are complementary (Tab. 5). Nevertheless, all the ablated approaches still provide an exogenous time series that helps models perform better when they consider it as additional input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Additional details on VISUELLE</head><p>The VISUELLE dataset from <ref type="bibr" target="#b36">[35]</ref> contains the following, multi-modal information for each product: i) Images, ii) Text tags, iii) Google Trends, iv) Sales curves.</p><p>-Images: RGB images with an average size of 577 ? 227; -Text: Three types of text tags (category, dominant color, fabric) that come partially from the technical sheet of the product (category, fabric), while the dominant color has been automatically extracted by the authors of the dataset. -Google Trends: Three 52-step long time series for each product, one for each of the tags listed above, extracted by the Google Trend platform in robust way <ref type="bibr" target="#b26">[25]</ref> for the 52 weeks prior the sale start date; -Sales curve: 12-step long weekly time series, reporting the sales of a particular item.</p><p>The testing product set is composed of the 497 most novel products of the two most recent seasons (Spring-Summer and Autumn-Winter 2019). The rest of the dataset (5080 products) is used for training. We utilise all of this data for the experiments in Sec 4.1 of the main paper. Additional forecasting results are displayed in <ref type="figure" target="#fig_3">Fig. 8</ref> (in terms of WAPE), exhibiting the behaviour of the best performing model for all possible forecasting horizons.  <ref type="bibr" target="#b36">[35]</ref> on the VISUELLE dataset. After six weeks there is a long enough history to model tendencies in the sales without considering product discounts or replenishments, unlike longer horizons. This is also reflected in the WAPE values, which keep increasing for forecasting horizons longer than six weeks. POP improves the forecasts for any horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Complete results on Fashion Forward</head><p>We show the full new style popularity forecasting results on all dataset partitions of Fashion Forward in Tab. 6, providing an extended version of Tab. 4 from the main text. The results show how POP can be used to estimate the intra dataset popularity trends of products from Fashion Forward with relatively high accuracy for all partitions. We would like to emphasize again that we use POP as input to forecasting models and compare the predictions with the ground-truth testing set that comes from the FF time series. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Qualitative Results</head><p>In this section we report a qualitative analysis of the POP signal, which in the main paper was limited to <ref type="figure" target="#fig_1">Fig.3</ref> due to space limitations. These results give additional insight on the significance of our time-dependent, data-centric approach.  In all the cases, using POP gives better results than not using it or using other, similar exagenous series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Under the hood of the POP signal</head><p>In <ref type="figure" target="#fig_5">Fig. 10</ref> and <ref type="figure" target="#fig_6">Fig. 11</ref> we report two examples of the (automatically) downloaded images used for the formation of the POP signal. In both figures, the probe images from which we extract the textual attributes to index the search are depicted. The analysis for each figure is reported in the corresponding caption. We also report in the figures some pruned images by the confident learning step, marked by a red cross. : Examples of images downloaded for the query 'Grey Long Sleeves" (after pruning by confident learning). One may note that mismatching images are very few, intended as those images which are not containing any "Grey Long Sleeves". An example would be the green sleeve + blue jeans in the bottom row. It is worth noting how most of the fashionable items have no printed logos, texture or tight sleeves. On the contrary,"Unfashionable Grey Long Sleeves" have big logo on them, with a winter theme, and many colors accompanying a gray background. In some cases, the gray color actually covers a small portion of the clothing item. : Examples of images downloaded for the query 'Violet Long Sleeve" (after pruning by confident learning). The "Fashionable Violet Long Sleeve" items seem to have a darker tone in most cases. Very long sleeves fade into dresses, indicating the length of the garment as an important aspect for making it fashionable. Curiously, "Unfashionable Violet Long Sleeve" contain brighter colors, short garments (like pyjamas) with writings or printed images. Pruned images are marked with a red cross.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">The importance of a time-dependent query</head><p>In the main paper, we report in Tab. 3 various ablation studies on POP. Here we focus on two aspects in particular: the "Time Dependent Query Expansion", and the "misaligned Past". The obtained results suggest that exploiting (un)fashionable images not related to the date of delivery on the market gives worse results in terms of forecasting. <ref type="figure">Fig. 12</ref> qualitatively demonstrates why this is the case. As it is visible, what made a garment of a particular type and color fashionable in 2017 ( <ref type="figure">Fig. 12, top)</ref> does not correspond to the same visual elements that can be found in 2019 <ref type="figure">(Fig. 12, bottom)</ref>. More specifically, throughout the spring/summer season of 2017, the green kimonos tend to be heavily associated with white patterns and the color white in general. In 2019, the kimonos are almost all in different shades of green or even dark green.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Ethical Concerns and Societal Impact</head><p>Ethical implications could, in principle, arise from the web image search: observed images can, for example, contain copyrighted images. Nevertheless, just as a normal user would use Google Images to gather an opinion of what could be trending in fashion, so do we, albeit automatically. In particular, we do not need to personally look at the web images (apart for the ones shown in the paper and here for explanatory purposes), since POP is a just a numerical time series.</p><p>As for the societal impact, our approach can be highly beneficial for fast fashion, which is the third most polluting industry in the world. Having a precise estimation of sales or popularity can improve the situation by solving supply chain issues and our pipeline can play a leading part. Ameliorated forecasts with POP also have a big impact at the economic level, in terms of profit. The best forecasting model on the VISUELLE dataset (which uses our generated time series, as seen in <ref type="table" target="#tab_2">Tables 1 and 2</ref> in the main paper), allows to spare 21% w.r.t. ordinary guidelines for new fast-fashion products, reducing a loss of $4.390.400 US dollars to $3.491.600 US dollars, assuming a general price of 28$ per piece for all products (independently on the category).</p><p>?Fashionable? Green Kimono Dress (over the years) t-K past t 2019-09-12 2018-09-13 2017-10-01 2016-10-02 <ref type="figure">Fig. 12</ref>: Examples of Fashionable downloaded images for particular timedepended queries. In this particular case, for the query "green kimono dress", it can be seen how the notion of fashionability can have significant variations over time. Notably, green kimonos in 2017, as seen in the latter half of the first figure, tend to be heavily associated with white patterns and the color white in general. In 2019, this trend appears to be dying out, with the kimonos being of different shades of green or even dark green.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Pipeline insights on Time-dependent Query Expansion (Sec. 3.2), Image Web Search (Sec. 3.3) and Learning From Noisy Labels (Sec. 3.4) steps. This figure reports a real world excerpt of the download and processing of N =2600 images (N = 2(M ? Kpast), M = 25, Kpast=52).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative results for the sales forecast of two different products on VI-SUELLE, considering all 12 time-steps. In all cases, using POP provides better forecasts. In the bottom row (bottom right plot), we show a forecasting failure case, where the product is discounted in its final week of sales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 :</head><label>8</label><figDesc>WAPE for different forecasting horizons and exogenous signals, using GTM-Transformer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 :</head><label>9</label><figDesc>Qualitative results on VISUELLE, considering all the 12 time-steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10: Examples of images downloaded for the query 'Grey Long Sleeves" (after pruning by confident learning). One may note that mismatching images are very few, intended as those images which are not containing any "Grey Long Sleeves". An example would be the green sleeve + blue jeans in the bottom row. It is worth noting how most of the fashionable items have no printed logos, texture or tight sleeves. On the contrary,"Unfashionable Grey Long Sleeves" have big logo on them, with a winter theme, and many colors accompanying a gray background. In some cases, the gray color actually covers a small portion of the clothing item. Pruned images are marked with a red cross.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11</head><label>11</label><figDesc>Pruned images are marked with a red cross.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Forecasting WAPE results per clothing category; the larger the blob, the higher the # of items in that category; the color below each category name indicates the type of training setup which gives the best WAPE.</figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">No Signal GoogleTrends POP</cell></row><row><cell>WAPE</cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>capris</cell><cell>culottes</cell><cell>doll dress</cell><cell>drop sleeve</cell><cell>gitana skirt</cell><cell>kimono dress</cell><cell>long cardigan</cell><cell>long coat</cell><cell>long dress</cell><cell>long duster</cell><cell>long sleeve</cell><cell>maxi</cell><cell>medium cardigan</cell><cell>medium coat</cell><cell>miniskirt</cell><cell>patterned</cell><cell>printed</cell><cell>sheath dress</cell><cell>shirt dress</cell><cell>short cardigan</cell><cell>short coat</cell><cell>short sleeves</cell><cell>shorts</cell><cell>sleeveless</cell><cell>solid colours</cell><cell>tracksuit</cell><cell>trapeze dress</cell></row><row><cell cols="2">Fig. 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on VISUELLE with the first order setup; "W" stands for WAPE, "M" for MAE. Lower is better for all metrics.<ref type="bibr" target="#b36">35</ref>.02 0.43 63.31 34.41 0.42 64.26 34.92 0.44 59.49 32.33 0.38 56.62 30.93 0.37 Google Trends 64.29 35.12 0.43 64.11 34.84 0.43 68.11 37.02 0.47 58.70 31.90 0.38 56.83 31.05 0.35 POP Signal 63.75 34.83 0.42 58.09 31.73 0.39 58.88 32.16 0.39 57.78 31.56 0.38 53.41 29.18 0.32</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">First Order Setup (Kbest = 28 weeks)</cell></row><row><cell>Exogenous</cell><cell cols="2">Gradient</cell><cell></cell><cell>Concat</cell><cell></cell><cell>Residual</cell><cell>X-Attention</cell><cell>GTM</cell></row><row><cell>Signal</cell><cell cols="2">Boosting</cell><cell cols="2">MM RNN</cell><cell cols="2">MM RNN</cell><cell>RNN</cell><cell>Transformer</cell></row><row><cell></cell><cell cols="2">[15] 2020</cell><cell></cell><cell>[9] 2020</cell><cell></cell><cell>[9] 2020</cell><cell>[9] 2020</cell><cell>[35] 2021</cell></row><row><cell></cell><cell>W</cell><cell cols="2">M ERP W</cell><cell cols="2">M ERP W</cell><cell>M ERP W</cell><cell>M ERP W</cell><cell>M ERP</cell></row><row><cell>No Signal</cell><cell>64.10</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Release Setup (Kbest = 52 weeks)</cell></row><row><cell>Exogenous</cell><cell cols="2">Gradient</cell><cell></cell><cell>Concat</cell><cell></cell><cell>Residual</cell><cell>X-Attention</cell><cell>GTM</cell></row><row><cell>Signal</cell><cell></cell><cell>Boosting</cell><cell cols="2">MM RNN</cell><cell cols="2">MM RNN</cell><cell>RNN</cell><cell>Transformer</cell></row><row><cell></cell><cell cols="2">[15] 2020</cell><cell></cell><cell>[9] 2020</cell><cell></cell><cell>[9] 2020</cell><cell>[9] 2020</cell><cell>[35] 2021</cell></row><row><cell></cell><cell>W</cell><cell cols="2">M ERP W</cell><cell cols="2">M ERP W</cell><cell>M ERP W</cell><cell>M ERP W</cell><cell>M ERP</cell></row><row><cell>No Signal</cell><cell>64</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>on VISUELLE with the release setup; "W" stands for WAPE, "M" for MAE. Lower is better for all metrics..10 35.02 0.43 63.31 34.41 0.42 64.26 34.92 0.44 59.49 32.33 0.38 56.62 30.93 0.37 Google Trends 63.52 34.70 0.42 65.87 35.80 0.44 68.46 37.21 0.48 59.02 32.08 0.38 55.24 30.18 0.33 POP Signal 63.38 34.62 0.42 57.43 31.37 0.36 58.38 31.89 0.39 57.36 31.33 0.36 52.39 28.62 0.29 also compute the Mean Absolute Error (MAE) to demonstrate the error on an absolute scale</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>show slightly different performances, promoting the general idea of learning from webly data. No learning gives the worse performance, indicating that a fine tuning on the web data is beneficial (53.03 and 53.83 WAPE); when learning is done on the web data, there is some increase (52.81 and 53.59 WAPE); when learning is robust to label noise, with SCE, performances are better (52.63 and 53.58 WAPE); removing some outliers with SELFIE gives a further help (52.56 and 53.51 WAPE). Confident learning remains the best solution, with 52.39 and 53.41 WAPE, while removing 0.8% and 1.1% of fashionable and unfashionable items respectively, from the 14,500,200 images mined using our cross-modal pipeline.</figDesc><table><row><cell>Sales</cell><cell cols="5">25 50 75 100 MAE: 38.499 -WAPE: 87.663 No Signal GT Forecasted Sales</cell><cell cols="5">20 40 60 80 MAE: 21.774 -WAPE: 49.579 Sales Google Trends</cell><cell cols="3">20 40 60 80 MAE: 10.991 -WAPE: 25.026 POP</cell></row><row><cell>(a)</cell><cell>1</cell><cell>4</cell><cell>8 Weeks</cell><cell>12</cell><cell></cell><cell>1</cell><cell>4</cell><cell>8 Weeks</cell><cell>12</cell><cell></cell><cell>1</cell><cell>4</cell><cell>8 Weeks</cell><cell>12</cell></row><row><cell></cell><cell cols="4">60 MAE: 12.248 -WAPE: 37.210 No Signal</cell><cell cols="5">60 MAE: 20.216 -WAPE: 61.415 Google Trends</cell><cell cols="4">60 MAE: 8.327 -WAPE: 25.296 POP</cell></row><row><cell>Sales</cell><cell>40</cell><cell></cell><cell></cell><cell>Sales</cell><cell cols="2">40</cell><cell></cell><cell></cell><cell>Sales</cell><cell cols="2">40</cell><cell></cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell cols="2">20</cell><cell></cell><cell></cell><cell></cell><cell cols="2">20</cell><cell></cell></row><row><cell>(b)</cell><cell>1</cell><cell>4</cell><cell>8 Weeks</cell><cell>12</cell><cell></cell><cell>1</cell><cell>4</cell><cell>8 Weeks</cell><cell>12</cell><cell></cell><cell>1</cell><cell>4</cell><cell>8 Weeks</cell><cell>12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Average results over all Fashion Forward [1] dataset partitions and specific results for the Dresses partition, where POP outperforms even the original GT style popularity time series (Oracle).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Global Average</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Signals</cell><cell>Mean</cell><cell>Last</cell><cell>Drift</cell><cell>AR</cell><cell>ARIMA</cell><cell>SES</cell></row><row><cell></cell><cell cols="6">MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE</cell></row><row><cell>Oracle</cell><cell cols="6">0.136 0.170 0.093 0.114 0.174 0.222 0.271 0.403 0.136 0.167 0.094 0.116</cell></row><row><cell cols="7">GoogleTrends 0.846 1.000 0.846 1.000 0.846 1.000 0.846 1.000 0.846 1.000 0.846 1.000</cell></row><row><cell>POP</cell><cell cols="6">0.152 0.192 0.116 0.144 0.182 0.229 0.281 0.418 0.235 0.293 0.125 0.156</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dresses</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Signals</cell><cell>Mean</cell><cell>Last</cell><cell>Drift</cell><cell>AR</cell><cell>ARIMA</cell><cell>SES</cell></row><row><cell></cell><cell cols="6">MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE</cell></row><row><cell>Oracle</cell><cell cols="6">0.155 0.197 0.130 0.158 0.203 0.263 0.307 0.409 0.173 0.209 0.129 0.157</cell></row><row><cell cols="7">GoogleTrends 0.849 1.000 0.849 1.000 0.849 1.000 0.849 1.000 0.849 1.000 0.849 1.000</cell></row><row><cell>POP</cell><cell cols="6">0.119 0.157 0.108 0.127 0.173 0.216 0.229 0.334 0.162 0.193 0.109 0.130</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Alternative versions of the signal forming step, comparing to the one proposed in Sec. 3.5, represented here as POP, on both the first order setup and release setup of VISUELLE. Lower is better for both metrics.</figDesc><table><row><cell></cell><cell cols="3">First Order Setup Release Setup</cell></row><row><cell>Strategy</cell><cell cols="2">WAPE MAE</cell><cell>WAPE MAE</cell></row><row><cell>Negative</cell><cell>52.68</cell><cell>28.78</cell><cell>53.90 29.44</cell></row><row><cell>Positive and Negative</cell><cell>52.97</cell><cell>28.94</cell><cell>54.35 29.69</cell></row><row><cell>POP</cell><cell>52.39</cell><cell>28.62</cell><cell>53.41 29.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results across all the Fashion Forward<ref type="bibr" target="#b0">[1]</ref> datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Global Average</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Signals</cell><cell>Mean</cell><cell>Last</cell><cell>Drift</cell><cell>AR</cell><cell>ARIMA</cell><cell>SES</cell></row><row><cell></cell><cell cols="6">MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE</cell></row><row><cell>Oracle</cell><cell cols="6">0.136 0.170 0.093 0.114 0.174 0.222 0.271 0.403 0.136 0.167 0.094 0.116</cell></row><row><cell cols="7">GoogleTrends 0.846 1.000 0.846 1.000 0.846 1.000 0.846 1.000 0.846 1.000 0.846 1.000</cell></row><row><cell>POP</cell><cell cols="6">0.152 0.192 0.116 0.144 0.182 0.229 0.281 0.418 0.235 0.293 0.125 0.156</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dresses</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Signals</cell><cell>Mean</cell><cell>Last</cell><cell>Drift</cell><cell>AR</cell><cell>ARIMA</cell><cell>SES</cell></row><row><cell></cell><cell cols="6">MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE</cell></row><row><cell>Oracle</cell><cell cols="6">0.155 0.197 0.130 0.158 0.203 0.263 0.307 0.409 0.173 0.209 0.129 0.157</cell></row><row><cell cols="7">GoogleTrends 0.849 1.000 0.849 1.000 0.849 1.000 0.849 1.000 0.849 1.000 0.849 1.000</cell></row><row><cell>POP</cell><cell cols="6">0.119 0.157 0.108 0.127 0.173 0.216 0.229 0.334 0.162 0.193 0.109 0.130</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Shirts</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Signals</cell><cell>Mean</cell><cell>Last</cell><cell>Drift</cell><cell>AR</cell><cell>ARIMA</cell><cell>SES</cell></row><row><cell></cell><cell cols="6">MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE</cell></row><row><cell>Oracle</cell><cell cols="6">0.122 0.149 0.075 0.097 0.148 0.190 0.301 0.371 0.126 0.159 0.080 0.103</cell></row><row><cell cols="7">GoogleTrends 0.840 1.000 0.840 1.000 0.840 1.000 0.840 1.000 0.840 1.000 0.840 1.000</cell></row><row><cell>POP</cell><cell cols="6">0.144 0.175 0.109 0.152 0.166 0.215 0.274 0.336 0.139 0.189 0.111 0.151</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Tops&amp;Tees</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Signals</cell><cell>Mean</cell><cell>Last</cell><cell>Drift</cell><cell>AR</cell><cell>ARIMA</cell><cell>SES</cell></row><row><cell></cell><cell cols="6">MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE MAE MAPE</cell></row><row><cell>Oracle</cell><cell cols="6">0.132 0.165 0.074 0.087 0.172 0.212 0.206 0.429 0.108 0.133 0.073 0.087</cell></row><row><cell cols="7">GoogleTrends 0.848 1.000 0.848 1.000 0.848 1.000 0.848 1.000 0.848 1.000 0.848 1.000</cell></row><row><cell>POP</cell><cell cols="6">0.193 0.245 0.131 0.153 0.206 0.257 0.341 0.585 0.405 0.497 0.156 0.186</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A commercial example is Trendstop https://www.trendstop.com/ and its "Trend Platform Membership" service 4 Several such tools are available, for instance https://www.tg3ds.com/ 3d-fashion-design-tools.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://datacentricai.org/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We drop the index i for clarity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by the Italian MIUR through the project "Dipartimenti di Eccellenza 2018-2022".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fashion forward: Forecasting visual style in fashion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data-centric explanations: Explaining training data of machine learning systems to promote transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Anik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrating human judgement into quantitative forecasting methods: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fahimnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Siemsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Omega</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ljung</surname></persName>
		</author>
		<title level="m">Time Series Analysis: Forecasting and Control</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the marriage of lp-norms and edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth international conference on Very large data bases</title>
		<meeting>the Thirtieth international conference on Very large data bases</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">https:/doi.ieeecomputersociety.org/10.1109/ICCV.2015.168</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/ICCV.2015.168" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fashion meets computer vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hidayati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention based Multi-Modal New Product Sales Time-series Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ekambaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manglik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S K</forename><surname>Sajja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raykar</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3394486.3403362</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3394486.3403362" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning object categories from google&apos;s image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision (ICCV&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1816" to="1823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2005.142</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2005.142" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Retail forecasting: Research and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fildes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolassa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fashion forecasting: an overview from material culture to industry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fashion Marketing and Management</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>An International Journal</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Forecasting: Principles and Practice. OTexts, Australia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Athanasopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>2nd edn.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Explainable boosted linear regression for time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>G?rg?l?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cevik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Baydogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fashionq: An interactive tool for analyzing fashion style trend with quantitative criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visualrank: Applying pagerank to large-scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1877" to="1890" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning from large-scale noisy web data with ubiquitous reweighting for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2961910</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2961910" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1808" to="1814" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dressing for Attention: Outfit Based Fashion Popularity Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2019.8803461</idno>
		<ptr target="https://doi.org/10.1109/ICIP.2019.8803461" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="2381" to="8549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge Enhanced Neural Fashion Trend Forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3372278.3390677</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/3372278.3390677" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval. ACM, Dublin Ireland</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval. ACM, Dublin Ireland</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Knowledge enhanced neural fashion trend forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3372278.3390677</idno>
		<ptr target="https://doi.org/10.1145/3372278.3390677" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval. ICMR &apos;20, Association for Computing Machinery</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval. ICMR &apos;20, Association for Computing Machinery<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766462.2767755</idno>
		<ptr target="https://doi.org/10.1145/2766462.2767755" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 43-52. SIGIR &apos;15</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 43-52. SIGIR &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The proper use of google trends in forecasting models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Medeiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Pires</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A data-centric approach for training deep neural networks with less data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Motamedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sakharnykh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kaldewey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03613</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A chat with andrew on mlops: From model-centric to data-centric ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=06-AZXmwHjo" />
		<imprint>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Confident learning: Estimating uncertainty in dataset labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pervasive label errors in test sets destabilize machine learning benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chipbrain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comparative study on fashion demand forecasting models with multiple sources of uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Demand forecasting in retail operations for fashionable products: methods, practices, and real case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Siqin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<idno>PMLR (10-15</idno>
		<ptr target="https://proceedings.mlr.press/v80/shazeer18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>Dy, J., Krause, A.</editor>
		<meeting>the 35th International Conference on Machine Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Googling fashion: forecasting fashion consumer behaviour using google trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">?</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fashion Retail: Forecasting Demand for New Items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.01960" />
	</analytic>
	<monogr>
		<title level="m">26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Well googled is half done: Multimodal forecasting of new fashion product sales with image-based google trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Skenderi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Joppi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denitto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.09824</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The fundamentals of fashion design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sorger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Udale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Bloomsbury Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
