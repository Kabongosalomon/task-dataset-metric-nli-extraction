<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Matters in Unsupervised Optical Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Stone</surname></persName>
							<email>austinstone@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
							<email>barron@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
							<email>konolige@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
							<email>anelia@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Robotics at Google</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Google AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What Matters in Unsupervised Optical Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We systematically compare and analyze a set of key components in unsupervised optical flow to identify which photometric loss, occlusion handling, and smoothness regularization is most effective. Alongside this investigation we construct a number of novel improvements to unsupervised flow models, such as cost volume normalization, stopping the gradient at the occlusion mask, encouraging smoothness before upsampling the flow field, and continual self-supervision with image resizing. By combining the results of our investigation with our improved model components, we are able to present a new unsupervised flow technique that significantly outperforms the previous unsupervised state-of-theart and performs on par with supervised FlowNet2 on the KITTI 2015 dataset, while also being significantly simpler than related approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Optical flow is a key representation in computer vision that describes the pixellevel correspondence between two images. Since optical flow is useful for estimating motion, disparity, and semantic correspondence, improvements in optical flow directly benefit downstream tasks such as visual odometry, stereo depth estimation, and object tracking. The performance of optical flow techniques has recently seen dramatic improvements, due to the widespread adoption of deep learning. Because ground-truth labels for dense optical flow are difficult to obtain for real image pairs, supervised optical flow techniques are primarily trained using synthetic data <ref type="bibr" target="#b4">[5]</ref>. Although models trained on synthetic data often generalize well to real images, there is an inherent mismatch between these two data sources that those approaches may struggle to overcome <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref> Though non-synthetic data for training supervised optical flow techniques is scarce, the data required to train an unsupervised model is abundant: all that training requires is unlabeled video, of which there are countless hours freely available on the internet. If an unsupervised approach could leverage this abundant and diverse real data, it would produce an optical flow model that does not suffer from any mismatch between its training data and its test data, and could presumably produce higher-quality results. The core assumption shared by unsupervised optical flow techniques is that an object's appearance does not change as it moves, which allows these models to be trained using unlabeled video as follows: The model is used to estimate a flow field between two images, that flow field is used to warp one image to match the other, and then the model weights are updated so as to minimize the difference between those two imagesand to accommodate some form of regularization.</p><p>Although all unsupervised optical flow methods share this basic idea, their details vary greatly. In this work we systematically compare, improve, and integrate key components to further our understanding and provide a unified framework for unsupervised optical flow. Our contributions are: 1. We systematically compare key components of unsupervised optical flow, such as photometric losses, occlusion estimation techniques, self-supervision, and smoothness constraints, and we analyze the effect of other choices, such as pretraining, image resolution, data augmentation, and batch size. 2. We propose four improvements to these key components: cost volume normalization, gradient stopping for occlusion estimation, applying smoothness at the native flow resolution, and image resizing for self-supervision. 3. We integrate the best performing improved components in a unified framework for unsupervised optical flow (UFlow for short) that sets a new state of the art -even compared to substantially more complex methods that estimate flow from multiple frames or co-train flow with monocular or stereo depth estimation. To facilitate future research, our source code is available at https: //github.com/google-research/google-research/tree/master/uflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The motion between an object and a viewer causes apparent movement of brightness patterns in the image <ref type="bibr" target="#b6">[7]</ref>. Optical flow techniques attempt to invert this relationship to recover a motion estimate <ref type="bibr" target="#b15">[16]</ref>. Classical methods infer optical flow for a pair of images by minimizing a loss function that measures photometric consistency and smoothness <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref>. Recent approaches reframe optical flow estimation as a learning problem in which a CNN-based model regresses from a pair of images to a flow field <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>. Some models incorporate ideas from earlier methods, such as cost volumes and coarse-to-fine warping <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32]</ref>. These supervised approaches require representative training data with accurate optical flow labels. Though such data can be generated for rigid objects with known geometry <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, recovering this ground truth flow for arbitrary scenes is laborious, and requires approaches as unusual as manually painting scenes with textured fluorescent paint and imaging it under ultraviolet light <ref type="bibr" target="#b0">[1]</ref>. Since such approaches scale poorly, supervised methods have mainly relied on synthetic data for training, and often for evaluation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Synthesizing "good" training data (such that learned models generalize to real images) is itself a hard research problem, requiring careful consideration of scene content, camera motion, lens distortion, and sensor degradation <ref type="bibr" target="#b16">[17]</ref>. Unsupervised approaches circumvent the need for labels by optimizing photometric consistency with some regularization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>, similar to the classical optimization-based methods mentioned above. Where traditional methods solve an optimization problem for each image pair, unsupervised learning jointly op-timizes an objective across all pairs in a dataset and learns a function that regresses a flow field from images. This approach has two advantages: 1) inference is fast because optimization is only performed during training, and 2) by jointly optimizing across the whole train set, information is shared across image pairs which can potentially improve performance. This unsupervised approach was extended to use edge-aware smoothness <ref type="bibr" target="#b29">[30]</ref>, a bi-directional Census loss <ref type="bibr" target="#b17">[18]</ref>, different forms of occlusion estimation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref>, self-supervision <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, and estimation from multiple frames <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. Other extensions introduced geometric reasoning through epipolar constraints <ref type="bibr" target="#b34">[35]</ref> or by co-training optical flow with depth and ego-motion models from monocular <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> or stereo input <ref type="bibr" target="#b28">[29]</ref>.</p><p>These works have pushed the state of the art and generated a range of ideas for unsupervised optical flow. But since each of them evaluates a different combination of ideas, it is unclear how individual ideas compare to each other and which ideas combine well together. For example, the methods OAFlow <ref type="bibr" target="#b29">[30]</ref> and DDFlow <ref type="bibr" target="#b13">[14]</ref> use different photometric losses and different ways to mask occlusions, and OAFlow uses an edge-aware smoothness loss while DDFlow regularizes learning through self-supervision. DDFlow performs better than OAFlow, but does this mean that every component of DDFlow is better than every component of OAFlow? The ablation studies often presented in these papers show that each novel contribution of each work does indeed improve the performance of each individual model, but they do not provide a guarantee that each such contribution will always improve performance when added to any other model. Our work addresses this problem by systematically comparing and combining photometric losses (L1, Charbonnier <ref type="bibr" target="#b23">[24]</ref>, Census <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, and structural similarity <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>), different methods for occlusion estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>, first order and second order edge-aware smoothness <ref type="bibr" target="#b26">[27]</ref>, and self-supervision <ref type="bibr" target="#b13">[14]</ref>. Our work also improves cost volume computation, occlusion estimation, smoothness, and self-supervision and integrates all components into an state of the art framework for unsupervised optical, while being simpler than many proposed methods to form a solid base for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries on Unsupervised Optical Flow</head><p>The task of estimating optical flow can be defined as follows: Given two color images I <ref type="bibr" target="#b0">(1)</ref> , I (2) ? R H?W ?3 , we want to estimate the flow field V (1) ? R H?W ?2 , which for each pixel in I (1) denotes the relative position of its corresponding pixel in I <ref type="bibr" target="#b1">(2)</ref> . Note that optical flow is an asymmetric representation of pixel motion: V (1) provides a flow vector for each pixel in I <ref type="bibr" target="#b0">(1)</ref> , but to find a mapping from image 2 back to image 1, one would need to estimate V <ref type="bibr" target="#b1">(2)</ref> .</p><p>In the context of unsupervised learning, we want to find a function V (1) = f ? (I <ref type="bibr" target="#b0">(1)</ref> , I <ref type="bibr" target="#b1">(2)</ref> ) with parameters ? learned from a set of image sequences D = {(I <ref type="bibr" target="#b0">(1)</ref> , I <ref type="bibr" target="#b1">(2)</ref> , . . . , I (N ) )}. Because we lack ground truth flow, we must define a proxy objective L(D, ?), such as photometric consistency between I (1) and I <ref type="bibr" target="#b1">(2)</ref> after it has been warped according to some estimated V <ref type="bibr" target="#b0">(1)</ref> . To enforce photometric consistency only for pixels that can be reconstructed from the other image, we must also estimate an occlusion mask O (1) ? R H?W , for example based on the estimated forward and backward flow fields O (1) = g(V <ref type="bibr" target="#b0">(1)</ref> , V <ref type="bibr" target="#b1">(2)</ref> ). L(?) might also include other terms for, e.g. for smoothness or self-supervision. If L(?) is differentiable with respect to ?, the parameters that minimize this loss ? * = arg min(L(D, ?)) can be recovered using gradient-based optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Key Components of Unsupervised Optical Flow</head><p>This section compares and improves key components of unsupervised optical flow. We will first discuss a model f ? (?), which we base on PWC-Net <ref type="bibr" target="#b24">[25]</ref>, and improve through cost-volume normalization. Then we go through different components of the objective function L(?): occlusion-aware photometric consistency, smoothness, and self-supervision. Here, we propose improvements to each component: stopping the gradient at the occlusion mask, computing smoothness at the native flow resolution, and image resizing for self-supervision. We end this section by discussing data augmentation and optimization.</p><p>As shown in <ref type="figure">Fig. 1</ref>, our model feeds images I <ref type="bibr" target="#b0">(1)</ref> and I <ref type="bibr" target="#b1">(2)</ref> into a shared CNN that generates a feature pyramid, where features are used as input for warping (W), cost volume computation (C), and flow estimation (F). At each level , the estimated flow V (1, +1) from the level above is upscaled, passed down to the lower level asV <ref type="bibr">(1, +1)</ref> , and then used to warp F <ref type="bibr">(2, )</ref> , the features of image 2. The warped features w(F <ref type="bibr">(2, )</ref> ,V <ref type="bibr">(1, )</ref> ) together with F <ref type="bibr">(1, )</ref> are used to compute a cost volume. The cost volume considers feature correlations for all pixels and all 81 combinations of shifting w(F <ref type="bibr">(2, )</ref> ,V <ref type="bibr">(1, )</ref> ) up to 4 pixels up/down and left/right. This results in a cost volume C ? R W 2 ? H 2 ?81 that describes how closely each pixel in F <ref type="bibr">(1, )</ref> resembles the 81 pixels around its location in F <ref type="bibr">(2, )</ref> . The cost volume, the features from image 1, the higher level flow and the context -the output of the second to last layer of the flow estimation network -are fed into a CNN that estimates a flow V <ref type="bibr">(1, )</ref> . After a number of flow estimation levels, there is a final stage of flow refinement at level two in which the flow and context are fed into a context network (CN), which is a stack of dilated convolutions.</p><p>Model Shrinking, Level Dropout and Cost Volume Normalization: PWC-Net was designed for supervised learning of optical flow <ref type="bibr" target="#b24">[25]</ref>. To deal with increased memory requirements for unsupervised learning due to bi-directional losses, occlusion estimation, and self-supervision, we remove level six, use 32 channels in all levels, and add residual connections to all flow estimation modules (the "+" in the bottom right of <ref type="figure">Fig. 1</ref>). Additionally, we dropout residual flow estimation at all levels to further regularize learning, i.e. we randomly pass the resized and rescaled flow estimate from the level above directly to the level below.</p><p>Another difference when using this model for unsupervised rather than supervised learning is that unsupervised losses are typically only imposed on the final output (presumably because photometric consistency and other objectives work better at higher resolutions). But without supervised losses on intermediate flow predictions, the model has difficulty learning flow estimation at higher levels. We found that this is caused by very low values in the estimated cost volumes as a result of vanishing feature activations at higher levels.</p><p>We address this problem by cost volume normalization. Let us denote features for image i at level as</p><formula xml:id="formula_0">F (i, ) ? R H 2 ? W 2</formula><p>?d . The cost volume between images 1 and 2 for all image locations (x, y) and all considered image shifts (u, v) is the inner product of the normalized features of the two images:</p><formula xml:id="formula_1">C ( ) x,y,u,v = d F (1, ) x,y,d ? ? (1, ) ? (1, ) F (2, ) x+u,y+v,d ? ? (2, ) ? (2, ) .<label>(1)</label></formula><p>Where ? (i, ) and ? (i, ) are the sample mean and standard deviation of F (i, ) over its spatial and feature dimensions. We found that cost volume normalization improves convergence and final performance in unsupervised optical flow. These findings are consistent with prior work that used a similar form of normalization to improve geometric matching <ref type="bibr" target="#b22">[23]</ref>.</p><p>Unsupervised Learning Objectives: Defining a learning objective L(?) that specifies the task of learning optical flow without having access to labels is the core problem of unsupervised optical flow. Similar to related work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref>, we train our model by estimating optical flow and applying the respective losses in both directions. In this work we consider a learning objective that consists of three terms: occlusion-aware photometric consistency, edge-aware smoothness, and self-supervision, which we will now discuss in detail. Photometric Consistency: The photometric consistency term encourages the estimated flow to align image patches with a similar appearance by penalizing photometric dissimilarity. The metric for measuring appearance similarity is critical for any unsupervised optical flow technique. Related approaches use three different objectives here (sometimes in combination), (i) the generalized Charbonnier loss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, (ii) the structural similarity index (SSIM) loss <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>, and (iii) the Census loss <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. We compare all three losses in this paper. The generalized Charbonnier loss <ref type="bibr" target="#b23">[24]</ref> is L C = 1 n (I (1) ? w(I (2) ) 2 + 2 ? . Our experiments use = 0.001 and ? = 0.5 and also compare to using a modified L1 loss L L1 = |I (1) ? w(I <ref type="bibr" target="#b1">(2)</ref> ) + | with = 10 ?6 . For the SSIM [31] loss, we use an occlusion-aware implementation from recent work <ref type="bibr" target="#b8">[9]</ref>. For the Census loss, we use a soft Hamming distance on Census-transformed image patches <ref type="bibr" target="#b17">[18]</ref>. Based on the empirical results discussed below, we use the Census loss unless otherwise stated. All photometric losses are computed using an occlusion-masked average over all pixels <ref type="bibr" target="#b29">[30]</ref>. Occlusion Estimation: By definition, occluded regions do not have a correspondence in the other image, so they should be discounted when computing the photometric loss. Related approaches estimate occlusions by (i) checking for consistent forward and backward flow <ref type="bibr" target="#b29">[30]</ref>, (ii) using the range map of the backward flow <ref type="bibr" target="#b2">[3]</ref>, and (iii) learning a model for occlusion estimation <ref type="bibr" target="#b11">[12]</ref>. We are considering and comparing the first two variants here and improve the second variant through gradient stopping. In addition to taking into account occlusions, we also mask "invalid" pixels whose flow vectors point outside of the frame of the image <ref type="bibr" target="#b29">[30]</ref>. The forward-backward consistency check defines occlusions a pixels for which the flow and the back-projected backward flow disagree by more than a threshold, such that the occlusion mask is defined as</p><formula xml:id="formula_2">O (1) = 1 |V (1) ?w(V (2) )| 2 &lt;?1(|V (1) | 2 ?|w(V (2) )| 2 )+?2 ,</formula><p>where ? 1 = 0.01 and ? 2 = 0.5 <ref type="bibr" target="#b25">[26]</ref>. An alternative approach computes a "range map" R (i) ? R H?W -a soft histogram of how many pixels in the other image map onto a given pixel, which is constructed by having each flow vector distribute a total weight of 1 to the four pixels around its end point according to a bilinear kernel <ref type="bibr" target="#b29">[30]</ref>. Pixels that none of the reverse flow vectors point to are assumed to have no correspondence in the other image, and are therefore occluded. As proposed by Wang et al. <ref type="bibr" target="#b29">[30]</ref>, we compute an occlusion mask O (i) ? R W ?H by thresholding the range map at 1. Based on the empirical results below, we use range-map based occlusion estimation by default, but use the forward-backward consistency check on KITTI, where it significantly improves performance. Gradient Stopping at Occlusion Masks: Although prior work does not mention this issue <ref type="bibr" target="#b29">[30]</ref>, we found that propagating the gradient of the photometric loss into the occlusion estimation consistently degraded performance or caused divergence when the occlusion estimation was differentiable, as is the case for range-map based occlusion. This behavior is to be expected because when computing the occlusion-weighted average over photometric dissimilarity, there should be a gradient towards masking pixels with high photometric error. We address this problem by stopping the gradient at the occlusion mask, which eliminates divergence and improves performance.</p><p>Smoothness: Different forms of smoothness are commonly used to regularize optical flow in traditional methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref> as well as most recent unsupervised approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. In this work, we consider edge-aware first and second order smoothness <ref type="bibr" target="#b26">[27]</ref>, where flows are encouraged to align their boundaries with visual edges in the image I <ref type="bibr" target="#b0">(1)</ref> . Formally, we define kth order smoothness as:</p><formula xml:id="formula_3">L smooth(k ) = 1 n exp ? ? 3 c ?I (1, ) c ?x ? k V (1, ) ?x k + exp ? ? 3 c ?I (1, ) c ?y ? k V (1, ) ?y k .<label>(2)</label></formula><p>Where ? modulates edge weighting based on I for color channel c ? [0, 2]. By default, we use first order smoothness on Flying Chairs and Sintel and second order smoothness on KITTI, which we ablate in different experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smoothness at Flow Resolution:</head><p>A question that we have not seen addressed is at which level , smoothness should be applied. Since we follow the commonly used method of estimating optical flow at = 2, i.e. at a quarter of the input resolution, followed by upsampling through bilinear interpolation, our model produces piece-wise linear flow fields. As a result, only every fourth pixel can possibly have a non-zero second order derivative, which might not be aligned with the corresponding image edge and thereby reduce the effectiveness of edge-aware smoothness. To address this, we apply smoothness at level = 2 where flow is generated and downsample the image instead of upsampling the flow. This of course does not affect evaluation, which is done at the original image resolution. Self-supervision: The idea of self-supervision in unsupervised optical flow is to generate optical flow labels by applying the learned model on a pair of images, then modify the images to make flow estimation more difficult and train the model to recover the originally estimated flow <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Since we see the main utility of this technique in learning flow estimation for pixels that go out of the image boundary -where cost-volume computation is not informative and photometric losses do not apply -we build on and improve ideas about self-supervised image crops <ref type="bibr" target="#b13">[14]</ref>. For our self-supervised objective, we apply our model on the full images, crop the images by removing 64 pixels from each edge, apply the model again, and use the cropped estimated flow from the full images as supervision for flow estimation from the cropped images. We define the self-supervision objectives as an occlusion-weighted Charbonnier loss, that takes into account only pixels that have low forward-backward consistency in the "student" flow from cropped image and high forward-backward consistency in the "teacher" flow from the original images, similar to DDFlow <ref type="bibr" target="#b13">[14]</ref>. Continual Self-supervision and Image Resizing: Unlike related work, we do not first train and then freeze a teacher model to supervise a separate student model but rather have a single model that supervises itself, which simplifies the approach, reduces the required memory, and allows the self-supervision signal to improve continually. To stabilize learning, we stop gradients of the self-supervision objectives to be propagated into the "teacher" flow. Additionally, we resize the image crops to match the original resolution before feeding them into the model (and we rescale the self-generated flow labels accordingly) to make the self-supervision examples more representative of the problem of extrapolating flow beyond the image boundary in the original size.</p><p>Optimization: To train our model f ? (?) we minimize a weighted sum of losses:</p><formula xml:id="formula_4">L(D, ?) = w photo ? L photo + w smooth ? L smooth + w self ? L self ,<label>(3)</label></formula><p>where L photo is our photometric loss, L smooth is smoothness regularization, and L self is the self-supervision Charbonnier loss. We set w photo to 1 for experiments using the Census loss and to 2 when we compare to the SSIM, Charbonnier, or L1 losses. We set w self to 2 when using first order, and to 4 for second order smoothness and use an edge-weight of ? = 150. We use w self = 0 during the first half of training, linearly increase it 0.3 during the next 10% of gradient steps and keep it constant afterwards. RGB image values are scaled to [?1, 1], and augmentated by randomly swapping the color channels and randomly shifting the hue. Sintel images are additionally randomly flipped up/down and left/right. All models are trained using with Adam <ref type="bibr" target="#b12">[13]</ref> (? 1 = 0.9, ? 2 = 0.999, = 10 ?8 ) with a learning rate of 10 ?4 for m steps, followed by another <ref type="bibr">1 5</ref> m steps during which the learning rate is exponentially decayed to 10 ?8 . All ablations use m = 50K with batch size 32, but the final model was trained using m = 1M with batch size 1, which produced slightly better performance as described below. Either way, the training takes about three days. Experiments on Sintel and KITTI start from a model that was first trained on Flying Chairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our model on the standard optical flow benchmark datasets: Flying Chairs <ref type="bibr" target="#b4">[5]</ref>, Sintel <ref type="bibr" target="#b3">[4]</ref>, and KITTI 2012/2015 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>. We divide Flying Chairs and Sintel according to its standard train/test split. For KITTI, we train on the multi-view extension on the KITTI 2015 dataset, and we do not train on any data from KITTI 2012 because it does not have moving objects.</p><p>Related work is inconsistent in their use of train/test splits. For Sintel, it is common to train on the training set, report the benchmark performance on the test set, and evaluate ablations on the training set only (because test set labels are not public), which does not test generalization very well. Others "download the Sintel movie and extract ?10,000 images" <ref type="bibr" target="#b14">[15]</ref> including the test set images, which is intended to demonstrate the ability of unsupervised methods to train on raw video data, but unfortunately also includes the benchmark test images in the training set. For KITTI, other works train on the raw KITTI dataset with and without excluding the evaluation set, or most commonly train on frames 1-8 and 13-20 of the multi-view extension of KITTI 2012/2015 datasets and evaluate on frames 10/11. But this split can mask overfitting to the trained sequences -either in the ablation results or also in the benchmark results, when the multiview-extensions of both the train and the test set are used. We therefore adopt the training regimen of Zhong et al. <ref type="bibr" target="#b34">[35]</ref> and train two models for each dataset, one on the training set and one on test set (or for KITTI on their multiview extension) and evaluate these models appropriately.</p><p>Following the conventions of the KITTI benchmark, we report endpoint error ("EPE") and error rates ("ER"), where a prediction is considered erroneous if its EPE is &gt; 3 pixels and if the distance between the predicted point and the true end point is &gt; 5% of the length of the true flow vector. We compute these metrics for all pixels ("occ" in the KITTI benchmark, which we call "all" in this paper). We use the common practice of pretraining on the train split of the Flying Chairs dataset before training on Sintel / KITTI. We evaluate on all images in the native resolution, but have the model perform inference on a resolution that is divisible by 32, output at a four times smaller resolution, and then resize the output to the original resolution for evaluation. On KITTI, we observe that performance improves when using a square input resolution instead of a resolution in the original aspect ratio -perhaps because KITTI is dominated by horizontal motion. Accordingly, we use the following resolutions in our experiments: Flying Chairs: 384?512, Sintel: 448?1024, KITTI: 640?640.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We evaluate our model in an extensive comparison and ablation study, from which we identify the best combination of components, tested in the "full" setting, which is often different from the components that work best individually in our "minimal" setting (more details below). We then compare our resulting model to the best published methods on unsupervised optical flow, and show that it outperforms all methods on all benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablations and Comparisons of Key Components</head><p>To determine which aspects of unsupervised optical flow are most important, we perform an extensive series of ablation studies. We find that a) occlusion-masking, self-supervision, and smoothness are all important, b) level dropout and cost volume normalization improve performance, c) the Census loss outperforms other photometric losses, d) range-map based occlusion estimation requires gradient stopping to work, c) edge-aware smoothness and smoothness level matters significantly, d) self supervision helps especially for KITTI, and is improved by our changes, e) losses might be the current performance bottleneck, f) changing the resolution can substantially improve results, g) data augmentation and pretraining are helpful.</p><p>In each ablation study we train one model per domain (on Flying Chairs, KITTI-test, and Sintel-test), and evaluate those on the corresponding validation split from the same domain, taking into account occluded and non-occluded pixels "(all)". To estimate the noise in our results, we trained models with six different random seeds for each domain and computed their standard deviations per metric: Flying Chairs: 0.0162, Sintel Clean: 0.0248, Sintel Final: 0.0131, KITTI-2015: 0.0704, 0.0718%. We now describe the findings of each study. Core Components: <ref type="table" target="#tab_0">Table 1</ref> shows how performance varies as each core component of our model (occlusion masking, smoothness, and self-supervision) is removed. We see that every component contributes to the overall performance. Since the utility of different components depends on what other components are used, all following experiments compare to the "minimal" (first row) and "full" (last row) versions of our method. Qualitative results for rows 9, 4, 3, and 1 are shown in <ref type="figure" target="#fig_2">Figure 2</ref> (from left to right). Note how the flow error ?V increases with each removal of a core component.    Model Improvements: <ref type="table" target="#tab_1">Table 2</ref> shows that level dropout (LD) and cost volume normalization (CVN) improve performance in the full setting (but not generally in the minimal setting). CVN appears to be more important for Chairs and Sintel while LD helps most for KITTI. Photometric Losses: <ref type="table" target="#tab_2">Table 3</ref> compares commonly used photometric losses and shows that it is important to test every component with the full method, rather than looking at isolated performance. By itself, the commonly-used Charbonnier loss works better, but in the full setting, it underperforms the simpler L1 loss. For KITTI, Census works best in both settings. But for Sintel (in particular Sintel Final), the SSIM loss significantly outperforms Census in the minimal setting (5.41 vs. 6.98) but does not perform as well when used with all components in the full setting. Occlusion Estimation: <ref type="table" target="#tab_3">Table 4</ref> compares different approaches to occlusion estimation (forward-backward consistency and range maps). We see that range-map based occlusion consistently diverges unless we stop the gradient of the photometric loss. But when gradients are stopped, this method works well, especially for Flying Chairs and Sintel Clean. Forward-backward consistency works best for KITTI, especially if not applied from the beginning.    Smoothness: Prior work suggests that photometric and smoothness losses taken together work better at higher resolutions <ref type="bibr" target="#b7">[8]</ref>. But our analysis of the smoothness loss alone shows an advantage of applying this loss at the resolution of flow estimation, rather than at the image resolution, in particular for Flying Chairs and Sintel <ref type="table" target="#tab_4">(Table 5</ref>). Our results also show that first order smoothness works better on Chairs and Sintel while second order smoothness works better on KITTI ( <ref type="table">Table 6</ref>). We see that context is important because in the minimal setting, the best second order smoothness weight for KITTI is 8, but in the full setting, it is 2. Comparing different edge-weights ? (Eq. 2) in <ref type="table" target="#tab_5">Table 7</ref>, we see that nonzero edge-weights improve performance, particularly in the full setting. To our surprise, the simple strategy of only optimizing the Census loss and second order smoothness without edge-awareness, occlusion, or self-supervision (first row) produces performance on KITTI that improves on previous the state of the art.</p><formula xml:id="formula_5">2015 I ?V KITTI V * V Final I ?V Sintel V * V Ground truth Full ?Occlusion ?Self-supervision ?Smoothness</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Supervision:</head><p>In <ref type="table" target="#tab_6">Table 8</ref> we ablate the use of self-supervision and our proposed changes, and confirm that self-supervision on image crops is instrumental in achieving good results on KITTI, where errors are dominated by fast motion near the image edges. We also see that selfsupervision is most effective when the image crop is resized as proposed by our method. Freezing the teacher network, as done in other works, seems to be important only when not using the other regularizing components. With these components in place, sharing the same model for both student and teacher appears to be beneficial.</p><p>Loss Comparison to Ground Truth: Photometric loss functions used in unsupervised optical flow rely on the brightness consistency assumption: that pixel intensities in the camera image are invariant to motion in the world. But pho-tometric consistency is an imperfect indicator of flow quality (e.g. in regions of shadows and specularity). To analyze this issue, we compute photometric and smoothness losses not only for the flow field produced by our model, but also for a flow field filled with zeros and for the ground truth flow. <ref type="table" target="#tab_7">Table 9</ref> shows that our model is able to achieve comparable or better photometric consistency (and overall loss) than the ground truth flow. This trend is more pronounced on Sintel Final, which we believe violates the consistency assumption more than Sintel Clean. This result suggests that the loss functions currently used may be a limiting factor in unsupervised methods. Resolution: <ref type="table" target="#tab_0">Table 10</ref> shows, perhaps surprisingly, that estimating flow at a different resolution and aspect ratio can substantially improve performance on KITTI-15 (2.93 vs. 3.80), presumably because the motion field in this dataset is dominated by horizontal motion. We have not observed this effect in other datasets.   Data Augmentation: <ref type="table" target="#tab_0">Table 11</ref> evaluates the importance of color augmentation (color channel swapping and hue randomization) for all domains, as well as image flipping for Sintel. The results show that both augmentation techniques improve performance, in particular image flipping for Sintel (which is a much smaller dataset than Chairs or KITTI). Pretraining: Pretraining is a common strategy in supervised <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref> and unsupervised <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref> optical flow. The results in <ref type="table" target="#tab_0">Table 12</ref> confirm that pretraining on Chairs improves performance on Sintel and KITTI. Gradient Steps and Batch Size: All experiments up to this point have trained the model for 60K steps at a batch size of 32. <ref type="table" target="#tab_0">Table 13</ref> shows a comparison to another training regime that trains longer with smaller batches, which consistently improves performance. We use this regime for our comparison to other published methods.</p><p>Comparison to State of the Art: We show qualitative results in <ref type="figure" target="#fig_4">Figure 3</ref> and quantitatively evaluate our model trained on KITTI and Sintel data in the corresponding benchmarks in <ref type="table" target="#tab_0">Table 14</ref>, where we compare against state-of-the-art techniques for unsupervised and supervised optical flow. Results not reported by prior work are indicated with "-". Among unsupervised approaches (H-U), our model sets a new state of the art for Sintel Clean (5.21 vs. 6.18), Sintel Final (6.50 vs. 7.40), and KITTI-15 (11.13% vs. 14.19%) -where, for a lack of comparability, we had to disregard results in braces that came from (partially) training on the test set. UFlow is only outperformed (1.8 vs. 1.9) on KITTI-12, which does not include moving objects, by a stereo-depth and motion based approach (P).</p><p>The top-performing supervised models finetuned on data from the evaluation domain (models A-D) do outperform our unsupervised model, as one may expect. But on KITTI-15, our model performs on par with the supervised FlowNet2. Of course, fine-tuning on the domain is only possible because the KITTI training data also contains ground-truth flow, which we ignore but which supervised techniques require. This sort of supervision is hard to obtain (KITTI being virtually the only non-synthetic dataset with this information), which demonstrates the value of unsupervised flow techniques such as ours. Without access to the ground truth labels of the test domain, our unsupervised method compares more favorably to its supervised counterparts, significantly outperforming them on KITTI. Our final experiment analyses cross-domain generalization in more detail.    <ref type="table" target="#tab_0">Table 15</ref> evaluates out-of-domain generalization by training and evaluating models across three datasets. While performance is best when training and test data are from the same domain, our model shows good generalization. It consistently outperforms DDFlow and it outperforms the supervised PWC-Net in all but one generalization task (training on Chairs and testing on Sintel Clean).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a study into what matters in unsupervised optical flow that systematically analyzes, compares, and improves a set of key components. This study results in a range of novel observations about these components and their interactions, from which we integrate the best components and improvements into a unified framework for unsupervised optical flow. Our resulting UFlow model substantially outperforms the state of the art among unsupervised methods and performs on par with the supervised FlowNet2 on the challenging KITTI 2015 benchmark, despite not using any labels. In addition to its strong performance, our method is also significantly simpler than many related approaches, which we hope will make it useful as a starting point for further research into unsupervised optical flow. Our code is available at https://github.com/google-research/ google-research/tree/master/uflow.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 +Fig. 1 .</head><label>11</label><figDesc>Model overview. Left: Feature pyramids feed into a top-down flow estimation. Right: A zoomed in view on a "W, C, F" (warping, cost volume, flow estimation) block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative ablation results of our model on random images not seen during training. Flow quality deteriorates as we progressively ablate core components</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Results for our model on random examples not seen during training taken from KITTI 2015 and Sintel Final. These qualitative results show the model's ability to estimate fast motions, relatively fine details, and substantial occlusions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Core components: OM: occlusion masking, SM: smoothness, SS: self-supervision; "div.": divergence</figDesc><table><row><cell></cell><cell></cell><cell>Chairs</cell><cell>Sintel train</cell><cell>KITTI-15 train</cell></row><row><cell cols="2">OM SM SS</cell><cell>test</cell><cell>Clean Final</cell><cell>all noc ER%</cell></row><row><cell>-</cell><cell>--</cell><cell>3.58</cell><cell>4.20 6.80</cell><cell>13.07 2.47 21.21</cell></row><row><cell>-</cell><cell>-</cell><cell>2.99</cell><cell>3.34 5.18</cell><cell>11.36 2.30 18.61</cell></row><row><cell>-</cell><cell>-</cell><cell>2.84</cell><cell>3.37 5.19</cell><cell>11.37 2.17 19.31</cell></row><row><cell>-</cell><cell></cell><cell>2.74</cell><cell>3.12 4.56</cell><cell>3.28 2.08 9.97</cell></row><row><cell></cell><cell>--</cell><cell>3.28</cell><cell>3.78 5.85</cell><cell>div. div. div.</cell></row><row><cell></cell><cell>-</cell><cell>2.91</cell><cell>3.26 4.72</cell><cell>3.02 2.11 9.89</cell></row><row><cell></cell><cell>-</cell><cell>2.63</cell><cell>3.20 4.63</cell><cell>4.15 2.05 13.15</cell></row><row><cell></cell><cell></cell><cell>2.55</cell><cell>3.00 4.18</cell><cell>2.94 1.98 9.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Model improvements. CVN: cost volume normalization, LD: level dropout</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Chairs</cell><cell>Sintel train</cell><cell cols="2">KITTI-15 train</cell></row><row><cell></cell><cell>CVN</cell><cell>LD</cell><cell>test</cell><cell>Clean Final</cell><cell>all</cell><cell>noc ER%</cell></row><row><cell>Minimal</cell><cell>--</cell><cell>--</cell><cell>5.01 5.29 4.86 3.58</cell><cell>4.52 6.67 4.40 6.59 4.19 6.69 4.20 6.80</cell><cell cols="2">13.30 2.72 21.69 12.75 2.49 21.30 13.294 2.59 21.54 13.07 2.47 21.21</cell></row><row><cell></cell><cell>-</cell><cell>-</cell><cell>3.78</cell><cell>3.41 4.70</cell><cell cols="2">39.09 30.19 98.77</cell></row><row><cell>Full</cell><cell>-</cell><cell>-</cell><cell>3.21 2.54</cell><cell>3.45 4.61 3.07 4.31</cell><cell cols="2">2.96 1.96 9.77 3.16 2.04 10.35</cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.55</cell><cell>3.00 4.18</cell><cell cols="2">2.94 1.98 9.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Photometric losses. Best results of L1 and Charbonnier underlined</figDesc><table><row><cell></cell><cell></cell><cell>Chairs</cell><cell>Sintel train</cell><cell cols="2">KITTI-15 train</cell></row><row><cell></cell><cell>Method</cell><cell>test</cell><cell>Clean Final</cell><cell>all</cell><cell>noc ER%</cell></row><row><cell>Minimal</cell><cell>L1 Charbonnier SSIM Census</cell><cell>4.27 4.31 3.51 3.54</cell><cell>5.51 7.74 5.50 7.64 4.01 5.41 4.23 6.98</cell><cell cols="2">17.02 6.11 32.96 16.94 6.09 32.84 11.99 2.46 21.72 11.66 2.37 21.15</cell></row><row><cell></cell><cell>L1</cell><cell>2.83</cell><cell>4.23 5.75</cell><cell cols="2">5.53 3.17 18.65</cell></row><row><cell>Full</cell><cell>Charbonnier SSIM</cell><cell>2.86 2.54</cell><cell>4.24 5.81 3.08 4.52</cell><cell cols="2">5.56 3.21 18.82 3.29 2.04 10.41</cell></row><row><cell></cell><cell>Census</cell><cell>2.61</cell><cell>3.00 4.20</cell><cell cols="2">3.08 2.01 10.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Occlusion estimation. RM: range-map based occllusion, FB: forward-backward consistency check</figDesc><table><row><cell></cell><cell></cell><cell>Chairs</cell><cell>Sintel train</cell><cell>KITTI-15 train</cell></row><row><cell></cell><cell>Method</cell><cell>test</cell><cell>Clean Final</cell><cell>all noc ER%</cell></row><row><cell></cell><cell>None</cell><cell>3.51</cell><cell>4.15 6.69</cell><cell>12.89 2.41 21.17</cell></row><row><cell>Minimal</cell><cell>RM (w/o grad stop) RM (w/ grad stop) FB (from step 1)</cell><cell>div. 3.27 3.57</cell><cell>div. div. 3.78 5.86 3.71 4.83</cell><cell>div. div. div. 10.65 2.29 18.76 8.99 2.16 17.71</cell></row><row><cell></cell><cell>FB (after 20% steps)</cell><cell>3.49</cell><cell>3.76 4.92</cell><cell>9.75 2.13 18.38</cell></row><row><cell></cell><cell>None</cell><cell>2.73</cell><cell>3.84 5.13</cell><cell>3.28 2.10 10.07</cell></row><row><cell></cell><cell>RM (w/o grad stop)</cell><cell>div.</cell><cell>div. div.</cell><cell>div. div. div.</cell></row><row><cell>Full</cell><cell>RM (w/ grad stop)</cell><cell>2.58</cell><cell>3.01 4.25</cell><cell>3.10 2.04 9.86</cell></row><row><cell></cell><cell>FB (from step 1)</cell><cell>3.28</cell><cell>3.49 4.45</cell><cell>2.96 1.99 9.65</cell></row><row><cell></cell><cell>FB (after 20% steps)</cell><cell>3.14</cell><cell>3.12 4.13</cell><cell>2.88 1.95 9.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Level for smoothness loss</figDesc><table><row><cell></cell><cell cols="2">Smoothn.</cell><cell>Chairs</cell><cell>Sintel train</cell><cell cols="2">KITTI-15 train</cell></row><row><cell></cell><cell cols="2">level</cell><cell>test</cell><cell>Clean Final</cell><cell>all</cell><cell>noc ER%</cell></row><row><cell>Minimal</cell><cell>0 1 2</cell><cell></cell><cell>3.05 2.94 2.85</cell><cell>4.10 5.22 3.65 5.07 3.33 5.21</cell><cell cols="2">12.16 2.32 20.33 11.94 2.24 19.98 11.43 2.23 19.38</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell>2.87</cell><cell>3.65 4.63</cell><cell cols="2">2.95 2.02 9.87</cell></row><row><cell>Full</cell><cell>1</cell><cell></cell><cell>2.74</cell><cell>3.13 4.29</cell><cell cols="2">2.96 1.99 9.78</cell></row><row><cell></cell><cell>2</cell><cell></cell><cell>2.58</cell><cell>3.00 4.24</cell><cell cols="2">2.93 1.99 9.63</cell></row><row><cell cols="7">Table 6. Comparison of weights for</cell></row><row><cell cols="6">first/second order smoothness</cell></row><row><cell></cell><cell cols="2">wsmooth</cell><cell>Chairs</cell><cell>Sintel train</cell><cell cols="2">KITTI-15 train</cell></row><row><cell></cell><cell cols="2">1st 2nd</cell><cell>test</cell><cell>Clean Final</cell><cell cols="2">all occ ER%</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>4.55</cell><cell>4.16 6.84</cell><cell cols="2">div. div. div.</cell></row><row><cell>Minimal</cell><cell>0 0 4</cell><cell>2 8 0</cell><cell>3.13 4.02 2.85</cell><cell>3.77 6.32 3.50 6.08 3.35 5.05</cell><cell cols="2">11.37 2.17 19.33 7.27 2.11 14.70 7.23 2.30 18.58</cell></row><row><cell></cell><cell cols="2">16 0</cell><cell>4.37</cell><cell>4.78 6.03</cell><cell cols="2">9.58 4.09 22.82</cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>2.92</cell><cell>3.27 4.77</cell><cell cols="2">2.92 2.07 9.75</cell></row><row><cell></cell><cell>0</cell><cell>2</cell><cell>2.79</cell><cell>div. div.</cell><cell cols="2">2.93 1.98 9.61</cell></row><row><cell>Full</cell><cell>0</cell><cell>8</cell><cell>2.75</cell><cell>3.33 4.77</cell><cell cols="2">2.94 1.91 9.85</cell></row><row><cell></cell><cell>4</cell><cell>0</cell><cell>2.60</cell><cell>3.00 4.17</cell><cell cols="2">5.39 2.03 16.58</cell></row><row><cell></cell><cell cols="2">16 0</cell><cell>3.68</cell><cell>4.22 5.30</cell><cell cols="2">8.71 4.01 21.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Smoothness edge-weights</figDesc><table><row><cell></cell><cell></cell><cell>Chairs</cell><cell>Sintel train</cell><cell>KITTI-15 train</cell></row><row><cell></cell><cell>?</cell><cell>test</cell><cell>Clean Final</cell><cell>all noc ER%</cell></row><row><cell>Minimal</cell><cell>0 10 150</cell><cell>4.93 4.33 2.83</cell><cell>6.00 6.65 5.32 6.12 3.36 5.12</cell><cell>4.15 2.36 12.50 4.22 2.17 12.28 11.41 2.21 19.37</cell></row><row><cell></cell><cell>0</cell><cell>4.87</cell><cell>5.78 6.40</cell><cell>3.86 2.84 11.81</cell></row><row><cell>Full</cell><cell>10</cell><cell>3.75</cell><cell>4.62 5.34</cell><cell>3.14 2.11 10.27</cell></row><row><cell></cell><cell>150</cell><cell>2.56</cell><cell>3.02 4.20</cell><cell>2.87 1.95 9.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Self-supervision ablation</figDesc><table><row><cell></cell><cell></cell><cell>Chairs</cell><cell>Sintel train</cell><cell>KITTI-15 train</cell></row><row><cell></cell><cell>Self-supervision</cell><cell>test</cell><cell>Clean Final</cell><cell>all noc ER%</cell></row><row><cell>Minimal</cell><cell>None No resize Frozen teacher Default</cell><cell>3.48 3.16 3.10 2.99</cell><cell>4.10 6.62 3.53 5.67 3.36 5.24 3.34 5.18</cell><cell>13.05 2.48 21.23 12.87 2.35 20.22 8.11 2.38 13.90 11.36 2.30 18.61</cell></row><row><cell></cell><cell>None</cell><cell>2.67</cell><cell>3.18 4.60</cell><cell>4.10 2.02 12.95</cell></row><row><cell>Full</cell><cell>No resize Frozen teacher</cell><cell>2.51 2.66</cell><cell>3.14 4.48 3.04 4.24</cell><cell>3.53 2.02 11.13 2.99 1.99 9.70</cell></row><row><cell></cell><cell>Default</cell><cell>2.61</cell><cell>2.99 4.23</cell><cell>2.86 1.95 9.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Losses on Sintel for zero flow, ground truth flow, and predicted flow</figDesc><table><row><cell></cell><cell></cell><cell>L1</cell><cell>SSIM</cell><cell cols="2">Census</cell><cell>SM</cell><cell cols="2">Census + SM</cell></row><row><cell></cell><cell>Flow</cell><cell>noc all</cell><cell>noc all</cell><cell>noc</cell><cell>all</cell><cell>all</cell><cell>noc</cell><cell>all</cell></row><row><cell>Clean</cell><cell>Zero GT UFlow</cell><cell>.146 .161 .031 .052 .031 .042</cell><cell>.927 .946 .191 .241 .203 .247</cell><cell cols="2">3.160 3.193 2.041 2.122 2.06 2.130</cell><cell>0. .032 .024</cell><cell cols="2">3.160 3.193 2.073 2.154 2.085 2.154</cell></row><row><cell>Final</cell><cell>Zero GT UFlow</cell><cell>.126 .142 .034 .055 .032 .037</cell><cell>.731 .751 .185 .233 .167 .226</cell><cell cols="2">3.037 3.075 2.086 2.154 2.044 2.091</cell><cell>0. .063 .045</cell><cell cols="2">3.037 3.075 2.149 2.217 2.089 2.136</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 .</head><label>10</label><figDesc>Resolution</figDesc><table><row><cell></cell><cell></cell><cell>KITTI-15 train</cell></row><row><cell></cell><cell>Resolution</cell><cell>all noc ER%</cell></row><row><cell>Min.</cell><cell>384?1280 640?640</cell><cell>13.25 2.79 21.38 12.91 2.42 21.17</cell></row><row><cell>Full</cell><cell>384?1280 640?640</cell><cell>3.80 2.13 10.88 2.93 1.96 9.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 .</head><label>11</label><figDesc>Data</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">augmentation. F: im-</cell></row><row><cell cols="7">age flipping up/down and left/right (not</cell></row><row><cell cols="7">used for KITTI), C: color augmentation</cell></row><row><cell></cell><cell></cell><cell>Chairs</cell><cell>Sintel train</cell><cell cols="3">KITTI-15 train</cell></row><row><cell></cell><cell>F C</cell><cell>test</cell><cell>Clean Final</cell><cell>all</cell><cell cols="2">noc ER%</cell></row><row><cell>Minimal</cell><cell>----</cell><cell>3.47 3.56 3.49</cell><cell>4.39 6.56 4.38 6.58 4.23 6.73</cell><cell cols="3">13.27 2.56 22.13 13.07 2.47 21.21 ---</cell></row><row><cell></cell><cell></cell><cell>3.58</cell><cell>4.20 6.80</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>--</cell><cell>2.53</cell><cell>3.84 5.14</cell><cell cols="3">3.06 2.03 9.82</cell></row><row><cell>Full</cell><cell>-</cell><cell>2.61</cell><cell>3.78 5.23</cell><cell cols="3">2.94 1.98 9.65</cell></row><row><cell></cell><cell>-</cell><cell>2.57</cell><cell>3.02 4.22</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>2.55</cell><cell>3.00 4.18</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 .</head><label>12</label><figDesc>Pretraining on Chairs</figDesc><table><row><cell></cell><cell>Pretraining</cell><cell>Sintel train</cell><cell cols="2">KITTI-15 train</cell></row><row><cell></cell><cell>on Chairs</cell><cell>Clean Final</cell><cell>all</cell><cell>noc ER%</cell></row><row><cell>Min.</cell><cell>-</cell><cell>4.41 7.53 4.20 6.80</cell><cell cols="2">12.93 2.44 21.24 13.07 2.47 21.21</cell></row><row><cell>Full</cell><cell>-</cell><cell>3.38 4.81 3.00 4.18</cell><cell cols="2">3.08 2.04 10.00 2.94 1.98 9.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 .</head><label>13</label><figDesc>Gradient</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">steps (S) and batch</cell></row><row><cell cols="3">size (B)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Chairs</cell><cell>Sintel train</cell><cell cols="2">KITTI-15 train</cell></row><row><cell></cell><cell>S</cell><cell>B</cell><cell>test</cell><cell>Clean Final</cell><cell>all</cell><cell>noc ER%</cell></row><row><cell>test</cell><cell cols="2">60K 32 1.2M 1</cell><cell>{3.16} {2.82}</cell><cell>3.04 3.01 4.09 4.23</cell><cell cols="2">2.92 1.96 9.71 2.84 1.96 9.39</cell></row><row><cell>train</cell><cell cols="2">60K 32 1.2M 1</cell><cell>2.57 2.55</cell><cell>{2.47} {3.92} {2.50} {3.39}</cell><cell cols="2">{2.74} {1.87} {9.04} {2.71} {1.88} {9.05}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 .</head><label>14</label><figDesc>Our model (yellow) compared to state of the art. Supervised models in gray fine-tune on their evaluation domain, which is often not possible in practice. Braces indicate models whose training set includes its evaluation set, and so are not comparable: "()" trained on the labeled evaluation set, "{}" trained on the unlabeled evaluation set, and "[]" trained on data related to the evaluation set (e.g. &lt; 5 frames away in KITTI, or having the same content in Sintel). The best unsupervised and supervised (without finetuning) results are in bold. Methods that use additional modalities are denoted with MDM: mono depth/motion, SDM: stereo depth/motion, MF: multi-frame flow</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Sintel Clean [4]</cell><cell cols="2">Sintel Final [4]</cell><cell cols="2">KITTI 2012 [6]</cell><cell></cell><cell cols="2">KITTI 2015 [19]</cell></row><row><cell></cell><cell></cell><cell cols="2">EPE</cell><cell cols="2">EPE</cell><cell cols="2">EPE</cell><cell cols="2">EPE EPE (noc)</cell><cell cols="2">ER in %</cell></row><row><cell></cell><cell>Method</cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>test</cell><cell>train</cell><cell>train</cell><cell>train</cell><cell>test</cell></row><row><cell></cell><cell>(A) FlowNet2-ft [11]</cell><cell>(1.45)</cell><cell>4.16</cell><cell cols="2">(2.01) 5.74</cell><cell>(1.28)</cell><cell>1.8</cell><cell>(2.30)</cell><cell>-</cell><cell>(8.61)</cell><cell>11.48</cell></row><row><cell>Supervised</cell><cell>(B) PWC-Net-ft [25] (C) SelFlow-ft [15] (D) VCN-ft [32] (E) FlowNet2 [11] (F) PWC-Net [25]</cell><cell cols="2">(1.70) (1.68) [3.74] 3.86 (1.66) 2.81 2.02 3.96 2.55 -</cell><cell cols="2">(2.21) 5.13 (1.77) {4.26} (2.24) 4.40 3.14 6.02 3.93 -</cell><cell>(1.45) (0.76) -4.09 4.14</cell><cell>1.7 1.5 ---</cell><cell>(2.16) (1.18) (1.16) 9.84 10.35</cell><cell>-----</cell><cell>(9.80) -(4.10) 28.20 33.67</cell><cell>9.60 8.42 6.30 --</cell></row><row><cell></cell><cell>(G) VCN [32]</cell><cell>2.21</cell><cell>-</cell><cell>3.62</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.36</cell><cell>-</cell><cell>25.10</cell><cell>-</cell></row><row><cell></cell><cell>(H) Back2Basics [34]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>11.30</cell><cell>9.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>(I) DSTFlow [22]</cell><cell cols="2">{6.16} 10.41</cell><cell cols="2">{7.38} 11.28</cell><cell cols="2">[10.43] 12.4</cell><cell>[16.79]</cell><cell>[6.96]</cell><cell cols="2">[36.00] [39.00]</cell></row><row><cell></cell><cell>(J) OAFlow [30]</cell><cell cols="2">{4.03} 7.95</cell><cell cols="2">{5.95} 9.15</cell><cell>[3.55]</cell><cell>[4.2]</cell><cell>[8.88]</cell><cell>-</cell><cell>-</cell><cell>[31.20]</cell></row><row><cell></cell><cell>(K) UnFlow [18]</cell><cell>-</cell><cell>-</cell><cell>7.91</cell><cell>10.21</cell><cell>3.29</cell><cell>-</cell><cell>8.10</cell><cell>-</cell><cell>23.27</cell><cell>-</cell></row><row><cell></cell><cell>(L) GeoNet [33] (MDM)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>10.81</cell><cell>8.05</cell><cell>-</cell><cell>-</cell></row><row><cell>Unsupervised</cell><cell cols="3">(M) DF-Net [36] (MDM) (N) CCFlow [21] (MDM) (O) MFOccFlow [12] (MF) {3.89} 7.23 ----(P) UnOS [29] (SDM) --</cell><cell cols="2">--{5.52} 8.81 ----</cell><cell>3.54 --1.64</cell><cell>4.4 --1.8</cell><cell>{8.98} 5.66 [6.59] 5.58</cell><cell>--[3.22] -</cell><cell cols="2">{26.01} {25.70} 20.93 25.27 -22.94 -18.00</cell></row><row><cell></cell><cell>(Q) EPIFlow [35]</cell><cell>3.94</cell><cell>7.00</cell><cell>5.08</cell><cell>8.51</cell><cell>2.61</cell><cell>3.4</cell><cell>5.56</cell><cell>2.56</cell><cell>-</cell><cell>16.95</cell></row><row><cell></cell><cell>(R) DDFlow [14]</cell><cell cols="2">{2.92} 6.18</cell><cell cols="2">{3.98} 7.40</cell><cell>[2.35]</cell><cell>3.0</cell><cell>[5.72]</cell><cell>[2.73]</cell><cell>-</cell><cell>14.29</cell></row><row><cell></cell><cell>(S) SelFlow [15] (MF)</cell><cell cols="2">[2.88] [6.56]</cell><cell cols="2">{3.87} {6.57}</cell><cell>[1.69]</cell><cell>2.2</cell><cell>[4.84]</cell><cell>[2.40]</cell><cell>-</cell><cell>14.19</cell></row><row><cell></cell><cell>(T) UFlow-test</cell><cell>3.01</cell><cell>-</cell><cell>4.09</cell><cell>-</cell><cell>1.58</cell><cell>-</cell><cell>2.84</cell><cell>1.96</cell><cell>9.39</cell><cell>-</cell></row><row><cell></cell><cell>(U) UFlow-train</cell><cell cols="2">{2.50} 5.21</cell><cell cols="2">{3.39} 6.50</cell><cell>1.68</cell><cell>1.9</cell><cell>{2.71}</cell><cell>{1.88}</cell><cell cols="2">{9.05} 11.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 .</head><label>15</label><figDesc>Generalization across datasets. Performance when training on one dataset and testing on different one (gray if same)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Chairs</cell><cell cols="2">Sintel train</cell><cell cols="2">KITTI-15 train</cell></row><row><cell></cell><cell></cell><cell>Method</cell><cell>test</cell><cell cols="2">Clean Final</cell><cell>all</cell><cell>noc</cell><cell>ER%</cell></row><row><cell>Train on</cell><cell>Chairs</cell><cell>PWC-Net [25] DDFlow [14] UFlow-test UFlow-train</cell><cell>2.00 2.97 {2.82} 2.55</cell><cell>3.33 4.83 4.36 3.43</cell><cell>4.59 4.85 5.12 4.17</cell><cell cols="2">13.20 17.26 15.68 11.27 5.66 30.31 -41.79 --7.96 32.69</cell></row><row><cell>Train on</cell><cell>Sintel</cell><cell>PWC-Net [25] DDFlow [14] UFlow-test UFlow-train</cell><cell>3.69 3.46 3.39 3.25</cell><cell cols="2">(1.86) (2.31) {2.92} {3.98} 3.01 4.09 {2.50} {3.39}</cell><cell>10.52 12.69 7.67 9.40</cell><cell>--3.77 17.41 30.49 -4.53 20.02</cell></row><row><cell>Train on</cell><cell>KITTI</cell><cell>DDFlow [14] UFlow-test UFlow-train</cell><cell>6.35 5.25 5.05</cell><cell>6.20 6.34 5.58</cell><cell>7.08 7.01 6.31</cell><cell cols="2">[5.72] 2.84 {2.71} {1.88} {9.05} --1.96 9.39</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">L</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">S</forename><surname>Beauchemin</surname></persName>
		</author>
		<title level="m">Performance of optical flow techniques. IJCV</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Andr?s Bruhn, Nils Papenberg, and Joachim Weickert. High accuracy optical flow estimation based on a theory for warping. ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Haz?rba?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flownet</surname></persName>
		</author>
		<title level="m">Learning optical flow with convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Perception of the Visual World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<pubPlace>Houghton Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">DDFlow: Learning optical flow with unlabeled data distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Selflow: Self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DARPA Image Understanding Workshop</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">What makes good synthetic training data for learning disparity and optical flow estimation? IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unflow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint 3d estimation of vehicles and scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Workshop on Image Sequence Analysis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Manduchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unos: Unified unsupervised optical-flow and stereo-depth estimation by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised deep epipolar flow for stationary or dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">DF-Net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
