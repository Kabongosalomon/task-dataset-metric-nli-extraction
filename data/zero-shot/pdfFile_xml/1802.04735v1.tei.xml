<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Scene Completion Combining Colour and Depth: preliminary experiments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><forename type="middle">B S</forename><surname>Guedes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Te?filo E. de Campos Universidade de Bras?lia Bras?lia-DF</orgName>
								<address>
									<postCode>70910-900</postCode>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">Hilton</forename><surname>Cvssp</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey Guildford</orgName>
								<address>
									<postCode>GU2 7XH</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Scene Completion Combining Colour and Depth: preliminary experiments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic scene completion is the task of producing a complete 3D voxel representation of volumetric occupancy with semantic labels for a scene from a single-view observation. We built upon the recent work of Song et al. <ref type="bibr" target="#b12">[13]</ref>, who proposed SSCnet, a method that performs scene completion and semantic labelling in a single end-to-end 3D convolutional network. SSCnet uses only depth maps as input, even though depth maps are usually obtained from devices that also capture colour information, such as RGBD sensors and stereo cameras. In this work, we investigate the potential of the RGB colour channels to improve SSCnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of reasoning about scenes in 3D is one of the seminal goals of Computer Vision <ref type="bibr" target="#b7">[8]</ref>. If the 3D geometry of a scene is known, robots are able to plan trajectories, avoid collisions or clean surfaces. If the semantic labels of each surface or voxel is also known a robot can also figure interact with the environment and perform more complex tasks, such as moving objects from one location to another; opening/closing doors, drawers, windows; operating kitchen appliances etc. Three-dimensional maps with labelled voxels have several other applications, including surveillance, assistive computing, augmented reality and so on. One issue is that capturing the full geometry of a scene can be time consuming (if it is done using a scanning technique <ref type="bibr" target="#b8">[9]</ref>) or expensive (if it is done using a rig of calibrated sensors).</p><p>It is well known that vision is a combination of so called bottom-up and top-down processes <ref type="bibr" target="#b7">[8]</ref>. Bottom-up information can be obtained by matching local features for stere-The work described in this extended abstract and in the attached poster was presented at the ICCV 2017 Workshop on 3D Reconstruction meets Semantics (3DRMS). opsis and top-down is the use of prior knowledge from related scenes and objects. If both types of cues are combined, it is possible to estimate a complete scene geometry by using a single visual and depth map of a scene. This is well illustrated in <ref type="figure">Figure 2</ref> of <ref type="bibr" target="#b12">[13]</ref>. A visual sensor captures a single view of a scene which provides measurements (e.g. RGB and Depth) of the visible objects but it is not possible to measure the geometry of occluded regions. However, if the class of the objects is identified, it is possible to infer the complete scene geometry, enabling a full 3D representation to be proposed.</p><p>Solid computational demonstrations of this have started to be published recently. Notably, Song et al. <ref type="bibr" target="#b12">[13]</ref> introduced the problem of Semantic Scene Completion (SSC), i.e., given an depth map, the goal is to generate a 3D image where each voxel is associated to one out of N + 1 labels, where there are N known object labels plus an 'empty space' label.</p><p>In <ref type="bibr" target="#b12">[13]</ref>, this problem is approached using a Deep 3D Convolutional Neural Network coined SSCNet. That paper demonstrates impressive results on completing and labelling a full 3D scene generated from a single depth map. Using a combination of bottom-up dues (from the depth sensor) and top-down cues (learnt from the training set), their method is able to infer the geometry and labels of the whole scene, including heavily occluded regions, such as the regions under tables and behind sofas, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> of <ref type="bibr" target="#b12">[13]</ref>.</p><p>However, one of the main limitations of SSCNet is that it was not designed to use any colour information, only depth maps are used. This clearly impairs the method as indoor scenes generally include various sources of error in depth and geometry estimation. Highly reflective scenes with glass, mirrors or shiny surfaces usually induce false depth. If depth is captured using stereo cameras, textureless and non-Lambertian surfaces often result in errors in feature detection and matching. Colour information also 1 disambiguates between different objects that have similar shape or that are co-planar, like posters on the wall. Furthermore, it is clear that colour offers crucial information for semantic labelling that strongly complement depth information, as seen in papers that focus on semantic segmentation from RGBD images, e.g. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>In this paper, we propose to use colour in addition to depth for Semantic Scene Completion. For that, we propose modifications of the SSCnet architecture in order to fuse RGB and depth. A new input layer was proposed to encode colour in the visible frustum and we combined a feature extraction training technique for multiple view learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Colour SSCNet</head><p>Depth maps are acquired using an RGB-D sensor and using the sensor's intrinsic calibration parameters, a 3D point cloud is generated. The observed geometry is then encoded using flipped Truncated Signed Distance Function (fTSDF), proposed in <ref type="bibr" target="#b12">[13]</ref>. This method associates a value to each point in the 3D space to a function of its distance to the nearest surface point. The sign of this value indicates if it is visible or occluded. Apart from the occlusion coding, this method is viewpoint independent.</p><p>The fTSDF encoding of voxels describe the geometry of the space, but it does not carry any information about the colour or grey level of the visible objects. We propose to encode the RGB values of the visible surfaces in another voxel representation of the scene. The three channels are normalised to range from 0 to 1. Empty spaces and occluded regions are coded with the -1 value for the three colour channels.</p><p>We apply these two encoding techniques to RGB and Depth signals and run them through a 3D CNN that learns to map from RGBD to a labelled 3D volume. Labelled volumes were obtained as described in <ref type="bibr" target="#b12">[13]</ref>, i.e., the binvox voxelisation technique <ref type="bibr" target="#b9">[10]</ref> was applied to 3D models, which accounts for both surface and interior voxels using a space carving approach.</p><p>We built upon the 3D CNN architecture of SSCNet <ref type="bibr" target="#b12">[13]</ref>. To combine RGB and Depth, we propose the two fusion schemes described below.</p><p>Early fusion. The first layer of SSCNet was adapted so that it takes as input a concatenation of fTSDF and the three colour channels encoded as described above. The remaining of the network is the same.</p><p>Mid-level fusion. This architecture is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, drawn using Caffe <ref type="bibr" target="#b5">[6]</ref> (better viewed on a screen). The numbers in brackets are: kernel size, stride, pad and dilation factor, respectively. The branch on the left is essentially a copy of SSCNet. A colour 3D CNN was built following a similar architecture to SSCNet up to the concatenation layer. This layer originally aggregated the output of five scales gathered from previous layers and it is followed by three 3D convolutional layers. Although this network does not have any fully connected (FC) layer, the last three layers perform the same class as FC layers in classification CNNs, as they are closer to the output, which produces labelled data. A typical mid-level fusion using CNNs is done at the input of the first FC layer. Therefore, we believe the most appropriated layer to fuse SSCNet-like sub-networks is at the concatenation layer. However, the original SSCNet is very memory-intensive and it requires about 7GB of GPU memory to process a single depth image 1 . If we were to duplicate all layers of the network up to the concatenation layer, much more than 12GB would be required, whereas most GPU models available nowadays have up to 12GB of RAM. To add to the challenge, our RGB coding uses three channels per image, rather than one as in the fTSDF model, though this only affects the first convolutional layer. Therefore, some of the convolutional layers were removed from the colour branch of SSCNet, but we have preserved all the dilated convolution layers, as this is a significant feature of SSCNet which widely expands the receptive field of the network <ref type="bibr" target="#b14">[15]</ref>.</p><p>In addition, we also evaluated a colour-only SSCNet, which follows the same architecture as the colour branch of the mid-level RGB-D fusion network, but it is followed by the top three convolutional layers, without aggregating activations from the fTSDF branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments and Training Strategies</head><p>Our evaluations focused on the NYU depth v2 dataset <ref type="bibr" target="#b11">[12]</ref>, using the standard split of 795 training samples and 654 test samples 2 . However, instead of the standard semantic segmentation labels, we used the labels devised for scene completion, where objects are grouped into 7 categories plus window, wall, floor, ceiling and another category that identifies free space. This set of labels originated from <ref type="bibr" target="#b4">[5]</ref>. As explained in <ref type="bibr" target="#b12">[13]</ref>, ground truth volumes were obtained from 3D mesh annotations of <ref type="bibr" target="#b2">[3]</ref>. Our implementation was developed using the Caffe framework <ref type="bibr" target="#b5">[6]</ref>.</p><p>We evaluated the two architectures proposed in Section 2: early and mid-level fusion and compared it against the original SSCNet and colour-only. For all methods that we proposed, training was done following these strategies:</p><p>? Random initialisation: all parameters were randomly initialised and the whole network was trained from scratch.</p><p>? Feature learning: we kept the original SSCNet parameters trained by Song et al. <ref type="bibr" target="#b12">[13]</ref> for all the original layers and optimised only the colour layers, i.e., the original SSCNet parameters were frozen.</p><p>? Fine tuning: this is similar to the strategy above, except that instead of freezing the original layers, we also enabled their parameters to be optimised, but with the learning rate ratio of 0.2 times the ratio of the new layers.</p><p>? Surgery: was applied only for the early fusion approach. It is similar to fine tuning, except that the weights of the input layer which related to depth were set to the original parameters of the first layer of SSC-Net and the other weights (linked to the colour channels) of the same convolutional kernel were initialised randomly.</p><p>Voxel labelling is done by applying soft-max to the scores of the last convolutional layer of the networks and optimisation is done using cross-entropy as a loss function, averaged out over all classes.</p><p>The results were evaluated using the Intersection over the Union (IoU) between predicted class labels and ground truth, averaging out over all voxels in the test set and all classes. We followed <ref type="bibr" target="#b12">[13]</ref> and evaluated our results both in terms of completion (i.e., the ability to detect if an occluded voxel is occupied or free space) and in terms of semantic labelling of voxels of all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Discussion</head><p>Our results so far show that none of the proposed architectures and training strategies actually lead to results that are better than the original SSCNet based only on depth observations, i.e., through the training iterations, our results peaked at scene completion IoU of 56.6 and average semantic scene completion of 30.5, which are both results obtained by the original SSCNet on the test set of the NYU depth v2 dataset. In other words, our experiments in the NYU depth v2 dataset (with the 12 category labels <ref type="bibr" target="#b4">[5]</ref>) show that the proposed method for coding colour information is not as discriminative as fTSDF, neither it complements depth information.</p><p>However, the performance of our colour-only network, initialised with randrom weights, followed a monotonic increase as the number of training iterations increased, though it did not converge with the same number of iterations as the architectures that use depth. Therefore, there is certainly relevant information in RGB, but it should probably be combined with fTSDF in a different way, perhaps using late fusion. Even if early or mid-level fusion are not the ideal strategies in this problem, further investigation is also needed to understand why RGB has not complemented Depth at all. It might be an artefact of the dataset and set annotated classes, as it is possible that geometry alone is already very discriminative. A suggestion is to verify this using more complex scenes with more occlusions or with finer object class labels.</p><p>Our results have also shown that unconstrained Fine Tuning leads to a higher decrease in the loss function than the constrained optimisation methods (Feature Learning and Surgery). However, after 1000 iterations, the test set performance (measured by IoU) starts to decrease due to over-fitting. Although the loss is lower for Fine Tuning, we did not observe a significant difference between the methods in terms of test set performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we reported ongoing work that considers the problem of Semantic Scene Completion in 3D from a single RGBD image. Starting from the 3D CNN architecture of SSCNet <ref type="bibr" target="#b12">[13]</ref>, which used only depth maps as input, we proposed to combine RGB and Depth information using early and mid-level fusion schemes.</p><p>Our preliminary results were not better than the original depth-only method. Therefore, further investigation is needed in order to verify if the dataset (NYU depth v2 with 12 labels obtained from <ref type="bibr" target="#b4">[5]</ref>) prilidedges structural information such that depth is already very discriminative. A finer set of labels or a more complex dataset should be evaluated. Other directions of future work are to evaluate late fusion scheme and investigate other ways to encode RGB information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>We are grateful for the valuable comments and suggestions provided by anonymous reviewers of the first version of this manuscript. TEdC's attendance to this workshop is sponsored by  ?Depth and Colour encoding -The input point cloud is converted to a 3D volume aligned with gravity and following Manhattan assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-</head><p>Depth is encoded using the flipped Truncated Signed Distance Function (fTSDF).</p><p>-Colour is encoded for each voxel as an RGB triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>To compute the colour of a voxel, the inverse of the voxelation process is applied and colour is computed by averaging out the original RGB region.</p><p>? Colour values are normalised to be between 0 and 1.</p><p>? Occluded or empty voxels have their colour set to -1.</p><p>?Learning strategies -For the early fusion network, we tried these strategies:</p><p>? Feature learning: where only the first layer was trained to learn colour parameters and the remaining parameters where locked.</p><p>? Fine tuning: where a learning rate of 0.2 was used for the previously learnt layers and 1.0 for the new layer.</p><p>-For the mid-level fusion network, we also tried learning all parameters from scratch with random initialisation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding and learning</head><p>Results on the NYU dataset ? It is likely that the data balancing strategy is not inducing a high penalty when an empty voxel is predicted as containing an object. ? Colour information will be more crucial on datasets with finer object labels (e.g., poster on the wall).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Network architecture for mid-level fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Funda??o de Apoio a Pesquisa do Distrito Federal (FAP-DF), edital 01/2017, protocolo n o 18708.76.44500.14072017 -Introduced by Song et al. CVPR 2017 (figure extracted from their paper) ?SSC from depth only by Song et al. CVPR 2017</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Savva, and T. Funkhouser. Semantic scene completion from a single depth image. CVPR 2017.T. deCampos thanks FAPDF, process 18708.76.44500.14072017, for partially funding his attendance to ICCV and the 3DMRS workshop.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">conv (12,1,1,1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">conv (12,1,1,1)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">conv (128,1,1,1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">conv (128,1,1,1)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="5">conv (128,1,1,1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">conv (128,1,1,1)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>c o n c a t</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a d d</cell><cell>a d d</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">dilated (64,3,1,2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dilated (64,3,1,2)</cell><cell>dilated (64,3,1,2)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">dilated (64,3,1,2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dilated (64,3,1,2)</cell><cell>dilated (64,3,1,2)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a d d</cell><cell>a d d</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">dilated (64,3,1,2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dilated (64,3,1,2)</cell><cell>dilated (64,3,1,2)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">dilated (64,3,1,2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dilated (64,3,1,2)</cell><cell>dilated (64,3,1,2)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a d d</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fusion schemes</cell><cell>?Early fusion: concatenation at the first layer</cell><cell cols="5">conv (16,7,2,1) conv (32,3,1,1) conv (32,3,1,1) conv (64,3,1,1) conv (64,3,1,1) conv (64,3,1,1) conv (64,3,1,1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?Mid-level fusion conv(32,1,1,1) conv(32,1,1,1)</cell><cell>a d d a d d p o o l i n g conv (16,7,2,1) conv (32,3,1,1) conv (32,3,1,1) conv (64,3,1,1) conv (64,3,1,1) conv (64,3,1,1) conv (64,3,1,1)</cell><cell>p o o l i n g conv (16,7,2,1) conv (64,3,1,1) conv (64,3,1,1)</cell><cell></cell><cell></cell></row><row><cell>Problem statement</cell><cell></cell><cell>observed surface</cell><cell>observed free</cell><cell>occluded</cell><cell>outside view</cell><cell>outside room</cell><cell>wall table table wall</cell><cell>chair chair</cell><cell>floor floor</cell><cell>a) surface labeling b) shape completion c) completion+labeling</cell><cell>Previous method</cell><cell></cell><cell>This work</cell><cell>Main reference:</cell><cell>We thank the authors above for sharing code and pre-trained</cell><cell>SSCnet parameters.</cell><cell>Acknowledgements:</cell></row></table><note>S. Song, F. Yu, A. Zeng, A. X. Chang, M.A. Hilton thanks the EPSRC Programme Grant EP/L000539/1 (S3A).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>resuts bellow illustrates one of the potential causes for the degeneration of our results</figDesc><table><row><cell>?Semantic Scene Completion</cell><cell>-inferring labels in 3D</cell><cell>Learning Precision Recall IoU</cell><cell>Strategy (%) (%) (%)</cell><cell cols="2">Mid-level Fine -34.23 17.19</cell><cell>fusion tuning</cell><cell>Feature 36.92 45.03 27.45</cell><cell>learning</cell><cell>Learning -18.30 05.92</cell><cell>from</cell><cell>scratch</cell><cell>Colour Fine 21 46 16</cell><cell>only tuning</cell><cell>Depth Song et al --30.50</cell><cell>only CVPR'17</cell><cell></cell><cell></cell><cell>view of prediction</cell><cell>?Observations</cell><cell>? Phantom cones are being</cell><cell>predicted.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?Top</cell></row><row><cell></cell><cell></cell><cell>IoU</cell><cell>(%)</cell><cell>56.3</cell><cell></cell><cell></cell><cell>56.1</cell><cell></cell><cell>54.20</cell><cell></cell><cell>54.31</cell><cell></cell><cell>46.60</cell><cell></cell><cell></cell><cell>56.6</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Recall</cell><cell>(%)</cell><cell>92.4</cell><cell></cell><cell></cell><cell>93.1</cell><cell></cell><cell>84.11</cell><cell></cell><cell>82.26</cell><cell></cell><cell>100.0</cell><cell></cell><cell></cell><cell>92.9</cell><cell></cell><cell></cell></row><row><cell>?Scene completion</cell><cell>-inferring geometry</cell><cell>Learning Precision</cell><cell>Strategy (%)</cell><cell>Early Fine 59.4</cell><cell cols="2">Fusion tuning</cell><cell>Feature 58.9</cell><cell>learning</cell><cell>Mid-level Fine 61.50</cell><cell>fusion tuning</cell><cell>Feature 62.46</cell><cell>learning</cell><cell>Learning 45.60</cell><cell>from</cell><cell>scratch</cell><cell>Depth Song et al 59.3</cell><cell>only CVPR'17</cell><cell>Sample result</cell></row></table><note>?The</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The original implementation from the authors, obtained from https: //github.com/shurans/sscnet, actually requires almost 12GB. We removed some redundancy from their code, freeing about 5GB of memory.<ref type="bibr" target="#b1">2</ref> We used the train+validation split for training and the test split for testing, following the sample indices available from https://github.com/shelhamer/fcn.berkeleyvision. org/tree/master/data/nyud.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D indoor scene modeling from RGB-D data: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="278" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 15th Int Conf on Computer Vision</title>
		<meeting>15th Int Conf on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Predicting complete 3D models of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.02437</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indoor scene understanding with RGB-D images: Bottom-up segmentation, object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding real world indoor scenes with synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01" />
			<biblScope unit="page" from="4077" to="4085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient 3D scene labeling using fields of trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2013.380</idno>
	</analytic>
	<monogr>
		<title level="m">Proc 14th Int Conf on Computer Vision</title>
		<meeting>14th Int Conf on Computer Vision<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3064" to="3071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Vision: a computational investigation into the human representation and processing of visual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Marr</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/9780262514620.001.0001</idno>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ISMAR</title>
		<meeting>ISMAR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simplification and repair of polygonal models using volumetric techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Nooruddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<ptr target="http://www.patrickmin.com/binvox/" />
	</analytic>
	<monogr>
		<title level="m">the implementation of binvox, available from Patrik Min&apos;s website</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="191" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Indoor scene segmentation using a structured light sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc ICCV Workshops</title>
		<meeting>ICCV Workshops</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://cs.nyu.edu/?silberman/datasets/nyu_depth_v2.html" />
	</analytic>
	<monogr>
		<title level="m">Proc European Conf on Computer Vision</title>
		<meeting>European Conf on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
	<note>NYU depth v2 dataset available from</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08974</idno>
	</analytic>
	<monogr>
		<title level="m">Proc 30th IEEE Conf on Computer Vision and Pattern Recognition</title>
		<meeting>30th IEEE Conf on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint available as technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised joint feature learning and encoding for RGB-D scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2015.2465133</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
	</analytic>
	<monogr>
		<title level="m">Proc Int Conf on Learning Representations ICLR</title>
		<meeting>Int Conf on Learning Representations ICLR<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Preprint available as arXiv technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
