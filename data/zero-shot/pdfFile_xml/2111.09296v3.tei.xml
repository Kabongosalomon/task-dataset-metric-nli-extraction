<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XLS-R: SELF-SUPERVISED CROSS-LINGUAL SPEECH REPRESENTATION LEARNING AT SCALE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Lakhotia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kritika</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Von Platen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatharth</forename><surname>Saraf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meta</forename><forename type="middle">Ai</forename><surname>Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ai</forename><forename type="middle">?</forename><surname>Outreach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">XLS-R: SELF-SUPERVISED CROSS-LINGUAL SPEECH REPRESENTATION LEARNING AT SCALE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets a new state of the art on VoxLin-gua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can perform as well as English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world. Models and code are available at www.github.com/ pytorch/fairseq/tree/master/examples/wav2vec/xlsr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Self-supervised learning of generic neural representations has gathered much recent interest with a large body of work in natural language processing (NLP; <ref type="bibr" target="#b43">Radford et al. 2018;</ref><ref type="bibr" target="#b15">Devlin et al. 2019;</ref><ref type="bibr" target="#b44">Raffel et al. 2019)</ref>, computer vision <ref type="bibr" target="#b23">He et al., 2020;</ref><ref type="bibr" target="#b8">Caron et al., 2021)</ref> as well as speech processing <ref type="bibr" target="#b51">Schneider et al., 2019;</ref><ref type="bibr" target="#b5">Baevski et al., 2020b;</ref><ref type="bibr" target="#b25">Hsu et al., 2021b;</ref><ref type="bibr" target="#b11">Chung et al., 2021)</ref>. Self-supervised learning provides general representations that can be used across domains and languages.</p><p>Multilingually pretrained NLP models such as mBERT <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref>, XLM-R  or mT5 <ref type="bibr" target="#b64">(Xue et al., 2020)</ref> brought significant improvements in multilingual language understanding <ref type="bibr" target="#b12">(Conneau et al., 2018;</ref><ref type="bibr" target="#b26">Hu et al., 2020;</ref><ref type="bibr" target="#b50">Ruder et al., 2021)</ref>. These models offer a promising path towards more ubiquitous NLP technology by improving performance for low-resource languages through leveraging data from high-resource languages. Furthermore, it is only necessary to maintain a single multilingual model instead of a myriad of monolingual models.</p><p>For speech processing, self-supervised approaches such as wav2vec 2.0 <ref type="bibr" target="#b5">(Baevski et al., 2020b;</ref> have also been extended to the multilingual setting <ref type="bibr" target="#b32">(Kawakami et al., 2020;</ref>. The recent XLSR  leverages cross-lingual transfer from highresource languages to build better representations for languages with little unlabeled data. The largest model, XLSR-53, was trained on about 50K hours of public training data in 53 languages and comprises about 300M parameters . But such models only scratch the surface of self-supervised cross-lingual speech representation learning.  <ref type="figure">Figure 1</ref>: Self-supervised cross-lingual representation learning. We pre-train a large multilingual wav2vec 2.0 Transformer (XLS-R) on 436K hours of unannotated speech data in 128 languages. The training data is from different public speech corpora and we fine-tune the resulting model for several multilingual speech tasks.</p><p>In natural language processing, language models are trained on very large datasets, spanning billions of documents such as CC100  or mC4 <ref type="bibr" target="#b64">(Xue et al., 2020)</ref> to fit models with tens of billions and even trillions of parameters <ref type="bibr" target="#b7">(Brown et al., 2020;</ref><ref type="bibr" target="#b21">Goyal et al., 2021;</ref><ref type="bibr" target="#b18">Fedus et al., 2021)</ref> with strong results on established benchmarks. In contrast, scaling efforts in speech have focused either on supervised multilingual models <ref type="bibr" target="#b35">(Li et al., 2021a)</ref> or monolingual self-supervised models, counting a billion or more parameters <ref type="bibr" target="#b66">(Zhang et al., 2020;</ref>, while cross-lingually pretrained speech models are much smaller in scale.</p><p>To this end, we present XLS-R, a large-scale cross-lingually pretrained wav2vec 2.0 model (see illustration in <ref type="figure">Figure 1</ref>) whose name is inspired by XLM-R in NLP. It leverages new publicly available VoxPopuli data, comprising 372K hours of unannotated speech <ref type="bibr" target="#b58">(Wang et al., 2021a)</ref>, the MLS corpus <ref type="bibr" target="#b42">(Pratap et al., 2020)</ref>, CommonVoice <ref type="bibr" target="#b1">(Ardila et al., 2020)</ref>, <ref type="bibr">BABEL (Gales et al., 2014)</ref> and VoxLingua107 <ref type="bibr" target="#b54">(Valk &amp; Alum?e, 2020)</ref> to cover 128 different languages from various regions of the world. To our knowledge, this is the largest effort to date, in making speech technology accessible for many more languages using publicly available data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Our work builds on  who pretrain wav2vec 2.0 models on data from multiple languages. wav2vec 2.0 contains a convolutional feature encoder f : X ? Z to map raw audio X to latent speech representations z 1 , . . . , z T which are input to a Transformer g : Z ? C to output context representations c 1 , . . . , c T <ref type="bibr" target="#b4">(Baevski et al., 2020a)</ref>. Each z t represents 25ms of audio strided by 20ms and the Transformer architecture follows BERT <ref type="bibr" target="#b56">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b15">Devlin et al., 2019)</ref>.</p><p>During training, feature encoder representations are discretized to q 1 , . . . , q T with a quantization module Z ? Q to represent the targets in the objective. The quantization module uses a Gumbel softmax to choose entries form the codebooks and the chosen entries are concatenated to obtain q <ref type="bibr" target="#b30">(Jegou et al., 2011;</ref><ref type="bibr" target="#b29">Jang et al., 2016;</ref><ref type="bibr" target="#b4">Baevski et al., 2020a)</ref>.</p><p>The model is trained by solving a contrastive task over masked feature encoder outputs. At training time, spans of ten time steps with random starting indices are masked. The objective requires identifying the true quantized latent q t for a masked time-step within a set of K = 100 distractors Q t sampled from other masked time steps. ? log exp(sim(ct,qt)) q?Q t exp(sim(ct,q)) where c t is the output of the Transformer, and sim(a, b) denotes cosine similarity. The objective is augmented by a codebook diversity penalty to encourage the model to use all codebook entries <ref type="bibr" target="#b16">(Dieleman et al., 2018)</ref>.</p><p>The model is trained on multiple languages to obtain cross-lingual representations. Specifically, training batches contain samples from multiple languages L <ref type="bibr" target="#b15">(Devlin et al., 2019;</ref><ref type="bibr" target="#b34">Lample &amp; Conneau, 2019;</ref> by sampling from a distribution p l ? n l N ? where l = 1, . . . , L, while n l is the amount of unlabeled data for each language, and ? is the upsampling factor which controls the trade-off between high-and low-resource languages during pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA AND EVALUATION</head><p>In this section, we outline the datasets on which XLS-R is pretrained as well as the language coverage. We also describe the downstream tasks on which we evaluate our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TRAINING DATA</head><p>We pretrain our models on a total of 436K hours of publicly available data from the following sources:</p><p>? VoxPopuli (VP-400K) comprises a total of 372K hours of data 2 in 23 European languages of parliamentary speech from the European parliament <ref type="bibr" target="#b58">(Wang et al., 2021a)</ref>. This makes it the largest publicly available speech corpus for semi-supervised learning. ? Multilingual Librispeech (MLS) contains data in eight European languages totaling around 50K hours of data <ref type="bibr" target="#b42">(Pratap et al., 2020)</ref>. The majority of the data is English (44K hours). ? CommonVoice (CV) is a corpus of read speech. We use the December 2020 release (v6.1; <ref type="bibr" target="#b1">Ardila et al. 2020</ref>) which covers 60 languages and over 7K hours of speech audio, ranging from over 1.6K hours for English to less than one hour for languages such as Hindi. ? VoxLingua107 (VL) is a dataset of 6.6K hours of data in 107 languages based on YouTube content <ref type="bibr" target="#b54">(Valk &amp; Alum?e, 2020)</ref> with an average of 62 hours of data per language. ? BABEL (BBL) is a multilingual corpus of conversational telephone speech of about 1K hours of data in 17 African and Asian languages <ref type="bibr" target="#b20">(Gales et al., 2014)</ref>.</p><p>To the best of our knowledge, this is the largest dataset used for training a publicly available selfsupervised speech model to date. <ref type="figure">Figure 2</ref> shows the data distribution across the 128 different languages in our training dataset. There are about 24 high-resource languages with more than 1K hours of data each, almost all of which are European, except for Kinyarwanda which is African. Then there is a small number of 17 mid-resource languages which have more than 100 hours of data (but less than 1K hours) which includes Catalan, Persian, Turkish, Russian, and Basque. Finally, the remaining 88 languages are low-resource and have less than 100 hours of data each. <ref type="table">Table 1</ref> lists all the languages, together with their ISO code, language family, sub-grouping and the amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DOWNSTREAM EVALUATION</head><p>We evaluate on a broad and diverse set of downstream tasks to showcase the generalization ability of our pretrained models across tasks, data regimes, domains and languages.  <ref type="table">Table 1</ref>: Languages on which XLS-R is trained on together with their ISO code, language family, sub-grouping and the amount of data for pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">AUTOMATIC SPEECH TRANSLATION (AST)</head><p>CoVoST-2 AST. For speech translation evaluation we adopt CoVoST-2 , a multilingual speech translation benchmark based on CommonVoice <ref type="bibr" target="#b1">(Ardila et al., 2020)</ref>. 3 It provides data for translating from English into 15 languages (En ? X) and from 21 languages into English (X ? En). The En ? X languages are: Arabic (ar), Catalan (ca), Welsh (cy), German (de), Estonian (et), Persian (fa), Indonesian (id), Japanese (ja), Latvian (lv), Mongolian (mn), Slovenian (sl), Swedish (sv), Tamil (ta), Turkish (tr), Chinese (zh) where each direction comprises about 430 hours of training data. The X ? En languages include all target languages of En ? X as well as Spanish (es), French (fr), Italian (it), Dutch (nl), Portuguese (pt). We group the latter into high-resource (136-264h of train data; fr, de, es, ca), mid-resource (10-49h of train data; fa, it, ru, pt, zh), and low-resource (2-7h of train data; tr, ar, et , mn, nl, sv, lv, sl, ta, ja, id, cy) for ease of presentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">AUTOMATIC SPEECH RECOGNITION (ASR)</head><p>BABEL ASR. BABEL is a challenging speech recognition benchmark from IARPA consisting of noisy telephone conversational data. <ref type="bibr">4</ref> We evaluate on five languages: Assamese (as), Tagalog (tl), Swahili (sw), Lao (lo), and Georgian (ka). Training sets comprise between 30 and 76 hours of annotated data. Following , we use 10% of the training set for validation, and report test results on the BABEL dev set. We report word error rate (WER) and use n-gram language models trained on CommonCrawl data.</p><p>Multilingual LibriSpeech ASR. Multilingual LibriSpeech (MLS; <ref type="bibr" target="#b42">Pratap et al. 2020</ref>) is a large corpus derived from read audiobooks of Librivox and consists of eight European languages: Dutch (nl), English (en), French (fr), German (de), Italian (it), Polish (pl), Portuguese (pt), Spanish (es). <ref type="bibr">5</ref> Training sets comprise between 104 hours for Polish and 44.7K hours for English. We use the 10 hour training splits of  and report word error rate with the official n-gram models provided by the MLS dataset.</p><p>CommonVoice ASR. Following <ref type="bibr" target="#b49">Rivi?re et al. (2020)</ref>, we use ten languages of CommonVoice for ASR evaluation: Spanish (es), French (fr), Italian (it), Kyrgyz (ky), Dutch (nl), Russian (ru), Swedish (sv), Turkish (tr), Tatar (tt) and Chinese-Hong Kong (zh-HK). 6 CommonVoice contains read speech primarily from Wikipedia sentences. Following prior work <ref type="bibr" target="#b49">(Rivi?re et al., 2020;</ref>, we fine-tune models on just one hour of labeled data per language, a few-shot scenario. Results are reported in terms of phoneme error rate (PER) without a language model.</p><p>VoxPopuli ASR. Following <ref type="bibr" target="#b58">Wang et al. (2021a)</ref>, we evaluate on languages which have at least ten hours of labeled data which is a total of 14 languages: English (en), German (de), Italian (it), French (fr), Spanish (es), Polish (pl), Romanian (ro), Hungarian (hu), Dutch (nl), Czech (cs), Slovenian (sl), Finnish (fi), Croatian (hr), Slovakian (sk). 7 Models are fine-tuned on the full train set, which ranges from 543 hours (English) to ten hours (Slovenian). We report word error rate without language models.</p><p>LibriSpeech ASR. LibriSpeech is a widely-used evaluation benchmark for speech recognition research <ref type="bibr" target="#b40">(Panayotov et al., 2015)</ref>. It consists of 960 hours of English annotated data. Following <ref type="bibr" target="#b5">Baevski et al. (2020b)</ref>, we use the 10 minute, 1 hour and 10 hour training splits. We compare the performance of XLS-R models against English-only wav2vec 2.0 models. We report word error rate without language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">SPEECH CLASSIFICATION (LID AND SPEAKER ID)</head><p>VoxLingua107 LangID. For spoken language identification we consider VoxLingua107 <ref type="bibr" target="#b54">(Valk &amp; Alum?e, 2020)</ref> which spans 107 languages. 8 It consists of short speech segments automatically extracted from YouTube videos. The total amount of speech data in the training set is 6,628 hours and the average per language is 62 hours. We report accuracy on the official test set which covers 33 different languages.</p><p>VoxCeleb1 SpeakerID. We use VoxCeleb1 for speaker identification <ref type="bibr" target="#b38">(Nagrani et al., 2017)</ref>. 9 VoxCeleb1 is an audio-visual dataset consisting of short clips of human speech, extracted from interview videos uploaded to YouTube. It consists of 1,251 unique speakers and 153K utterances. We use the official dataset splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETUP</head><p>In this section, we give more details on architectures and hyperparameters used during pretraining and finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PRETRAINING</head><p>We use the wav2vec 2.0 implementation available in fairseq <ref type="bibr" target="#b39">(Ott et al., 2019</ref>) and evaluate several model architectures detailed in <ref type="table" target="#tab_2">Table 2</ref>. We consider models with between 0.3B parameters to 2B parameters. To optimize GPU memory usage, we use a fully sharded backend <ref type="bibr" target="#b46">(Rajbhandari et al., 2021)</ref> as well as activation checkpointing <ref type="bibr" target="#b9">(Chen et al., 2016)</ref> as implemented in FairScale <ref type="bibr">(Baines et al., 2021)</ref>.</p><p>Models are optimized with Adam (Kingma &amp; Ba, 2015) and the learning rate is warmed up for the first 32K steps followed by polynomial decay to zero for the remainder of training. Training audio sequences are cropped to a maximum of 320K samples, or 20 seconds and all models were pretrained for a total of one million updates. XLS-R (0.3B) was trained on 128 GPUs with nearly 2M samples on each GPU, totaling about 4.3h of data in a batch. Larger models were trained on 200 GPUs with 800K to 1M samples on each GPU giving an effective batch size of about 2.8-3.6 hours.</p><p>Our training data covers 128 languages and five training corpora with different characteristics. To balance data from the different languages and corpora we upsample both training corpora and languages. We first upsample the languages within a particular corpus using the strategy outlined in ?2 and then balance the different corpora using the same strategy by treating each corpus as a different language. We use ? = 0.5 in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SPEECH TRANSLATION</head><p>To build speech translation models, we multilingually fine-tune XLS-R models by training on the combined labeled data of all En ? X or X ? En language directions without upsampling any direction. We stack a decoder network on top of XLS-R which is a  decoder network is initialized with weights from multilingually fine-tuned mBART <ref type="bibr" target="#b37">(Liu et al., 2020;</ref><ref type="bibr" target="#b36">Li et al., 2021b;</ref><ref type="bibr">Tang et al., 2021)</ref> and uses the same vocabulary with 250K types. The total size of the decoder network is 459M parameters.</p><p>In our ablations, we also consider bilingually fine-tuned speech translation models which use a much smaller decoder of 16M parameters which has seven layers, embedding size 256, 4 attention heads and feed forward network dimension 2048. For this, a 10K byte-pair encoding (BPE; <ref type="bibr" target="#b52">Sennrich et al. 2016</ref>) vocabulary is built on the CoVoST 2 target text for each target language.</p><p>We fine-tune with Adam (Kingma &amp; Ba, 2015), a learning rate of 3e-4, label smoothing with probability 0.1, an effective batch size of 66M samples, or nearly 68 minutes, layer drop 0.05, a masking strategy similar to wav2vec 2.0 with mask length 5 and mask probability 0.15. During fine-tuning, the wav2vec 2.0 encoder is not updated for the first 10K updates. Models are fine-tuned for 250K updates in total and the best checkpoint is selected based on validation BLEU. We choose the learning rate by searching in the interval [3e ? 5, 3e ? 4]. Translations are generated with a beam size of 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">AUTOMATIC SPEECH RECOGNITION</head><p>For fine-tuning, we follow the settings of <ref type="bibr" target="#b5">Baevski et al. (2020b)</ref> by adding a linear layer on top of the pretrained model to predict the output vocabulary and train using Connectionist Temporal Classification (CTC; <ref type="bibr" target="#b22">Graves et al. 2006)</ref>. The output vocabulary is characters for all benchmarks, except for CommonVoice where we use phonemes. We fine-tune using Adam and the learning rate is warmed up for the first 10% of total updates, kept constant for the next 40% and then decayed to zero in the remaining 50% of updates. Since the amount of labeled data differs widely for each dataset we found the following number of training updates to be effective: 20K updates for BABEL, 13K updates for CommonVoice, 20K updates for MLS and 50K updates for VoxPopuli.</p><p>We found large batch sizes to be very effective. For XLS-R (0.3B) and XLS-R (1B), we use an effective batch size of 0.44 hours and for XLS-R (2B) we used 1.06 hours. Learning rate as well as batch size was tuned based on dev error rate and we searched the range [2e ? 5, 3e ? 4] for XLS-R (0.3B) and XLS-R (1B) as well as [3e ? 6, 3e ? 5] for XLS-R (2B). To reduce overfitting for XLS-R (2B), we increase stochastic depth to 15% compared to 10% for all other setups <ref type="bibr" target="#b27">(Huang et al., 2016;</ref>. We use a masking probability of between 30-75%, depending on the setup and which is determined on the development set.</p><p>For MLS and BABEL, we use a language model for decoding. We tune the language model weight within the interval [0, 5] and a word insertion penalty within the interval [?5, 5] using Bayesian optimization. <ref type="bibr">10</ref> We run 128 trials with beam 500 and choose the best set of parameters according to the dev error rate. For Common Voice and Vox Populi we do not use a language model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>Next, we analyze the results of our pretrained models on all downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SPEECH TRANSLATION</head><p>We conduct an extensive study on the CoVoST-2 speech translation benchmark. The task entails translating speech audio in one language into another language with text as output. Performance is evaluated in terms of BLEU <ref type="bibr" target="#b41">(Papineni et al., 2002)</ref>. Models are simultaneously fine-tuned either on all 21 translation directions with English as target language (X ? En) or on all the 15 directions where English is the input language (En ? X; see ?3.2.1), resulting in only two models instead of 36.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">X ? ENGLISH</head><p>For X ? English directions we group languages into high-resource, mid-resource and low-resource directions ( ?3.2.1) and compare to several baselines: in order to directly compare to XLSR-53 , and VP-100K <ref type="bibr" target="#b58">(Wang et al., 2021a)</ref>, we fine-tune these publicly available models following the same protocol as XLS-R. We also compare to <ref type="bibr" target="#b36">Li et al. (2021b)</ref>, the best known results from the literature who either use an English-pretrained wav2vec 2.0 model (XMEF-En) for En ? X directions or the multilingually pretrained XLSR-53 (XMEF-X) for X ? En directions. There is a trend of larger capacity in pretrained models enabling few-shot learning for speech translation, similar to wav2vec 2.0 enabling few-shot speech recognition <ref type="bibr" target="#b5">(Baevski et al., 2020b;</ref>. For example, on language pairs with only two hours of labeled speech translation data, XLS-R (2B) improves over XLS-R (0.3B) as follows: from 10.3 BLEU to 29.6 BLEU on Swedish-English, from 1.4 BLEU to 16.5 BLEU on Indonesian-English and from 3.0 BLEU to 17.1 BLEU on Arabic-English (see Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">ENGLISH ? X</head><p>For English ? X directions we compare to previous cross-lingually pretrained models (XLSR-53, VP-100K) as well as baselines with English-only pretraining: XMEF JT, the best performing setup of <ref type="bibr" target="#b36">Li et al. (2021b)</ref> for En ? X directions as well as wav2vec 2.0 pre-trained on 60K hours of English Libri-light data and fine-tuned following the same protocol as XLS-R <ref type="bibr" target="#b31">(Kahn et al., 2020;</ref><ref type="bibr" target="#b5">Baevski et al., 2020b)</ref>. The latter has the advantage of being pre-trained on exactly the same language as the input data for all translation directions while cross-lingually pretrained models need to be able to represent many different languages which puts them at a disadvantage. <ref type="table" target="#tab_5">Table 4</ref> shows that XLSR-53 now performs similarly to XLS-R (0.3B) while for X ? English XLS-R (0.3B) performed much better (see ?5.1.1). This is likely because English data dominates the training corpus of XLSR-53 which is not the case for XLS-R ( ?3.1). Both XLS-R (1B) and XLS-R (2B) outperform XMEF JT showing that larger capacity results in better performance.</p><p>We also compare to prior work using the English-only pretrained wav2vec 2.0 LV-60K model <ref type="bibr" target="#b60">(Wang et al., 2021c)</ref> which additionally uses self-training and a language model for decoding. We do not use these techniques. Their results represent the state of the art on these four directions. <ref type="bibr" target="#b60">Wang et al. (2021c)</ref> achieves an average BLEU of 25.6 on the four directions while as XLS-R (2B) rivals this at an average BLEU of 25.5. We note that self-training and LM decoding methods are equally applicable to our approach.</p><p>XLS-R (2B) also performs well compared to English-only pretraining at 27.8 average BLEU compared to 26.6 BLEU for a wav2vec 2.0 model pretrained on 60K hours of Libri-light data and 720M parameters. This confirms that with sufficient capacity, cross-lingual pretraining can perform as well as strong monolingual models .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">ABLATIONS</head><p>We build speech translation models by adopting two design decision from <ref type="bibr" target="#b36">Li et al. (2021b)</ref>: multilingual fine-tuning of pretrained models on labeled speech translation data in multiple translation directions and initializing the decoder network with mBART <ref type="bibr" target="#b36">(Li et al., 2021b;</ref><ref type="bibr">Tang et al., 2021)</ref>. In the following we ablate these two choices to better understand their impact.</p><p>We first compare multilingual fine-tuning to bilingual fine-tuning. For faster experimental turn-around we consider a reduced setup of four English ? X language directions (en-ca, en-ar, en-de, en-tr) as well as all high-resource and mid-resource X ? English directions. <ref type="bibr">11</ref> We compare bilingual fine-tuning to models fine-tuned on all 15 English ? X or all 21 X ? English directions. <ref type="table" target="#tab_6">Table 5</ref> shows that multilingual fine-tuning is particularly effective for X ? English directions where the average improvement is 3.3 BLEU (20.9 BLEU to 24.2 BLEU). The amount of labeled data ranges from 264 hours for French ? English to 10 hours for Chinese ? English and multilingual fine-tuning leverages supervision from high-resource languages to improve performance for languages <ref type="bibr">11</ref> We also did not use mBART initialization for this ablation.</p><p>with less labeled data. Languages with less data benefit both from cross-lingual transfer during pretraining, through training on unlabeled data in other languages, and fine-tuning, through labeled data from other languages <ref type="bibr" target="#b2">(Arivazhagan et al., 2019;</ref>. For English ? X, multilingual fine-tuning performs roughly on par to bilingual fine-tuning which is a desirable outcome given that transfer between language directions is diminished. This is supported by the larger size of the decoder network in multilingual fine-tuning ( ?3.2.1). Next, we analyze the impact of initializing the decoder network with mBART which was pretrained on additional labeled text-to-text machine translation data <ref type="bibr" target="#b37">(Liu et al., 2020;</ref><ref type="bibr" target="#b36">Li et al., 2021b;</ref><ref type="bibr">Tang et al., 2021)</ref>. Specifically, we use MBART-ML50N1 (49 languages to English) for X ? English directions and MBART-ML501N (English to 49 languages) for English ? X directions. <ref type="bibr">12</ref> We observe that mBART initialization has little impact on English ? X but it leads to large improvements for X ? English, especially on mid-and low-resource language directions. Initializing the decoder network with mBART resulted in some low-resource languages moving from 1-3 BLEU to 10+ BLEU. The labeled translation data used to train mBART helps speech translation to adapt faster to the low supervision in mid/low-resource language pairs of the CoVoST-2 benchmarks where many language directions have only a few hours of labeled data. This shows that pretraining both the encoder and decoder, multilingual fine-tuning, as well as the use of extra machine translation data through mBART, enables few-shot learning for some speech translation directions which have only a few hours of labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SPEECH RECOGNITION</head><p>Our experiments cover four common speech recognition benchmarks, 26 different languages, three different domains and both low-data and high-data regimes. The BABEL dataset evaluates models on noisy speech ( ?5.2.1), CommonVoice presents a few-shot setup with just one hour of labeled data per language ( ?5.2.2), MLS contains read speech in multiple European languages ( ?5.2.3), and VoxPopuli contains parliamentary speech with varying amounts of labeled data ( ?5.2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">BABEL</head><p>BABEL consists of the hardest speech recognition setting among our four benchmarks which results in higher word error rates. Languages are low-resource, the speech is very noisy and corresponds to natural telephone conversation. Many competitions have tackled this dataset <ref type="bibr" target="#b0">(Alum?e et al., 2017;</ref><ref type="bibr" target="#b45">Ragni et al., 2018;</ref><ref type="bibr" target="#b28">Inaguma et al., 2019)</ref> and baselines are thus well tuned. We compare to the best numbers we have found in the literature, as well as our own best baselines. <ref type="table" target="#tab_8">Table 7</ref> shows that XLS-R (0.3B) outperforms the equally sized XLSR-53, which was the previous state of the art on all languages by an average of 1.4 WER, e.g., on Assamese (as), WER decreases from 44.1 to 42.9, on Swahili (sw) WER decreases from 26.5 to 24.3 and on Georgian (ka) WER drops from 31.1 to 28.0 WER. XLSR-53 and XLS-R were both pretrained on the same BABEL data, and the better performance of XLS-R (0.3B) shows that pretraining on additional out-of-domain datasets such as VoxPopuli does help performance on BABEL. This is similar to findings for monolingual pretraining <ref type="bibr" target="#b24">(Hsu et al., 2021a)</ref>.</p><p>Using additional capacity, XLS-R (1B) outperforms XLS-R (0.3B) by 2.5 WER on average. On Georgian (ka), this corresponds to improvements of 6 WER and 7.1 WER compared to  and <ref type="bibr" target="#b0">Alum?e et al. (2017)</ref>, respectively. Compared to results from only three years ago from <ref type="bibr" target="#b45">Ragni et al. (2018)</ref> and <ref type="bibr" target="#b28">Inaguma et al. (2019)</ref>, XLS-R (1B) reduces word error rate by more than 10 WER, from 40.6 to 30.6 on Tagalog and from 35.5 to 21.2 on Lao. XLS-R (2B) improves over XLS-R (1B) by 0.8 BLEU on average showing that additional capacity can further improve performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">COMMONVOICE</head><p>CommonVoice is an easier task than BABEL because it is read-speech but we use a reduced labeled data setup which introduces a different challenge. Specifically, we use the train/dev/test splits of <ref type="bibr" target="#b49">Rivi?re et al. (2020)</ref> which corresponds to a few-shot scenario where only one hour of training data is available per language.</p><p>On English speech recognition, pretraining has been shown to be particularly beneficial for low labeled data settings <ref type="bibr" target="#b5">(Baevski et al., 2020b)</ref>. This is similar to cross-lingual pretraining  where pretraining on the large MLS corpus significantly improved performance over pretraining only on CommonVoice data, e.g., on Dutch accuracy improved from 14 PER to 5.8 PER. <ref type="table" target="#tab_9">Table 8</ref> shows that the additional training data of XLS-R compared to XLSR-53 results in better performance of 1.1 PER on average for XLS-R (0.3B). XLS-R uses the same training data as XLSR-53 plus the very large VP-400K corpus of parliamentary speech as well as the much smaller VoxLingua-107 which consists of YouTube data, both of which are out of domain with respect to the read audiobook domain of CommonVoice. This confirms that pretraining on more out of domain data can still improve performance <ref type="bibr" target="#b24">(Hsu et al., 2021a)</ref>.</p><p>Furthermore, accuracy improves even on languages for which XLS-R does not add any pretraining data compared to XLSR-53, e.g., Kyrgyz (ky) improves from 6.1 PER to 5.1 PER for XLS-R (0.3B) and 4.1 PER for XLS-R (1B) and both models are pretrained on only about 11 hours of Kyrgyz data -0.003% of the total pretraining data. This shows that there is cross-lingual transfer that benefits low-resource languages and that additional capacity is important to realize this effect. Chinese improves the least and gains are particularly large for languages for which the training corpus of XLS-R contains more data due to VoxPopuli, e.g., for Swedish VP-400K adds more than 16K hours of unannontated speech and performance improves from 12.2 PER to 5.5 PER when comparing XLSR-53 to XLS-R (1B). Finally, XLS-R (2B) performs slightly better than XLS-R (1B) on average with some languages improving while as others are performing slightly worse. The modest average improvement is likely because error rates are already low on this benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">MULTILINGUAL LIBRISPEECH</head><p>Multilingual LibriSpeech is a common benchmark for evaluating multilingual speech recognition on eight European languages. We consider a setup where we use ten hours of labeled data for each language  and compare to the prior work of <ref type="bibr" target="#b42">Pratap et al. (2020)</ref> which uses all available labeled data as well as XLSR-53  which uses the same ten hour reduced labeled data setup. <ref type="table" target="#tab_10">Table 9</ref> shows that XLS-R can outperform XLSR-53 on average by 1 WER at equal capacity and that additional model capacity results in an improvement of 2.9 WER on average for XLS-R (1B). This result rivals the performance of the supervised models of <ref type="bibr" target="#b42">Pratap et al. (2020)</ref> which is based on significantly more labeled data compared to the ten hour setup of XLS-R. Finally, on average XLS-R (2B) does not show improvements over XLS-R (1B), which is similar to CommonVoice.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">LIBRISPEECH</head><p>On LibriSpeech English ASR, we compare XLS-R (0.3B) and XLS-R (1B) to the wav2vec 2.0 English baseline. We see in <ref type="table" target="#tab_13">Table 11</ref> that with the same capacity and same fine-tuning procedure, the English wav2vec 2.0 significantly outperforms the XLS-R (0.3B) in all data regimes, showing the capacity dilution and interference problem of our multilingual model. However, when increasing the capacity, the model is able to catch up with the monolingual results. In particular, XLS-R (1B) outperforms wav2vec 2.0 LV-60k on the 10 minute setting, but is at a disadvantage in the 10 hour setting, where the English-focused monolingual model outperforms it by 0.7 WER on average. This suggests that higher-capacity models can circumvent the interference problem and can get strong results on high-resource languages, while still leveraging their cross-lingual transfer ability for lower-resource languages .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SPEECH CLASSIFICATION</head><p>Finally, we evaluate our approach on two speech utterance classification tasks, language identification and speaker identification. For these tasks we use our smallest model as these tasks require less 5.9 10.5 5.9 10.6 capacity given the lower complexity of the tasks compared to the structured prediction problems of speech recognition and speech translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">LANGUAGE IDENTIFICATION</head><p>For language identification we adopt the setup of VoxLingua107 <ref type="bibr" target="#b54">(Valk &amp; Alum?e, 2020</ref>) which provides data for 107 different languages. We train our model on the official train set, and report results on the development set, comprising 33 languages. <ref type="table" target="#tab_2">Table 12</ref> shows that our best model outperforms previous work, improving the best known prior work of <ref type="bibr" target="#b47">Ravanelli et al. (2020)</ref> by 1% absolute, a relative error reduction of 15%. For comparison, we also fine-tune the English-only wav2vec 2.0 pretrained on Libri-Light which performs surprisingly well on this multilingual task but is outperformed by the XLS-R model by 1.5% error rate on average.  <ref type="table" target="#tab_3">(Table 13)</ref> show that our cross-lingual model also performs very well for speaker identification, even though utterances are mostly in English. <ref type="table" target="#tab_3">Table 13</ref>: Speaker identification accuracy on VoxCeleb1 in terms of accuracy. We report baselines from previous work as well as XLS-R (B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy (%)</head><p>Previous work CNN <ref type="bibr" target="#b38">(Nagrani et al. (2017)</ref>) 80.5 SUPERB-Hubert Large <ref type="bibr">(Yang et al., 2021)</ref> 90.3</p><p>This work XLS-R (0.3B) 95.8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">DISCUSSION</head><p>Single model. Cross-lingual training results in a single model for multiple languages compared to a separate model for each language. Training a cross-lingual model requires more effort than a single monolingual model but the resulting model can be used for many different languages. Advances in architectures and training can also be deployed more easily since we only need to retrain a single model rather than many different ones.</p><p>In terms of accuracy, prior work in self-supervised learning for speech established that cross-lingually pretrained models are very competitive to monolingually pretrained models for speech recognition . Our experiments show a similar trend for speech-translation: XLS-R can perform very competitively to English-only pretrained models for English ? X speech translation where the encoder only needs to encode English speech -a setting which favors monolingually pretrained models.</p><p>Performance trends. Overall, XLS-R performs best for low-resource and mid-resource languages. For speech translation, we observe strong improvements for low-and mid-resource X ? English directions and comparatively smaller gains on high-resource directions. Many low-resource directions which previously had performance in the 1-5 BLEU range improve to over 10-20 BLEU due to the better cross-lingual speech representations. For English ? X directions, large enough cross-lingual models can even surpass the performance of English-only pretrained models.</p><p>Similarly, for speech recognition, we see strong improvements on BABEL, CommonVoice and VoxPopuli, benchmarks which include low-and mid-resource tasks. <ref type="bibr">13</ref> We find that models trained on more data from more languages can perform as well or better than comparable models of the same size and we we observe this trend across all speech recognition benchmarks. Keeping everything else equal, larger capacity models often further improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>XLS-R is a new self-supervised cross-lingual speech representation model which scales the number of languages, the amount of training data as well as model size. The training corpus is an order of magnitude larger than prior work and covers 128 languages in 436K hours of recorded speech audio.</p><p>The resulting model enables state of the art results for X ? English speech translation on CoVoST-2, outperforming prior art by a sizeable margin with the largest improvements on mid-and low-resource directions. It also performs competitively to the best English ? X work, without the use of equally applicable techniques such as self-training and language model decoding.</p><p>On speech recognition, XLS-R sets a new state of the art on CommonVoice, VoxPopuli, several languages of BABEL, while performing competitively on MLS with much less labeled data. These datasets cover a wide range of languages, data regimes and domains, demonstrating the generalization ability of XLS-R. Our model also sets a new state of the art on the VoxLingua107 language identification benchmark. The largest XLS-R model comprises 2B parameters which enables it to outperform a strong English-only pretrained model on English ? X speech translation, a setting which favors monolingually pretrained models. This shows that cross-lingually trained models with sufficient capacity can perform as well as specialized monolingually pretrained models. We hope XLS-R will help catalyze research in speech technology for many more languages of the world. Models and code are publicly available on several platforms. Prior work XLSR-53 0.7 0.6 20.5 2.8 1.9 0.5 0.1 0.4 0.6 5. <ref type="formula">6</ref>    <ref type="table" target="#tab_6">Table 15</ref>: CoVoST-2 English ? X full results. We report baselines results from XLSR-53 , VP-100K <ref type="bibr" target="#b58">(Wang et al., 2021a)</ref>, XMEF-JT <ref type="bibr" target="#b36">(Li et al., 2021b)</ref> and wav2vec 2.0 LV-60K with self-training <ref type="bibr" target="#b60">(Wang et al., 2021c)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A DETAILED SPEECH TRANSLATION RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " U o 4 s F A N + 3 p v Z 3 U m v 6 / J o B H X 7 l 0 Q = " &gt; A A A B 8 n i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I Q d 0 V 3 L h w U c U + Y D q U T J p p Q z P J k G S E M v Q z 3 L h Q x K 1 f 4 8 6 / M d P O Q l s P B A 7 n 3 E v O P W H C m T a u + + 2 U 1 t Y 3 N r f K 2 5 W d 3 b 3 9 g + r h U U f L V B H a J p J L 1 Q u x p p w J 2 j b M c N p L F M V x y G k 3 n N z k f v e J K s 2 k e D T T h A Y x H g k W M Y K N l f x + j M 2 Y Y J 7 d z Q b V m l t 3 5 0 C r x C t I D Q q 0 B t W v / l C S N K b C E I 6 1 9 j 0 3 M U G G l W G E 0 1 m l n 2 q a Y D L B I + p b K n B M d Z D N I 8 / Q m V W G K J L K P m H Q X P 2 9 k e F Y 6 2 k c 2 s k 8 o l 7 2 c v E / z 0 9 N d B V k T C S p o Y I s P o p S j o x E + f 1 o y B Q l h k 8 t w U Q x m x W R M V a Y G N t S x Z b g L Z + 8 S j o X d a 9 R v 7 5 v 1 J o P R R 1 l O I F T O A c P L q E J t 9 C C N h C Q 8 A y v 8 O Y Y 5 8 V 5 d z 4 W o y W n 2 D m G P 3 A + f w C J t Z F + &lt; / l a t e x i t &gt;S elf-supervised pre-training Transformer VoxPopuli 372k hours -23 langs Parliament speech</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>? (English, German, ...) Low resource ? (Sinhala, Zulu, ...) Hours of training data Figure 2: Illustration of the unlabeled training data distribution across the 128 languages of XLS-R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model architectures. We show details for prior work, XLSR-53, VP-100K<ref type="bibr" target="#b58">(Wang et al., 2021a)</ref>, and the XLS-R models: the number of languages (#lgs), the pretraining data (Datasets), the number of blocks (B), the number of hidden states (H m ), the inner dimension of feed-forward blocks (H f f ), the number of attention heads (A) and the total number of parameters (#params).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Speech translation: results for X ? English directions on CoVoST-2 in terms of average BLEU for 21 directions grouped into high/mid/low-resource labeled data directions. Appendix A lists detailed results for all languages. high mid low Avg.</figDesc><table><row><cell>Prior work</cell><cell></cell><cell></cell></row><row><cell cols="2">XLSR-53 (Conneau et al., 2021) 30.3 11.1</cell><cell>3.2 10.3</cell></row><row><cell>VP-100K (Wang et al., 2021b)</cell><cell>27.7 13.2</cell><cell>4.6 11.1</cell></row><row><cell>XMEF-En (Li et al., 2021b)</cell><cell>32.4 16.8</cell><cell>4.0 12.4</cell></row><row><cell>XMEF-X (Li et al., 2021b)</cell><cell>34.2 20.2</cell><cell>5.9 14.7</cell></row><row><cell>This work</cell><cell></cell><cell></cell></row><row><cell>XLS-R (0.3B)</cell><cell>30.6 18.9</cell><cell>5.1 13.2</cell></row><row><cell>XLS-R (1B)</cell><cell cols="2">34.3 25.5 11.7 19.3</cell></row><row><cell>XLS-R (2B)</cell><cell cols="2">36.1 27.7 15.1 22.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table /><note>shows a new state of the art with XLS-R (2B), improving over the previous best result (Li et al., 2021b), by 7.4 BLEU on average over all 21 directions (14.7 BLEU vs. 22.1 BLEU). This is largely due to improvements on mid-resource (+7.5 BLEU) and low-resource (+9.2 BLEU) language directions. Model capacity has a large impact: XLS-R (1B) improves over XLS-R (0.3B) by an average of 6.1 BLEU and XLS-R (2B) improves by an average of 2.8 BLEU compared to XLS-R (1B). Appendix A shows detailed results for all translation directions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Speech translation: results for English ? X directions on CoVoST-2 in terms of BLEU. We show detailed results for four language pairs: English-German (en-de), English-Catalan (en-ca), English-Arabic (en-ar) and English-Turkish (en-tr) as well as the average performance over all 15 directions. For faster experimental turnaround we do not use self-training and LM-decoding as<ref type="bibr" target="#b60">Wang et al. (2021c)</ref> and we expect these methods to be equally applicable to XLS-R. Appendix A lists detailed results for all translation directions.</figDesc><table><row><cell>en-ca en-ar en-de en-tr Avg. (15 dir)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Speech translation: multilingual vs. bilingual fine-tuning. Multilingually fine-tuned models are trained on labeled data from either 21 X ? En directions or 15 En ? X directions. Bilingually fine-tuned models are trained on a single pair. We show average BLEU for XLS-R (1B) over four En ? X directions en-(ca, ar, de, tr), and the average of high-and mid-resource directions for X ? En. Note: results are not directly comparable to the rest of the paper and are hence denoted by ( * ).</figDesc><table><row><cell>Bilingual fine-tuning</cell><cell>20.9</cell><cell>23.4</cell></row><row><cell>Multilingual fine-tuning</cell><cell>24.2</cell><cell>23.6</cell></row></table><note>X ? En* En ? X*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Speech translation: multilingual fine-tuning performance ablation in terms BLEU. mBART decoder initialization primarily improves mid-and low resource directions which benefit from the labeled translation data mBART was trained on.</figDesc><table><row><cell></cell><cell>En ? X</cell><cell cols="2">X ? En High Mid Low Avg</cell></row><row><cell>XLS-R (1B)</cell><cell cols="3">26.0 34.3 25.5 11.7 19.3</cell></row><row><cell>-mBART init</cell><cell cols="2">25.1 32.4 17.7</cell><cell>3.6 12.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="6">: Speech recognition results on BABEL in terms of word error rate (WER) on Assamese (as),</cell></row><row><cell cols="2">Tagalog (tl), Swahili (sw), Lao (lo) and Georgian (ka).</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>as</cell><cell>tl</cell><cell>sw</cell><cell>lo</cell><cell>ka</cell></row><row><cell>Labeled data</cell><cell cols="5">55h 76h 30h 59h 46h</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Phoneme recognition performance on CommonVoice in terms of phoneme error rate (PER) when using one hour of labeled data to fine-tune each language. We compare to m-CPC<ref type="bibr" target="#b49">(Rivi?re et al., 2020)</ref>,<ref type="bibr" target="#b19">Fer et al. (2017)</ref>,XLSR-10 (Conneau et al., 2021)  and XLSR-53.</figDesc><table><row><cell></cell><cell>es</cell><cell>fr</cell><cell>it</cell><cell>ky</cell><cell>nl</cell><cell>ru</cell><cell>sv</cell><cell>tr</cell><cell cols="3">tt zh-HK Avg</cell></row><row><cell>Labeled data</cell><cell>1h</cell><cell>1h</cell><cell>1h</cell><cell>1h</cell><cell>1h</cell><cell>1h</cell><cell>1h</cell><cell>1h</cell><cell>1h</cell><cell>1h</cell><cell></cell></row><row><cell>Previous work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>m-CPC</cell><cell cols="9">38.0 47.1 40.5 41.2 42.5 43.7 47.5 47.3 42.0</cell><cell cols="2">55.0 44.5</cell></row><row><cell cols="10">Fer et al. (2017) 36.6 48.3 39.0 38.7 47.9 45.2 52.6 43.4 42.5</cell><cell cols="2">54.3 44.9</cell></row><row><cell>XLSR-10</cell><cell cols="3">7.9 12.6 11.7</cell><cell cols="2">7.0 14.0</cell><cell cols="2">9.3 20.6</cell><cell>9.7</cell><cell>7.2</cell><cell cols="2">22.8 12.3</cell></row><row><cell>XLSR-53</cell><cell>2.9</cell><cell>5.0</cell><cell>5.7</cell><cell>6.1</cell><cell>5.8</cell><cell cols="2">8.1 12.2</cell><cell>7.1</cell><cell>5.1</cell><cell>18.3</cell><cell>7.6</cell></row><row><cell>This work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLS-R (0.3B)</cell><cell>3.1</cell><cell>5.4</cell><cell>4.9</cell><cell>5.1</cell><cell>5.8</cell><cell>6.0</cell><cell>7.2</cell><cell>6.0</cell><cell>4.1</cell><cell>17.0</cell><cell>6.5</cell></row><row><cell>XLS-R (1B)</cell><cell>2.0</cell><cell>3.9</cell><cell>3.5</cell><cell>4.1</cell><cell>4.2</cell><cell>4.1</cell><cell>5.5</cell><cell>4.4</cell><cell>3.4</cell><cell>15.7</cell><cell>5.1</cell></row><row><cell>XLS-R (2B)</cell><cell>2.2</cell><cell>4.0</cell><cell>3.5</cell><cell>4.0</cell><cell>4.7</cell><cell>3.7</cell><cell>5.0</cell><cell>4.0</cell><cell>2.9</cell><cell>14.8</cell><cell>4.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Speech recognition performance on Multilingual LibriSpeech (MLS) in terms of WER. Models are fine-tuned with 10h or all available labeled data (full) for each language. We compare to<ref type="bibr" target="#b42">Pratap et al. (2020)</ref> who uses all labeled data (nearly 45K hours for English). Results are based on a 4-gram language model, except for Polish (pl) where we report Viterbi results for all settings since performance with the provided language model data resulted in inferior accuracy, as previously reported<ref type="bibr" target="#b42">(Pratap et al., 2020)</ref>; we denote this with ( * ).The VoxPopuli corpus provides about 1.8K hours of labeled speech data in 14 languages, ranging from 543 hours for English to 35 hours for Slovakian, as well as about 400K hours of unlabeled speech. This dataset is representative of a setting where a lot of unannotated data in the same domain as the labeled data is available. We compare to the work of<ref type="bibr" target="#b58">Wang et al. (2021a)</ref> which used a cross-lingually pretrained wav2vec 2.0 Base model on an earlier version of VoxPopuli that contained about 10K hours of unlabeled speech.</figDesc><table><row><cell></cell><cell>#ft</cell><cell>en</cell><cell>de</cell><cell>nl</cell><cell>fr</cell><cell>es</cell><cell>it</cell><cell>pt</cell><cell>pl  *  Avg.</cell></row><row><cell>Full labeled data (h)</cell><cell></cell><cell cols="8">44.7K 2K 1.6K 1.1K 918 247 161 104</cell></row><row><cell>Previous work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Pratap et al. (2020) full</cell><cell cols="3">5.9 6.5 12.0</cell><cell cols="5">5.6 6.1 10.5 19.5 19.4 10.7</cell></row><row><cell>XLSR-53</cell><cell>10h</cell><cell cols="8">14.6 8.4 12.8 12.5 8.9 13.4 18.2 17.8 13.8</cell></row><row><cell>This work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLS-R (0.3B)</cell><cell>10h</cell><cell cols="8">15.9 9.0 13.5 12.4 8.1 13.1 17.0 13.9 12.8</cell></row><row><cell>XLS-R (1B)</cell><cell>10h</cell><cell cols="8">12.9 7.4 11.6 10.2 7.1 12.0 15.8 10.5 10.9</cell></row><row><cell>XLS-R (2B)</cell><cell>10h</cell><cell cols="7">14.0 7.6 11.8 10.0 6.9 12.1 15.6</cell><cell>9.8 11.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell>shows that cross-lingual pretraining (VP-10K) reduces WER from an average of 37.5 for</cell></row><row><cell>supervised-only training (No pretraining) to 15.3 WER. XLS-R uses a lot more unlabeled VoxPopuli</cell></row><row><cell>data and this results in improved performance: XLS-R (0.3B) improves over VP-10K by an average</cell></row><row><cell>of 2.5 WER. The largest gains are on English, where WER improves from 16.2 to 10.2, likely due</cell></row><row><cell>to the use of more English data from MLS during pretraining (over 44K hours). Increasing model</cell></row><row><cell>capacity to 1B parameters results in even better performance, reducing WER from an average of 15.3</cell></row><row><cell>for VP-10K to 10.6.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10</head><label>10</label><figDesc></figDesc><table><row><cell cols="9">: VoxPopuli ASR results in terms of WER. We report the supervised-only baseline (No</cell></row><row><cell cols="8">pretraining) of Wang et al. (2021b) as well as their pretrained model (VP-10K).</cell></row><row><cell></cell><cell>en</cell><cell>de</cell><cell>it</cell><cell>fr</cell><cell>es</cell><cell>pl</cell><cell>ro</cell><cell>hu</cell></row><row><cell>Labeled data</cell><cell cols="8">543h 282h 91h 211h 166h 111h 89h 63h</cell></row><row><cell cols="6">Baselines from previous work (Wang et al., 2021b)</cell><cell></cell><cell></cell></row><row><cell cols="9">No pretraining 30.0 29.3 45.2 30.5 31.4 25.6 27.7 27.9</cell></row><row><cell>VP-10K</cell><cell cols="6">16.2 16.2 21.5 15.4 11.0 12.5</cell><cell cols="2">9.4 12.0</cell></row><row><cell>This work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLS-R (0.3B)</cell><cell cols="4">10.2 13.0 19.2 12.6</cell><cell>9.8</cell><cell>9.6</cell><cell cols="2">7.9 11.6</cell></row><row><cell>XLS-R (1B)</cell><cell cols="4">8.8 11.5 15.1 10.8</cell><cell>8.2</cell><cell>7.7</cell><cell>7.3</cell><cell>9.6</cell></row><row><cell></cell><cell>nl</cell><cell>cs</cell><cell>sl</cell><cell>fi</cell><cell>hr</cell><cell cols="2">sk Avg</cell></row><row><cell>Labeled data</cell><cell>53h</cell><cell cols="2">62h 10h</cell><cell>27h</cell><cell>43h</cell><cell>35h</cell><cell></cell></row><row><cell cols="6">Baselines from previous work (Wang et al., 2021b)</cell><cell></cell><cell></cell></row><row><cell cols="8">No pretraining 38.3 27.7 96.5 41.6 40.2 32.7 37.5</cell></row><row><cell>VP-10K</cell><cell cols="7">19.7 11.8 26.1 17.1 14.1 11.1 15.3</cell></row><row><cell>This work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLS-R (0.3B)</cell><cell cols="5">14.8 10.5 24.5 14.2 12.3</cell><cell cols="2">8.9 12.8</cell></row><row><cell>XLS-R (1B)</cell><cell>12.5</cell><cell cols="4">8.7 19.5 11.3 10.0</cell><cell cols="2">7.1 10.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>LibriSpeech ASR results in terms of WER. Models are fine-tuned with 10min, 1h or 10h of annotated data. We compare XLS-R to wav2vec 2.0<ref type="bibr" target="#b5">(Baevski et al., 2020b)</ref> with the same number of parameters (317M). We do not use any language model for these experiments. Cross-lingual training with higher capacity such as for XLS-R (1B) obtains competitive performance.</figDesc><table><row><cell>Model</cell><cell cols="4">dev clean other clean other test</cell></row><row><cell>10 min labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wav2vec 2.0 LV-60K (0.3B)</cell><cell>31.7</cell><cell>35.0</cell><cell>32.1</cell><cell>34.5</cell></row><row><cell>XLS-R (0.3B)</cell><cell>33.3</cell><cell>39.8</cell><cell>34.1</cell><cell>39.6</cell></row><row><cell>XLS-R (1B)</cell><cell>28.4</cell><cell>32.5</cell><cell>29.1</cell><cell>32.5</cell></row><row><cell>1h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wav2vec 2.0 LV-60K (0.3B)</cell><cell>13.7</cell><cell>16.9</cell><cell>13.7</cell><cell>17.1</cell></row><row><cell>XLS-R (0.3B)</cell><cell>17.1</cell><cell>23.7</cell><cell>16.8</cell><cell>24.0</cell></row><row><cell>XLS-R (1B)</cell><cell>13.2</cell><cell>17.0</cell><cell>13.1</cell><cell>17.2</cell></row><row><cell>10h labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wav2vec 2.0 LV-60K (0.3B)</cell><cell>5.7</cell><cell>9.2</cell><cell>5.6</cell><cell>9.4</cell></row><row><cell>XLS-R (0.3B)</cell><cell>8.3</cell><cell>15.1</cell><cell>8.3</cell><cell>15.4</cell></row><row><cell>XLS-R (1B)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Language identification on VoxLingua107. We report the error rate on the development set spanning 33 languages. consider speaker identification on VoxCeleb1 where we fine-tune our model to distinguish between a fixed set of speakers given an utterance. We compare to prior work including results published as part of the recently introduced SUPERB benchmark(Yang et al., 2021)  but note that their results are not strictly comparable because they do not fine-tune the underlying pre-trained model. All parameters of XLS-R are fine-tuned, similar to the evaluation of all other tasks. The results</figDesc><table><row><cell></cell><cell></cell><cell>Error Rate (%)</cell><cell></cell></row><row><cell></cell><cell cols="3">0...5 sec 5...20 sec Average</cell></row><row><cell>Previous work</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Valk &amp; Alum?e (2020)</cell><cell>12.3</cell><cell>6.1</cell><cell>7.1</cell></row><row><cell>Ravanelli et al. (2021)</cell><cell>-</cell><cell>-</cell><cell>6.7</cell></row><row><cell>This work</cell><cell></cell><cell></cell><cell></cell></row><row><cell>wav2vec 2.0 LV-60K (300M)</cell><cell>11.5</cell><cell>6.3</cell><cell>7.2</cell></row><row><cell>XLS-R (0.3B)</cell><cell>9.1</cell><cell>5.0</cell><cell>5.7</cell></row><row><cell>5.3.2 SPEAKER IDENTIFICATION</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Finally, we</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 14 :</head><label>14</label><figDesc>CoVoST-2 X ? English full results. We report baseline results from XLSR-53, VP-100K<ref type="bibr" target="#b58">Wang et al. (2021a)</ref>, XMEF-En and XMEF-X<ref type="bibr" target="#b36">Li et al. (2021b)</ref>.</figDesc><table><row><cell>English ? X Train Hours</cell><cell cols="8">ca 430h 430h 430h 430h 430h 430h 430h 430h ar de tr zh fa et mn</cell></row><row><cell>Prior work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLSR-53</cell><cell cols="8">29.0 16.5 23.6 15.3 33.7 19.4 19.8 13.3</cell></row><row><cell>VP-100K</cell><cell cols="8">26.1 14.5 20.8 13.5 30.7 17.1 17.4 12.0</cell></row><row><cell>XMEF-JT</cell><cell cols="8">30.9 18.0 25.8 17.0 33.3 21.5 22.1 14.8</cell></row><row><cell>Wav2vec 2.0 (0.3B)</cell><cell cols="4">32.4 17.4 23.8 15.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>+ self-training + LM</cell><cell cols="4">35.6 20.8 27.2 18.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">This work -monolingual pretraining (LV-60K)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Wav2vec 2.0 (0.72B) 32.7 19.4 27.0 17.7 37.4 21.8 22.9 15.4</cell></row><row><cell cols="3">This work -cross-lingual pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLS-R (0.3B)</cell><cell cols="8">28.7 16.3 23.6 15.0 33.5 19.0 19.6 13.2</cell></row><row><cell>XLS-R (1B)</cell><cell cols="8">32.1 19.2 26.2 17.1 36.7 21.3 22.4 14.9</cell></row><row><cell>XLS-R (2B)</cell><cell cols="8">34.2 20.7 28.3 18.6 38.5 22.9 24.1 16.2</cell></row><row><cell>English ? X Train Hours</cell><cell cols="8">sv 430h 430h 430h 430h 430h 430h 430h lv sl ta ja id cy Avg</cell></row><row><cell>Prior work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLSR-53</cell><cell cols="8">29.6 19.3 22.8 15.8 37.1 27.6 28.8 23.4</cell></row><row><cell>VP-100K</cell><cell cols="8">26.3 16.7 20.2 13.9 33.9 24.9 26.0 20.9</cell></row><row><cell>XMEF-JT</cell><cell cols="8">29.6 21.5 25.1 17.8 39.3 29.9 30.6 25.1</cell></row><row><cell>Wav2vec 2.0 (0.3B)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>+ self-training + LM</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="4">This work -monolingual pretraining (LV-60K)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="9">Wav2vec 2.0 (0.72B) 33.0 22.5 26.1 18.4 40.3 31.2 32.8 26.6</cell></row><row><cell cols="3">This work -cross-lingual pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLS-R (0.3B)</cell><cell cols="8">29.1 19.3 22.4 15.6 36.9 27.4 28.9 23.2</cell></row><row><cell>XLS-R (1B)</cell><cell cols="8">32.3 22.0 25.4 18.1 39.9 30.3 31.8 26.0</cell></row><row><cell>XLS-R (2B)</cell><cell cols="8">34.5 23.5 27.6 19.8 41.5 32.5 33.8 27.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">There are 400K hours before removing leading and trailing silences.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/facebookresearch/covost 4 https://catalog.ldc.upenn.edu/byyear LDC2016S06, LDC2016S13, LDC2017S05, LDC2017S08, LDC2016S12 5 https://github.com/flashlight/wav2letter/blob/main/recipes/mls 6 https://dl.fbaipublicfiles.com/cpc_audio/common_voices_splits.tar.gz</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/facebookresearch/voxpopuli 8 http://bark.phon.ioc.ee/voxlingua107/ 9 https://www.robots.ox.ac.uk/?vgg/data/voxceleb/vox1.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/facebook/Ax</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://github.com/pytorch/fairseq/tree/main/examples/multilingual# mbart50-models</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13">MLS is a notable exception and we attribute the different performance pattern to prior work having pretrained on large amounts of in-domain data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Emmanuel Dupoux for early access to VP-400K, and Christophe Ropers for his advise on language categorization. We also thank Min Xu, Jacob Kahn, Shruti Bhosale, Anjali Sridhar, and Tatiana Likhomanenko for help with infrastructure.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The 2016 bbn georgian telephone speech keyword spotting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alum?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Common voice: A massivelymultilingual speech corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosana</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsay</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Massively multilingual neural machine translation in the wild: Findings and challenges. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><forename type="middle">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cloze-driven pretraining of self-attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fairscale: A general purpose modular pytorch library for high performance and large scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Caggiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lefaudeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Sheiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/fairscale" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Proc. of NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>abs/1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xnli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.05053</idno>
		<title level="m">Evaluating cross-lingual sentence representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The challenge of realistic music generation: modelling raw audio at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Sander Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reducing transformer depth on demand with structured dropout. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>abs/2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multilingually trained bottleneck features in spoken language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radek</forename><surname>Fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Mat?jka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franti?ek</forename><surname>Gr?zl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Old?ich</forename><surname>Plchot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Honza?ernock?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speech recognition and keyword spotting for low-resource languages: Babel project research at cued</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><forename type="middle">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakti</forename><forename type="middle">P</forename><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technologies for Under-Resourced Languages</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Larger-scale transformers for multilingual masked language modeling. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giri</forename><surname>Anantharaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno>abs/2105.00572</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01027</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How much can a bad teacher benefit ASR pre-training?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4411" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1603.09382</idno>
		<title level="m">Deep networks with stochastic depth. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transfer learning of language-independent end-to-end asr with language model fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirofumi</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murali</forename><forename type="middle">Karthick</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Libri-light: A benchmark for asr with limited or no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning robust and multilingual speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Scaling end-to-end models for large-scale multilingual asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multilingual speech translation with efficient finetuning of pretrained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>abs/2001.08210</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Voxceleb: a large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL System Demonstrations</title>
		<meeting>of NAACL System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mls: A large-scale multilingual dataset for speech research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineel</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<title level="m">Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Confidence estimation and deletion prediction using bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In SLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Zero-infinity: Breaking the GPU memory wall for extreme scale deep learning. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno>abs/2104.07857</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Multi-task self-supervised learning for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titouan</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aku</forename><surname>Rouhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loren</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nauman</forename><surname>Dawalatabad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelwahab</forename><surname>Heba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju-Chieh</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Lin</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Wei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Feng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Rastorgueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Aris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwidong</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>SpeechBrain: A general-purpose speech toolkit. arXiv</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining transfers well across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Rivi?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Emmanuel</forename><surname>Mazar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Xtreme-r: Towards more challenging and nuanced multilingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07412</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Setffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multilingual translation from denoising pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Voxlingua107: a dataset for spoken language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Valk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanel</forename><surname>Alum?e</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SLT</title>
		<meeting>of SLT</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Covost 2 and massively multilingual speech-to-text translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Talnikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgane</forename><surname>Riviere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Talnikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Haziza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Largescale self-and semi-supervised learning for speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ccnet: Extracting high quality monolingual datasets from web crawl data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grave</forename><surname>And?douard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4003" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Self-training and pre-training are complementary for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paden</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Self-training and pre-training are complementary for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Likhomanenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paden</forename><surname>Tomasello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3030" to="3034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11934</idno>
		<title level="m">Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Han</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Sung</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I Jeff</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kushal</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yist</forename><forename type="middle">Y</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuankai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guan-Ting</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Hsien</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko-Tik</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Rong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<editor>Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung-yi Lee</editor>
		<imprint/>
	</monogr>
	<note>SUPERB: speech processing universal performance benchmark. arXiv, abs/2105.01051, 2021</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pushing the limits of semi-supervised learning for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS SAS Workshop</title>
		<meeting>of NeurIPS SAS Workshop</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
		<editor>Khe Chai Sim, Bhuvana Ramabhadran, Tara N. Sainath, Fran?oise Beaufays, Zhifeng Chen, Quoc V. Le, Chung-Cheng Chiu, Ruoming Pang, and Yonghui Wu</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
