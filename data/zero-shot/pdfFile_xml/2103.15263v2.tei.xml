<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-shot Adversarial Quantization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>wongjun@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-shot Adversarial Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model quantization is a promising approach to compress deep neural networks and accelerate inference, making it possible to be deployed on mobile and edge devices. To retain the high performance of full-precision models, most existing quantization methods focus on fine-tuning quantized model by assuming training datasets are accessible. However, this assumption sometimes is not satisfied in real situations due to data privacy and security issues, thereby making these quantization methods not applicable. To achieve zero-short model quantization without accessing training data, a tiny number of quantization methods adopt either post-training quantization or batch normalization statisticsguided data generation for fine-tuning. However, both of them inevitably suffer from low performance, since the former is a little too empirical and lacks training support for ultra-low precision quantization, while the latter could not fully restore the peculiarities of original data and is often low efficient for diverse data generation. To address the above issues, we propose a zero-shot adversarial quantization (ZAQ) framework, facilitating effective discrepancy estimation and knowledge transfer from a full-precision model to its quantized model. This is achieved by a novel two-level discrepancy modeling to drive a generator to synthesize informative and diverse data examples to optimize the quantized model in an adversarial learning fashion. We conduct extensive experiments on three fundamental vision tasks, demonstrating the superiority of ZAQ over the strong zero-shot baselines and validating the effectiveness of its main components. Code is available at https://git.io/Jqc0y.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Although deep neural networks (DNNs), especially deep convolutional networks (DCNs), have achieved remarkable performance in a broad range of computer vision tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref>, their ever-growing complexitiesa large number of model parameters -inhibit the appli-* Corresponding author. cations on cloud and edge devices. As a consequence, model quantization, converting high-precision parameters to low-precision ones, becomes one of the main paradigms in model compression and acceleration <ref type="bibr" target="#b9">[10]</ref>. To mitigate the performance degradation issue due to model quantization, quantization-aware fine-tuning approaches have been extensively studied to optimize quantized models on the full training datasets <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b37">38]</ref>. However, in real situations, original training data is sometimes inaccessible due to privacy and security issues. For instance, electronic health records usually contain patients' private information. As such, the quantization-aware fine-tuning methods are no longer applicable. Post-training quantization methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47</ref>] therefore emerge to quantize weights and activations in DNNs through correction strategies, without fine-tuning. However, there is a negligible gap between the strategies and the goals of target tasks, causing the quantized models to suffer from performance degradation. This issue is even amplified for the ultra-low precision situation. To address this, batch normalization statistics (BNS)-guided data generation is leveraged by recent methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref>. They aim at synthesizing data samples that match the real-data statistics encoded in the batch normalization layers of full-precision deep models. The synthetic data is further leveraged to fine-tune the quantized models by directly optimizing on target tasks supervised by its full-precision model, as shown in <ref type="figure" target="#fig_0">Figure 1(a)</ref>. Although the performance of ultra-low precision models is boosted to some extent, thanks to fine-tuning, data generated by batch normalization statistics is hard to fully recover the peculiarities of training data and the generation process itself is time-consuming due to data redundancy. These issues make the results still far from satisfactory.</p><p>This paper seeks to promote the development of data-free model quantization by addressing the above-mentioned issues. We, therefore, present a novel learning framework named Zero-shot Adversarial Quantization (ZAQ) to perform model quantization without utilizing any sample from training data. Specifically, we devise a two-level discrepancy modeling strategy for ZAQ to measure the gap between a quantized model and its corresponding full-precision model. We consider not only the output discrepancy from models' top layers, just similar as existing data-free model quantization methods, but also fuses a new intermediate inter-channel discrepancy based on feature maps. A generator in ZAQ is responsible for generating informative and diverse data examples in an adversarial learning manner <ref type="bibr" target="#b14">[15]</ref> -optimization based on a minimax game -to enable effective discrepancy estimation and knowledge transfer, as depicted <ref type="figure" target="#fig_0">Figure 1</ref>(b). In addition, activation regularization is adopted to facilitate the generator to obtain examples more sensitive to the network. To sum up, our contributions are as follows:</p><p>? We propose a zero-shot adversarial quantization framework to support effective data generation and knowledge transfer. To our best knowledge, it represents the first effort to apply adversarial learning to data-free model quantization.</p><p>? A novel two-level discrepancy modeling strategy is devised to measure the discrepancy between a quantized model and its full-precision model, thereby guiding the training of the quantized model and generator.</p><p>? We conduct extensive experiments on image classification, segmentation, and object detection tasks, showing our ZAQ framework achieves state-of-the-art results in data-free situation, works well for ultra-low precision scenarios, and is efficient compared to the approaches of BNS-guided data generation for model quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Model quantization is a promising model compression methods aiming to store parameters with fewer bits so that computation can be executed on integer-arithmetic units rather than on power-hungry floating-point ones <ref type="bibr" target="#b15">[16]</ref>. An important challenge with quantization is that it can lead to significant performance degradation, especially in ultra-low precision settings. To cope with this, PACT <ref type="bibr" target="#b6">[7]</ref> used an activation clipping parameter to find the right quantization scale. Zhu et al. <ref type="bibr" target="#b48">[49]</ref> built a flexible and unified INT8 training framework for vision tasks. Flexpoint <ref type="bibr" target="#b17">[18]</ref>, MPT <ref type="bibr" target="#b27">[28]</ref> and DFP <ref type="bibr" target="#b8">[9]</ref> all use 16-bit floating-point to train DNNs with accuracy comparable to full-precision model. And there are some approaches to decrease induced degradation by quantizationaware training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref> or reducing the dynamic range of activations by clipping outliers <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b1">2]</ref>. Instead of focusing on improving the quantization process itself, <ref type="bibr" target="#b26">[27]</ref> explored an equivalent weight arrangement that make the net less sensitive to quantization. However, all above quantization methods generally require access to the entire training data which is not always available as aforementioned.</p><p>Data-free model compression has been a hot topic and draw more and more attention in recent years, which is a challenge to compress model without training data. Srinivas and Babu <ref type="bibr" target="#b38">[39]</ref>, the pioneers in data-free compression, introduced a channel pruning method without original training data. Since then, more and more kinds of data-free or zero-shot compression methods were proposed, including quantization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42]</ref>, weight factorization <ref type="bibr" target="#b29">[30]</ref> and knowledge distillation (KD) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b22">23]</ref>. DFQ <ref type="bibr" target="#b29">[30]</ref> and ACIQ <ref type="bibr" target="#b1">[2]</ref> are both post-training quantization methods relying on weight equalization or bias correction without fine-tuning on the entire dataset. But when applied to ultralow precision (i.e., lower than 6-bit) model, these kinds of quantization methods cannot prevent quantization models from performance degradation. Most of the data-free KD methods attempt to reconstruct the original data from pretrained teacher model utilizing prior information about the underlying data distribution, such as BNS <ref type="bibr" target="#b43">[44]</ref>, Dirichlet distribution <ref type="bibr" target="#b30">[31]</ref> and category information <ref type="bibr" target="#b4">[5]</ref>. However, they ignore the intermediate features to guide the student network learning.</p><p>Two recent data-free quantization studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42]</ref> quantize and fine-tune models without needing original data. Their core idea is to reconstruct some samples from full-precision models to fine-tune quantized models. To be specific, Ze-roQ <ref type="bibr" target="#b3">[4]</ref> directly reconstructs samples by optimizing from random noises according to BNS of full-precision models. GDFQ <ref type="bibr" target="#b41">[42]</ref> further adopts a generator to reconstruct samples guided by BNS and extra category label information, which limits its application to classification tasks. To sum up, there is still a large gap between the data generated based on BNS and original training data after a time-consuming generation process. Moreover, both ZeroQ and GDFQ poorly support high-level vision tasks due to the lack of considering information from intermediate layers of full-precision models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Computational Framework</head><p>Framework Overview: <ref type="figure" target="#fig_1">Figure 2</ref> depicts the basic framework of ZAQ. It contains pretrained full-precision model P , In what follows, we first introduce the preliminary of the quantization function used in this paper. Then we detail the proposed framework.</p><formula xml:id="formula_0">adaptive weight minimize maxmize G Q P z ? P ? Q ? o ? f ? a ? P ? Q 1 RGM C H W HW C C f 1 f 2 f 3 f 1 f 2 f C original synthetic gap transfer G BNS Full-precision model Quantized model (a) (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>A common practise in training a neural network with low-precision weights and activations is to introduce a quantization function. Considering the general case of k-bit quantization <ref type="bibr" target="#b47">[48]</ref>, we define the uniform quantization function q(?) as:</p><formula xml:id="formula_1">q(v) = round S ? (v ? Z) ,<label>(1)</label></formula><p>where v denotes the full-precision (float32) value, S is the scaling factor, and Z is the zero point in float32. According to whether the parameter Z is zero, uniform quantization can be divided into two categories: symmetric quantization and asymmetric quantization. Here we use symmetric quantization and set Z = 0. Consequently, S is formulated as:</p><formula xml:id="formula_2">S = 2 k?1 ? 1 max(|x f |) ,<label>(2)</label></formula><p>where x f is any one of float32-point numbers.</p><p>The key of model quantization is to reduce the discrepancy D between full-precision model P and low-precision model Q through optimizing Q, which can be expressed as: <ref type="figure">Figure 3</ref>. Illustration of obtaining channel relation map.</p><formula xml:id="formula_3">Q * = min Q D(P, Q) . (3) G Q ? Q CRM C H W HW C C f 1 f 2 f 3 f 1 f 2 f C original synthetic gap transfer G BN Full-precision model Quantized model (a) (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Two-level Discrepancy Modeling</head><p>As aforementioned, the framework ZAQ leverages a novel two-level discrepancy function to model the discrepancy between full-precision and quantized models. First, we assume there is a data example x g generated by G, i.e., x g = G(z) where z is random noise. We denote the corresponding prediction outputs for full-precision model P and quantized model Q as P (x g ) and Q(x g ), respectively. There are some distance metrics that can be used to measure the discrepancy, such as Kullback-Leibler (KL) divergence. KL divergence is efficient in data-driven knowledge transfer or distillation, but it is insufficient to maximize the discrepancy when training generator G. This is because some unexpected samples may be similar in prediction, making the negative KLD too small to optimize. Instead, we adopt L1 loss to measure the output discrepancy D o in a more direct way:</p><formula xml:id="formula_4">D o (P, Q; G) = E xg 1 N P (x g ) ? Q(x g ) 1 ,<label>(4)</label></formula><p>where N is element number in the outputs, for instance, class number for classification and label map size for segmentation. Inspired by the idea of harnessing intermediate feature maps to improve performance in knowledge distillation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b31">32]</ref>, we further propose Channel Relation Map (CRM) to gain intermediate inter-channel discrepancy. Noting that although there are a few studies <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22]</ref> modeling relations between data instances, we are the first to consider similarity relations between different channels of feature maps, which are introduced later.</p><p>Specifically, we define intermediate inter-channel discrepancy as below:</p><formula xml:id="formula_5">D f (P, Q; G) = E xg L l ? (l) C (l) 2 R (l) P (x g ) ? R (l) Q (x g ) 1 ,<label>(5)</label></formula><p>where L is the total number of layers exploited for ZAQ, R (l) P (?) and R (l) Q (?) represent CRM extracted from the l-th layer of P and Q, respectively. ? (l) is the adaptive weight allocated to the l-th layer, and C (l) is the output channel number of the l-th layer. Actually, we usually select the last layer in each group or block for residual neural networks, and the layer number is 3 ? 4 for VGG in our experiments.</p><p>A conventional manner to measure the discrepancy of intermediate layers relies on correlating feature maps of P and Q, just as what KD commonly does <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45]</ref>. However, since the numerical spans of P and Q are very different because of precision settings, the gap between feature maps in P and Q is relatively large (verified in <ref type="table">Table 4</ref>). Therefore, we introduce CRM to address this issue. Gram matrix can represent certain relationships between feature vectors to reflect the characteristics of images, and is commonly used in style transfer <ref type="bibr" target="#b13">[14]</ref>. But it is unreliable to measure the feature discrepancy between two networks by directly using feature vectors with different precision. Here we extend it to channel relation map to capture the relations towards different channels in the same layer of one model. It cannot only shield the influence of feature maps with different numerical spans, but also represent the high-dimensional features of samples. <ref type="figure">Figure 3</ref> illustrates the procedures of obtaining CRM, which are the same for both P and Q. Taking the feature mapF (l) ? R C?H?W extracted from the l-th layer of P (or Q) for clarification, it can be flattened into</p><formula xml:id="formula_6">F (l) ? R C?HW , which is composited by C channel- wise feature vectors: f (l) 1 f (l) 2 ? ? ? f (l) C</formula><p>. Then the consine similarity between channel features f (l) i and f (l) j is defined as below:</p><formula xml:id="formula_7">R (l) ij = &lt; f (l) i , f (l) j &gt; f (l) i 2 f (l) j 2 .<label>(6)</label></formula><p>Based on R Q (x g ) can be obtained. To adaptively determine ? (l) in Eq. 5, we use the discrepancy calculated for the two models and define the following computational equation:</p><formula xml:id="formula_8">? (l) = exp EMA T E xg ?B t R (l) P (xg) ? R (l) Q (xg) 1 L l exp EMA T E xg ?B t R (l ) P (xg) ? R (l ) Q (xg) 1 ,<label>(7)</label></formula><p>where EMA T denotes exponential moving averaging, T is the training steps in an epoch, and L is the number of layers exploited for ZAQ. By this way, more attention will be paid to the model layer that has a larger difference in CRMs. Besides, in order to avoid breaking the balance in long-term training, ? (l) needs to be re-initialized by 1 L when a new epoch starts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adversarial Knowledge Transfer</head><p>Our ZAQ framework trains quantized model Q and generator G in an adversarial minimax game, which contains discrepancy estimation and knowledge transfer stages. In discrepancy estimation stage, the generator G aims at maximizing the two-level discrepancy between Q and P to search for discrepancy represent space. The loss is defined as follows:</p><formula xml:id="formula_9">L DE = ?D o (P, Q; G) ? ?D f (P, Q; G) ,<label>(8)</label></formula><p>where ? is a hyperparameter to balance D o and D f . In knowledge transfer stage, quantized model Q is optimized to minimize the two-level discrepancy to approximate full-precision model P , denoted as:</p><formula xml:id="formula_10">L KT = D o (P, Q; G) + ?D f (P, Q; G) .<label>(9)</label></formula><p>As a consequence, the knowledge is transferred from P to Q progressively in the zero-shot situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Activation Regularization</head><p>Although the L1 loss function can relieve the model from falling into some abnormal sample points in discrepancy estimation, they exist all the time and interfere with the generator's exploration of the original input domain. These unexpected samples could make the prediction distributions of the two networks consistent but they are not in the working domain of full-precision model. We assume the infinite discrepancy space between model P and Q is ?, in which the generator G explore valuable samples for transfer learning. In fact, ? consists of two subspaces ? P and ? U , which means ? = ? P ? ? U . ? P is the subspace that is equal to the original training data domain, or the working domain of pretrained model P . And ? U is an infinite subspace outside the working domain of P . The goal of the generator is to synthesize samples distributed in the subspace ? P , rather in ? U .</p><p>According to several researches about interpretability of DNNs <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b10">11]</ref> or sample reconstruction <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5]</ref>, the activation layer reflects the sensitivity of the neural network to the input data, and higher activation means more correlation between synthetic samples and working domain of P . Hence, we further leverage activation regularization to constraint the generator to explore and synthesize valuable samples. We denote the i-th channel activation map extracted by the last convolution layer of network P as h P i , i ? {1, 2, . . . , M }, where M is the number of activation maps. Then, the activation regularization can be formulated as</p><formula xml:id="formula_11">L a = ? 1 M M i h P i 1 .<label>(10)</label></formula><p>With the intuition that high activation values mean a better matching between a given input example and training data, we incorporate L a into Eq. 8 and minimize the following loss to guide generator training.</p><formula xml:id="formula_12">L DE = ?D o (P, Q; G) ? ?D f (P, Q; G) + ?L a .<label>(11)</label></formula><p>Finally, the detailed procedures of the proposed framework ZAQ is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Zero-shot Adversarial Quantization</head><p>Input: A pretrained full-precision model P (x; ? p ), quantization precision. Output: Quantized model Q(x; ? q ) 1 Quantize the model P as Q by Eq. 3; 2 for number of epochs do  Estimate L DE by Eq. 11;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head><p>Fix ? q , update ? g :</p><formula xml:id="formula_13">? g ? ? g ? ? ?L DE ?? g</formula><p>Update adaptive weights ? (l) by Eq. 7; 9 # Knowledge Transfer 10 z ? N (0, I), x g ? G(z; ? g ); <ref type="bibr" target="#b10">11</ref> Calculate L KT by Eq. 9;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12</head><p>Fix ? g , update ? q : We evaluate our approach on the following six datasets: CIFAR10, CIFAR100, and ImageNet for classification, Cityscapes and CamVid for segmentation, and VOC2012 for object detection.</p><formula xml:id="formula_14">? q ? ? q ? ? ?L KT ??</formula><p>CIFAR. CIFAR10 <ref type="bibr" target="#b18">[19]</ref> and CIFAR100 consist of 32?32 color images with 10 and 100 classes, respectively. Both are split into a 50,000-image train set and a 10,000-image test set.</p><p>ImageNet. The 1,000-class dataset from ILSVRC 2012 [36] provides 1.2 million images for training, and 50,000 for validation.</p><p>Cityscapes. Cityscapes <ref type="bibr" target="#b7">[8]</ref> is for urban scene understanding and contains 30 classes with only 19 classes used for evaluation. It provides 3,975 images with fine segmentation annotations, including 2,975 images for training and 500 images for testing. <ref type="bibr" target="#b2">[3]</ref> is an automotive dataset, containing 367 training and 233 testing images. We perform on the commonly used 11 different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CamVid. CamVid</head><p>VOC2012. A total of 11540 images are included in PAS-CAL VOC2012 <ref type="bibr" target="#b11">[12]</ref>, where each image contains a set of objects, out of 20 different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines</head><p>To evaluate the effectiveness and advantages of our proposed method, we compared it with both data-free fine-tuning methods and post-training quantization methods. The baselines are briefly described as follows.</p><p>FT. We use original training data to Fine-Tune (FT) a quantized model.</p><p>RQ. Raw Quantization (RQ) method directly testing the model after quantization without any fine-tuning.</p><p>DFQ <ref type="bibr" target="#b29">[30]</ref>. A post-training quantization method uses a weight equalization scheme to remove outliers in both weights and activations.</p><p>ACIQ <ref type="bibr" target="#b1">[2]</ref>. It analytically computes a clipping range, as well as a per-channel bit allocation for neural networks without any fine-tuning/training.</p><p>ZeroQ <ref type="bibr" target="#b3">[4]</ref>. It retrains a quantized model by reconstructed data instead of original data.</p><p>GDFQ <ref type="bibr" target="#b41">[42]</ref>. It is also a fine-tuning method by recovering fake data via a conditional generator. Yet it only supports classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation Details</head><p>We implement all networks and quantization methods in Pytorch. For all datasets, we adopt the same data augmentation procedure on pretraining as <ref type="bibr" target="#b36">[37]</ref> for making fair comparisons. We adopt SGD with momentum 0.9 and weight decay 5 ? 10 ?4 in both pretraining and fine-tuning. All the models are pretrained for 200 epochs and the learning rates are decayed by 0.1 for every 80 epochs on datasets, except ImageNet, on which we directly use the official pretrained models. We construct a generator following DCGAN <ref type="bibr" target="#b32">[33]</ref> with 256-dimension noise and is trained with Adam <ref type="bibr" target="#b16">[17]</ref>. But for CIFAR, we just reduce the channels of all layers in the generator to a quarter and set the dimension of noise to 100, due to the smaller size of the samples. Moreover, the learning rates of quantized models and generators are initialized to 0.1 and 1 ? 10 ?3 , respectively. The learning rates of SGD and Adam are decayed by different steps in different tasks. In training, we set the batch size to 256 for CIFAR, 64 for ImageNet and VOC2012, and 16 for segmentation datasets. As for the hyperparameters, we set ? = 0.1 and ? = 0.05 by default. More detailed implementation and settings for different datasets are illustrated in the following parts.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Performance Test for Image Classification</head><p>For image classification, we take the top-1 accuracy (abbr. Acc) as the metric. The number of fine-tuning epochs is 200 for CIFAR, while 300 for ImageNet. In each epoch, the training steps are set to 40 for CIFAR and 50 for ImageNet. The learning rates of SGD and Adam are decayed every 80 epochs for CIFAR and 100 for ImageNet. Besides, we use "W-A-" to denote the quantization bits used for weights (W) and activations (A), and "float32" as full-precision models. <ref type="table" target="#tab_1">Table 1</ref> shows the classification results. First, we find DFQ and ACIQ suffer from dramatic performance degradation when taking ultra-low precision, especially for CI-FAR100 and ImageNet. This verifies that due to the lack of fine-tuning, post-training quantization methods do not work well for ultra-low precision. Then we observe our framework achieves the best performance on the three classification datasets, indicating its advantages over the other quantization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Performance Test for Image Segmentation</head><p>In this part, we mainly compare ZAQ with ZeroQ and DFQ on Cityscapes and CamVid, the images of which are all resized to 256. GDFQ requires labels as conditions to synthesize data, so it does not naturally support high-level vision tasks such as segmentation and detection. The ImageNetpretrained MobileNetV2 and ResNet50 models are used as feature extractors within DeepLabv3 <ref type="bibr" target="#b5">[6]</ref>. The hyperparameters ? = 0.5 and ? = 0.1. We adopt mean IoU of all classes (mIoU) as the evaluation metric for segmentation. In fine-tuning, we set the size of the synthetic image as 128 ? 128, which is enough for representing model discrepancy and transferring knowledge. <ref type="table" target="#tab_2">Table 2</ref> shows the performance of quantized models finetuned by different methods, from which we can see that our method still exhibits superior performance, especially for ultra-low precision situations. This observation is consistent with what we find in image classification tasks.</p><p>Furthermore, we randomly select two real examples from Cityscapes and CamVid, respectively, and visualize the segmentation results of 4-bit DeeplabV3(MobileNetV2) learned by different model quantization methods. The results are shown in <ref type="figure" target="#fig_5">Figure 4</ref>, where the first two rows correspond to Cityscapes and the last two rows correspond to CamVid. Obviously, DFQ does not work properly for the examples in 4-bit quantization and thus it is hard to retain model performance. By comparing ZAQ with ZeroQ, we find it exhibits better qualitative results in complex details and small object segmentation, as shown in the second row of the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Performance Test for Object Detection</head><p>To demonstrate the application on object detection, we apply ZAQ to the model MobileNetV2 SSD <ref type="bibr" target="#b20">[21]</ref> and evaluate it on VOC2012. our method compared to other quantization methods. In particular, ZAQ is comparable with FT that utilizes the original training dataset. Finally, we end up the introduction of the performance tests for three image-based tasks with <ref type="figure" target="#fig_6">Figure 5</ref>, which provides an overview of how performance changes with different bits. The curves of different quantization methods in the figures reflect that ZAQ is consistently better and it gains greater improvements in ultra-low precision situation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Ablation Study</head><p>In this part, we conduct an ablation study to validate the con- bring 1 ? 2% performance improvement, while the activation regularization has a smaller contribution of about 0.5% improvement. But it could prevent the generator from falling into some abnormal samples that are not sensitive to the full-precision models, which is also utilized in the previous study for KD <ref type="bibr" target="#b4">[5]</ref>. We further demonstrate the effectiveness of CRM in model quantization by comparing it with two alternatives for learning intermediate knowledge: <ref type="bibr" target="#b0">(1)</ref> Gram which directly uses Gram matrix in discrepancy modeling; (2) AT <ref type="bibr" target="#b44">[45]</ref> which directly aligns normalized feature maps in knowledge transfer. <ref type="table">Table 4</ref> shows the performance of the abovementioned methods, from which we can see CRM is much better than the other two methods. This verifies the necessity of considering different numerical spans in designing quantization-aware fine-tuning methods. In addition, we choose CIFAR100 to visualize the computed CRMs by ZAQ. In <ref type="figure" target="#fig_4">Figure 7</ref>, (a) and (b) are CRMs from the 2-nd exploited layer of full-precision and 4-bit ResNet18, respectively. By comparison, we can find the two CRMs are consistent with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Efficiency Analysis</head><p>We conduct efficiency test on a single GPU (GTX 2080Ti) for ZAQ and the data generation-based quantization methods, i.e., ZeroQ and GDFQ. The number of synthesized images determined for each method is conditioned on its performance convergence state following <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b41">[42]</ref>. Due to the poor diversity of synthetic images, ZeroQ and GDFQ need to synthesize more samples in training. Besides, the images in Cityscapes have high resolution, making ZeroQ cost too much time in synthesizing procedure. So we conduct the comparative experiment on Cityscapes with the same number of samples approximate to the original dataset in each epoch. <ref type="table">Table 5</ref> shows the results, where our method reduces GPU time by 41.8% compared to GDFQ on CIFAR100, while 57.5% compared to ZeroQ on Cityscapes. The conclusion is intuitive since ZeroQ needs 500 to 1500 iterations to generate per image and GDFQ is prone to generate redundant images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Method </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Case Study of Generated Images</head><p>This part conducts case studies on the generated data by different model quantization methods. We take CIFAR and CamVid for illustration. For CIFAR, the randomly selected images are shown in <ref type="figure">Figure 8</ref>. The first row of the images corresponds to CIFAR10 and the second row corresponds to CIFAR100. The first column shows the original images. The images in the middle three columns are gotten from MobileNetV2 (quantized to 8 bits) and the last column is from ResNet20 (quantized to 4 bits). By investigating the image patterns generated by GDFQ and ZeroQ, we find there is a big gap between them and those of the original images.</p><p>Although the image samples by ZAQ seem to be not recognizable by humans or be similar to the original data, their goal is to represent the discrepancy between two models with different precision. The comparison between the synthetic images of ZQA and those of GDFQ and ZeroQ indicates that ZQA could generate more diverse images, while GDFQ and ZeroQ suffer from more repeated patterns in their generated images. This empirically shows the efficiency of knowledge transfer in ZAQ.</p><p>Furthermore, we visualize the semantic image samples generated by ZAQ and ZeroQ in <ref type="figure">Figure 9</ref>. The observation is accordant with what we have observed in image classification datasets. That is, ZAQ tends to generate semantic images with more diversity, while ZeroQ reconstructs semantic images with some duplicated local patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we have proposed ZAQ, a novel zero-shot adversarial quantization framework without needing to access any original training data. Its main innovations lie in applying adversarial learning to data-free model quantization through alternating two-level discrepancy estimation and knowledge transfer. Our framework is welcomed for its ability of modeling prediction discrepancy, as well as intermediate inter-channel discrepancy between full-precision and quantized models. Extensive experiments on various deep neural models for three common vision tasks demonstrate the superiority of ZAQ, especially for ultra-low precision situations. In the future work, we consider applying the proposed method to other domains such as BERT quantization <ref type="bibr" target="#b37">[38]</ref>, and extending ZAQ to automatic mixed precision quantization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of our framework. The methods based on sample reconstruction are shown as part (a), and part (b) is the overview of our framework. BNS is short for batch normalization statistics stored in the BN layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Framework of ZAQ. quantized model Q, and generator G. G is responsible for generating informative and diverse data examples, which are used by a two-level discrepancy function to compute the discrepancy between P and Q. The discrepancy function is composed of output discrepancy D o and intermediate inter-channel discrepancy D f . Consequently, Q and G are optimized through a minimax game, where the adversarial learning of the two-level discrepancy modeling is conducted. In addition, activation regularization L a encourages G to generate more informative and diverse examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ij (i, j ? {1, 2, . . . , C}), the corresponding matrices R (l) P (x g ) and R (l)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 Initialize adaptive weights by 1 L ; 4 for number of training steps do 5 # Discrepancy Estimation 6 z</head><label>3456</label><figDesc>? N (0, I), x g ? G(z; ? g );</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>7</head><label>7</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of segmentation results for Cityscapes (the first two rows) and CamVid (the last two rows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Performance change versus different quantization precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>tributions of the main components in ZAQ. First of all, Figure 6 presents the benefits of output discrepancy D o ('a'), intermediate inter-channel discrepancy D f ('b'), and activation regularization L a ('c') on ImageNet (using model ResNet18) and Cityscapes (using model DeeplabV3(ResNet50)). Since output discrepancy D o is directly associated with the final model output, it should not be removed anytime. As we can see, the intermediate inter-channel discrepancy could Effectiveness of different components of the proposed ZAQ method. Noting that 'a':Do, 'b':D f , 'c':La.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .Figure 8 .Figure 9 .</head><label>789</label><figDesc>(a) full-precision model (b) quantization model Visualization of CRMs on CIFAR100. Generated samples about CIFAR. Sematic images about CamVid generated by ZAQ and ZeroQ based on DeeplabV3(MobileNetV2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results of image classification on three datasets.</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="2">Model</cell><cell>size (MB)</cell><cell>bit</cell><cell cols="2">size (MB) float32</cell><cell>RQ</cell><cell>ZeroQ GDFQ DFQ ACIQ ZAQ</cell></row><row><cell></cell><cell></cell><cell cols="2">MobileNetV2</cell><cell>9.0</cell><cell>W6A6</cell><cell>1.7</cell><cell>92.39 78.90 89.90</cell><cell>91.27 85.43 91.04 92.15</cell></row><row><cell cols="2">CIFAR10</cell><cell cols="2">VGG19</cell><cell>149</cell><cell>W4A8</cell><cell>25.1</cell><cell>93.49 92.42 92.69</cell><cell>92.84 92.66 92.48 93.06</cell></row><row><cell></cell><cell></cell><cell cols="2">ResNet20</cell><cell>1.1</cell><cell>W5A5</cell><cell>0.2</cell><cell>69.58 49.54</cell><cell>65.7</cell><cell>66.12 59.42 60.19 67.94</cell></row><row><cell cols="2">CIFAR100</cell><cell cols="2">ResNet18</cell><cell>43</cell><cell>W4A4</cell><cell>5.4</cell><cell>77.38 17.00 70.25</cell><cell>71.53 40.35 54.73 72.67</cell></row><row><cell></cell><cell></cell><cell cols="2">MobileNetV2</cell><cell>14</cell><cell>W8A8</cell><cell>3.5</cell><cell>71.88 67.09 70.88</cell><cell>70.17 70.58 68.92 71.43</cell></row><row><cell cols="2">ImageNet</cell><cell cols="2">ResNet50 ResNet50</cell><cell>98 98</cell><cell>W4A4 W2A2</cell><cell>12.3 6.1</cell><cell>76.13 64.90 69.30 76.13 11.25 63.12</cell><cell>68.69 10.32 59.34 70.06 64.96 1.48 3.25 65.52</cell></row><row><cell>RGB</cell><cell cols="2">Ground Truth</cell><cell>FT</cell><cell>ZeroQ</cell><cell>DFQ</cell><cell>Ours</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results on Cityscapes and CamVid (mIoU).</figDesc><table><row><cell>Dataset</cell><cell cols="5">Method W8A8 W6A6 W4A4 W2A2</cell></row><row><cell></cell><cell>FT</cell><cell>61.25</cell><cell>59.64</cell><cell>55.98</cell><cell>45.77</cell></row><row><cell>Cityscapes (63.39)</cell><cell>RQ DFQ ZeroQ</cell><cell>58.42 57.34 59.52</cell><cell>55.33 55.29 57.97</cell><cell>29.16 19.06 52.73</cell><cell>0.44 3.13 43.18</cell></row><row><cell></cell><cell>ZAQ</cell><cell>60.18</cell><cell>58.12</cell><cell>55.12</cell><cell>44.93</cell></row><row><cell></cell><cell>FT</cell><cell>52.76</cell><cell>50.75</cell><cell>49.13</cell><cell>40.06</cell></row><row><cell>CamVid (53.34)</cell><cell>RQ DFQ ZeroQ</cell><cell>44.96 51.02 49.92</cell><cell>43.20 46.13 48.56</cell><cell>10.05 11.78 43.83</cell><cell>0.02 2.33 36.44</cell></row><row><cell></cell><cell>ZAQ</cell><cell>50.89</cell><cell>49.77</cell><cell>47.62</cell><cell>39.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>briefly demonstrates the advantages of</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable methods for 8-bit training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5145" to="5153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aciq: analytical clipping for integer quantization of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Nahshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zeroq: A novel zero shot quantization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="13169" to="13178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data-free learning of student networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanjian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3514" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Jen</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayalakshmi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06085</idno>
		<title level="m">Pact: Parameterized clipping activation for quantized neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mixed precision training of convolutional neural networks using integer operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Mellempudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Kalamkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasikanth</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Georganas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model compression and hardware acceleration for neural networks: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="532" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05493</idno>
		<title level="m">Towards interpretable deep neural networks by leveraging adversarial examples</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge 2012 (voc2012) development kit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis, Statistical Modelling and Computational Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data-free adversarial distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongfan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengchao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flexpoint: An adaptive numerical format for efficient training of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>K?ster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oguz</forename><surname>Constable</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Elibol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hornof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1742" to="1752" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-05" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge distillation via instance relationship graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiang</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7096" to="7104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive multi-teacher multi-level knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">415</biblScope>
			<biblScope unit="page" from="106" to="113" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Datafree knowledge distillation for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Raphael Gontijo Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thad</forename><surname>Fenu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Discovering low-precision networks close to full-precision networks for efficient embedded inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rathinakumar</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepika</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Izzet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharmendra S</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04191</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Same, same but different: Recovering neural network quantization error through weight factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Meller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Almog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Grobman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4486" to="4495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ganesh Venkatesh, et al. Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">8-bit inference with tensorrt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Migacz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPU technology conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data-free quantization through weight equalization and bias correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart</forename><surname>Van Baalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Blankevoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero-shot knowledge distillation in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konda</forename><surname>Gaurav Kumar Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaisakh</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Venkatesh Babu Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakraborty</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of Machine Learning Research</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Qbert: Hessian based ultra low precision quantization of bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8815" to="8821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh Babu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06149</idno>
		<title level="m">Data-free parameter pruning for deep neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Quantized convolutional neural networks for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4820" to="4828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generative lowbitwidth data free quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuangrun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Data-free knowledge amalgamation via group-stack dual-gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page" from="12516" to="12525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dreaming to distill: Data-free knowledge transfer via deepinversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Hongxu Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8715" to="8724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving neural network quantization without retraining using outlier channel splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritchie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Dotzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7543" to="7552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<title level="m">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards unified int8 training for convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1969" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
