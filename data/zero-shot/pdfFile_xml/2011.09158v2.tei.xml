<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Privileged Knowledge Distillation for Online Action Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Zhao</surname></persName>
							<email>pszhao@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajie</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<email>yazhang@sjtu.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
							<email>wangyanfeng@sjtu.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff5">
								<orgName type="institution">Huawei Cloud &amp; AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Privileged Knowledge Distillation for Online Action Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online Action Detection (OAD) in videos is proposed as a per-frame labeling task to address the real-time prediction tasks that can only obtain the previous and current video frames. This paper presents a novel learningwith-privileged based framework for online action detection where the future frames only observable at the training stages are considered as a form of privileged information. Knowledge distillation is employed to transfer the privileged information from the offline teacher to the online student. We note that this setting is different from conventional KD because the difference between the teacher and student models mostly lies in input data rather than the network architecture. We propose Privileged Knowledge Distillation (PKD) which (i) schedules a curriculum learning procedure and (ii) inserts auxiliary nodes to the student model, both for shrinking the information gap and improving learning performance. Compared to other OAD methods that explicitly predict future frames, our approach avoids learning unpredictable unnecessary yet inconsistent visual contents and achieves state-of-the-art accuracy on two popular OAD benchmarks, TVSeries and THUMOS14.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action detection in videos has been widely studied under offline settings <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b43">43]</ref>, where the entire video is observed when locating the actions. In some real-time situations such as autonomous driving and online surveillance where the video frames come in streams, real-time predictions based on the available data are required. To address this problem, Online Action Detection (OAD) is proposed as a per-frame labeling task with existing observations. Early OAD methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b29">29]</ref> only use the available video frames (i.e., previous and current frames) to train the online models. Considering that future observations are ac-  <ref type="figure" target="#fig_2">Figure 1</ref>. "Pass or Dunk?" When watching the basketball game at time t, we prefer that Kobe will dunk since we may have seen him completing dunk before. This experience helps us make decisions. Inspired by this, it is beneficial to teach the OAD model by knowledge distillation with a teacher who can see full video data. tually available at the training stage, F 2 G <ref type="bibr" target="#b40">[40]</ref> and TRN <ref type="bibr" target="#b35">[35]</ref> propose to first learn to anticipate either future video frames or future frame features; then utilize these anticipations together with the existing observations to better classify actions. This anticipation&amp;classification-based decision mechanism has been shown to improve the performance of OAD. However, the anticipation may have a different target than the classification. Predicting the detailed visual appearance or general features extracted with a pretrained model may not correlate well with the action semantics. Furthermore, the anticipation&amp;classification framework inevitably suffers from the efficiency issue, i.e., making explicit predictions especially predicting future video frames needs considerable computations and is not suitable for online tasks.</p><p>We deem the additional future frames available at the training stage as privileged information <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b33">33]</ref> and re-formulate the problem into online action detection with privileged information. To leverage the privileged information but not explicitly anticipate it as the anticipa-tion&amp;classification methods do, we introduce a Knowledge Distillation (KD) based framework, named Privileged Knowledge Distillation (PKD), which consists of an offline teacher and an online student, and anticipation is replaced with knowledge distillation from offline teacher models to online student models. The KD process can be seen as an implicit way of learning to anticipate high-level semantic features from offline models. With this KD framework, both the teacher and student have the same learning target of classifying current actions. To the best of our knowledge, this is the first study that introduces KD for the OAD task.</p><p>The KD for OAD is different from conventional KD because the difference between the teacher and student models mostly lies in the available input data rather than the network architecture. Given the different inputs, it is not feasible for the student to learn the same representation as teachers. Additional auxiliary nodes are thus inserted to each layer of the student network so that only part of the student features are regularized by their teachers. Moreover, the information gap between the teacher and the student may not be easily captured through the knowledge distillation, especially at the early stage of an action. To facilitate the information propagation from the teacher to the student, a curriculum learning procedure is further introduced. Instead of directly letting the student learn from the teacher, we formulate several intermediate teacher models that can see increasing lengths of future frames, representing teachers with different levels of knowledge. The student then learns from these intermediate teachers in the order of from easy to hard. In this way, the privileged knowledge from offline models is gradually distilled to the online student model.</p><p>We validate the effectiveness of the proposed PKD on two popular benchmarks TVSeries and THUMOS14. The experimental results show that our proposed auxiliary nodes and curriculum learning procedure indeed improves the performance of the OAD model and achieves the state-of-theart performance of 86.44% mcAP on TVSeries and 64.45% mAP on THUMOS14. Further analysis shows that PKD performs better when detecting action at some early stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Offline Action Detection. Offline action detection aims to detect the start and end of each action instance with fullyobserved videos. Existing methods, such as TAL-Net <ref type="bibr" target="#b6">[6]</ref>, BSN <ref type="bibr" target="#b19">[19]</ref>, BMN <ref type="bibr" target="#b18">[18]</ref>, MGG <ref type="bibr" target="#b22">[22]</ref>, and the most recent works <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b43">43]</ref> are inspired by the two-stage object detection framework <ref type="bibr" target="#b26">[26]</ref>, that decomposed this task into the action proposal and action classification stages. Besides, AFO-TAD <ref type="bibr" target="#b31">[31]</ref> and A2Net <ref type="bibr" target="#b39">[39]</ref> proposed an anchor-free method that directly regress the action boundaries without pre-defined action proposals. PGCN <ref type="bibr" target="#b41">[41]</ref> and G-TAD <ref type="bibr" target="#b36">[36]</ref> introduced GCN <ref type="bibr" target="#b17">[17]</ref> to model the relationship between action proposals and temporal nodes. All these offline methods need to observe the entire video.</p><p>Online Action Detection. Different from the offline settings, online action detection can only observe the current and previous video frames. Geest et al. <ref type="bibr" target="#b14">[14]</ref> first defined the online action detection task and introduced a dataset, TVSeries. Gao et al. <ref type="bibr" target="#b10">[10]</ref> proposed a reinforced encoderdecoder network to anticipate future actions that can be used for online action detection. TRN <ref type="bibr" target="#b35">[35]</ref> also predicted future information and proposed a TRN cell to use these predicted features as well as the current and past features to detect actions. Besides, Shou et al. <ref type="bibr" target="#b29">[29]</ref> proposed a new metric to evaluate the quality of action start points and named this task as Online Detection of Action Start (ODAS). Start-Net <ref type="bibr" target="#b12">[12]</ref> followed the new metric and addressed ODAS by optimizing long-term localization rewards using policy gradient methods. Recently, <ref type="bibr" target="#b8">[8]</ref> changed the state update rules in LSTM and proposed an Information Discrimination Unit (IDU) to model the relationship between the current frame and the past frames. Then the IDU could decide whether to accumulate input information based on its relevance to the current feature. Most of the online action detection methods are designed based on RNN models that are more suitable for modeling sequence data.</p><p>Knowledge Distillation. Ordinarily, knowledge distillation is an effective technique that can transfer the knowledge from teachers, i.e., large complex models, to students, i.e., small simple models. After Hinton et al. <ref type="bibr" target="#b16">[16]</ref> addressed knowledge distillation by training the student with the soft target provided by the teacher, knowledge distillation has been widely adopted in a variety of learning tasks. <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b21">21]</ref> let the student imitate the output logits of the teacher; <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b37">37]</ref> were focused on minimizing the representations between intermediate layers of the student and teacher. Besides, some studies introduced the idea of progressive training. TAKD <ref type="bibr" target="#b23">[23]</ref> and DGKD <ref type="bibr" target="#b30">[30]</ref> introduced medium size models, called teacher assistant, to bridge the architecture gap between the teacher and student models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Models and Notations. Most OAD methods <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b8">8]</ref> adopt the LSTM-based network to model the input sequence data. However, the LSTM structure is less efficient than the CNN based network. Thus, we adopt the causal convolution <ref type="bibr" target="#b24">[24]</ref> to build our OAD model. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (a), a two-layer causal convolution model, named S, can only observe the previous and current features, which satisfies the online settings. Besides, we also introduce an offline model to teach the student model S in <ref type="figure" target="#fig_1">Figure 2</ref>   xt is the extracted video features. h 1 t and h 2 t denote original student features; a 1 t and a 2 t denote auxiliary nodes; g 1 t and g 2 t denote teacher features. y S t and y T t are output logits. (c) shows the curriculum learning procedure that the student model S are trained from T1 to T4. T1, T2, T3 are also teacher models that can see different length of future data between S and T4.</p><p>This model does not only keep the same receptive fields in previous observations with the online model but also obtains the future observations. In terms of the extra increasing numbers of connections from the future nodes in convolution kernels, the offline teacher models are named T 1 , T 2 , T 3 , and T 4 . Problem Definition. Given a streaming video, the goal of OAD is to recognize the current action without access to future frames. We denote x t ? R C as the input features to represent the streaming video and y t ? R M +1 as the classification logits. At the time step t = 0 with the twolayer models shown in <ref type="figure" target="#fig_1">Figure 2</ref>, online model S and offline model T 4 are formulated as:</p><formula xml:id="formula_0">y S 0 = S({x t } 0 t=?8 ), y T 0 = T 4 ({x t } 8 t=?8 ). where y S 0 ? R M +1 and y T 0 ? R M +1</formula><p>are the output logits at the time step t = 0; M denotes the number of action classes and +1 means the background class. Both the student and teacher models are supervised by classification labels y gt t :</p><formula xml:id="formula_1">L cls = 1 N t?N H CE (y gt t , y t ),<label>(1)</label></formula><p>where N is the number of time steps; H CE represents the cross-entropy. The goal of KD for OAD is to distill the privileged knowledge from T 4 to S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Privileged KD for OAD</head><p>Knowledge distillation is an effective technique that can transfer knowledge from the teacher to the student. It is natural to introduce KD to the OAD task which can distill the knowledge from the offline to online models. However, different from the conventional KD that both the teacher and student take the same input data, the KD in OAD settings takes the different inputs for teacher and student models. Therefore, the main difference between the teacher and student comes from the input information gap rather than model structures. This information gap makes it a challenge to let the student learn from the teacher with existing KD methods. Thus, we propose a novel Privileged Knowledge Distillation (PKD) method to cushion the information gap between them by (i) scheduling a curriculum learning process for KD and (ii) inserting auxiliary nodes for the student model. These designs are illustrated in Sections 4.1 and 4.2, and we summarize the loss function in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Curriculum Knowledge Distillation</head><p>Curriculum Learning <ref type="bibr" target="#b4">[4]</ref> is proposed by Bengio et al. that teaches a model starting from easy samples to hard samples. Inspired by this training strategy, we introduce a curriculum learning procedure to PKD that can cushion the information gap between the teacher and student. In this section, we first introduce the Vanilla KD and then describe the curriculum learning procedure.</p><p>Vanilla KD. The output logits from a classification model can be regarded as a soft label that contains the knowledge of how similar between different classes. This similarity knowledge is helpful for classification tasks, which is proved by previous KD methods <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b15">15]</ref>. With observing the future data, logits from the offline model contain a more complete understanding of this similarity knowledge than logits from the online model. So we train the online model to imitate the output logits of the offline model by minimizing the KL divergence between them at each time step t. Therefore, the loss function of the vanilla knowledge distillation is formulated as:</p><formula xml:id="formula_2">L L = 1 N t ? 2 L KL (?(y S t /? ), ?(y T t /? )),<label>(2)</label></formula><p>where N is the number of time steps; ? is the temperature parameter <ref type="bibr" target="#b16">[16]</ref> to control the softening of y T ; ? is the softmax function; L KL (?, ?) measures the KL divergence.</p><p>Curriculum Learning Procedure. The information gap between S and T 4 is large since half of the input data is not available. We find it is difficult to directly train the student model S to learn from T 4 . Therefore, to better reduce the information gap between the teacher and student, we schedule a curriculum learning procedure to relieve this problem. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (c), we first introduce some intermediate teacher models T 1 , T 2 , and T 3 which can see the increasing length of future frames between S and T 4 ; then let the student model S learn from the teacher models from T 1 to T 4 . This learning process is notated as </p><formula xml:id="formula_3">S ? T 1 ? T 2 ? T 3 ? T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning with Auxiliary Nodes</head><p>Conventional KD methods usually force the student model to have the same representations as the teacher model does. It is appropriate to add this constrain since they have the same input data. However, with different input data, it is difficult to let the student model S to learn the same representations as the teacher does. Therefore, directly minimizing the distance between their feature representations is not appropriate in this setting. So we introduce a new KD method that inserts auxiliary nodes for the student to relieve this problem. As illustrated in <ref type="figure" target="#fig_1">Figure 2 (a)</ref>, each two-color node in the feature layer represents a combined feature representation that concatenates the original student features h l t ? R C l with the auxiliary nodes a l t ? R C l . l stands for the feature layer and the C l represents the feature dimension. At the training stage, we let these auxiliary nodes learn from the teacher representations g l t ? R C l which are shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b) by orange nodes. In this case, the auxiliary nodes can serve as the additional information that will not conflict with original feature representations of the student model and can also provide privileged knowledge. Auxiliary Nodes for Final Feature Layer. At the time step t in <ref type="figure" target="#fig_1">Figure 2</ref> (a), auxiliary nodes a 2 t in the final feature layer are used to classify actions at that moment. So these auxiliary nodes of the final feature layer should be supervised by the corresponding teacher representations g 2 t . Thus, the loss function is formulated as:</p><formula xml:id="formula_4">L AN = 1 N t ||a 2 t ? g 2 t || 2 ,<label>(3)</label></formula><p>where N is the number of time steps and || ? || 2 measures the mean square error. Auxiliary Nodes for Intermediate Feature Layer. Different from the conventional KD with features, directly applying Eq. (3) for intermediate feature layers makes an unmatched-receptive-field problem. It is because that the teacher and student models use the different convolution types. Specifically, at the time step t = 0 in layer 2, the student feature d 2 0 has the receptive fields of [?8, 0], while the teacher feature g 2 0 has the receptive fields of [?8, ?8] at the same position. However, at layer 1, the corresponding features</p><formula xml:id="formula_5">{d 1 t } 0 t=?4 in S have the same receptive fields of [?8, 0] but {g 1 t } 0 t=?4 in T 4</formula><p>have the receptive fields of [?8, +4] in the same position. To solve this unmatchedreceptive-field problem, we let the student features d 1 t at layer 1 to learn from the g 1 t with seeing more future nodes to align the receptive fields to [?8, +8]. So the final loss function of the KD with AN is formulated as:</p><formula xml:id="formula_6">L AN = 1 N t ||d 2 t ? g 2 t || 2 + ||d 1 t ? 1 p + 1 t+p t g 1 t || 2 ,<label>(4)</label></formula><p>where N is the number of time steps and p is the number of future nodes that the model should be seen to align the receptive fields. p is set to be 4 for KD from T4 to S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Loss Function</head><p>Instead of using the fixed teacher models, we jointly optimize the student and teacher models to obtain a better performance which has been confirmed by <ref type="bibr" target="#b42">[42]</ref>. Therefore, using the Eq. (1), Eq. (2), and Eq. (4), the overall loss function for training the OAD model with KD is formulated as:</p><formula xml:id="formula_7">L = L cls + L KD = L S cls + L T cls + ?L L + ?L AN ,<label>(5)</label></formula><p>where L S cls and L T cls are the classification losses for the student and teacher models. L L denotes the vanilla KD loss while L AN is the KD loss for auxiliary nodes. ? and ? are the trade-off parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Metric</head><p>TVSeries. The TVSeries dataset <ref type="bibr" target="#b14">[14]</ref> consists of 16 hours of 27 untrimmed videos with 30 action classes, e.g., "stand up", "open door", and "drive car". The dataset is collected from the six recent TV series on DVD. So it is challenging with diverse actions, multiple actors, heavy occlusions, and a large proportion of background video frames. THUMOS14. The THUMOS14 dataset includes 413 untrimmed videos with 20 action classes. Following the public data split, 200 of them are used for training and 213 are used for testing. Each video contains more than an average of 15 action instances and 70% backgrounds. Evaluation Metric. To evaluate the performance of the online action detection, we follow the previous studies <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b8">8]</ref> and use the conventional metric mean average precision (mAP), which first calculate the average precision (AP) of all frames for each action class; then average these APs over all classes. The mAP is used for the THUMOS14 dataset. As for TVSeries, <ref type="bibr" target="#b14">[14]</ref> proposed a mean calibrated average precision (mcAP) to better evaluate the performance of the online action detection, which balance the effectiveness of the foreground-background proportion. The calibrated average precision is formulated as:</p><formula xml:id="formula_8">cAP = k cPrec(k) ? I(k) P ,<label>(6)</label></formula><p>where the calibrated precision cPrec = TP TP+FP/? ; I(k) is equal to 1 if frame k is a True Positive (TP); P is the number of TP. The coefficient ? is the ratio between the number of background and action frames. Thus, the mcAP is calculated by averaging cAPs over all classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We use the Pytorch framework <ref type="bibr" target="#b25">[25]</ref> to implement our online action detection models as well as the training process. All the proposed models T 1 -T 4 and S are trained with the , the number of epochs is set to be 40. The initial learning rate is set to be 5 ? 10 ?4 and decreased by a factor of 0.1 at the epoch 30. The hyperparameter of temperature ? is set to be 5. The trade-off parameter ? and ? are set to be 0.4 and 0.01 that can balance the KD loss L L and L AN under the similar numerical value with classification losses L S cls and L S cls . Moreover, all the feature channels are set to be 512 for h l t , a l t , and g l t . Features. To make comparisons with the previous works that process the video with extracted features, we also extract the two-stream features for these two datasets. The dimension for both RGB and optical flow is 1024. The twostream input features x t concatenate the RGB and optical flow features. The toolkit for extracting features is provided by <ref type="bibr" target="#b20">[20]</ref> and the extractor is pre-trained on Kinetics <ref type="bibr" target="#b5">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison to the State-of-the-arts</head><p>In this section, we make comparisons between our proposed method and other state-of-the-art methods on TVSeries and THUMOS14 datasets. As illustrated in Table 1, the inputs of F 2 G [40] are video frames while other methods take extracted two-stream features as the input data. F 2 G is designed to first predict future video frames and then use these predicted frames as well as previous and current video frames to classify current actions. RED <ref type="bibr" target="#b10">[10]</ref> and TRN <ref type="bibr" target="#b35">[35]</ref> also use the anticipation-classification framework for OAD but the prediction stages are based on the feature level which is built in a reinforced encoder-decoder network or an LSTM structure, respectively. IDN <ref type="bibr" target="#b8">[8]</ref> is an improved LSTM structure for the OAD task that decides whether to accumulate input information based on its relevance to the current input frame. Besides, * TRN <ref type="bibr" target="#b35">[35]</ref> and * IDN <ref type="bibr" target="#b8">[8]</ref> also use the feature extractor that is pre-trained on Kinetics dataset <ref type="bibr" target="#b5">[5]</ref>, which allows a fair comparison with our proposed method. On the THUMOS14 dataset, our pro- posed PKD method achieves 64.5% in terms of mAP that improves absolute 4.2% over the previous best method IDN. As for the TVSeries dataset, PKD also performs better than the previous method. Since the performance is relatively high on this dataset, the improvement is only absolute 0.3% in terms of mcAP over the previous best method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Studies on PKD</head><p>In this section, we first show the effectiveness of introducing KD to the OAD task, then investigate the effectiveness of our proposed curriculum learning process and auxiliary nodes on two datasets: TVSeries and THUMOS14. Besides, we also explore the performance of PKD with different numbers of auxiliary nodes. Effectiveness of the KD. As illustrated in <ref type="table" target="#tab_3">Table 2 and Table 3</ref>, the first two rows of each table show the performances on S, T 1 , T 2 , T 3 , and T 4 models of each dataset. The baseline online model S achieves 85.40% and 61.65% in terms of the mcAP on TVSeries and the mAP on THUMOS14, respectively. While the best offline model T 4 can achieve 87.94% and 66.91%. So the performance gaps between the online and the offline models are absolute 2.54% on TVSeries and 5.26% on THUMOS14, which shows the upper bounds for online models through distilling knowledge from offline models. We denote "T 1 ? S", "T 2 ? S", "T 3 ? S", and "T 4 ? S" as the one step distillation settings which means the online model S is trained from only one offline model. The results show that the student model S obtains a better performance when it learns from a better teacher who can see more future frames. Therefore, the baseline KD for OAD that only uses the vanilla KD loss L L in "T 4 ? S" improves the performance of S from 85.40% to 85.98% on TVSeries dataset and from 61.65% to 62.72% on THUMOS14 dataset. Considering the upper bounds from offline models. The effectiveness of the baseline KD for OAD can reduce this gap by 22.8% on TVSeries Effectiveness of the auxiliary nodes. Apart from distilling the knowledge from the output logits, we also design auxiliary nodes for S to distill the privileged knowledge through feature representations. As shown in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref>, with adding the loss L AN , the performance on both datasets is improved on different KD path. In "T 4 ? S" setting, the performance can achieve 86.14% on TVSeries and 63.96% on THUMOS14. Therefore, the effectiveness of the auxiliary nodes can further reduce the upper bound gap to 29.1% on TVSeries and 43.9% on THUMOS14.</p><p>Effectiveness of the curriculum learning. As for KD with more distillation path, the notations "T 4 ? T 2 ? S" and "T 4 ? T 3 ? T 2 ? T 1 ? S" follow the TAKD <ref type="bibr" target="#b23">[23]</ref> training strategy that train the online model S starting from a strong teacher model T 4 ; then distill the knowledge to some intermediate teacher models; finally, the student model S is taught with the closest teacher model. As for the notations "S ? T 2 ? T 4 " and "S ? T 1 ? T 2 ? T 3 ? T 4 " represent the curriculum learning process for PKD in Algorithm 1. As shown in <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref>, "T 4 ? T 2 ? S" and "T 4 ? T 3 ? T 2 ? T 1 ? S" on both datasets can improve the performance from the baseline model S. However, they obtain about the same or even worse results as the single path "T 4 ? S" dose. TAKD has been proved to be effective in conventional KD settings where the knowledge gap mainly comes from the model structure. So it can alleviate the ability of feature representations from strong to weak. But in offline to online settings, the representation ability of student S is not weak compared to teacher T 4 . So, KD in TAKD way loses the input information since the final knowledge distillation stage is completed by a weaker teacher. Therefore, it is proper to let the student approach the teacher instead of letting the teacher approach the student. Following the training strategy in Algorithm 1, the curriculum learning procedure "S ? T 1 ? T 2 ? T 3 ?   The number of auxiliary nodes. As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, we also investigate the performance of PKD with different numbers of auxiliary nodes based on "</p><formula xml:id="formula_9">S ? T 1 ? T 2 ? T 3 ? T 4 " setting.</formula><p>In previous experiments, we introduce auxiliary nodes to each feature layer of S which has the same feature dimension as the teacher. To match the feature dimension with the teacher when changing the number of auxiliary nodes, we use an additional causal convolution layer to map this dimension difference. Results show that PKD achieves better performance with more auxiliary nodes. However, only using 16 auxiliary nodes can also improve 1.14% mAP from the "Baseline" model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Predicting Features vs. PKD</head><p>To compare our proposed PKD with anticipation methods, we also implement an anticipation&amp;classification pipeline with the same network structure S. The anticipation stage is made under the input feature level x t . It is designed like the proposed auxiliary nodes that we predict input features supervised by different average future inputs. For each input feature x t , the predicted input feature is notated asx t . Thus,x t and x t are concatenated as the new input features for time t.x t is supervised as:</p><formula xml:id="formula_10">L predict = ||x t ? 1 P P i=1</formula><p>x t+i || 2 .</p><p>As illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>, the performance on the THU-MOS14 dataset of the prediction method is shown in a blue solid line with different average future steps P , while the KD methods are shown in dashed lines. By predicting more  future features the online model can achieve better performance but when the average future step is large than 3, the performance decreases. The phenomenon is the same as the prediction method TRN <ref type="bibr" target="#b35">[35]</ref> which also shows that a larger number of prediction steps do not guarantee better performance. It is because anticipation accuracy usually decreases for longer future sequences, and thus creates more noise in the input features. At the average step of 3, the anticipation method achieves the best performance of 62.73%. Comparing the prediction method and KD methods, we find that the prediction method can achieve a similar performance of baseline KD. However, this explicit anticipation way still has an obvious gap around the absolute 1.7% between our proposed PKD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Evaluation of Different Stages of Action</head><p>One of the significant characteristics of OAD is to detect an action at the early stage. Therefore, we compare our proposed PKD method with the previous methods in different portions of action stages on the TVSeries dataset in <ref type="table">Table 4</ref>. For example, "0%-10%" means that only action frames at the duration range of "0%-10%" are evaluated in terms of mcAP. The results show that our proposed PKD outperforms all the previous methods at all action portions. We notice that CNN <ref type="bibr" target="#b14">[14]</ref> and LSTM <ref type="bibr" target="#b14">[14]</ref> obtain almost the same result on different action portions since they do not utilize the temporal information. i.e., these methods only use a single frame or few successive frames at each output time. As for FV-SVM <ref type="bibr" target="#b14">[14]</ref>, TRN <ref type="bibr" target="#b35">[35]</ref>, and IDN <ref type="bibr" target="#b8">[8]</ref> which use the accumulated information, the performance increases when observing more completed actions. Different from previous methods, our PKD takes the 1D temporal convolution as our base model that processes the input sequence within a receptive window, thus reaches its maximum 89.0% near the middle and back of the action. <ref type="table">Table 4</ref>. Comparisons on different portions of the action instance in terms of mcAP on the TVSeries dataset. The portion labels are only used in evaluation when the results is predicted by an OAD model. * means the feature extractor is pre-trained on Kinetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Proportion of Action 0%-10% 10%-20% 20%-30% 30%-40% 40%-50% 50%-60% 60%-70% 70%-80% 80%-90% 90%-100% CNN <ref type="bibr" target="#b14">[14]</ref> 61.0 61.0 61.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Qualitative Results</head><p>As illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>, we visualize some OAD results on TVSeries (above) and THUMOS14 (below) datasets. Comparing the baseline model S with our proposed PKD method, we find that PKD can detect more action instances that have been missed in the baseline model the on TVSeries dataset. e.g., the actions "Fall", "Answer phone", and "Drive car" are missed by the baseline model but found by the PKD method. It is because that the PKD model obtains the privileged knowledge from the offline models, which can help to classify the ongoing actions. Besides, the PKD method can output more stable predic-tions than the baseline which are shown inside the "Hammer Throw" action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we propose a novel algorithm that applies knowledge distillation (KD) to online action detection (OAD) so that the target model is guided by a teacher model that sees complete video data. The key difficulty lies in that teacher and student models receives different input data. To bridge the information gap, we present a privileged KD pipeline that leverages curriculum learning and inserts auxiliary nodes to assist the student model. Experiments on TVSeries and THUMOS14 demonstrate the state-of-the-art performance of our approach.</p><p>Our research enlightens a new path that improves video understanding by offering auxiliary guidance (e.g., predicting unseen features) in an implicit space. This is probably related to unsupervised of self-supervised representation learning, which we will investigate in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) and (b) show the model structures of the student S and teacher T4. Both of them use the 1D temporal convolution layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Curriculum Learning Procedure. Input: extracted video features x t , target labels y gt t Output: distilled student model S 1 Train the student S with Eq. (1) 2 Train the teachers T 1 , T 2 , T 3 , and T 4 with Eq. (1) 3 for i ? 1 to 4 do 4 load the parameters of the models S, T i 5 do 6 inference the model S and T i with x t 7 compute the gradients with Eq. (5) 8 update the parameters of S and T i 9 while S is not converged; 10 save the parameters of the model S 11 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Comparisons between different number of auxiliary nodes in terms of mAP on the THUMOS14 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparisons between prediction method and KD methods in terms of mAP on the THUMOS14 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Comparisons between the PKD and the baseline OAD model S in terms of mAP on TVSeries (above) and THUMOS14 (below).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Pass or Dunk at the time ?</figDesc><table><row><cell>-2</cell><cell>-1</cell><cell>+1</cell><cell>+2</cell></row><row><cell>Student:</cell><cell>Knowledge</cell><cell>Teacher:</cell><cell></cell></row><row><cell>Online Model</cell><cell></cell><cell cols="2">Offline Model</cell></row><row><cell>Pass</cell><cell>Dunk</cell><cell>Dunk</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b4">4</ref> and the detailed training procedure is shown in Algorithm 1. In addition, TAKD<ref type="bibr" target="#b23">[23]</ref> also uses intermediate teacher models to help KD. But the distillation path is opposite from ours, so it will lose the input information in the final distillation stage when training the student model. The detailed comparisons are shown in the experimental results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparisons to state-of-the-art methods based on two datasets: THUMOS14 and TVSeries. * means the feature extractor is pre-trained on Kinetics dataset.</figDesc><table><row><cell>Method</cell><cell>THUMOS14 mAP (%)</cell><cell>TVSeries mcAP (%)</cell></row><row><cell>RED [10]</cell><cell>45.3</cell><cell>79.2</cell></row><row><cell>F 2 G [40]</cell><cell>45.8</cell><cell>-</cell></row><row><cell>TRN [35]</cell><cell>47.2</cell><cell>83.7</cell></row><row><cell>IDN [8]</cell><cell>50.0</cell><cell>84.7</cell></row><row><cell>TRN *  [13]</cell><cell>51.0</cell><cell>-</cell></row><row><cell>IDN *  [8]</cell><cell>60.3</cell><cell>86.1</cell></row><row><cell>Ours *</cell><cell>64.5</cell><cell>86.4</cell></row><row><cell cols="3">Adam optimizer. All the experiment results are the aver-</cell></row><row><cell>aged values of 3 times.</cell><cell></cell><cell></cell></row></table><note>Parameter settings. At each training process including training the single model (e.g., T 4 or S) or training with KD loss (e.g., distilling T 4 to S)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Comparisons in terms of mcAP (%) on the TVSeries dataset. The first two rows show the baseline performance on the online and different offline models. Rest of the table illustrate the effectiveness of different KD paths and KD losses for online S.</figDesc><table><row><cell>models</cell><cell>S</cell><cell>T 1</cell><cell>T 2</cell><cell>T 3</cell><cell>T 4</cell></row><row><cell>mcAP</cell><cell>85.40</cell><cell>86.66</cell><cell>87.56</cell><cell>87.77</cell><cell>87.94</cell></row><row><cell cols="3">KD path T 1 ? S T 2 ? S T 3 ? S T 4 ? S T 4 ? T 2 ? S T 4 ? T 3 ? T 2 ? T 1 ? S S ? T 2 ? T 4 S ? T 1 ? T 2 ? T 3 ? T 4</cell><cell cols="3">S w/ L L S w/ L L +L AN 85.57 86.07 85.74 86.13 85.94 86.14 85.98 86.14 85.73 86.20 85.96 86.30 86.11 86.32 86.43 86.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparisons in terms of mAP (%) on the THUMOS14 dataset. The first two rows show the baseline performance on the online and different offline models. Rest of the table illustrate the effectiveness of different KD paths and KD losses for model S.</figDesc><table><row><cell>models</cell><cell>S</cell><cell>T 1</cell><cell>T 2</cell><cell>T 3</cell><cell>T 4</cell></row><row><cell>mAP</cell><cell>61.65</cell><cell>63.92</cell><cell>64.91</cell><cell>65.75</cell><cell>66.91</cell></row><row><cell cols="3">KD path T 1 ? S T 2 ? S T 3 ? S T 4 ? S T 4 ? T 2 ? S T 4 ? T 3 ? T 2 ? T 1 ? S S ? T 2 ? T 4 S ? T 1 ? T 2 ? T 3 ? T 4</cell><cell cols="3">S w/ L L S w/ L L +L AN 62.02 62.94 62.35 63.23 62.70 63.77 62.72 63.96 62.75 63.43 62.35 63.47 62.89 64.31 63.64 64.45</cell></row><row><cell cols="3">and 20.3% on THUMOS14.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>T 4 " achieves the best performance on both datasets, which can improve the online model S from 85.40% to 86.44% on TVSeries and from 61.65% to 64.45% on THUMOS14. Therefore, the effectiveness of the curriculum learning can further reduce the upper bound gap to 40.9% on TVSeries and 53.2% on THUMOS14.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge distillation from internal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7350" to="7357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwen</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th Annual International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking the faster r-cnn architecture for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1130" to="1139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the efficacy of knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4794" to="4802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to discriminate information for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjun</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="809" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate temporal action proposal generation with relation-aware pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10810" to="10817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Red: Reinforced encoder-decoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengya</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09168</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Residual knowledge distillation. In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Startnet: Online detection of action start in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5542" to="5551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Woad: Weakly supervised online action detection in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03732</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Roeland De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Variational student: Learning compact and sparser networks in knowledge distillation framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinidhi</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjitha</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramya</forename><surname>Hebbalaguppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwajith</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bsn: Boundary sensitive network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haisheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Completeness modeling and context separation for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daochang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge flow: Improve upon your teachers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen</forename><surname>Iou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-granularity generator for temporal action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3604" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved knowledge distillation via teacher assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Seyed Iman Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghasemzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meal: Multi-model ensemble via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4886" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online action detection in untrimmed, streaming videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Giro-I-Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="534" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Densely guided knowledge distillation using multiple teacher assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonchul</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08825</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Afo-tad: Anchor-free one-stage detector for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08250</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning using privileged information: similarity control and knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rauf</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2023" to="2049" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A new learning paradigm: Learning using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Vashist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="544" to="557" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal recurrent networks for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5532" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kernel based progressive distillation for adder neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Snapshot distillation: Teacher-student optimization in one generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2859" to="2868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Revisiting anchor mechanisms for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8535" to="8548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A novel online action detection framework from untrimmed video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Gyu</forename><surname>Da Hye Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Whan</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bottom-up temporal action localization with mutual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peisen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="555" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
