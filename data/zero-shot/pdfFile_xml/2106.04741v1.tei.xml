<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Marginalizable Density Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dar</forename><surname>Gilboa</surname></persName>
							<email>dar_gilboa@fas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Pakman</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Vatter</surname></persName>
							<email>thibault.vatter@columbia.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Marginalizable Density Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Probability density models based on deep networks have achieved remarkable success in modeling complex high-dimensional datasets. However, unlike kernel density estimators, modern neural models do not yield marginals or conditionals in closed form, as these quantities require the evaluation of seldom tractable integrals. In this work, we present the marginalizable density model approximator (MDMA), a novel deep network architecture which provides closed form expressions for the probabilities, marginals and conditionals of any subset of the variables. The MDMA learns deep scalar representations for each individual variable and combines them via learned hierarchical tensor decompositions into a tractable yet expressive CDF, from which marginals and conditional densities are easily obtained. We illustrate the advantage of exact marginalizability in several tasks that are out of reach of previous deep network-based density estimation models, such as estimating mutual information between arbitrary subsets of variables, inferring causality by testing for conditional independence, and inference with missing data without the need for data imputation, outperforming state-of-the-art models on these tasks. The model also allows for parallelized sampling with only a logarithmic dependence of the time complexity on the number of variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Estimating the joint probability density of a set of random variables is a fundamental task in statistics and machine learning that has witnessed much progress in recent years. While traditional estimators such as histograms [1, 2] and kernel-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> have appealing theoretical properties and typically perform well in low dimensions, they become computationally impractical above 5-10 dimensions. Conversely, recent density models based on neural networks [5-10] scale efficiently with the number of random variables, but lack a crucial feature available to traditional methods: the ability to compute probabilities, marginalize over subsets of variables, and evaluate conditionals. These tasks require integrals of the estimated density, which are intractable for modern neural density models. Thus, while such operations are central to many applications (e.g., inference with missing data, testing for conditional (in)dependence, or performing do-calculus [11]), approaches based on neural networks require estimating separate models whenever marginals or conditionals are needed.</p><p>Alternatively, one could model the cumulative distribution function (CDF), making computing probabilities and marginalization straightforward. But evaluating the density requires taking d derivatives of the CDF, which incurs an exponential cost in d for a generic computational graph. This observation has made direct CDF modeling traditionally challenging <ref type="bibr" target="#b11">[12]</ref>.</p><p>In this work, we present the marginalizable density model approximator (MDMA), a novel deep network architecture preserving most of the expressive power of neural models for density estimation, Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>while providing closed form expressions for the probabilities, marginals and conditionals of any subset of the variables. In a nutshell, the MDMA learns many deep scalar representations for each individual variable and combines them using hierarchical tensor decompositions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> into a tractable multivariate CDF that can be fitted using stochastic gradient descent. Additionally, sampling from MDMA can be parallelized along the input dimension, resulting in a very low space complexity and a time complexity that scales only logarithmically with the number of variables in the problem (as opposed to linearly in naive autoregressive sampling, see below in Section 2).</p><p>As could be expected, the architectural choices that allow for easy marginalization take a minor toll in terms of performance. Indeed, while competitive, our model admittedly does not beat state-of-the-art models in out-of-sample log-likelihood of high-dimensional datasets. On the other hand, it does beat those same models in a task for which the latter are ill-prepared: learning densities from data containing missing values, a common setting in some application areas such as genomics <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. While our model is able to deal optimally with missing values by evaluating, for every data point, the marginal likelihood over its non-missing values, other models must resort to data imputation. Consequently, we significantly outperform state-of-the-art neural density estimators trained using a number of common data imputation strategies. We also show MDMA can be used to test for conditional independence, which is useful for discovering the causal structure in graphs, a task on which it outperforms existing methods, and that it enables estimation of mutual information between arbitrary subsets of variables after fitting a single model. Additionally, we prove that the model class is a universal approximator over the space of absolutely continuous multivariate distributions.</p><p>The structure of this paper is as follows. In Section 2 we review related works. In Section 3 we present our new model and its theoretical properties. We present our experimental results in Section 4, and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Modern approaches to non-parametric density estimation, based on normalizing flows <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> (see <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> for recent reviews), model expressive yet invertible functions that transforms a simple (usually uniform or Gaussian) density to the target density. Nonetheless, as previously mentioned, such architectures lead to intractable derived quantities such as probabilities, marginals and/or conditional distributions.</p><p>Moreover, many normalizing flow models rely on an autoregressive construction, which makes the cost of generating samples scale linearly with the dimension. This can be circumvented using inverse autoregressive flows <ref type="bibr" target="#b18">[19]</ref>, but in this dual case a linear cost is incurred instead in density evaluations and hence in training. Another solution is training a feed-forward network using the outputs of a trained autoregressive model <ref type="bibr" target="#b19">[20]</ref>. With MDMA, fast inference and sampling is achieved without requiring this distillation procedure.</p><p>Tensor decompositions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, which are exploited in this work, have been used in various applications of signal processing, machine learning, computer vision, and more <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. Recently, such decompositions have been used to speed-up or reduce the number of parameters in existing deep architectures <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. In addition to their practical appeal, tensor methods have been widely studied to understand the success of deep neural networks <ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> 3 Marginalizable Density Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>In the following, we use a capital and lowercase Roman letter (e.g., F and f ) or Greek letters along with dot above (e.g., ? and?) to denote respectively absolutely continuous CDFs of arbitrary dimensions and the corresponding densities. When dealing with multivariate distributions, the marginal or conditional distribution over a subset of variables will be indicated by the argument names (i.e., F (x 1 |x 2 , x 3 ) = P [X 1 ? x 1 |X 2 = x 2 , X 3 = x 3 ]).</p><p>For a positive integer p ? N \ 0, let [p] = {1, . . . , p}. Denote the space of absolutely continuous univariate and d-dimensional CDFs respectively by F 1 and F d . For any F ? F 1 , the density f : R ? R + is f (x) = ?F (x)/?x. Similarly, for any F ? F d , the density f (x) : R d ? R + is f (x) = ? d F (x)/?x 1 ? ? ? ?x d , and F j (x) = lim z?? F (z, . . . , x, . . . , z) ? F 1 for j ? [d] is the jth marginal distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The bivariate case</head><p>For the task of modeling joint distributions of two variables supported on R 2 , consider a family of univariate CDFs {? i,j } i?[m], j? <ref type="bibr" target="#b1">[2]</ref> with ? i,j ? F 1 , i.e., the functions ? i,j :</p><formula xml:id="formula_0">R ? [0, 1] satisfy lim x??? ? i,j (x) = 0 , lim x?? ? i,j (x) = 1 ,? i,j (x) = ??(x)/?x ? 0 .</formula><p>These functions are our basic building block, and we model them using a simple neural architecture proposed in <ref type="bibr" target="#b36">[37]</ref> and described in Section 3.6. If A is an m ? m matrix of nonnegative elements satisfying m i,j=1 A i,j = 1, we can combine it with the univariate CDFs to obtain</p><formula xml:id="formula_1">F (x 1 , x 2 ) = m i,j=1 A i,j ? i,1 (x 1 )? j,2 (x 2 ).<label>(1)</label></formula><p>The coefficients A i,j encode the dependencies between the two variables, and the normalization ensures that F is a valid CDF, that is F ? F 2 . Even though in each summand the interaction is modeled by a single scalar parameter, such a model can be used to approximate well complex interactions between x 1 and x 2 if m is sufficiently large, as we show in Section 3.6. The advantage of this construction is that {? i,j } i?[m], j? <ref type="bibr" target="#b1">[2]</ref> , the family of densities corresponding to the univariate CDFs, leads immediately to</p><formula xml:id="formula_2">f (x 1 , x 2 ) = m i,j=1 A i,j?i,1 (x 1 )? j,2 (x 2 ).</formula><p>It is similarly straightforward to obtain marginal and conditional quantities, e.g.:</p><formula xml:id="formula_3">F (x 1 ) = m i,j=1 A i,j ? i,1 (x 1 ), F (x 1 |x 2 ) = m i,j=1 A i,j ? i,1 (x 1 )? j,2 (x 2 ) m i,j=1 A i,j?j,2 (x 2 )</formula><p>, and the corresponding densities result from replacing ? i,1 by? i,1 . Deriving these simple expressions relies on the fact that (1) combines the univariate CDFs linearly. Nonetheless, it is clear that, with m ? ? and for judiciously chosen univariate CDFs, such a model is a universal approximator of both CDFs and sufficiently smooth densities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The multivariate case</head><p>To generalize the bivariate case, consider a collection of univariate CDFs {? i,j } i?[m], j? <ref type="bibr">[d]</ref> with ? i,j ? F 1 for each i and j, and define the tensor-valued function ? :</p><formula xml:id="formula_4">R d ? [m] d ? [0, 1] by ?(x) i1,...,i d = d j=1 ? ij ,j (x j ) for x ? R d .</formula><p>Furthermore, denote the class of normalized order d tensors with m dimensions in each mode and nonnegative elements by </p><formula xml:id="formula_5">A d,m = {A ? R m?????m : A i1,...,i d ? 0, m i1,...,i d =1 A i1,...,i d = 1}.<label>(2)</label></formula><formula xml:id="formula_6">F A,? (x) = A, ?(x) = m i1,...,i d =1 A i1,...,i d d j=1 ? ij ,j (x j ).<label>(3)</label></formula><p>If is clear that the conditions on ? and A imply that F A,? ? F d . As in the bivariate case, densities or marginalization over x j are obtained by replacing each ? i,j respectively by? i,j or 1. As for conditioning, considering any disjoint subsets R = {k 1 , . . . , k r } and S = {j 1 , . . . , j s } of [d] such that R ? S = ?, we have</p><formula xml:id="formula_7">F A,? (x k1 , . . . , x kr |x j1 , . . . , x js ) = m i1,...,i d =1 A i1,...,i d k?R ? i k ,k (x k ) j?S? ij ,j (x j ) m i1,...,i d =1 A i1,...,i d j?S? ij ,j (x j )</formula><p>.</p><p>For a completely general tensor A with m d parameters, the expression (3) is computationally impractical, hence some structure must be imposed on A. For instance, one simple choice is</p><formula xml:id="formula_8">A i1,...,i d = a i1 ? i1,...,i d , which leads to F A,? (x) = m i=1 a i d j=1 ? i,j (x j ), with a i ? 0 and m i=1 a i = 1.</formula><p>Instead of this top-down approach, A can be tractably constructed bottom-up, as we explain next.</p><formula xml:id="formula_9">Assuming d = 2 p for integer p, define ? : R d ? R m?2 p? +1</formula><p>for ? {1, . . . , p} recursively by</p><formula xml:id="formula_10">? i,j (x) = ? ? ? ? i,j (x j ), = 1 m k=1 ? ?1 i,k,j ? ?1 k,2j?1 (x)? ?1 k,2j (x), = 2, . . . , p<label>(4)</label></formula><formula xml:id="formula_11">for i ? [m], j ? [2 p? +1 ], and where ? is a non-negative m ? m ? 2 p? +1 tensor, normalized as m k=1 ? i,k,j = 1.</formula><p>The joint CDF can then be written as</p><formula xml:id="formula_12">F A HT ,? (x) = m k=1 ? p k ? p k,1 (x)? p k,2 (x),<label>(5)</label></formula><p>with</p><formula xml:id="formula_13">? p ? R m + satisfying m k=1 ? p k = 1.</formula><p>It is easy to verify that the underlying A HT satisfies A HT ? A d,m defined in <ref type="bibr" target="#b1">(2)</ref>. A graphical representation of this tensor is provided in <ref type="figure" target="#fig_0">Figure 10</ref> in the supplementary materials.</p><p>For example, for d = 4, we first combine (x 1 , x 2 ) and (x 3 , x 4 ) into</p><formula xml:id="formula_14">? 2 i,1 (x) = m k=1 ? 1 i,k,1 ? k,1 (x 1 )? k,2 (x 2 ) , ? 2 i,2 (x) = m k=1 ? 1 i,k,2 ? k,3 (x 3 )? k,4 (x 4 ) ,</formula><p>and then merge them as</p><formula xml:id="formula_15">F A HT ,? (x) = m k=1 ? 2 k ? 2 k,1 (x)? 2 k,2 (x),<label>(6)</label></formula><p>from which we can read off that</p><formula xml:id="formula_16">A HT i1,i2,i3,i4 = m k=1 ? 2 k ? 1 k,i1,1 ? 1 k,i3,2 ? i1,i2 ? i3,i4</formula><p>. Note that the number of parameters required to represent A HT is only poly(m, d). The construction is easily generalized to dimensions d not divisible by 2. Also, the number of ? factors combined at each iteration in (4) (called the pool size), can be any positive integer. This construction is a variant of the hierarchical Tucker decomposition of tensors <ref type="bibr" target="#b37">[38]</ref>, which has been used in the construction of tensor networks for image classification in <ref type="bibr" target="#b31">[32]</ref>.</p><p>Given a set of training points {x} N i=1 , we fit MDMA models by maximizing the log of the density with respect to both the parameters in {? i,j } i?[m],j? <ref type="bibr">[d]</ref> and the components of A. We present additional details regarding the choice of architectures and initialization in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">A non-marginalizable MDMA</head><p>We can construct a more expressive variant of MDMA at the price of losing the ability to marginalize and condition on arbitrary subsets of variables. We find that the resulting model leads to state-ofthe-art performance on a density estimation benchmark. We define v = x + T?(x) where T is an upper-triangular matrix with non-negative entries and 0 on the main diagonal. Note that ?v ?x = 1.</p><formula xml:id="formula_17">Given some density f (v 1 , . . . , v d ) = d j=1? j (v j ), we have f (x 1 , . . . , x d ) = ?v ?x f (v 1 , . . . , v d ) = f (v 1 (x), . . . , v d (x)).<label>(7)</label></formula><p>We refer to this model nMDMA, since it no longer enables efficient marginalization and conditioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">MDMA sampling</head><p>Given an MDMA as in <ref type="formula" target="#formula_6">(3)</ref>, we can sample in the same manner as for autoregressive models: from u 1 , . . . , u d independent U (0, 1) variables, we obtain a sample from F A,? by computing</p><formula xml:id="formula_18">x 1 = F ?1 A,? (u 1 ), x 2 = F ?1 A,? (u 2 |x 1 ) ? ? ? x d = F ?1 A,? (u d |x 1 , . . . , x d?1 )</formula><p>, where, unlike with autoregressive model, the order of taking the conditionals does not need to be fixed. The main drawback of this method is that due to the sequential nature of the sampling the computational cost is linear in d and cannot be parallelized.</p><p>However, the structure of F A,? can be leveraged to sample far more efficiently. Define by R A a vector-valued categorical random variable taking values in [m] ? ? ? ? ? [m], with distribution P [R A = (i 1 , . . . , i d )] = A i1,...,i d . The fact that A ? A d,m with A d,m from (2) ensure the validity of this definition. Consider a vector (X 1 , . . . ,X d , R A ) where R A is distributed as above, and</p><formula xml:id="formula_19">P X 1 ? x 1 , . . . ,X d ? x d |R A = r = d i=1 ? ri,i (x i ),</formula><p>for the collection {? i,j } of univariate CDFs. Denoting the distribution of this vector byF A,? , marginalizing over R A gives</p><formula xml:id="formula_20">F A,? (x) = r?[m] d P [R A = r] d j=1 ? rj ,j (x j ) = F A,? (x).</formula><p>Instead of sampling directly from the distribution F A,? , we can thus sample fromF A,? and discard the sample of R A . To do this, we first sample from the categorical variable R A . Denoting this sample by r, we can sample fromX i by inverting the univariate CDF ? ri,i . This can be parallelized over i.</p><p>The approach outlined above is impractical since R A can take m d possible values, yet if A can be expressed by an efficient tensor representation this exponential dependence can be avoided. Consider the HT decomposition <ref type="bibr" target="#b4">(5)</ref>, which can be written as Note that the time complexity of this sampling procedure depends only logarithmically on d. The reason is that a simple hierarchical structure of Algorithm 1, where Mult denotes the multinomial distribution. Algorithm 1: Sampling from the HT MDMA Result:</p><formula xml:id="formula_21">F A HT ,? (x) = A HT , ?(x) = p =1 2 p? j =1 m k ,j =1 ? k +1, j /2 ,k ,j ,j ?(x) k1,1,...,k 1,d/2 ,<label>(8)</label></formula><formula xml:id="formula_22">x j for j = 1, . . . , d k p,1 ? Mult(? p * ); for ? p ? 1 to 1 by ?1 do k ,j ? Mult(? k +1, j/2 , * ,j ), j = 1, . . . , 2 p? ; end x j ? ? k 1, j/2 ,j , j = 1, . . . , 2 p ;</formula><p>The logarithmic dependence is only in the sampling from the categorical variables, which is inexpensive to begin with. We thus avoid the linear dependence of the time complexity on d that is common in sampling from autoregressive models. Furthermore, the additional memory required for sampling scales like log m (since storing the categorical samples requires representing integers up to size m), and aside from this each sample requires evaluating a single product of univariate CDFs (which is independent of m). In preliminary experiments, we have found that even for densities with d ? 10, this sampling scheme is faster by 1.5 to 2 orders of magnitude than autoregressive sampling. The relative speedup should only increase with d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Universality of the MDMA</head><p>To model functions in F 1 , we use ? l,r,? , a class of constrained feedforward neural networks proposed in <ref type="bibr" target="#b36">[37]</ref> with l hidden layers, each with r neurons, and ? a nonaffine, increasing and continuously differentiable elementwise activation function, defined as</p><formula xml:id="formula_23">? l,r,? = {? : R ? [0, 1], ?(x) = sigmoid ? L l ? ? ? L l?1 ? ? ? ? ? ? ? ? L 1 ? ? ? L 0 (x)}, where L i : R ni ? R n i+l is the affine map L i (x) = W i x + b i for an n i+1 ? n i weight matrix W i</formula><p>with nonnegative elements and an n i+1 ? 1 bias vector b i , with n l+1 = n 0 = 1 and n i = r for i ? [l]. The constraints on the weights and the final sigmoid guarantee that ? l,r,? ? F 1 , and for any ? ? ? l,r,? , the corresponding density?(x) = ??(x)/?x can be obtained with the chain rule. The universal approximation property of the class ? l,r,? is expressed in the following proposition. Proposition 2. ? l,r ? l,r,? is dense in F 1 with respect to the uniform norm.</p><p>While the proof in the supplementary assumes that lim x??? ?(x) = 0 and lim x?? ?(x) = 1, it can be easily modified to cover other activations. For instance, in our experiments, we use ?(x) = x + a tanh(x) following <ref type="bibr" target="#b36">[37]</ref>, and refer to the supplementary material for more details regarding this case. In the multivariate case, consider the class of order d tensored-valued functions with m dimensions per mode defined as</p><formula xml:id="formula_24">? m,d,l,r,? = {? : R d ? [m] d ? [0, 1], ?(x) i1,...,i d = d j=1 ? ij ,j (x j ), ? i,j ? ? l,r,? }.</formula><p>Combining ? m,d,l,r,? with the A d,m , the normalized tensors introduced in Section 3.3, the class of neural network-based MDMAs can then be expressed as</p><formula xml:id="formula_25">MDMA m,d,l,r,? = {F A,? : R d ? [0, 1], F A,? (x) = A, ?(x) , A ? A d,m , ? ? ? m,d,l,r,? }. Proposition 3. The set ? m,l,r MDMA m,d,l,r,? is dense in F d with respect to the uniform norm.</formula><p>The proof relies on the fact that setting m = 1 yields a class that is dense in the space of d-dimensional CDFs with independent components. All proofs are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Additional experimental details for all experiments are provided in Appendix C. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Toy density estimation</head><p>We start by considering 3D augmentations of three popular 2D toy probability distributions introduced in [8]: two spirals, a ring of 8 Gaussians and a checkerboard pattern. These distributions allow to explore the ability of density models to capture challenging multimodalities and discontinuities <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>. The results, presented in <ref type="figure" target="#fig_0">Figure 1</ref>, show that MDMA captures all marginal densities with high accuracy, and samples from the learned model appear indistinguishable from the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mutual information estimation</head><p>Given a multivariate probability distribution over some variables X = (X 1 . . . , X d ), estimating the mutual information</p><formula xml:id="formula_26">I(Y ; Z) = dp X (x) log p X (x) p Y (y)p Z (z) ,<label>(9)</label></formula><p>where Y, Z are random vectors defined by disjoint subsets of the X i , requires evaluating p Y , p Z which are marginal densities of X. Typically, Y and Z must be fixed in advance, yet in some cases it is beneficial to be able to flexibly compute mutual information between any two subsets of variables. Estimating both I(Y, Z) and I(Y , Z ) may be highly inefficient, e.g. if Y and Y are highly overlapping subsets of X. Using MDMA however, we can fit a single model for the joint distribution and easily estimate the mutual information between any subset of variables by simply marginalizing over the remaining variables to obtain the required marginal densities. Thus a Monte Carlo estimate of (9) can be obtained by evaluating the marginal densities at the points that make up the training set. <ref type="figure">Figure 2</ref> presents an example of this method, showing the accuracy of the estimates.</p><p>x 1</p><formula xml:id="formula_27">x 2 x 1 x 3 x 1 x 2 x 3 x 1 x 2 x 3</formula><p>Samples from model</p><formula xml:id="formula_28">x 1 x 2 f (x 1 , x 2 ) x 1 x 3 f (x 1 , x 3 ) x 1 x 2 f (x 1 , x 2 |x 3 = 0) x 1 x 2 f (x 1 , x 2 |x 3 = 0.5) x 1 x 2 x 3</formula><p>Training data</p><formula xml:id="formula_29">x 1 x 2 f (x 1 , x 2 ) x1 x 2 x3</formula><p>Training data</p><formula xml:id="formula_30">x 1 x 2 f (x 1 , x 2 )</formula><p>Training data  </p><formula xml:id="formula_31">I((X 1 , ..., X k ); (X k+1 , ..., X d ))</formula><p>Ground Truth MDMA <ref type="figure">Figure 2</ref>: Mutual information estimation between subsets of a random vector. We fitted a single MDMA model to samples from a zero-mean d = 16 Gaussian, with covariance</p><formula xml:id="formula_32">? ij = ? ij + (1 ? ? ij )(i + j ? 2)/(5d). Monte</formula><p>Carlo estimates of the mutual information <ref type="formula" target="#formula_26">(9)</ref> between (X 1 , . . . , X k ) and (X k+1 , . . . , X d ) for any k = 1, . . . , d ? 1 are easily obtained and match closely the exact values. For each k we average over 5 repetitions of drawing the dataset and fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Density estimation with missing values</head><p>Dealing with missing values in multivariate data is a classical challenge in statistics that has been studied for decades <ref type="bibr" target="#b38">[39]</ref>. The standard solution is the application of a data imputation procedure (i.e., "filling in the blanks"), which requires making structural assumptions. In some cases, this is natural, as for the matrix completion problem under a low-rank assumption <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>, where the imputed values are the main object of interest. But the artifacts introduced by data imputation <ref type="bibr" target="#b41">[42]</ref> are generally a price that one must unwillingly pay in order to perform statistical inference in models that  <ref type="figure">Figure 3</ref>: Density estimation with missing data Test NLL on two density estimation benchmarks, varying the proportion of entries in each data matrix that are designated missing and not used for fitting. We compare MDMA which can fit marginal densities directly with BNAF which achieves state-of-the-art results on the POWER dataset, after performing data imputation using MICE. As the proportion of missing data increases, MDMA outperforms BNAF.</p><p>require fully-observed data points. Two popular, generic techniques for imputation are MICE <ref type="bibr" target="#b42">[43]</ref> and k-NN imputation <ref type="bibr" target="#b43">[44]</ref>. The former imputes missing values by iteratively regressing each missing variable against the remaining variables, while the latter uses averages over the non-missing values at k-nearest datapoints.</p><p>More formally, let X ? R d be distributed according to some density p with parameters ?, let X (0) , X (1) be the non-missing and missing entries of X respectively, and M ? {0, 1} d a vector indicating the missing entries. In the missing-at-random setting (i.e. M is independent of X (1) ), likelihood-based inference using the full likelihood of the model is equivalent to inference using the marginal likelihood <ref type="bibr" target="#b38">[39]</ref> L(X (0) |?) = p(X|?)dX <ref type="bibr" target="#b0">(1)</ref> . Standard neural network-based density estimators must resort to data imputation because of the impossibility of computing this marginal likelihood. MDMA however can directly maximize the marginal likelihood for any pattern of missing data at the same (actually slightly cheaper) computational cost as maximizing the full likelihood, without introducing any bias or variance due to imputation.</p><p>As a demonstration of this capability, we consider the UCI POWER and GAS datasets, following the same pre-processing as <ref type="bibr" target="#b44">[45]</ref>. We construct a dataset with missing values by setting each entry in the dataset to be missing independently with a fixed probability . We compare MDMA to BNAF <ref type="bibr" target="#b8">[9]</ref>, a neural density model which achieves state-of-the-art results on a number of density estimation benchmarks including GAS. We train MDMA directly on the log marginal likelihood of the missing data, and BNAF by first performing data imputation using MICE <ref type="bibr" target="#b42">[43]</ref> and then training using the full log likelihood with the imputed data. The validation loss is the log marginal likelihood for MDMA and the log likelihood of the imputed validation set for BNAF. The test set is left unchanged for both models and does not contain any missing values. We train BNAF using the settings specified in <ref type="bibr" target="#b8">[9]</ref> that led to the best performance (2 layers and 40d hidden units where d is the dimensionality of the dataset). The results are shown in <ref type="figure">Figure 3</ref>. We find that, as the probability of missingness increases, MDMA significantly outperforms BNAF on both datasets. Note that, while the proportion of missing values might seem extreme, it is not uncommon in some applications (e.g., proteomics data). We also trained BNAF using k-NN imputation <ref type="bibr" target="#b43">[44]</ref>, finding that performance was worse than MICE imputation for all values of ?. A comparison of the two methods is provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Conditional independence testing and causal discovery</head><p>Randomized control trials <ref type="bibr" target="#b45">[46]</ref> remain the golden standard for causal discovery. Nonetheless, experiments or interventions are seldom doable, e.g. due to financial or ethical considerations. Alternatively, observational data can help uncovering causal relationships <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. In this context, a class of popular methods targeted at recovering the full causal graph, like PC or FCI <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref>, rely on conditional independence (CI) tests. Letting X, Y and Z be random variables, the CI of X and Y given Z, denoted X ? ? Y | Z, means that given Z, no information about X (or Y ) can be gained by knowing the value of Y (or X). And testing H 0 : X ? ? Y | Z against H 1 : X ? ? Y | Z is a problem tackled in econometrics <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>, statistics <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, and machine learning <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Following <ref type="bibr" target="#b54">[55]</ref>, denote U 1 = F (X | Z) and U 2 = F (Y | Z). It is clear that H 0 implies U 1 ? ? U 2 , although the converse does not hold [see e.g., 56]. Nonetheless, U 1 ? ? U 2 implies H 1 , so a test based <ref type="table">Table 1</ref>: MDMA for causal discovery. Conditional densities from a trained MDMA model can be used for causal discovery by allowing to test for conditional independence between variables. Both on synthetic DAG data and real data from a protein signaling network, MDMA infers the graph structure more accurately than a competing method based on quantile regression <ref type="bibr" target="#b54">[55]</ref>. The metrics are the structural Hamming distance for the directed (SHD(D)) and undirected (SHD) graph.</p><p>Model  <ref type="bibr" target="#b54">[55]</ref> is based on estimating the conditional CDFs through quantile regression, we proceed similarly, albeit using the MDMA as a plugin for the conditional distributions. Our approach is especially appealing in the context of causal discovery, where algorithms require computing many CI tests to create the graph's skeleton. Instead of having to regress for every test, MDMA estimates the full joint distribution, and its lower dimensional conditionals are then used for the CI tests.</p><p>In <ref type="table">Table 1</ref>, we present results on inferring the structure of causal graphs using the PC algorithm <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref>. As a benchmark, we use the vanilla (i.e., Gaussian) CI test, and compare it to the PC algorithm obtained with the CI test from <ref type="bibr" target="#b54">[55]</ref>, albeit using the MDMA for the conditional distributions. Synthetic random directed acyclic graphs (DAGs) along with sigmoidal or polynomial mechanisms linking parents to children are sampled using <ref type="bibr" target="#b60">[61]</ref>. Each dataset is d = 10 dimensional and contains 20,000 observations. We also compare the two algorithms on data from a protein signaling network with d = 11 <ref type="bibr" target="#b61">[62]</ref> for which the ground truth causality graph is known. Performance is assessed based on the structural Hamming distance (SHD) <ref type="bibr" target="#b62">[63]</ref>, that is the L 1 norm of the difference between learned adjacency matrices and the truth, as well as a variant of this metric for directed graphs SHD(D) which also accounts for the direction of the edges. <ref type="table">Table 1</ref> shows averages over 8 runs for each setting. In all cases, MDMA outperforms the vanilla PC in terms of both metrics. For the synthetic data, we note the large standard deviations, due in part to the fact that we sample randomly from the space of DAG structures, which has cardinality super-exponential in d. An example of the inferred graphs is presented in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Density estimation on real data</head><p>We trained MDMA/nMDMA and the non-marginalizable variant described in Section 3.4 on a number of standard density estimation benchmarks from the UCI repository, 2 following the pre-processing described in <ref type="bibr" target="#b44">[45]</ref>. <ref type="table" target="#tab_4">Table 2</ref> compares test log likelihoods of MDMA/nMDMA with several other neural density models. We find the performance of MDMA on the lower-dimensional datasets comparable to state-of-the-art models, while for higher-dimensional datasets it appears to overfit. nMDMA achieves state-of-the-art performance on the POWER (d = 6) dataset, but at the cost of losing the ability to marginalize or condition over subsets of the variables. The width of MDMA was chosen based on a grid search over {500, 1000, 2000, 3000, 4000} for each dataset, and the marginal CDF parameters by a search over {(l = 2, w = 3) , (l = 4, w = 5)}. All models were trained using ADAM with learning rate 0.01, and results for MDMA and nMDMA are averaged over 3 runs. Additional experimental details are provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>MDMAs offer the ability to obtain, from a single model, closed form probabilities, marginals and conditionals for any subset of the variables. These properties enable one to straightforwardly use the model to solve a diverse array of problems, of which we have demonstrated only a few: mutual information estimation between arbitrary subsets of variables, inference with missing values, and conditional independence testing targeted at multivariate causal discovery. In addition to these, MDMA's marginalization property can be used for anomaly detection with missing values <ref type="bibr" target="#b63">[64]</ref>. We have shown that MDMA can fit data with missing values without requiring imputation, yet if one is interested in data imputation for downstream tasks, the ability to sample from arbitrary conditional distributions means that MDMA can be used for imputation as well.</p><p>Additionally, in some application areas (e.g., financial risk management), powerful models exist for the univariate distributions, and marginal distributions are then glued together using copulas <ref type="bibr" target="#b64">[65]</ref>. However, popular copula estimators suffer from the same drawbacks as modern neural network density estimators with regard to marginalization and conditioning. Using MDMA for copula estimation (say by replacing the kernel density estimator by MDMA in the formulation of <ref type="bibr" target="#b65">[66]</ref>), one can then obtain copula estimators that do not suffer from these deficiencies.</p><p>The main shortcoming of MDMA is the linearity in the combination of the products of univariate CDFs which appears to limit the expressivity of the model. The study of tensor decompositions is an active area of research, and novel constructions, ideally adapted specifically for this task, could lead to improvements in this regard despite the linear structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material A Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Proposition 1</head><p>The proof follows directly from Algorithm 1. The distribution F A HT ,? ) = A HT , ? is a mixture model, and thus in order to sample from it we can first draw a single mixture component (which is a product of univariate CDFs) and then sample from this single component. The mixture weights are the elements of the tensor A HT given by the diagonal HT decomposition <ref type="bibr" target="#b7">(8)</ref>. In the next section, we add details on the sampling process for the sake of clarity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Details on the sampling for the HT model</head><formula xml:id="formula_33">= R p 1,1 , define the event p =1 ? 2 p? j =1 R k +1, j /2 ,j = k ,j = R p 1,1 = k p,1 ? 2 j=1 R p?1 kp,1,j = k p?1,j . . . ? d/2 j=1 R 1 k 2, j/2 ,j = k 1,j .</formula><p>Let (X 1 , . . . ,X d , R) be a random vector such that</p><formula xml:id="formula_34">P X 1 ? x 1 , . . . ,X d ? x d p =1 ? 2 p? j =1 R k +1, j /2 ,j = k ,j = d i=1 ? k 1, i/2 ,i (x i ),<label>(10)</label></formula><p>which implies that the distribution of (X 1 , . . . ,X d ) obtained after conditioning on a subset of the {R i,j } in this way is equal to a single mixture component in F HT = A, ? . Thus, based on a sample of R, one can sampleX i by inverting the univariate CDFs ? k 1, i/2 ,i numerically and parallelizing over i. Numerical inversion is trivial since the functions are increasing and continuously differentiable, and this can be done for instance using the bisection method. It remains to sample a mixture component.</p><p>Assume that a sample {R i,j } for a sequence of variables as in <ref type="formula" target="#formula_1">(10)</ref> is obtained e.g. from Algorithm 1.</p><p>With the convention ? p kp+1,1,k,1 = ? p k , since dependence is only in sampling from the categorical variables which is computationally cheap. This not only avoids the linear time complexity common in sampling from autoregressive models (without using distillation), but the space complexity is also essentially independent of m since only a single mixture component is evaluated per sample.</p><formula xml:id="formula_35">P p =1 ? 2 p? j =1 R k +1, j /2 ,j = k ,j = p =1 2 p? j =1 ? k +1, j /2 ,k ,j ,j ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Proposition 2</head><p>Assume that the activation function ? is increasing, continuously differentiable, and such that lim x??? ?(x) = 0 and lim x?? ?(x) = 1. Proposition 2 then follows immediately from Proposition 4 and the fact that ? r ? 1,r,? ? ? l,r ? l,r,? . Remark 1. In practice, we use the activation ?(x) = x + a tanh(x) for some a &gt; ?1. While it does not satisfy the assumptions, the arguments in the proof of Proposition 5 can be modified in a straightforward manner to cover this activation (see <ref type="bibr">Remark 2)</ref>. Proposition 4. ? r ? 1,r,? is dense in F 1 with respect to the uniform norm.</p><p>Letting</p><formula xml:id="formula_36">F 1 = { F : R ? R, F (x) = log F (x)/(1 ? F (x)), F ? F 1 }, ? l,r,? = { ? : R ? R, ?(x) = log ?(x)/(1 ? ?(x)), ? ? ? l,r,? },</formula><p>the proof of Proposition 4 relies on the following proposition.</p><p>Proposition 5. ? r ? 1,r,? is dense in F 1 with respect to the uniform norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Proof of Proposition 4</head><p>This proof is similar to that of <ref type="bibr" target="#b66">[67,</ref><ref type="bibr">Theorem 2]</ref>, which deals with functions with positive outputs. We want to show that, for any F ? F 1 , compact K ? R, and &gt; 0, there exists ? ? ? r ? 1,r,? such that  <ref type="figure">F (x)</ref>), so that F = ? ? F . By Proposition 5, there exists ? ? ? r ? 1,r,? such that</p><formula xml:id="formula_37">? ? F ?,K = sup x?K |?(x) ? F (x)| ? .</formula><formula xml:id="formula_38">sup x?K | ?(x) ? F (x)| ? 4 .</formula><p>Thus, letting ? = ? ? ?, we have</p><formula xml:id="formula_39">|?(x) ? F (x)| = |? ? ?(x) ? ? ? F (x)| ? sup x?K ?(x) | ?(x) ? F (x)| ? .</formula><p>Since ? 1,r,? = {? ? ? : ? ? ? 1,r,? }, the result follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Proof of Proposition 5</head><p>This proof is similar to that of [68, Theorem 3.1], which is incomplete and only deals with the sigmoid activation. First, note that F 1 is the space of strictly increasing and continuously differentiable functions. Therefore, for any F ? F 1 and interval K = [K 1 , K 2 ], we can write, for any x ? K,</p><formula xml:id="formula_40">F (x) = F (K 1 ) + F (K2) F (K1) 1 F (x)?u du = F (K 1 ) + F (K2) F (K1) 1 x? F ?1 (u) du,</formula><p>where the existence of the inverse F ?1 is guaranteed by the fact that F is strictly increasing and continuous. Thus, for F (</p><formula xml:id="formula_41">K 1 ) = u 0 &lt; u 1 &lt; ? ? ? &lt; u k = F (K 2 ) a partition of [ F (K 1 ), F (K 2 )] with u j+1 ? u j ? /2( F (K 2 ) ? F (K 1 )), x j = F ?1 (u j ) and G(x) = F (K 1 ) + k j=1 1 x?xj (u j ? u j?1 ),</formula><p>we have | F (x) ? G(x)| ? /2, namely the approximation error of the Riemann sum for increasing functions. Let a &gt; 0 and ? ? ? 1,k,? obtained by setting b 1 = F (K 1 ), as well as (W 1 ) 1,j = (u j ? u j?1 )/a &gt; 0, (b 0 ) j = ?ax j and (W 0 ) j,1 = a for 1 ? j ? k, then</p><formula xml:id="formula_42">|G(x) ? ?(x)| ? k j=1 (u j+1 ? u j ) 1 x?xj ? ?(a(x ? x j )) .</formula><p>By the assumptions on ?, it is clear that |1 x?0 ? ?(ax)| can be made arbitrarily small. Thus, taking a large enough so that |G(x) ? ?(x)| ? /2, we have</p><formula xml:id="formula_43">| F (x) ? ?(x)| ? | F (x) ? G(x)| + |G(x) ? ?(x)| ? .</formula><p>Remark 2. Let ?(x) = x + a tanh(x) for some a &gt; ?1 and ? ? ? 1,k,? obtained by setting b 1 = F (K 1 ) + 1/2, as well as</p><formula xml:id="formula_44">(W 1 ) 1,j = (u j ? u j?1 )/2a &gt; 0, (b 0 ) j = ?ax j and (W 0 ) j,1 = |a| for 1 ? j ? k, then |G(x) ? ?(x)| ? k j=1 (u j+1 ? u j ) 1 x?xj ? tanh(|a|(x ? x j ))/2 ? 1/2 + k j=1 (u j+1 ? u j )|x ? x j |/2|a|.</formula><p>Because a is arbitrary, one can take it large enough so that |G(x) ? ?(x)| ? /2 as above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Proposition 3</head><p>Consider the classes of order d tensored-valued functions with m dimensions per mode defined as</p><formula xml:id="formula_45">? m,d,l,r,? = {? : R d ? [m] d ? [0, 1], ?(x) i1,...,i d = d j=1 ? ij ,j (x j ), ? i,j ? ? l,r,? }, F m,d = {? : R d ? [m] d ? [0, 1], ?(x) i1,...,i d = d j=1 F ij ,j (x j ), F i,j ? F 1 }</formula><p>as well as the class of neural network-based and F 1 -based MDMAs, that is MDMA m,d,l,r,? = {F A,? :</p><formula xml:id="formula_46">R d ? [0, 1], F A,? (x) = A, ?(x) , A ? A d,m , ? ? ? m,d,l,r,? }, MDMA m,d,F1 = {F A,? : R d ? [0, 1], F A,? (x) = A, ?(x) , A ? A d,m , ? ? F m,d }.</formula><p>We can now state the following proposition. Proposition 6. ? l,r MDMA m,d,l,r,? is dense in MDMA m,d,F1 with respect to the uniform norm Proposition 3 then follows immediately from the fact that ? m MDMA m,d,F1 is the space of multivariate mixture distributions admitting a density, which is dense in F d with respect to the uniform norm (see e.g., <ref type="bibr" target="#b68">[69,</ref><ref type="bibr">Theorem 33.2]</ref>, <ref type="bibr" target="#b69">[70,</ref><ref type="bibr">Theorem 5]</ref>, or [71, Corollary 11]).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Proof of Proposition 6</head><p>With A ? A d,m , ? 1 ? ? m,d,l,r,? , and &gt; 0 and a compact K = K 1 ? ? ? ? ? K d ? R d , we want to prove that there exists A 2 ? A d,m and ? 2 ? F m,d , such that sup x?K |F A,?1 (x) ? F A2,?2 (x)| ? . Assuming that we can show sup x?K |? 1 (x) i1,...,i d ? ? 2 (x) i1,...,i d | ? , the result would then follow from setting A 2 = A and the fact that</p><formula xml:id="formula_47">F A,?1 (x) ? F A,?2 (x) = A, ? 1 (x) ? ? 2 (x) implies sup x?K |F A,?1 (x) ? F A,?2 (x)| = ? i1,...,i d A i1,...,i d sup x?K |? 1 (x) i1,...,i d ? ? 2 (x) i1,...,i d | = .</formula><p>With ? = 1/d , by Proposition 2, there exists l, w, and {?} i?[m],j? <ref type="bibr">[d]</ref> with ? i,j ? ? l,r,? , such that max</p><formula xml:id="formula_48">i?[m],j?[d] sup xj ?Kj |? i,j (x j ) ? F i,j (x j )| ? ?.</formula><p>Thus, we have that</p><formula xml:id="formula_49">|? 1 (x) i1,...,i d ? ? 2 (x) i1,...,i d | = | d j=1 ? ij ,j (x j ) ? d j=1 F i,j (x j )| ? ? d = .</formula><p>B Additional experimental results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Toy density estimation</head><p>Figures 4 and 5 show more results on the popular checkerboard and 8 Gaussians toy datasets studied in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><formula xml:id="formula_50">x 1 x 2 x 3 x 1 x 2 x 1 x 3 x 2 x 3 x 1 x 2 x 3</formula><p>Samples from model</p><formula xml:id="formula_51">x 1 x 2 f (x 1 , x 2 ) x 1 x 3 f (x 1 , x 3 ) x 2 x 3 f (x 2 , x 3 ) ?5 0 5 0.00 0.05 0.10 0.15 f (x 1 ) f (x 2 ) f (x 3 )</formula><p>Training data</p><p>x 1</p><formula xml:id="formula_52">x 2 f (x 1 , x 2 |x 3 = 0.5) x 1 x 2 f (x 1 , x 2 |x 3 = 0) x 1 x 2 f (x 1 , x 2 |x 3 = ?0.5)</formula><p>Training data </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Density estimation with missing data</head><p>We compare MICE imputation <ref type="bibr" target="#b42">[43]</ref> to k-NN imputation (with k = 3 neighbours) <ref type="bibr" target="#b43">[44]</ref> on the UCI POWER dataset in <ref type="figure">Figure 6</ref>, before performing density estimation with BNAF <ref type="bibr" target="#b8">[9]</ref>. Due to the size of the dataset, we were not able to use k-NN imputation on the full dataset, but instead split it up into 100 batches and performed the imputation per batch. Similar results were obtained on the UCI GAS dataset, and for this reason we only compare MDMA to MICE imputation in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Causal discovery</head><p>In <ref type="figure">Figure 7</ref>, we present examples of completely partially directed acyclical graphs (CPDAGs) learned using the PC algorithm, using either MDMA or the vanilla (Gaussian) method for testing conditional independence used in <ref type="bibr" target="#b54">[55]</ref>. See Appendix C.2 for additional details.  <ref type="figure">Figure 6</ref>: A comparison of data imputation methods on the UCI POWER dataset followed by density estimation with BNAF, showing that MICE imputation outperforms k-NN. We subsequently use MICE in the comparison with MDMA in the main text.  <ref type="figure">Figure 7</ref>: Recovered causal graphs: Top: Synthetic data from a random DAG with sigmoidal causality mechanism. The graph inferred using MDMA PC had directional SHD of 11, compared to 15 for the Gaussian PC. Bottom: Protein signaling graph <ref type="bibr" target="#b61">[62]</ref>. The graph inferred using MDMA PC had directional SHD of 27, compared to 32 for the Gaussian PC.   <ref type="figure">Figure 9</ref>: Feature learning in MDMA. We plot ten univariate PDFs? ij parameterized as in appendix E.1 for j = 4, both at initialization and after training on the UCI POWER dataset. Overlaid is a histogram of computed from 500 datapoints. We find that the features localize near the datapoints. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Causal discovery</head><p>In many application areas, causal relationships between random variables can be represented by a directed acyclical graph (DAG). The PC algorithm <ref type="bibr" target="#b46">[47]</ref> is an efficient algorithm for recovering sparse DAGs from observations. In general, this recovery is complicated by the fact that two DAGs can induce the same probability distribution, leading to them being called Markov equivalent. Hence, observational data can only help infer the Markov equivalence class of a given DAG. The equivalence class, known as a completely partially directed acyclical graph (CPDAG, also called essential graph) <ref type="bibr" target="#b71">[72]</ref>, encodes all the dependence information in the induced distribution. The object of the PC algorithm is therefore the recovery of a CPDAG that is consistent with the data. This is generally a hard problem, since the cardinality of the space of DAGs is super-exponential in the number of variables <ref type="bibr" target="#b72">[73]</ref>.</p><p>The PC algorithm requires repeatedly testing for independence between pairs of variables conditioned on subsets of the remaining variables. As mentioned in the main text, testing for conditional independence can be reduced to an independence test between variables that depend on conditional CDFs <ref type="bibr" target="#b54">[55]</ref>, which can be obtained easily after fitting the joint density using MDMA. In our experiments, the results of using MDMA as part of the PC algorithm for testing conditional independence are compared to the results obtained by using a Gaussian conditional independence test based on partial correlations.</p><p>The synthetic DAGs were generated using the the Causal Discovery Toolbox. <ref type="bibr" target="#b2">3</ref> When the sigmoidal causal mechanism is used, given a variable Y and parents {X 1 , . . . , X s }, then Y = r i=1 w i ?(X i + w 0 ) + ?, and if a polynomial mechanism is used then</p><formula xml:id="formula_53">Y = ? w 0 + s i=1 w 1i X i + s i=1</formula><p>w 2i X 2 i , where w i , w ij , ? are random. MDMA was trained with m = 1000, L = 2, r = 3 for 50 epochs and learning rate 0.1 on all datasets. In all experiments we find that the graphs recovered using MDMA are closer to the truth than those recovered using Gaussian PC, as measured by the structural Hamming distance. Example recovered graphs are shown in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Density estimation on real data</head><p>We train MDMA on four UCI datasets, details of the dataset sizes and hyperparameter choices are presented in <ref type="table" target="#tab_5">Table 3</ref>. In all experiments a batch size of 500 and learning rate of 0.01 were used. We use the same pre-processing as <ref type="bibr" target="#b44">[45]</ref>, which involves normalizing and adding noise. Details are provided in the attached code. <ref type="bibr" target="#b3">4</ref> The POWER dataset consists of measurements of power consumption from different parts of a house as well as the time of day. The GAS dataset contains measurements of chemical sensors used to discriminate between different gases. The HEPMASS and MINIBOONE datasets are both measurements from high-energy physics experiments, aiming respectively for the discovery of novel particles and to distinguish between different types of fundamental particles (electron and muon neutrinos). a) b) c) d) <ref type="figure" target="#fig_0">Figure 10</ref>: Tensor decompositions. a) Tensors of various orders (resp. vectors, matrices, delta tensors). Each edge represents an index, and connecting two edges represents contraction (summation over an index). b) The set of univariate CDFs ?, which can be viewed as an order 2d tensor. c) A general unstructured tensor of order 6. d) The hierarchical Tucker (HT) decomposition <ref type="bibr" target="#b4">(5)</ref>. After suitable normalization, the tensor in d) can be contracted with the tensor ? shown in b) to give a multivariate CDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Tensor decompositions</head><p>In constructing the MDMA estimator, we are confronted with the problem of combining products of univariate CDFs linearly, in a manner that is both computationally efficient and expressive. The linearity constraint reduces this to a tensor decomposition problem (with additional non-negativity and normalization constraints). There is an extensive literature on such efficient tensor decompositions (see <ref type="bibr" target="#b73">[74]</ref> for a review).</p><p>The analogy with tensor decompositions becomes clear when we consider discrete rather than continuous variables. Assume we wish to model the joint distribution of d discrete variables, each taking one of S possible values. The distribution is then a function ? S : [S] ? ? ? ? ? [S] ? R, which can also be viewed as an order d tensor. A general tensor ? S will require order of S d numbers to represent, and is thus impractical even for moderate d. The continuous analog of such a tensor is a multivariate function F : R d ? R, with the value of x j corresponding to the discrete index s j . We will thus use the same notation for the continuous case. A graphical representation of tensors and of the diagonal HT tensor used in MDMA is presented in <ref type="figure" target="#fig_0">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional design details E.1 Univariate marginal parameterization</head><p>We parameterize the univariate marginal CDF ?(x) for some scalar x ? R using a simple feedforward network following <ref type="bibr" target="#b36">[37]</ref>. Recall from section Section 3.6 that we model the univariate CDFs as functions</p><formula xml:id="formula_54">?(x) = sigmoid ? L l ? ? l?1 ? L l?1 ? ? l?2 ? ? ? ? ? 1 ? L 1 ? ? 0 ? L 0 (x),</formula><p>where L i : R ni ? R n i+l is the affine map L i (x) = W i x + b i for an n i+1 ? n i weight matrix W i with nonnegative elements and an n i+1 ? 1 bias vector b i , with n l+1 = n 0 = 1 and n i = r for i ? [l]. This is a slightly more general form than the one in Section 3.6 since we allow the nonlinearities to depend on the layer. For the nonlinearities, we use</p><formula xml:id="formula_55">? i (x) = x + a i tanh(x)</formula><p>for some vector a i ? R ni+1 with elements constrained to lie in [?1, 1] (the lower bound on a i is necessary to ensure that ? i are invertible, but the upper bound is not strictly required). This constraint, as well as the non-negativity constraint on the W i , is enforced by setting W i = softplus(W i , 10), a i = tanh(? i ) in terms of someW i ? R ni+1?ni ,? i ? R ni+1 . The softplus function is softplus(x, ?) = 1 ? log(1 + exp(?x)) and is a smooth, invertible approximation of the ReLU. We typically use small values for l, r in the experiments (see Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Adaptive variable coupling</head><p>One degree of freedom in constructing a HT decomposition is the choice of partitions of subsets of the variables at every layer over which the products are taken. This imposes a form of weight-sharing, and it will be natural to share weights between variables that are highly correlated. As a simple example, let d = 4 and consider two different HT decompositions F (x 1 , x 2 , x 3 , x 4 ) = i1,i2,k ? 2 k ? 1 k,i1,1 ? 1 k,i2,2 ? i1,1 (x 1 )? i1,2 (x 2 )? i2,3 (x 3 )? i2,4 (x 4 ),</p><formula xml:id="formula_56">F (x 1 , x 2 , x 3 , x 4 ) = i1,i2,k ? 2 k ? 1 k,i1,1 ? 1 k,i2,2 ? i1,1 (x 1 )? i1,3 (x 3 )? i2,2 (x 2 )? i2,4 (x 4 ),</formula><p>obtained by coupling X 1 in the first layer respectively with X 2 and X 3 . The univariate marginals for X 1 , X 2 and X 3 can then be written as</p><formula xml:id="formula_57">F (x 1 ) = i1,k ? 2 k ? 1 k,i1,1 ? i1,1 (x 1 ), F (x 2 ) = i1,k ? 2 k ? 1 k,i1,1 ? i1,2 (x 2 ), F (x 3 ) = i2,k ? 2 k ? 1 k,i2,2 ? i2,3 (x 3 ), F (x 1 ) = i1,k ? 2 k ? 1 k,i1,1 ? i1,1 (x 1 ), F (x 2 ) = i2,k ? 2 k ? 1 k,i2,2 ? i2,2 (x 2 ), F (x 3 ) = i1,k ? 2 k ? 1 k,i1,1 ? i1,3 (x 3 ).</formula><p>Assume that the variables X 1 and X 2 are identical. In F , both of their univariate marginals depend in an identical way on the tensor parameters. In F however, additional parameters are required to represent them. Hence F is a more parsimonious representation of the join distribution. If X 1 and X 3 are identical instead, then the converse holds and F is the more parsimonious representation. This property extends to any higher-dimensional (e.g., bivariate) marginals.</p><p>In data with spatial or temporal structure (e.g. if the variables are image pixels) there is a natural way to couple variables based on locality. When this is not present, we can adaptively construct the couplings based on the correlations in the data using a simple greedy algorithm. After constructing an empirical covariance matrix from a minibatch of data, we couple the two variables that are most correlated and have not yet been paired. We repeat this until we couple all the groups of variables. Then we "coarse-grain" by averaging over blocks of the covariance matrix arranged according to the generated coupling and repeat the process, this time coupling subsets of variables. We find that this coupling scheme improves performance compared to naive coupling that does not take correlations into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Initialization</head><p>As in the univariate case, the non-negativity constraint of the HT tensor parameters ? i k,k ,j is enforced by defining ? i k,k ,j = softplus ?i k,k ,j , 20 for some? i k,k ,j ? R.</p><p>As is standard, we initialize independently the elements of the univariate PDF weightsW i as zeromean gaussians with variance 1/n fanin , the? i as standard gaussians and the b i as 0. The initialization of the HT parameters is? i k,k ,j = m * ? k,k for 1 ? i ? p ? 1. This initialization is chosen so that after applying the softplus the matrix ? i ?,?,j is close to an identity at initialization, which we have found facilitates training compared to using a random initialization. Benefits of such "orthogonal" initialization schemes have also been shown for deep convolutional networks <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b75">76]</ref>. The final layer ? p k are initialized as zero mean gaussians with variance 0.3/m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 From HT to MERA</head><p>The choice of the diagonal HT decomposition (5) is convenient, yet there is a wealth of other tensor decompositions that can be explored. Here we highlight one such decomposition that generalizes the diagonal HT and could potentially lead to more expressive models. It is based on <ref type="bibr" target="#b76">[77]</ref>.</p><p>Let  <ref type="formula" target="#formula_10">(4)</ref>, and ? l a 2 ? m ? m ? d/2 l tensor with nonegative elements satisfying ? l 1,k,i,j + ? l 2,k,i,j = 1. The MERA parametrization of a distribution can then be written as </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Density estimation with closed-form marginals and conditionals. First Row: Panels 1,2: Empirical histograms of training data. Panel 3: Samples from the training data. Panel 4: Samples from the trained MDMA model. Second Row: Panels 1,2: The marginal density learned by MDMA plotted on a grid. Panels 3,4: Conditional densities learned by MDMA plotted on a grid. Third Row: Results on additional datasets: Panels 1,2: Training data and learned marginal density for a 3D checkerboard dataset. Panels 3,4: Similarly for a 3D mixture of Gaussians.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Define a collection of</head><label></label><figDesc>independent categorical variables R = {R i,j } taking values in [m], where ? [p], i ? [m] and for any , j ? [2 p? ]. These variables are distributed according to ? , i, j : P R i,j = k = ? i,k,j , where {? } p =1 are the parameters of the HT decomposition. The fact that the parameters are nonnegative and m k=1 ? k,i,j = 1 ensures the validity of this distribution. With the convention R p kp+1,1,1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>sampling from the categorical variables in this fashion is equivalent to sampling a mixture component. It follows that by first sampling a single mixture component and then sampling from this component, one obtains a sample from F HT . The main loop in Algorithm 1 samples such a mixture component, and there are p = log 2 d layers in the decomposition, so the time complexity of the main loop is O(log d), and aside from storing the decomposition itself this sampling procedure requires storing only O(d) integers. This logarithmic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Denote the sigmoid function by ?(x) = 1/(1 + e ?x ) and define the function F : R ? R by F (x) = log F (x)/(1 ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Density estimation with closed-form marginals and conditionals. Top Row: Samples from a 3D density, and 2D marginal histograms. Middle Row: Samples from MDMA after fitting the density, and plots of the learned 2D marginals. Bottom Row: Left: learned 1D marginals compared to 1D marginal histograms of the training data. Right: Learned conditional densities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>15 Figure 8 :</head><label>158</label><figDesc>Log marginal density on UCI datasets. The scatter plot is composed of 500 samples from the dataset. Left: The POWER dataset. One variable corresponds to the time of day, and the other to power consuption from the kitchen of a house. Note the small value of the density during night-time. The data is normalized during training, yet the labels on the horizontal axis reflect the value of the unnormalized variable for interpretability. Right: The HEPMASS dataset. Despite MDMA not achieving state-of-the-art results on test likelihood for this dataset, the model still captures accurately the non-trivial dependencies between the variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>F M, 1 :</head><label>1</label><figDesc>R d ? R m?d be a matrix-valued function such that F M,1 i,j (x) = F HT,1 i,j (x) = ? i,j (x j ).For l ? {2, . . . , log 2 d}, define the matrix-value functions F M,1 :R d ? R m?d/2 l?1 recursively by F M,l i,j (x) = m k ? l?1 k,i,j {? l?1 1,k,i,j ? M,l?1 k,2j?1 (x)? M,l?1 k,2j (x) + ? l?1 2,k,i,j ? M,l?1 k,2j?1 (x)? M,l?1 k+1,2j (x)}with k + 1 ? 1 when k = m, ? l as in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>F 1 k 2 ,? 1 i2,k 2 ,k1, 2 ?</head><label>12122</label><figDesc>with a ? R m Since the conditions on ? l and ? l imply m k=1 ? l k,i,j (? l 1,k,i,j + ? l 2,k,i,j ) = 1, this parametrization clearly results in a valid CDF. Note that ? l 1,k,i,j + ? l 2,k,i,j = 1 leads to ? l having only m ? m ? d/2 l free parameters. For d = 4, we haveF M (x) = m k1,k2,k 2 =1 a k1 ? 1 k2,k1,1 ? ? M,1 k2,1 (x)? M,1 k2+i1?1,2 (x)? M,1 k 2 ,3 (x)? M,1 k 2 +i2?1,4 (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? 3.0 15.6 ? 2.7 19.8 ? 4.1 18.9 ? 4.2 32 27 MDMA 15.6 ? 6.1 12.8 ? 5.2 17.9 ? 5.3 15.0 ? 4.5 30.3 ? 1.8 25.8 ? 0.7 on the independence between U 1 and U 2 can still have power. While the test from</figDesc><table><row><cell cols="2">Sigmoidal DAG, d=10</cell><cell cols="2">Polynomial DAG, d=10</cell><cell cols="2">Sachs [62], d=11</cell></row><row><cell>SHD(D)</cell><cell>SHD</cell><cell>SHD(D)</cell><cell>SHD</cell><cell>SHD(D)</cell><cell>SHD</cell></row><row><cell>Gaussian 18.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>General density estimation. Test log likelihood for density estimation on UCI datasets. The comparison results are reproduced from<ref type="bibr" target="#b9">[10]</ref>.</figDesc><table><row><cell>Model</cell><cell>POWER [d=6]</cell><cell>GAS [d=11]</cell><cell cols="2">HEPMASS [d=21] MINIBOONE [d=43]</cell></row><row><cell>Kingma et al. 2018 [5]</cell><cell>0.17 ? .01</cell><cell>8.15 ? .4</cell><cell>?18.92 ? .08</cell><cell>?11.35 ? .07</cell></row><row><cell>Grathwohl et al. 2019 [8]</cell><cell>0.46 ? .01</cell><cell>8.59 ? .12</cell><cell>?14.92 ? .08</cell><cell>?10.43 ? .04</cell></row><row><cell>Huang et al. 2018 [6]</cell><cell>0.62 ? .01</cell><cell>11.96 ? .33</cell><cell>?15.08 ? .4</cell><cell>?8.86 ? .15</cell></row><row><cell>Oliva et al. 2018 [7]</cell><cell>0.60 ? .01</cell><cell>12.06 ? .02</cell><cell>?13.78 ? .02</cell><cell>?11.01 ? .48</cell></row><row><cell>De Cao et al. 2019 [9]</cell><cell>0.61 ? .01</cell><cell>12.06 ? .09</cell><cell>?14.71 ? .38</cell><cell>?8.95 ? .07</cell></row><row><cell>Bigdeli et al. 2020 [10]</cell><cell>0.97 ? .01</cell><cell>9.73 ? 1.14</cell><cell>?11.3 ? .16</cell><cell>?6.94 ? 1.81</cell></row><row><cell>MDMA</cell><cell>0.57 ? .01</cell><cell>8.92 ? 0.11</cell><cell>?20.8 ? .06</cell><cell>?29.0 ? .06</cell></row><row><cell>nMDMA</cell><cell>1.78 ? .12</cell><cell>8.43 ? .04</cell><cell>?18.0 ? 0.91</cell><cell>?18.6 ? .47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Dimension and size of the UCI datasets, and the hyperparameters used for fitting MDMA on these datasets. m is the width of the MDMA model, l and r are respectively the depth and width of the univariate CDF models described in Appendix E.1.</figDesc><table><row><cell></cell><cell>POWER</cell><cell>GAS</cell><cell cols="2">HEPMASS MINIBOONE</cell></row><row><cell>d</cell><cell>6</cell><cell>8</cell><cell>21</cell><cell>43</cell></row><row><cell>Training set</cell><cell cols="2">1659917 852174</cell><cell>315123</cell><cell>29556</cell></row><row><cell cols="2">Validation set 184435</cell><cell>94685</cell><cell>35013</cell><cell>3284</cell></row><row><cell>Test set</cell><cell cols="2">204928 105206</cell><cell>174987</cell><cell>3648</cell></row><row><cell>m</cell><cell>1000</cell><cell>4000</cell><cell>1000</cell><cell>1000</cell></row><row><cell>l</cell><cell>2</cell><cell>4</cell><cell>2</cell><cell>2</cell></row><row><cell>r</cell><cell>3</cell><cell>5</cell><cell>3</cell><cell>3</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code for reproducing all experiments is available at https://github.com/dargilboa/mdma.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://archive.ics.uci.edu/ml/datasets.php</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://fentechsolutions.github.io/CausalDiscoveryToolbox/html/index.html<ref type="bibr" target="#b3">4</ref> The raw datasets are available for download at https://zenodo.org/record/1161203#.YLUMImZKjuU</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work of DG is supported by a Swartz fellowship. The work of AP is supported by the Simons Foundation, the DARPA NESD program, NSF NeuroNex Award DBI1707398 and The Gatsby Charitable Foundation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Samples from model</p><p>Training data</p><p>x 1</p><p>Training data </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Density estimation on real data</head><p>To demonstrate how MDMA allows one to visualize marginal densities we show in <ref type="figure">Figure 8</ref> learned bivariate marginals from the UCI POWER and HEPMASS datasets. The former is composed of power consumption measurements from different parts of a house, with one of the variables (X 6 ) being the time of day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental details</head><p>All experiments were performed on Amazon Web Services using Tesla V-100 GPUs. The total compute time was 3,623 hours, with the vast majority devoted to the experiments on density estimation with missing values (Section 4.3), where some of the runs of BNAF required over 72 hours to complete.</p><p>C.1 Mutual information estimation 10 6 samples from the true density are used for fitting MDMA and for estimating the integral over the log marginals in order to compute the mutual information. The MDMA model used had parameters r = 4, l = 5, m = 1000 and was trained with a batch size 500 and learning rate 0.01 for 2 epochs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On optimal and data-based histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="605" to="610" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Consistency of data-driven histogram methods for density estimation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Nobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="687" to="706" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Remarks on Some Nonparametric Estimates of a Density Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="832" to="837" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On estimation of a probability density function and mode. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Parzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural autoregressive flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2078" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformation autoregressive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junier</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3898" to="3907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Block neural autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1263" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning generative models using denoising density estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Siavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Bigdeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiziano</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Portenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zwicker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02728</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning and Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural likelihoods via cumulative distribution functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Chilinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Tensor spaces and numerical tensor calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Hackbusch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namgil</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh-Huy</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">P</forename><surname>Mandic</surname></persName>
		</author>
		<title level="m">Tensor networks for dimensionality reduction and large-scale optimization: Part 1 lowrank tensor decompositions. Foundations and Trends? in Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Genotype imputation. Annual review of genomics and human genetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristen</forename><surname>Willer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Sanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gon?alo</forename><surname>Abecasis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="387" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Genotype imputation for genome-wide association studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Marchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Howie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Genetics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="499" to="511" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Normalizing flows: An introduction and review of current methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Brubaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Normalizing flows for probabilistic modeling and inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">57</biblScope>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3918" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilinear analysis of image ensembles: Tensorfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetri</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="447" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Anh Huy Phan, and Shun-ichi Amari. Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Zdunek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tensor decompositions for learning latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matus</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2773" to="2832" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tensors for data mining and data fusion: Models, applications, and scalable algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Evangelos E Papalexakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sidiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2016" />
			<publisher>TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evangelos E Papalexakis, and Christos Faloutsos. Tensor decomposition for signal processing and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Nicholas D Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>De Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3551" to="3582" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speeding-up convolutional neural networks using fine-tuned cp-decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional neural networks with low-rank regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. PMLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tensorizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 29-th Conference on Natural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Deok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunhyeok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taelim</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjun</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. PMLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep multi-task representation learning: A tensor factorisation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sharing residual units through collective tensor factorization to improve deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="page" from="635" to="641" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the expressive power of deep learning: A tensor analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on learning theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="698" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Global optimality in tensor factorization, deep learning, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Haeffele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Generalization bounds for neural networks through tensor factorization. CoRR, abs/1506.08473, 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanie</forename><surname>Majid Janzamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beating the perils of non-convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanie</forename><surname>Majid Janzamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08473</idno>
	</analytic>
	<monogr>
		<title level="m">Guaranteed training of neural networks using tensor methods</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the expressive power of overlapping architectures of deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. PMLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Variational image compression with a scale hyperprior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ball?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Jin</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A new scheme for the tensor representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Hackbusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>K?hn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Fourier analysis and applications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="706" to="722" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Statistical analysis with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Roderick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald B Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">793</biblScope>
		</imprint>
	</monogr>
	<note>3rd Ed</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Cand?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="717" to="772" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Matrix completion with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="925" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Nearest neighbor imputation algorithms: a critical evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Beretta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Santaniello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medical informatics and decision making</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="208" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">mice: Multivariate imputation by chained equations in r</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>S Van Buuren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groothuis-Oudshoorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical software</title>
		<imprint>
			<biblScope unit="page" from="1" to="68" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Missing value estimation methods for dna microarrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Troyanskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Sherlock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Botstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ B</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="520" to="525" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Statistical methods for research workers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald Aylmer</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Especially Section</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A review of some recent advances in causal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preetam</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nandy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Big Data</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Approximate kernel-based conditional independence tests for fast non-parametric causal discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Strobl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visweswaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A consistent characteristic function-based test for conditional independence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="807" to="834" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A nonparametric hellinger metric test for conditional independence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Halbert</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Theory</title>
		<imprint>
			<biblScope unit="page" from="829" to="864" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Testing conditional independence using maximal nonlinear conditional correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzee-Ming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2047" to="2091" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The hardness of conditional independence testing and the generalised covariance measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1514" to="1538" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>AUAI Press</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Testing conditional independence via quantile regression based partial copulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><forename type="middle">Richard</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">70</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The partial copula: Properties and associated dependence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Spanhel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="76" to="83" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the pc-algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>B?hlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A kernel-based causal learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="855" to="862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Nonlinear directed acyclic structure learning with weakly additive noise models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Robert E Tillman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1847" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pc algorithm for nonparanormal graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Causal discovery toolbox: Uncovering causal relationships in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diviyan</forename><surname>Kalainathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Goudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritik</forename><surname>Dutta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">37</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Pe&amp;apos;er</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garry P</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">5721</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing Bayesian network structure learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006-10" />
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="31" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Anomaly detection in the presence of missing values. ODD v5.0: Outlier Detection De-constructed Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadesse</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zemicheal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Quantitative risk management: concepts, techniques and tools-revised edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?diger</forename><surname>Mcneil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Embrechts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Princeton university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Probit transformation for nonparametric kernel estimation of the copula density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gery</forename><surname>Geenens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Charpentier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Paindaveine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1848" to="1873" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Incorporating functional knowledge in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>B?lisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Monotone and partially monotone neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hennie</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Velikova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="906" to="917" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Asymptotic theory of statistics and probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">A course in approximation theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><forename type="middle">Ward</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Allan</forename><surname>Light</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">On approximations via convolution-defined mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mclachlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Theory and Methods</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="3945" to="3955" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning equivalence classes of bayesian-network structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="445" to="498" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Counting unlabeled acyclic digraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert W Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Combinatorial mathematics V</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1977" />
			<biblScope unit="page" from="28" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Tensor networks for dimensionality reduction and Large-Scale optimizations. part 2 applications and future perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A-H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mandic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Bahri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5393" to="5402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Beyond signal propagation: Is feature diversity necessary in deep neural network initialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Blumenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dar</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Quantum MERA channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Giovannetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Montangero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosario</forename><surname>Fazio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
