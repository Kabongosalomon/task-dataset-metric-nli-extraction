<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepLogo: Hitting Logo Recognition with the Deep Neural Network Hammer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
							<email>forresti@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ASPIRE Laboratory Unviersity of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anting</forename><surname>Shen</surname></persName>
							<email>antingshen@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ASPIRE Laboratory Unviersity of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gao</surname></persName>
							<email>pgao@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ASPIRE Laboratory Unviersity of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
							<email>keutzer@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ASPIRE Laboratory Unviersity of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepLogo: Hitting Logo Recognition with the Deep Neural Network Hammer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>IANDOLA, SHEN, GAO, KEUTZER: DEEPLOGO 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, there has been a flurry of industrial activity around logo recognition, such as Ditto's service for marketers to track their brands in user-generated images, and Lo-goGrab's mobile app platform for logo recognition. However, relatively little academic or open-source logo recognition progress has been made in the last four years. Meanwhile, deep convolutional neural networks (DCNNs) have revolutionized a broad range of object recognition applications. In this work, we apply DCNNs to logo recognition. We propose several DCNN architectures, with which we surpass published state-of-art accuracy on a popular logo recognition dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Logo recognition is the key problem in contextual ad placement, validation of product placement, and online brand management. Contextual ad placement is about placing relevant ads on webpages, images, and videos. An obvious problem in contextual advertising is "what brands appear in images and video content?" Logo recognition is the core solution to this.</p><p>As absurd as it may sound, ad networks often don't know what's in the ad JPEGs that they are placing in front of web content 1 . For example, a brand like Toyota can import an ad into Google AdWords without mentioning that "this is a Toyota ad, and we may want to place this on car-related content. <ref type="bibr">"</ref> We have observed that most ads contain their brand's logo, so logo recognition is the obvious solution to this. We provide a deeper explanation of these logo recognition applications in Section 2.2.</p><p>In this paper, we apply deep convolutional neural networks (DCNNs) to logo recognition. We look at three problem statements in logo recognition: classification, detection without localization, and detection with localization. We achieve state-of-the-art accuracy by customizing DCNNs for logo recognition.</p><p>The rest of the paper is organized as follows. In Section 2, we describe related work in logo recognition methods, logo recognition applications, and object recognition. Section 3 provides an overview of publicly-available datasets of labeled logo images. In Section 4, c 2015. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. <ref type="bibr">1</ref> Managers at three different digital advertising platforms told us about this problem.</p><p>arXiv:1510.02131v1 [cs.CV] 7 Oct 2015 we propose neural network architectures for logo recognition. In Section 5, we evaluate our classifiers in logo classification and detection. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Logo Recognition Methods</head><p>In the last few years (2007-2014), Scale Invariant Feature Transform (SIFT) features have been at the core of most logo recognition methods, replacing methods such as Fisher classifiers <ref type="bibr" target="#b19">[21]</ref>. Many previous works employ a bag-of-words framework to work with SIFT features, where local features are quantized into a vocabulary to allow for efficient matching of descriptors. Boia et al. <ref type="bibr" target="#b2">[4]</ref> applied Complete Rank Transform on top of bag-of-words SIFT features to gain additional invariance to illumination and monotonic changes in pixel intensities in their logo classifier, and achieve state of the art classification accuracies that we use as our baseline. For detection, Romberg et al. <ref type="bibr" target="#b14">[16]</ref> combined a cascaded index with bag-of-words SIFT features to achieve high precision at the expense of some recall while maintaining computational efficiency and scalability. Neural networks have played a part in logo detection as well. Psyllos et al. <ref type="bibr" target="#b12">[14]</ref> incorporated neural networks by applying a probabilistic neural network on top of SIFT for vehicle model recognition. Francesconi <ref type="bibr" target="#b5">[7]</ref> used recursive neural networks to classify blackand-white logos. Duffner &amp; Garcia <ref type="bibr" target="#b4">[6]</ref> fed pixel values directly into a convolutional neural network with two convolution layers to detect watermarks on television.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Logo Recognition Applications</head><p>In this section, we summarize real-world applications where logo recognition plays a crucial role.  <ref type="bibr" target="#b19">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logo Detection with Localization</head><p>Locate and classify logos in the image.</p><p>Augmented reality <ref type="bibr" target="#b0">[2]</ref> Logo Localization Locate the logos, but don't classify them.</p><p>Removing watermarks from television content <ref type="bibr" target="#b4">[6]</ref>  <ref type="bibr" target="#b18">[20]</ref> Brand tracking on Instagram. Since the rise of social media in the mid-2000s, companies like HootSuite and Sprout Social have provided dashboards for marketers to track their brands on Facebook and Twitter. Most of these social media analytics dashboards simply aggregate text data from tweets and Facebook posts. However, Instagram and Pinterest are social networks in which communications are based almost entirely on photos. Logo recognition is key to brand analytics on Instagram and Pinterest.</p><p>Used car ad verification. Psyllos et al. used logo recognition (e.g. Ford, Volkswagen, etc) as one of several signals for classifying cars by make and model <ref type="bibr" target="#b12">[14]</ref>. Their logo recognition system consists of SIFT features classified with a Probabilistic Neural Network. This would be useful for identifying inaccurate listings on eBay or Autotrader used car classified ads -e.g., flagging an ad labeled as a Honda CR-V that contains photos of a Volkswagen Passat.</p><p>Watermark removal. Yan et al. <ref type="bibr" target="#b18">[20]</ref> and Duffner et al. <ref type="bibr" target="#b4">[6]</ref> used logo recognition as part of a system for removing logos and watermarks from television content. Note that this application does not necessarily need to identify the type of logo (e.g. Apple or Adidas); it simply needs to localize logos and remove them.</p><p>We show how each of these applications uses logo recognition in <ref type="table" target="#tab_0">Table 1</ref>. Note that there are a few variants of the logo recognition problem, such as classification and detection.  There are several datasets of images labeled with logos, including BelgaLogos <ref type="bibr" target="#b9">[11]</ref>, FlickrLogos-27 <ref type="bibr" target="#b10">[12]</ref>, and FlickrLogos-32 <ref type="bibr" target="#b14">[16]</ref>. We evaluate our logo detection methods on the FlickrLogos-32 dataset, because it has more labeled logo images than any other dataset that we are aware of. We give an overview of FlickrLogos-32 in <ref type="table" target="#tab_1">Table 2</ref>. We show a few example images from the FlickrLogos-32 dataset in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Neural Networks for Object Recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><formula xml:id="formula_0">(a) (b) (c) HP</formula><p>Ritter Sport Background image (no logos) <ref type="figure">Figure 1</ref>: Diversity of the FlickrLogos-32 dataset. The logos vary widely in size, down to just a few pixels (a). Some images like (b) have multiple images of the same type. However, an image wouldn't contain both Ritter Sport and HP. "Background images" like (c) don't contain logos.</p><p>The FlickrLogos-32 dataset also provides bounding box annotations for all logos, which can optionally be used during training. In Sections 5.1 and 5.2, we ignore the bounding boxes. We use the bounding boxes in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Deep Convolutional Neural Network Architectures</head><p>We now review DCNN architectures from the literature, which we will use as inspiration to design specialized architectures for logo recognition. In 2012, Krizhevsky et al. designed AlexNet <ref type="bibr" target="#b11">[13]</ref>, a DCNN architecture containing 5 convolutional layers and 3 fully-connected layers. AlexNet won the ImageNet-2012 image classification challenge. By 2014, state-ofthe-art ImageNet DCNNs had evolved into extremely deep networks with up to 19 layers, such as VGG-19 <ref type="bibr" target="#b15">[17]</ref>. In addition to increasing depth, GoogLeNet <ref type="figure" target="#fig_1">(Figure 3</ref>) is comprised of "inception" meta-layers ( <ref type="figure">Figure 2</ref>), which contain multiple convolution filter resolutions. Inception layers are well equipped to classify images where objects may be large or small with respect to the overall image size. GoogLeNet and VGG-19 took the top two places in the ImageNet-2014 competition. We give a broader context of the speed/accuracy tradeoffs among these DCNN architectures in <ref type="table">Table 3</ref>. <ref type="table">Table 3</ref>: ImageNet accuracy for popular deep convolutional neural network architectures. Single-crop accuracy numbers are reported by the respective authors. We obtained these perframe speed and energy numbers on a NVIDIA K40 GPU using Caffe's native convolution implementation <ref type="bibr" target="#b8">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DCNN Architecture</head><p>Top-5 ImageNet-1k accuracy Speed (Caffe) Energy (Caffe) AlexNet <ref type="bibr" target="#b11">[13]</ref> 81.8% 417 fps 0.48 kJ/frame VGG-19 <ref type="bibr" target="#b15">[17]</ref> 91.0% 35.1 fps 8.37 kJ/frame GoogLeNet <ref type="bibr" target="#b16">[18]</ref> 89.9% 121 fps 1.64 kJ/frame <ref type="figure">Figure 2</ref>: An "inception meta-layer," as defined in GoogLeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Our Proposed DCNN architectures</head><p>GoogLeNet is a good start, but we believe that there are even better DCNN architectures for logo classification. Within the vast DCNN architectural space, we are on the hunt for architectures that are even better than GoogLeNet at handling a broad variety of logo resolutions (see <ref type="figure" target="#fig_5">Figure 6</ref>). Here are three examples of architectures that we have designed for this goal: GoogLeNet-GP, or GoogLeNet with Global Pooling, performs global average-pooling prior to fully connected layers. We implement this with the Caffe global pooling layer, which takes any input layer and pools it down to 1x1xchannels. This enables the network to process any input image size at training or test time. We exploit this property of GoogLeNet-GP by feeding in various image sizes at training time.</p><p>GoogLeNet-FullClassify. When making nets extremely deep, end-to-end backpropagation can become ineffective, as discussed in <ref type="bibr" target="#b15">[17]</ref>. One way to make a very deep net more trainable with backpropagation is to have multiple softmax classifiers hanging off the side of the net. For example, GoogLeNet has 3 softmax classifiers, situated after Inception3, Incep-tion6, and Inception9. Taking this idea to an extreme, we propose GoogLeNet-FullClassify, which has a classifier after every inception layer. At test time, we only use the final softmax layer's classification outputs.</p><p>Full-Inception. A "Full-Inception" net is like GoogLeNet, but the first layer is also an inception layer. Most DCNNs, including GoogLeNet, struggle to classify low-resolution logos in big images. Full-Inception aims to rectify this by having a range of DCNN filter sizes at the first layer, which operates on the input pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We now evaluate deep convolutional neural networks on the FlickrLogos-32 logo recognition dataset -we train on trainval and test on test. All of our experiments were conducted with the Caffe <ref type="bibr" target="#b8">[10]</ref> deep learning framework. We consider three variants of the logo recognition problem: classification, detection without localization, and detection with localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Logo Classification</head><p>We now turn to the problem of classifying logos in images. As we describe in <ref type="table" target="#tab_0">Table 1</ref>, we define this problem as "Which logo is in this image?" In this section, we assume that each image in the test set contains a logo.</p><p>We defined several DCNN architectures in Section 4. We apply these to logo classification, and we report accuracy results in <ref type="table">Table 4</ref>. In the previous work, the most accurate classifier on FlickrLogos-32 is the Complete Rank Transform, which is built on top of SIFT features <ref type="bibr" target="#b2">[4]</ref>. GoogLeNet-GP produces higher logo classification accuracy than the Complete Rank Transform. Increasing dropout boosts GoogLeNet logo classification accuracy. Higher dropout also leads to slower convergence in training <ref type="figure" target="#fig_3">(Figure 4</ref>), but our highest-accuracy classifier trains in just fifteen minutes.</p><p>Although we are close to 90% accuracy with GoogLeNet-GP, it is valuable to analyze the images that were incorrectly classified. We see two key failure modes: low-resolution logos, and variation within logo categories. First, we find that low-resolution logos are the most <ref type="table">Table 4</ref>: Results for logo classification on FlickrLogos-32 with no background samples. Pretraining: ImageNet (ILSVRC-2012-train); Fine-tuning: FlickrLogos-32-trainval; Testing: FlickrLogos-32-test. In "GoogLeNet-GP," we average pool down to 1x1xchannels (aka "globalpool") prior to the fc layers. common failure case for all of our classifiers (GoogLeNet-GP, AlexNet, etc) -we illustrate this pattern in <ref type="figure" target="#fig_4">Figures 5 and 6</ref>. Second, logos change frequently. Disney changed its logo more than 30 times from 1988 to 2015 <ref type="bibr" target="#b1">[3]</ref>. In FlickrLogos-32, we found that substantially different versions of the Texaco logo appear in the trainval and test sets (see <ref type="figure" target="#fig_6">Figure 7)</ref>. Training protocol for GoogLeNet experiments: We begin by pretraining GoogLeNet on ImageNet. Then, we re-initialize the softmax layers (cls1_fc2, cls2_fc2, and cls3_fc) to randomized weights with 32 outputs (for FlickrLogos-32) instead of 1000 outputs (for ImageNet's 1000 categories). Finally, we fine-tune GoogLeNet on FlickrLogos-32-val.</p><p>Training protocol for GoogLeNet-GP experiments: We begin with pretrained GoogLeNet weights. Then, we modify GoogLeNet by inserting global pooling layers prior to fullyconnected layers {cls1_fc1, cls2_fc1, cls3_fc}, which appear after Inception3, Inception6, and Inception9, respectively. In the off-the-shelf GoogLeNet, cls1_fc1 and cls2_fc1 take 4x4xchannels input and cls3_fc takes 1x1xchannels input. But, global pooling produces 1x1xchannels input for these fully-connected layers. So, we re-initialize cls1_fc1, cls2_fc1, and cls3_fc with random weights and an input size of 1x1xchannels.</p><p>Training protocol for Full-Inception experiments: We begin with pretrained GoogLeNet weights. Then, for the first two conv layers, we randomly select filters to warp to smaller or larger sizes. This way, we create inception layers out of pretrained model weights -instead of retraining from scratch.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Logo Detection without Localization</head><p>The problem of logo detection without localization is defined here as "which logo, if any, is in this image?"</p><formula xml:id="formula_1">(a) (b) (c) Apple</formula><p>Pepsi Starbucks <ref type="figure">Figure 8</ref>: Sample PR curves for Detection without Localization. Detection without localization exhibits fairly good detection performance, especially on distinctive logos such as that of Starbucks.</p><p>We used Fast R-CNN (FRCN for short) <ref type="bibr" target="#b6">[8]</ref> to do deep detection with localization of logos within images. FRCN is a framework for object detection built on top of Caffe <ref type="bibr" target="#b8">[10]</ref> that is significantly faster and more accurate than previous deep learning frameworks for object detection. It simplifies the training process for deep networks using a multi-task loss, achieving 9x faster training times than the previous version of R-CNN <ref type="bibr" target="#b7">[9]</ref>. These properties make Fast R-CNN a good choice of a framework to apply to our task.</p><p>Ordinarily, FRCN takes in region proposals and performs bounding box regression in order to refine these proposals. We will explore this functionality when we do localization with detection, but here, we used FRCN with only one region proposal per image (encompassing the entire image) and removed the bounding box regression functionality of FRCN. FRCN takes in this single region proposal and the image itself, which can be thought of as examining the entire image for logos. It then outputs a classification of the image as containing no logo, or, if a logo is present, which logo is in the image. We ran this modified FRCN with AlexNet and produced a 73.3% mean average precision (more details in <ref type="table" target="#tab_3">Table 5</ref>). We are not aware of a baseline for non-localized detection in the related work -FlickrLogos-32 (and other logo datasets) are most commonly used for evaluating retrieval algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Logo Detection with Localization</head><p>We then looked at the problem of detection with localization. Here, we define this problem as "If there is a logo in this image, which logo is it and where is it located?" Here, we use FRCN with region proposals. For both training and testing, FRCN takes in raw images and region proposals of significant features inside of those images in the form of bounding boxes. This is the localization step. It then classifies each bounding box region proposal as a type of logo or as "background," meaning that the area in the proposal is not a logo at all. If the region proposal contains a logo, it also outputs a bounding box regression offset that adjusts the region proposal to more closely highlight the region containing the logo. Our region proposals are generated using selective search <ref type="bibr" target="#b17">[19]</ref>. These regions along with images are then fed into FRCN for training/testing.</p><p>As in the previous section, we do not know of a non-localized detection baseline. We achieve a mean average precision of 73.5% with FRCN+AlexNet and 74.4% with FRCN +VGG16 <ref type="bibr" target="#b15">[17]</ref>. AP values per class and example precision-recall curves are shown in <ref type="table" target="#tab_4">Table 6</ref> and <ref type="figure">Figure 9</ref>.</p><p>(a) (b) (c) Apple Pepsi Starbucks <ref type="figure">Figure 9</ref>: Sample PR curves for Detection with Localization. Detection with localization exhibits even better detection performance than detection without localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Logo recognition is a key problem in marketing analytics, digital advertising, and augmented reality. We have developed deep convolutional neural network (DCNN) architectures that improve upon state-of-the-art logo classification results. We also use our DCNNs to establish a baseline for accuracy on logo detection. Collectively, we refer to this work as DeepLogo.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>In our view, logo recognition is an instantiation of the broader problem of object recognition. Recently, Deep Convolutional Neural Networks (DCNNs) have unleashed a torrent of progress on object recognition. In the ImageNet object classification challenge, DCNNs have posted accuracy improvements of several percentage points per year. Using DCNNs, Donohue et al. outperformed state-of-the-art accuracy on scene classification and fine-grained bird classification<ref type="bibr" target="#b3">[5]</ref>. DCNNs enabled Razavian et al. to outperform state-of-the-art accuracy human attribute detection and visual instance retrieval<ref type="bibr" target="#b13">[15]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>GoogLeNet consists almost entirely of 'inception meta-layers."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Convergence: fine-tuning our variants of GoogLeNet for FlickrLogos-32 classification with a batch size of 32. As we increase dropout, training takes slightly longer to converge. The training has mostly converged by 1000 iterations. On an NVIDIA K20 GPU, 1000 iterations of GoogLeNet (or GoogLeNet-GP) training takes 15 minutes and 32 seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Correlation between bounding box size and logo classification outcome forGoogLeNet-GP, dropout=0.9. Failures are more common for smaller logos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>6 :</head><label>6</label><figDesc>Failure case: low-resolution logos. We misclassified photos like (a) and (b) because the logos are small in relation to the image resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>7 :</head><label>7</label><figDesc>Variations within categories. Texaco training images have the circular logo (a), while some of the test images have the hexagonal Texaco logo (b). The script nVIDIA (c) is easier than the all-caps version (d). Correctly identified: (a), (c). Mistakes: (b), (d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Applications of logo classification and detection.</figDesc><table><row><cell></cell><cell>Problem Definition</cell><cell>Example Applications</cell></row><row><cell>Logo Classification</cell><cell>Which logo is in this image?</cell><cell>Checking for mistakes in used-car</cell></row><row><cell></cell><cell></cell><cell>ads [14]</cell></row><row><cell>Logo Detection w/o</cell><cell>If there is a logo, what type of logo</cell><cell>Marketing, brand tracking on Insta-</cell></row><row><cell>Localization</cell><cell>is it?</cell><cell>gram and Pinterest [1]; Document</cell></row><row><cell></cell><cell></cell><cell>classification</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Overview of the FlickrLogos-32 dataset. Foreground images contain logos; background images do not.</figDesc><table><row><cell></cell><cell>trainval</cell><cell>test</cell></row><row><cell>foreground</cell><cell>1280</cell><cell>960</cell></row><row><cell>background</cell><cell>3000</cell><cell>3000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>FlickrLogos-32 non-localized detection APs.</figDesc><table><row><cell></cell><cell cols="8">adidas aldi apple becks bmw carls chim coke</cell></row><row><cell></cell><cell>corona</cell><cell>dhl</cell><cell>erdi</cell><cell>esso</cell><cell cols="3">fedex ferra ford</cell><cell>fost</cell></row><row><cell></cell><cell cols="3">google guin hein</cell><cell>hp</cell><cell cols="4">milka nvid paul pepsi</cell></row><row><cell>Method</cell><cell>ritt</cell><cell cols="2">shell sing</cell><cell>starb</cell><cell>stel</cell><cell>texa</cell><cell>tsin</cell><cell>ups</cell><cell>mAP</cell></row><row><cell></cell><cell>44.0</cell><cell cols="2">67.6 80.6</cell><cell>72.1</cell><cell>79.2</cell><cell cols="2">56.5 73.0</cell><cell>55.9</cell></row><row><cell></cell><cell>90.9</cell><cell cols="2">53.3 76.9</cell><cell>86.9</cell><cell>68.7</cell><cell cols="2">90.9 84.3</cell><cell>85.4</cell></row><row><cell>FRCN +</cell><cell>81.2</cell><cell cols="2">86.4 73.3</cell><cell>N/A</cell><cell>46.0</cell><cell cols="2">52.5 97.6</cell><cell>37.5</cell></row><row><cell>AlexNet (ours)</cell><cell>63.0</cell><cell cols="2">55.6 88.8</cell><cell>98.1</cell><cell>83.8</cell><cell cols="2">80.2 85.2</cell><cell>75.0</cell><cell>73.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>FlickrLogos-32 localized detection APs.</figDesc><table><row><cell></cell><cell cols="8">adidas aldi apple becks bmw carls chim coke</cell></row><row><cell></cell><cell>corona</cell><cell>dhl</cell><cell>erdi</cell><cell>esso</cell><cell cols="3">fedex ferra ford</cell><cell>fost</cell></row><row><cell></cell><cell cols="3">google guin hein</cell><cell>hp</cell><cell cols="4">milka nvid paul pepsi</cell></row><row><cell>Method</cell><cell>ritt</cell><cell cols="2">shell sing</cell><cell>starb</cell><cell>stel</cell><cell>texa</cell><cell>tsin</cell><cell>ups</cell><cell>mAP</cell></row><row><cell></cell><cell>47.8</cell><cell cols="2">69.1 68.6</cell><cell>71.7</cell><cell>81.7</cell><cell cols="2">59.1 70.9</cell><cell>58.8</cell></row><row><cell></cell><cell>90.9</cell><cell cols="2">56.9 83.6</cell><cell>89.4</cell><cell>70.8</cell><cell cols="2">88.3 85.7</cell><cell>86.0</cell></row><row><cell>FRCN +</cell><cell>90.9</cell><cell cols="2">81.0 66.5</cell><cell>N/A</cell><cell>46.6</cell><cell cols="2">52.4 98.0</cell><cell>42.0</cell></row><row><cell>AlexNet (ours)</cell><cell>63.3</cell><cell cols="2">50.6 83.5</cell><cell>99.3</cell><cell>87.9</cell><cell cols="2">81.5 86.7</cell><cell>70.5</cell><cell>73.5</cell></row><row><cell></cell><cell>61.6</cell><cell cols="2">67.2 84.9</cell><cell>72.5</cell><cell>70.0</cell><cell cols="2">49.6 71.9</cell><cell>33.0</cell></row><row><cell></cell><cell>92.9</cell><cell cols="2">53.5 80.1</cell><cell>88.8</cell><cell>61.3</cell><cell cols="2">90.0 84.2</cell><cell>79.7</cell></row><row><cell>FRCN +</cell><cell>85.2</cell><cell cols="2">89.4 57.8</cell><cell>N/A</cell><cell>34.6</cell><cell cols="2">50.3 98.6</cell><cell>34.2</cell></row><row><cell>VGG16 (ours)</cell><cell>63.0</cell><cell cols="2">57.4 94.2</cell><cell>95.9</cell><cell>82.2</cell><cell cols="2">87.4 84.3</cell><cell>81.5</cell><cell>74.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Research partially funded by DARPA Award Number HR0011-12-2-0016, plus ASPIRE industrial sponsors and affiliates Intel, Google, Huawei, Nokia, NVIDIA, Oracle, and Samsung. The first author is funded by the US DOD NDSEG Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logograb</surname></persName>
		</author>
		<ptr target="www.logograb.com" />
		<imprint/>
	</monogr>
	<note>founded in 2012</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Logo variations -walt disney pictures. closinglogos.com/page/Logo+Variations+-+Walt+Disney+ Pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Beard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Aczel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local description using multi-scale complete rank transform for improved logo recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raluca</forename><surname>Boia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Bandrabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corneliu</forename><surname>Florea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COMM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural scheme for robust detection of transparent logos in tv programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Duffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks (ICANN)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Logo recognition by recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Francesconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marinai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Q</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Recognition Algorithms and Systems</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Fast r-cnn</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Logo retrieval with a contrario visual query expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Roelof van Zwol, and Yannis Avrithis. Scalable triangulation-based logo recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lluis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Pueyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trevisiol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vehicle model recognition from frontal view image measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Psyllos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kayafas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Standards and Interfaces</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR DeepVision Workshop</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable logo recognition in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Garcia Pueyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roelof</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<idno>abs/1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic video logo detection and removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Qi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic document logo detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
