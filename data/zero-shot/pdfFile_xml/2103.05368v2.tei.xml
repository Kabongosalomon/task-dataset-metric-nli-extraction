<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ChangeSim: Towards End-to-End Online Scene Change Detection in Industrial Indoor Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Man</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Hyuk</forename><surname>Jang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahng-Min</forename><surname>Yoo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun-Kyung</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ue-Hwan</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Jong-Hwan</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">ChangeSim: Towards End-to-End Online Scene Change Detection in Industrial Indoor Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a challenging dataset, ChangeSim, aimed at online scene change detection (SCD) and more. The data is collected in photo-realistic simulation environments with the presence of environmental non-targeted variations, such as air turbidity and light condition changes, as well as targeted object changes in industrial indoor environments. By collecting data in simulations, multi-modal sensor data and precise ground truth labels are obtainable such as the RGB image, depth image, semantic segmentation, change segmentation, camera poses, and 3D reconstructions. While the previous online SCD datasets evaluate models given well-aligned image pairs, ChangeSim also provides raw unpaired sequences that present an opportunity to develop an online SCD model in an end-to-end manner, considering both pairing and detection. Experiments show that even the latest pair-based SCD models suffer from the bottleneck of the pairing process, and it gets worse when the environment contains the non-targeted variations. Our dataset is available at https://sammica.github.io/ChangeSim/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Scene change detection (SCD), the task of localizing changes given two scenes, past and present, is one of the most important tasks for patrol robots. SCD divides into two categories: offline and online. On the one hand, offline SCD <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> performs two global mappings at two different time-steps; compares the two reconstructions; and detects changes between the two scenes. Since offline SCD can detect changes only after the second mapping is completed, immediate detection is impossible. On the other hand, online SCD performs frame-level detection on-the-fly during the second mapping. Therefore, it enables immediate response, which is a crucial prerequisite for surveillance. In this work, we focus on online SCD.</p><p>Prior to change detection, online SCD goes through a pairing process in which a reference frame whose camera view matches that of the query (current) frame gets extracted from the past scene. At the core of the pairing process lies localization as camera poses play a key role in measuring the similarity between two frames. In the case of online outdoor SCDs such as SCDs for street-view <ref type="bibr" target="#b4">[5]</ref> and remote sensing <ref type="bibr" target="#b10">[11]</ref>, the availability of the global positioning system (GPS) allows robust and accurate pose estimation regardless of how much the environment has visually changed. Therefore, most outdoor online SCD datasets assume perfect localization and provide paired image sets as input rather than pairs of raw image sequences <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>The authors are with the School of Electrical Engineering, KAIST, Daejeon, 34141, Republic of Korea. e-mail: {jmpark, jhjang, smyoo, sklee, uhkim, johkim}@rit.kaist.ac.kr. <ref type="bibr" target="#b0">1</ref> These authors contributed equally to this work. On the other hand, localization becomes a challenging task for online indoor SCD since a localization algorithm can only utilize visual (+ inertial) features but not GPS signals. In addition, indoor environments frequently contain non-targeted visual changes such as air turbidity or light intensity. These variations not only degrade change detection performance for well-matched pairs, but also cause a bottleneck by deteriorating the performance of visual localization, the preceding stage of actual change detection. Due to these challenges, online indoor SCD has not displayed much progress over the past decades despite its substantial significance. Furthermore, the lack of a corresponding dataset prevents the employment of recent advances in deep learning. Though a set of indoor SCD datasets was reported in the literature <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b9">[10]</ref>, they merely assume the offline setting-which is not realistic in practical deployment.</p><p>We claim the following properties as an effective online indoor SCD dataset:  We broadly categorize these datasets as being either in outdoor or indoor. The Images column indicates whether or not images are provided in a paired way (i.e., reference image (at time t0) and query image (at time t1), P: Paired, U: Unpaired). The Change annotation column indicates the type of change label (every frame: ground-truths are provided for each frame of a sequence, final map only: ground-truths are provided for the 3D reconstruction only). The Environment column indicates its domain (i.e., R: real, S: synthetic). Note that ChangeSim is the first dataset providing both raw unpaired images and frame-by-frame change annotations, which are the prerequisite for online indoor SCD.</p><formula xml:id="formula_0">?</formula><p>In this work, we introduce ChangeSim, a photo-realistic dataset towards end-to-end online indoor SCD, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. To the best of our knowledge, we are the first to explicitly define the task of online indoor SCD, collect a dataset-satisfying the claimed properties-for development and evaluation of online indoor SCD; and propose baseline models.</p><p>First of all, the proposed ChangeSim provides raw unpaired sequences captured at time t 0 and t 1 as input (for endto-end SCD), requiring models to perform the pairing process and change detection. At the same time, ChangeSim includes paired image sets so that conventional pair-based SCD models can also benefit from ChangeSim. Next, ChangeSim provides controllable non-target variations, such as air turbidity and light condition. We show in the experiment that stateof-the-art SCD models struggle in our dataset mainly due to the bottleneck of visual localization, especially when the non-target variations are applied in the environment. Finally, ChangeSim provides rich multi-modal data and annotations, as described in <ref type="table" target="#tab_1">Table I (last row)</ref>, including depth, semantic segmentation label, multi-class change segmentation label, 7-D pose, and 3D reconstructions.</p><p>In summary, the main contributions of this paper are (1) a large SCD dataset with raw multi-modal data and rich annotations in diverse challenging virtual indoor environments, (2) a data collection and annotation pipeline for multilabel SCD, and (3) verification of the proposed ChangeSim by evaluating a representative SLAM algorithm, semantic segmentation models, and SCD models, and (4) unveiling insights on future directions of the SCD models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATASET FEATURES</head><p>We adopt a leading game engine, Unreal Engine 4 (UE4), to implement the virtual environment for change detection, where UE4 delivers photo-realistic environments, high-quality textures, dynamic shadows, and a variety of rendering options. These features not only minimize the gap between real-world and simulation environments, but also drastically lower the cost of high-quality ground-truth labels, which enables the creation of large, richly-annotated datasets. In particular, we have found that most traditional change detection datasets suffer from their sizes (i.e., lack of labeled changed objects and images) restraining learningbased methods. We expect that the following dataset features made by photo-realistic simulation achieve better coverage of environments and scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Diverse and Realistic Indoor Industrial Domain</head><p>We select warehouse environments as the domain of ChangeSim. A warehouse is appropriate as the domain of change detection benchmark because it is 1) a large-scale indoor environment compared to the household environment, 2) an industrial environment that needs periodic surveillance, and 3) a challenging environment for object change detection due to the densely arranged objects. ChangeSim includes ten real-world-like warehouse environments, ranging from a small personal warehouse to a large logistics warehouse, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. ChangeSim contains 24 semantic categories, including domain-specific industrial objects (e.g., pallet, forklift, pallet jack, rack, etc.). <ref type="figure" target="#fig_2">Figure 3</ref>(a) displays the list of semantic categories, and its average occurrence (%) and area (%) per image. It reveals the long tail-shaped, which is a natural feature of the real-world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Raw Data for the Whole Pipeline for Online Change Detection</head><p>ChangeSim provides two unpaired raw sequence, reference sequence, S 0 = [f 1 0 , ..., f i 0 , ..., f n 0 ] and query sequence,</p><formula xml:id="formula_1">S 1 = [f 1 1 , ..., f j 1 , .</formula><p>.., f m 1 ], captured at time t 0 and t 1 , respectively, where n and m are the number of frames in S 0 and S 1 , respectively. Given a current frame, f j 1 , an online SCD model, D, detects changes as follows:</p><formula xml:id="formula_2">c = D(f j 1 , S 0 , [f 0 1 , ..., f j?1 1 ]),<label>(1)</label></formula><p>where c ? R w?h?l is a multi-class change segmentation.</p><p>Here, w, h, and l indicate width, height, and the number of change classes, respectively. Assuming that D infer c in a  two-stage process, D can be separated into two sub-models, matching frame extractor, E, and pair-based scene change detector, I, as follows:</p><formula xml:id="formula_3">f i 0 = E(f j 1 , S 0 , [f 0 1 , ..., f j?1 1 ]), c = I(f i 0 , f j 1 ),<label>(2)</label></formula><p>where f i 0 is the extracted frame from S 0 , where the camera view of f j 1 best matches that of f i 0 among frames in S 0 . While the traditional SCD datasets only provided the pairs, f j 1 and f i 0 , focusing only on training and evaluation of I, we generalize online SCD by providing the raw sequences, S 0 and S 1 , so that both E and I are considered. Furthermore, an end-to-end SCD model, D can be developed leveraging our dataset.</p><p>In ChangeSim, there are 80 sequences in total (20 sequences for S 0 , 60 sequences for S 1 ), and each sequence contains 500-6,500 frames. Each frame in S 0 has synchronized RGB, depth, semantic segmentation image, and camera pose. A 3D reconstruction is also provided for each reference sequence, S 0 . The organization of each frame in S 1 is the same as that in S 0 , except that a change segmentation label and a matching frame, f i 0 , are included. C. Rich Changes with Novel Multi-class Change Labels 1) Multi-class Change Labels: While most SCD datasets provide binary change segmentation labels for the purpose of binary SCD, ChangeSim provides multi-class change segmentation labels for four change types: missing, new, replaced, and rotated, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. In the binary SCD, detecting pixel-level change is sufficient, but for the multi-class SCD, the ability to comprehensively consider the location, semantics, and shape of an object is required.</p><formula xml:id="formula_4">t 1 t o t 1 GT GT t o</formula><p>2) Rich Changes: We analyze our dataset at the levels of image and map to show that our dataset contains rich changes. At the image-level, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(b-e), about 84% of all images contain changes, and each image contains an average of 8% changed pixels (b). Also, change blobs with fewer than 5,000 pixels are the most common (d), and the change blobs between 5 and 10 meters in depth are the most frequent (e). Besides, our dataset provides approximately 10 change blobs per image, which is the largest compared to the online SCD datasets providing image-level change labels (c). At the map-level, as summarized in TABLE II, we compare indoor/offline SCD datasets to our dataset according to an average number of changed objects per environment (left), and a total number of objects in the dataset (right). It shows that ChangeSim provides the most number of changed objects compared to the existing indoor SCD datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normal</head><p>Dusty-air Low-illumination </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Controllable Environmental Variations</head><p>For each reference sequence, S 0 , ChangeSim provides three versions of the query sequence, S 1 ,: normal, dustyair, and low-illumination, allowing to directly measure the impact of environmental variations on an online SCD model. <ref type="figure" target="#fig_4">Figure 5</ref> shows examples of each scenario. The normal situation is when no other change has occurred other than the change of the object. The other two situations are devised assuming extreme cases that can occur in industrial environments. In dusty-air, the indoor air becomes dusty and cloudy, making it challenging to identify distant objects. Also, light is scattered as it passes through the dusty air. Low-illumination is a situation in which most of the indoor lighting has been removed. Instead, the camera is equipped with several flashlights. The field of view is limited to the area illuminated by the flashlights. Furthermore, the color tone becomes darker, and the texture of objects changes according to the lighting intensity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>To collect the raw data reflecting real-world characteristics as well as detailed annotations with minimum human efforts, we design a four-stage pipeline as shown in <ref type="figure" target="#fig_5">Figure 6</ref>. The included stages are semantic labeling, reference sequence capturing (at time t 0 ), change labeling, and query sequence capturing (at time t 1 ). The semantic labeling process assigns class labels to objects in the map based on their names defined by the map creators (i.e., professional designers). Then, in the capturing process of a reference sequence, a quadrotortype drone empowered by AirSim <ref type="bibr" target="#b11">[12]</ref> moves along a handdrawn trajectory to acquire an RGBD sequence. After that, we create a modified environment where some of the objects change their state, and the environment can be controlled to have optional environmental variations such as dusty-air, low illumination, and minor variation of trajectories. Finally, in the query sequence capturing process, the drone traverses the changed environment and acquires the RGBD sequence and ground-truth labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Labeling</head><p>Each object in the environment has a unique name defined by professional designers, usually in the form of Mod-elingName Number (e.g., bottle 7), where ModelingName generally includes semantics and Number is a randomly assigned integer to distinguish the same objects. Using the form, we define various candidate names for each semantic category (e.g., candidate names for canister: can, bottle, container, etc.), and if a given object contains a candidate name, we label it with the corresponding category. <ref type="figure" target="#fig_5">Figure  6</ref>(a) shows the environment from the top-view where the semantic labeling is completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reference Sequence Capturing</head><p>1) Trajectory Drawing: Auto-generated trajectories have the advantage of being able to include various movements, so they are mainly used for synthetic SLAM datasets. However, auto-generated trajectories are often unsuitable for surveillance tasks, such as when the trajectory is unnatural or when it gets too close to an obstacle. As ChangeSim aims at surveillance-oriented change detection, we adopt hand-drawing trajectories to minimize this side-effect. We design the trajectories mainly in the form of traversing one lap through the corridors, keeping the distance to nearby obstacles greater than a pre-defined threshold to obtain a view suitable for change detection.</p><p>2) RGBD-SLAM based 3D Mapping: The multirotor traverses the environment and collects RGBD, 7-D pose, semantic labels for each frame, and 3D reconstruction for each sequence along the hand-drawn trajectories (see <ref type="figure" target="#fig_5">Figure 6</ref>(b)). One of the most latest RGBD-SLAM models, RTABMAP <ref type="bibr" target="#b12">[13]</ref>, is employed to collect pose and 3D reconstruction. The data is collected at a velocity of up to 3 m/s, an acquisition rate of 1-3 fps, and a clock-speed of 0.03. Each sequence took about 30 minutes to 1 hour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Change Labeling</head><p>We limit the candidate semantic classes to be changed to 16 classes out of 24 in total, mainly things (i.e., countable objects), because stuff objects (i.e., uncountable objects, such as floor, ceiling, etc.) have little probability of changing within a short time in the real world. <ref type="figure" target="#fig_5">Figure 6(c)</ref> shows the result of applying the object changes with the change labels.</p><p>Missing: One of the advantages of virtual environments, including the Unreal Engine, is that the types of rendering are selectable. We disable RGB rendering and Depth rendering for the selected object. Through this, the object becomes transparent, that is, it does not appear in the RGB and Depth images, but is still displayed in the segmentation image, resulting in missing type of change label. New: We randomly select 3D modelings and place them on the floor, shelves, or other objects in the environment. Replaced: After deleting the selected object, another 3D modeling is placed in the same place. The replaced one may or may not belong to the same semantic class as the deleted object. Rotated: The selected object is rotated randomly in the x, y, and z axes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Query Sequence Capturing</head><p>1) Data acquisition in a changed environment: As shown in <ref type="figure" target="#fig_5">Figure 6(d)</ref>, the multirotor traverses the changed environment and collects RGB, Depth, estimated semantic &amp; change labels, and 7-D poses. The poses are obtained employing RTABMAP's localization mode, where the SLAM algorithm stops mapping and does only localization (i.e., estimate 7-D poses), given a 3D reconstruction and its corresponding RGBD sequence. Note that the multirotor is localized in the 3D map reconstructed from the traverse at time t 0 , as shown in (d). At this time, the multirotor follows the noise-added version of the trajectory used in the previous mapping (b), and Gaussian noise N (0, 0.3 2 ) is applied to each coordinate of waypoints in the trajectory.</p><p>We capture three scenarios for each sequence: normal, dusty-air, low-illumination. In the case of Dusty-air, the weather control option provided by AirSim is used (Dust 0.5, Foggy 0.5). For the low-illumination, almost all lights in the environment are turned off, or their brightness is minimized. Instead, two forward-facing lights are installed at the front of the multirotor, ensuring a minimum amount of sight.</p><p>2) Matching frame extraction: For the current frame, a frame with the closest L1 distance from the reference sequence is selected using 7-D poses, and then the two frames are paired. While this method is the most intuitive and straightforward, it highly depends on the visual SLAM algorithm's localization performance and is therefore vulnerable to visual changes. As the pairing becomes inaccurate, the overlapping area within a pair also reduces, which leads to deterioration in change detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. BASELINE AND ANALYSIS</head><p>In order to verify the usefulness of ChangeSim, we propose a straightforward baseline model of a two-step strategy for online SCD and evaluate each module of this baseline: 1) matching frame extraction and 2) pair-based scene change detection. In matching frame extraction, a pairing process is performed using the estimated pose obtained through the SLAM algorithm. In pair-based scene change detection, changes are detected using the latest learning-based SCD models, CSCDNet <ref type="bibr" target="#b7">[8]</ref> and ChangeNet <ref type="bibr" target="#b13">[14]</ref>.</p><p>In particular, we measure the impact of environmental variation for each module and show that the existing models are vulnerable to environmental variation. Also, in 2), the effect of multi-class on SCD is measured. While existing models manage to identify changes, they show weakness in classifying the type of change, implying that they perform as a pixel-level change detector function rather than perceiving the essence of changes. As an example of using semantic labels, we show the effect of simple fine-tuning-based transfer learning on the online SCD. We hope that various approaches to online SCD will be developed and evaluated without restraint via ChangeSim, towards end-to-end, multiclass, and visually robust online SCD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Train-test Split</head><p>To establish concrete benchmarks, we split our data into training and test sets according to the provided maps. We selected 6 maps (12 sequences, 2 sequences for each map) as the training set (rows 1-3, <ref type="figure" target="#fig_1">Figure 2</ref>) and the remaining 4 maps (8 sequences, 2 sequences for each map) as the test set (rows 4-5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Matching Frame Extraction</head><p>Given the current frame, visual localization is performed estimating the 7-D pose through RTABMAP, and the frame with the least L1 distance is extracted from the reference sequence, S 0 . <ref type="table" target="#tab_1">Table III</ref> shows visual localization performance measured by absolute trajectory error (ATE), according to three environmental variations for six randomly selected sequences. Assuming that the degree of environmental variation becomes more severe in the order of normal&lt;dusty-air&lt;low-illumination, the visual localization tend to become more and more inaccurate as the environmental variation becomes more severe. <ref type="figure" target="#fig_6">Figure 7</ref>(a) visualizes the estimated trajectory for each of the three environmental variation scenarios for the two reference sequences, and examples of paired images are shown in <ref type="figure" target="#fig_6">Figure 7</ref>(b). The results confirm that the performance degradation of visual localization is directly linked to low pairing quality, which severely interferes with the following pair-based SCD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pair-based Scene Change Detection</head><p>Given a pair of the reference and query images, SCD is performed to infer changes for the present view. We   benchmarked the following representative pair-based SCD models: ChangeNet <ref type="bibr" target="#b13">[14]</ref> which consists of siamese networks <ref type="bibr" target="#b14">[15]</ref> and fully convolutional networks (FCNs), and CSCDNet <ref type="bibr" target="#b7">[8]</ref> which consists of siamese networks and correlation networks <ref type="bibr" target="#b15">[16]</ref>.</p><p>For the implementation details, we used the Adam optimizer with the learning rate of 1 ? 10 ?4 . The ImageNetpretrained ResNet backbone was used, unless otherwise specified. We used the batch size of 16 and minimized Cross-entropy loss for 20 epochs. Horizontal flips and color jittering were applied during training. The training time took approximately 24 hours with a single RTX2080Ti. The inputs to the models are a pair of RGB images, where the images are resized to 240?320, and 256?256 for the ChangeNet and CSCDNet, respectively, to meet their inputsize requirements. In particular, the pairs are obtained by the matching frame extraction; thus, both train/test sets contain mismatched pairs and well-matched pairs. We report macro-F1 (i.e., average of per-class F1) and mean intersection over union (mIoU) for the following SCD results. The results are from multi-class SCDs unless otherwise specified.</p><p>1) Impact of Multi-class Learning: In <ref type="table" target="#tab_1">Table IV</ref>, the effect of multi-class learning on SCD is reported. There is a significant performance drop when performing multi-class SCD, implying that the models' abilities to distinguish characteristics of each change category (missing, new, replaced, or rotated) are poor while they are good at detecting pixellevel changes. This is because multi-class SCD requires   a comprehensive understanding of changes in semantics, position, size, and pose of an object to distinguish change categories.</p><p>2) Impact of Environmental Variations: <ref type="table" target="#tab_8">Table V</ref> shows the impact of environmental variations between the two sequences, S 0 and S 1 , on the model performance. Note that environmental variations are not included in the training set. As expected, it is confirmed that model performances decrease as the environmental variations become more severe. In particular, the performance significantly decreases in lowillumination, which seems to be due to the incorrect pairing process. That is, two frames of a pair barely overlap their camera view.</p><p>3) Impact of Pre-training: We performed a simple finetuning-based transfer-learning experiment to confirm the effectiveness of ChangeSim's semantic labels for SCD. As an upstream task, we trained representative semantic segmentation models, DeepLab-V2 <ref type="bibr" target="#b18">[19]</ref> and DeepLab-V3 <ref type="bibr" target="#b19">[20]</ref>, using semantic segmentation labels. The original image size of 640?480 was used, and the cross-entropy loss was minimized using SGD, with the batch size of 3. Each model was trained up to 20 epochs without any augmentation.   other datasets, PASCAL VOC and Cityscapes, is analyzed to be due to a large number of classes and the appearance characteristics of industrial indoor environment objects, in particular, the presence of sharp and complex objects such as wires and racks. For the downstream task, we took the trained backbone ResNet50 of DeepLab-V3 as the backbone of ChangeNet and CSCDNet, respectively. The remaining implementation details were set the same as in other experiments. As shown in <ref type="table" target="#tab_1">Table VII</ref>, the SCD performance when using the backbone trained from ChangeSim was higher than that trained from ImageNet, implying that semantic-awareness helps SCD. <ref type="figure">Figure 8</ref> visualizes the SCD results of two pair-based SCD models, CSCDNet and ChangeNet. Binary SCD and multi-class SCD are denoted by B and M, respectively. Consistent with the results shown in <ref type="table" target="#tab_1">Table IV</ref>, CSCDNet caught the changes better than ChangeNet. In the multi-class SCD setting, the silhouette of changed objects is better captured compared to the binary SCD. However, the changes are often misclassified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Scene Change Detection:</head><p>2) Semantic Segmentation: We also show the qualitative results for the segmentation networks, as shown in <ref type="figure" target="#fig_7">Figure 9</ref>. The results show that the segmentation network trained on the train split of ChangeSim works well on both a) the test split of ChangeSim and b) real-word samples, proving that ChangeSim successfully represents the real-world characteristics.</p><p>V. CONCLUSION We have presented ChangeSim, a new dataset for online indoor scene change detection (SCD), satisfying the following properties for the effective development and evaluation of online SCDs: unpaired image sets; non-targeted variation; and diversity. We have verified the applicability of Chan-geSim through a two-stage online SCD model, whose stages are named as 1) matching frame extraction; and 2) pair-based SCD. In particular, we have found that existing pair-based SCD models suffer from the bottleneck of 1), mainly due to the vulnerability of visual localization. This result implies that an end-to-end and visually robust method should be developed for the application of online indoor SCD models in practice. We hope that ChangeSim will complement other datasets and benchmarks, and stimulate the development of the online indoor SCD, leveraging the claimed properties.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The proposed ChangeSim dataset, aimed at online scene change detection in industrial indoor environment (warehouse). While the previous online SCD datasets provided only paired images (b) and ground-truth labels (c), the proposed dataset also provides raw unpaired sequences (a) for the development of endto-end online SCD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>An overview of the ChangeSim environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>e t b a rr e l c e il in g s ig n c a n is te r fo rk li ft la m p p a ll e t_ ja c k b e a m la d d e r e x ti n g u is h e r d o o r b a g e le c tr ic _ b o x c o lu m n fe n c ChangSim dataset statistics. (a) ChangeSim dataset semantic label statistics. (left) Average probability of occurrence per image. (right) Average area per image. (b) ChangeSim dataset change label statistics. (left) Average probability of appearance per image. (right) Average area per image. (c) Comparison of change detection datasets on blobs per image. For fair comparison, the frequency values of Panoramic image datasets (TSUNAMI, GSV) are divided by four, ensuring the same field of view (90 ? ). (d) Histogram of blob size. (e) Histogram of average depth per blob.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Examples of changes in ChangeSim. Four categories of novel change types are defined, such as new (rows 1-2), missing (rows 3-4), replaced (rows 5-6), and rotated (rows 7-8), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Controllable environmental variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>An overview of the data collection pipeline, including data annotation. In (c-d), new, missing, replaced, and rotated objects are marked as brown, navy, lime, and green, respectively. Best viewed when magnified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative results matching frame extraction for the two reference sequences (blue). (a) The trajectories from the query sequences drawn as orange, green and red, respectively. (b) Pairing results are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Results on real-world samples Qualitative test results of the representative semantic segmentation networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Unpaired images: The dataset should provide raw unpaired image sequences rather than paired image sets, since localization takes a key role in online indoor SCD.</figDesc><table><row><cell>Dataset</cell><cell>Images</cell><cell>Change annotation</cell><cell>Size</cell><cell>#change class</cell><cell>#sem. class</cell><cell>3D recon.</cell><cell>Environment</cell><cell>Target change</cell><cell>Non-target change</cell></row><row><cell>Outdoor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VL-CMU-CD [5]</cell><cell>P</cell><cell>every frame</cell><cell>1362 images</cell><cell>1</cell><cell>9</cell><cell>-</cell><cell>R/street-view</cell><cell>structural change</cell><cell>weather, light</cell></row><row><cell>PCD [6]</cell><cell>P</cell><cell>every frame</cell><cell>200 images</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell>R/street-view</cell><cell>structural change</cell><cell>weather, light</cell></row><row><cell>CD2014 [7]</cell><cell>P</cell><cell>every frame</cell><cell>70,000 images</cell><cell>1</cell><cell>-</cell><cell>-</cell><cell cols="2">R/street-view, cctv dynamic object</cell><cell>-</cell></row><row><cell>PSCD* [8]</cell><cell>P</cell><cell>every frame</cell><cell>500 images</cell><cell>2</cell><cell>10</cell><cell>-</cell><cell>R/street-view</cell><cell>structural change</cell><cell>weather, light</cell></row><row><cell>CARLA-OBJCD* [9]</cell><cell>P</cell><cell>every frame</cell><cell>60,000 images</cell><cell>1</cell><cell>10</cell><cell>-</cell><cell>S/street-view</cell><cell>new/missing object</cell><cell>-</cell></row><row><cell>GSV-OBJCD* [9]</cell><cell>P</cell><cell>every frame</cell><cell>500 images</cell><cell>1</cell><cell>10</cell><cell>-</cell><cell>R/urban area</cell><cell>new/missing object</cell><cell>weather, light</cell></row><row><cell>Indoor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3RScan [1]</cell><cell>U</cell><cell>final map only</cell><cell>1482 scans of 478 scenes</cell><cell>1</cell><cell>7</cell><cell></cell><cell>R/household</cell><cell>dynamic object</cell><cell>- ?</cell></row><row><cell>InteriorNet [10]</cell><cell>U</cell><cell>final map only</cell><cell>Millions scans / unknown</cell><cell>-</cell><cell>23</cell><cell></cell><cell>S/household</cell><cell>dynamic object</cell><cell>light (color, intensity)</cell></row><row><cell>Langer et al. [2]</cell><cell>U</cell><cell>final map only</cell><cell>31 scans of 5 scenes</cell><cell>1</cell><cell>8</cell><cell></cell><cell>R/household</cell><cell>new object</cell><cell>- ?</cell></row><row><cell>Fehr et al. [3]</cell><cell>U</cell><cell>final map only</cell><cell>23 scans of 3 scenes</cell><cell>1</cell><cell>-</cell><cell></cell><cell>R/household</cell><cell>dynamic object</cell><cell>- ?</cell></row><row><cell>Ambrus et al. [4]</cell><cell>U</cell><cell>final map only</cell><cell>88 scans of 1 scenes</cell><cell>1</cell><cell>-</cell><cell></cell><cell>R/office</cell><cell>dynamic object</cell><cell>- ?</cell></row><row><cell>ChangeSim (ours)</cell><cell>P&amp;U</cell><cell>every frame</cell><cell>80 scans of 10 scenes, Approx. 130,000 images</cell><cell>4</cell><cell>24</cell><cell></cell><cell>S/warehouse</cell><cell>new/missing/ rotated/replaced object</cell><cell>dusty air, low-illumination</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Non-targeted variation: The dataset should include en-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">vironmental non-targeted variations (e.g., light condi-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">tion) because they frequently occur in real-world sce-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>narios.</cell><cell></cell><cell></cell><cell></cell></row></table><note>?? Diversity: The dataset should contain diverse types of environments and change classes. The claimed properties of online indoor SCD datasets en- hance the robustness and practicality of online indoor SCD models.arXiv:2103.05368v2 [cs.CV] 22 Jul 2021 ? Minor lighting changes with little visual variation included.* Not yet publicly released.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table /><note>Comparison of previous datasets for scene change detection.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Average number of changed objects per environment (left), total number of changed objects in dataset (right).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Impact of environmental variation on visual localization. Absolute trajectory errors (ATEs) were shown for the six randomly chosen query sequences.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Impact of multi-class learning. IoUs of Binary and Multi-class SCD are reported (C: change, S: static, M: missing, N: new, Re: replaced, Ro: rotated).</figDesc><table><row><cell></cell><cell></cell><cell>Normal</cell><cell cols="2">Dusty-air</cell><cell cols="2">Low-illumination</cell></row><row><cell></cell><cell cols="6">mIoU macro-F1 mIoU macro-F1 mIoU macro-F1</cell></row><row><cell>ChangeNet</cell><cell>23.0</cell><cell>29.8</cell><cell>21.3</cell><cell>26.0</cell><cell>20.9</cell><cell>24.6</cell></row><row><cell>CSCDNet</cell><cell>26.8</cell><cell>30.6</cell><cell>24.4</cell><cell>27.2</cell><cell>22.6</cell><cell>24.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Impact of environmental variations. mIoUs and macro-F1s are reported for the three cases of environmental variations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table</head><label></label><figDesc>VI shows the evaluation results in the test set. The performance difference of mIoU between ChangeSim and</figDesc><table><row><cell></cell><cell></cell><cell>ChangeSim</cell><cell></cell><cell cols="2">Pascal VOC Cityscapes</cell></row><row><cell></cell><cell cols="3">mIoU fwIoU P. Acc. (%)</cell><cell>mIoU</cell><cell>mIoU</cell></row><row><cell>DeepLab-V2</cell><cell>34.65</cell><cell>58.69</cell><cell>73.48</cell><cell>79.7</cell><cell>70.4</cell></row><row><cell>DeepLab-V3</cell><cell>39.80</cell><cell>62.95</cell><cell>76.68</cell><cell>85.7</cell><cell>81.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Semantic segmentation results for ChangeSim. For an intuitive comparison, the mIoU performances for PASCAL VOC<ref type="bibr" target="#b16">[17]</ref> and CityScapes<ref type="bibr" target="#b17">[18]</ref> are also reported.</figDesc><table><row><cell></cell><cell></cell><cell>Scratch</cell><cell cols="2">ImageNet</cell><cell cols="2">ChangeSim</cell></row><row><cell></cell><cell cols="5">mIoU macro-F1 mIoU macro-F1 mIoU</cell><cell>macro-F1</cell></row><row><cell>ChangeNet</cell><cell>18.8</cell><cell>22.0</cell><cell>21.1</cell><cell>25.0</cell><cell>23.0</cell><cell>29.8</cell></row><row><cell>CSCDNet</cell><cell>22.1</cell><cell>25.3</cell><cell>26.1</cell><cell>30.2</cell><cell>26.8</cell><cell>30.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII :</head><label>VII</label><figDesc>Impact of pre-training. SCD results are shown for the three pre-trained backbone ResNet.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work was supported by Institute for Information &amp; communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No.2020-0-00440, Development of artificial intelligence technology that continuously improves itself as the situation changes in the real world).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rio: 3d object instance re-localization in changing indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Avetisyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7658" to="7667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust and efficient object change detection by combining global semantic information and local geometric verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tsdf-based change detection for consistent longterm dense reconstruction and dynamic object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Furrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dryanovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gilitschenski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Robotics and automation (ICRA</title>
		<meeting>IEEE International Conference on Robotics and automation (ICRA</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5237" to="5244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised object segmentation through change detection in a long term autonomy scenario</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Folkesson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jensfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids)</title>
		<meeting>IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1181" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Streetview change detection with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Alcantarilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gherardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1301" to="1322" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Change detection from a street image pair using cnn features and superpixel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cdnet 2014: An expanded change detection benchmark dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="387" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly supervised silhouette-based semantic scene change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shibuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6861" to="6867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Epipolar-guided deep object matching for scene change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iwase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yokota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15540</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interiornet: Mega-scale multisensor photo-realistic indoor scenes dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzoumanikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Airsim: High-fidelity visual and physical simulation for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lovett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Field and Service Robotics (FSR)</title>
		<meeting>Field and Service Robotics (FSR)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="621" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rtab-map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="416" to="446" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Changenet: A deep learning architecture for visual change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gubbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Balamuralidhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Siamese recurrent architectures for learning sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
