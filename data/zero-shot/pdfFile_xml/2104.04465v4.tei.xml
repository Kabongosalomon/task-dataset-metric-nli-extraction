<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bootstrapping Semantic Segmentation with Regional Contrast</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dyson Robotics Lab</orgName>
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaifeng</forename><surname>Zhi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dyson Robotics Lab</orgName>
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Johns</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Robot Learning Lab</orgName>
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dyson Robotics Lab</orgName>
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bootstrapping Semantic Segmentation with Regional Contrast</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present ReCo, a contrastive learning framework designed at a regional level to assist learning in semantic segmentation. ReCo performs semi-supervised or supervised pixel-level contrastive learning on a sparse set of hard negative pixels, with minimal additional memory footprint. ReCo is easy to implement, being built on top of offthe-shelf segmentation networks, and consistently improves performance in both semisupervised and supervised semantic segmentation methods, achieving smoother segmentation boundaries and faster convergence. The strongest effect is in semi-supervised learning with very few labels. With ReCo, we achieve high quality semantic segmentation models, requiring only 5 examples of each semantic class. Code is available at https://github.com/lorenmt/reco. * Corresponding Author: shikun.liu17@imperial.ac.uk.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is an essential part of applications such as scene understanding and autonomous driving, whose goal is to assign a semantic label to each pixel in an image. Significant progress has been achieved by use of large datasets with high quality human annotations. However, labelling images with pixel-level accuracy is time consuming and expensive; for example, labelling a single image in CityScapes can take more than 90 minutes <ref type="bibr" target="#b5">[6]</ref>. When deploying semantic segmentation models in practical applications where only limited labelled data are available, high quality ground-truth annotation is a significant bottleneck.</p><p>To reduce the need for labelled data, there is a recent surge of interest in leveraging unlabelled data for semi-supervised learning. Previous methods include improving segmentation models via adversarial learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref> and self-training <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35]</ref>. Others focus on designing advanced data augmentation strategies to generate pseudo image-annotation pairs from unlabelled images <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In both semi-supervised and supervised learning, a semantic segmentation model often predicts smooth label maps, because neighbouring pixels are usually of the same class, and rarer high-frequency regions are typically only found in object boundaries. This learning bias nat- <ref type="figure">Figure 1</ref>: ReCo pushes representations within a class closer to the class mean representation, whilst simultaneously pushing these representations away from negative representations sampled in different classes. The sampling distribution from negative classes is adaptive to each query class. For example, due to the strong relation between bicycle and person class, ReCo will sample more representations in bicycle class, when learning person class, compared to other classes. urally produces blurry contours and regularly mis-labels rare objects. After carefully examining the label predictions, we further observe that wrongly labelled pixels are typically confused with very few other classes; e.g. a pixel labelled as rider has a much higher chance of being wrongly classified as person, compared to building or bus. By understanding this class structure, learning can be actively focused on the most challenging pixels to improve overall segmentation quality.</p><p>Here we propose ReCo, a contrastive learning framework designed at a regional level. Specifically, ReCo is a new loss function which helps semantic segmentation not only to learn from local context (neighbouring pixels), but also from global semantic class relationships across the entire dataset. ReCo performs supervised or semi-supervised contrastive learning on a pixel-level dense representation, as visualised in <ref type="figure">Fig. 1</ref>. For each semantic class in a training mini-batch, ReCo samples a set of pixel-level representations (queries), and encourages them to be close to the class mean averaged across all representations in this class (positive keys), and simultaneously pushes them away from representations sampled from other classes (negative keys).</p><p>For pixel-level contrastive learning with high-resolution images, it is impractical to sample all pixels which would cost huge memory footprint and considerably slow training time. In ReCo, we actively sample a sparse set of queries and keys, consisting of less than 5% of all available pixels. We sample negative keys from a learned distribution based on the relative distance between the mean representation of each negative key class and the query class. This distribution can be interpreted as a pairwise semantic class relationship, dynamically updated during training. We sample queries for those corresponding pixels having a low prediction confidence. Active sampling helps ReCo to rapidly focus on the most confusing pixels for each semantic class, and requires minimal additional memory.</p><p>ReCo enables a high-accuracy segmentation model to be trained with very few human annotations. We evaluate ReCo in both semi-supervised and supervised settings. For semisupervised learning, we propose two different modes: i) Partial Dataset Full Labels -a sparse subset of training images has full ground-truth labels, with the remaining data unlabelled; ii) Partial Labels Full Dataset -all images have some labels, but covering only a sparse subset of pixels. In both semi-supervised and supervised learning, we show that ReCo can consistently improve performance across all methods and datasets, obtaining sharper object boundaries and more accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic Segmentation The advances of semantic segmentation commonly rely on designing more powerful deep convolutional neural networks. Fully convolutional networks (FCNs) <ref type="bibr" target="#b18">[19]</ref> are the foundation of modern segmentation network design. They were later improved with dilated/atrous convolutions with larger receptive fields, capturing more long range information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. Alternative approaches include encoder-decoder architectures <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15]</ref>, sometimes using skip connections <ref type="bibr" target="#b25">[26]</ref> to refine filtered details.</p><p>A parallel direction is to improve optimisation strategies, by designing loss functions that better respect class imbalance <ref type="bibr" target="#b17">[18]</ref> or using point-wise rendering strategy to refine uncertain pixels from high-frequency regions improving the label quality <ref type="bibr" target="#b15">[16]</ref>. ReCo is built upon this line of research, a model-agnostic framework to improve segmentation by providing additional supervision on hard pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised Classification and Segmentation</head><p>The goal of semi-supervised learning is to improve model performance by taking advantage of a large amount of unlabelled data during training. Here consistency regularisation and entropy minimisation are two common strategies. The intuition is that the network's output should be invariant to data perturbation and geometric transformation. Based on these strategies, many semi-supervised methods have been developed for image classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>However, for segmentation, generating effective pseudo-labels and well-designed data augmentation are non-trivial. Some solutions improved the quality of pseudo-labelling, using adversarial learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref> and class activation maps <ref type="bibr" target="#b37">[38]</ref>; or enforcing consistency from different augmented images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>, perturbed features <ref type="bibr" target="#b24">[25]</ref> and different networks <ref type="bibr" target="#b12">[13]</ref>. In this work, we show that rather than designing a more advanced pseudo-labelling strategy, we can improve the performance of current semi-supervised segmentation methods by jointly training with a suitable auxiliary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contrastive Learning</head><p>Contrastive learning learns a similarity function to bring views of the same data closer in representation space, whilst pushing views of different data apart. Most recent contrastive frameworks learn similarity scores based on global representations of the views, parameterising data with a single vector <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>. Dense representations, on the other hand, rely on pixel-level representations and naturally provide additional supervision, capturing fine-grained pixel correspondence. Contrastive pre-training based on dense representations has recently been explored, and shows better performance in dense prediction tasks, such as object detection and keypoint detection <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Contrastive Learning for Semantic Segmentation Contrastive learning has been recently studied to improve semantic segmentation, with a number of different design strategies. <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b33">[34]</ref> both perform contrastive learning via pre-training, based on the generated auxiliary labels and ground-truth labels respectively, but at the cost of huge memory consumption. In contrast, ours performs contrastive learning whilst requiring much less memory, via active sampling. In concurrent work, <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b0">1]</ref> also perform contrastive learning with active sampling. However, whilst both these methods are applied to a stored feature bank, ours focuses on sampling features on-the-fly. Active sampling in <ref type="bibr" target="#b0">[1]</ref> is further based on learnable, classspecific attention modules, whilst ours only samples features based on relation graphs and prediction confidence, without introducing any additional computation overhead, which results in a simpler and much more memory-efficient implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Regional Contrast (ReCo)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pixel-Level Contrastive Learning</head><p>Let (X, Y) be a training dataset with training images x ? X and their corresponding C-class pixel-level segmentation labels y ? Y, where y can be either provided in the original dataset (supervised learning setting) or generated automatically as pseudo-labels (semi-supervised learning setting). A segmentation network f is then optimised to learn a mapping f ? : X ? Y, parameterised by network parameters ?. This segmentation network f can be decomposed into two parts: an encoder network: ? : X ? Z, and a decoder classification head ? c : Z ? Y. To perform pixel-level contrastive learning, we additionally attach a decoder representation head ? r on top of the encoder network ?, parallel to the classification head, mapping the encoded feature into a higher m-dimensional dense representation with the same spatial resolution as the input image: ? r : Z ? R, R ? R m . This representation head is only applied during training to guide the classifier using the ReCo loss as an auxiliary task, and is removed during inference.</p><p>A pixel-level contrastive loss is a function which encourages queries r q to be similar to the positive key r + k , and dissimilar to the negative keys r ? k . All queries and keys are sampled from the decoder representation head: r q , r +,? k ? R. In ReCo, we use a pixel-level contrastive loss in a supervised or semi-supervised manner across all available semantic classes in each mini-batch, with the distance between keys and queries measured by their normalised dot product. The general formation of the ReCo loss L reco is then defined as:</p><formula xml:id="formula_0">L reco = ? c?C ? r q ?R c q ? log exp(r q ? r c,+ k /?) exp(r q ? r c,+ k /?) + ? r ? k ?R c k exp(r q ? r ? k /?) ,<label>(1)</label></formula><p>for which C is a set containing all available classes in the current mini-batch, ? is the temperature control of the softness of the distribution, R c q represents a query set containing all representations whose labels belong to class c, R c k represents a negative key set containing all representations whose labels do not belong to class c, and r c,+ k represents the positive key which is the mean representation of class c. Suppose P is a set containing all pixel coordinates with the same resolution as R, these queries and keys are then defined as:</p><formula xml:id="formula_1">R c q = [u,v]?P 1(y [u,v] = c) r [u,v] , R c k = [u,v]?P 1(y [u,v] = c) r [u,v] , r c,+ k = 1 |R c q | ? r q ?R c q r q . (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Active Hard Sampling on Queries and Keys</head><p>To perform pixel-level contrastive learning on all available pixels in high-resolution training images would be computationally expensive and require massive memory. Here we introduce active hard sampling strategies to optimise only a sparse set of queries and keys.</p><p>Active Key Sampling When classifying a pixel, a semantic network might be only uncertain over a very few number of candidates among all available classes. The uncertainty from these candidates typically comes from a close spatial (e.g. rider and bicycle) or semantic (e.g. horse and cow) relationship. To reduce this uncertainty, we propose to sample negative keys non-uniformly, based on the relative distance between each negative key class and the query class. This involves building a pair-wise class relationship graph G, with G ? R |C|?|C| , computed and dynamically updated for each mini-batch. This pair-wise relationship is measured by the normalised dot product between the mean representation from a pair of two classes and is defined as:</p><formula xml:id="formula_2">G[p, q] = r p,+ k ? r q,+ k</formula><p>, ?p, q ? C, and p = q.</p><p>(</p><p>We further apply SoftMax to normalise these pair-wise relationships among all negative classes j for each query class c, producing a distribution:</p><formula xml:id="formula_4">exp(G[c, i])/ ? j?C,j =c exp(G[c, j]).</formula><p>We sample negative keys for each class i based on this distribution, to learn the corresponding query class c. This procedure allocates more samples to hard, confusing classes chosen specifically for each query class, helping the segmentation network to learn a more accurate decision boundary.</p><p>Active Query Sampling Due to the natural class imbalance in semantic segmentation, it is easy to over-fit on common classes, such as the road and building classes in the CityScapes dataset, or the background class in the Pascal VOC dataset. These common classes contribute to the majority of pixel space in training images, and so randomly sampling queries will under-sample rare classes and provide minimal supervision to these classes.</p><p>Therefore, we instead sample hard queries -for those corresponding pixel prediction confidence is below a defined threshold. Accordingly, ReCo loss would then guide the segmentation network providing suitable supervision on these less certain pixels. The easy and hard queries are defined as follows, and visualised in <ref type="figure" target="#fig_0">Fig. 2</ref>, where? q is the predicted confidence of label c after the SoftMax operation corresponding to the same pixel location as r q , and ? s is the user-defined confidence threshold.</p><formula xml:id="formula_5">R c, easy q = r q ?R c q 1(? q &gt; ? s )r q , R c, hard q = r q ?R c q 1(? q ? ? s )r q ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Semantic Segmentation with ReCo</head><p>ReCo can easily be added to modern supervised and semi-supervised segmentation methods without changing the training pipeline, with no additional cost at inference time. To incorporate ReCo, we simply add an additional representation head ? r as described in Section 3.1, and apply the ReCo loss (in Eq. 1) to this representation using the sampling strategy introduced in Section 3.2. Following prior contrastive learning methods <ref type="bibr" target="#b9">[10]</ref>, we only compute gradients on queries, for better training stabilisation.</p><p>In the supervised segmentation setting, where all training data have ground-truth annotations, we apply the ReCo loss on dense representations corresponding to all valid pixels. The overall training loss is then the linear combination of the supervised cross-entropy loss and the ReCo loss:</p><formula xml:id="formula_6">L total = L supervised + L reco .<label>(5)</label></formula><p>In the semi-supervised segmentation setting, where only part of the training data has groundtruth annotations, we apply the Mean Teacher framework <ref type="bibr" target="#b28">[29]</ref> following prior state-of-theart semi-supervised segmentation methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref>. Instead of using the original segmentation network f ? (which we call the student model), we instead use f ? (which we call the teacher model) to generate pseudo-labels from unlabelled images, where ? is a moving average of the previous state of ? during training optimisation:</p><formula xml:id="formula_7">? t = ?? t?1 + (1 ? ?)? t ,</formula><p>with a decay parameter ? = 0.99. This teacher model can be treated as a temporal ensemble of student models across training time t, resulting in more stable predictions for unlabelled images. The student model f ? is then used to train on the augmented unlabelled images, with pseudo-labels as the ground-truths.</p><p>For all pixels with defined ground-truth labels, we apply the ReCo loss similarly to the supervised segmentation setting. For all pixels without such labels, we only sample pixels whose predicted pseudo-label confidence is greater than a threshold ? w . This avoids sampling pixels which are likely to have incorrect pseudo-labels.</p><p>We apply the ReCo loss to a combined set of pixels from both labelled and unlabelled images. The overall training loss for semi-supervised segmentation is then the linear combination of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReCo Loss</head><p>Feature Space Masked Features <ref type="figure">Figure 3</ref>: Visualisation of the ReCo framework applied to semi-supervised segmentation and trained with three losses. A supervised loss is computed based on labelled data with groundtruth annotations. An unsupervised loss is computed for unlabelled data with generated pseudo-labels. And finally a ReCo loss is computed based on pixel-level dense representation predicted from both labelled and unlabelled images.</p><p>supervised cross-entropy loss (on ground-truth labels), unsupervised cross-entropy loss (on pseudo-labels) and ReCo loss:</p><formula xml:id="formula_8">L total = L supervised + ? ? L unsupervised + L reco ,<label>(6)</label></formula><p>where ? is defined as the percentage of pixels whose predicted confidence are greater than ? s , a scalar re-weighting the contribution for unsupervised loss, following prior methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref>. This makes sure the segmentation network would not be dominated by gradients produced by uncertain pseudo-labels, which typically occur during the early stage of training. <ref type="figure">Fig. 3</ref> shows a visualisation of the ReCo framework for semi-supervised segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate ReCo on supervised and semi-supervised segmentation. We introduce our new benchmark design and datasets in Section 4.1, with results and visualisations presented in Section 4.2 and 4.3. We provide an ablative analysis of important hyper-parameters along with the effect of query and key sampling strategies in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Semi-Supervised Segmentation Benchmark Redesign We propose two modes of semisupervised segmentation tasks aiming at different applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Partial Dataset Full Labels:</head><p>A small subset of the images is trained with complete groundtruth labels, whilst the remaining training images are unlabelled. When creating the labelled dataset, we sample labelled images based on two conditions: i) Each sampled image must contain a distinct number of classes greater than a manually-defined threshold. ii) Each sampled image must contain one of the least sampled classes in the previously sampled images. These two conditions ensure that the class distribution is more consistent across different random seeds, and ensures that all classes are represented. We are therefore able to evaluate the performance of semi-supervised methods with a very small number of labelled images without worrying about some rare classes being completely absent.</p><p>2. Partial Labels Full Dataset: All images are trained with partial labels, but only a few percentage of labels are provided for each class in each training image. We create the dataset by first randomly sampling a pixel for each class, and then continuously apply a <ref type="bibr">[5 ? 5]</ref> square kernel for dilation until we meet the percentage criteria.</p><p>The Partial Dataset Full Label evaluates learning to generalise semantic classes given few examples with perfect boundary information. The Partial Label Full Dataset evaluates learning semantic class completion given many examples with no or minimal boundary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We experiment on popular segmentation datasets: Cityscapes <ref type="bibr" target="#b5">[6]</ref> and Pascal VOC 2012 <ref type="bibr" target="#b6">[7]</ref> in both partial and full label setting. We also evaluate on a more difficult indoor scene segmentation dataset SUN RGB-D <ref type="bibr" target="#b27">[28]</ref> in the full label setting only, mainly due to the low quality annotations making it difficult to be fairly evaluated in the partial label setting.</p><p>In the full label setting, all three datasets are evaluated in four cases containing three semisupervised settings, and one fully supervised setting (training on all labelled images). In semi-supervised setting, we sample labelled images to make sure the least appeared class has appeared at least in 5, 15 and 50 images respectively, in all three datasets, among which, the labelled images for CityScapes, Pascal VOC and SUN RGB-D contain at least 12, 3 and 1 semantic classes, respectively.</p><p>In the partial label setting, both the CityScapes and Pascal VOC datasets are evaluated in four cases, by sampling 1, 1%, 5% and 25% labelled pixels for each semantic class in each training image. An example of the partially labelled dataset is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>.  Strong Baselines Prior semi-supervised segmentation methods are typically designed with different backbone architectures, and trained with different strategies, which makes it hard to compare them fairly. In this work, we standardise the baselines and implement four strong semi-supervised segmentation methods ourselves: i) S4GAN <ref type="bibr" target="#b20">[21]</ref>: an adversarial learning based semi-supervised method, ii) CutOut <ref type="bibr" target="#b8">[9]</ref>: an image augmentation strategy to generate new data by cutting out a random patch in an image, iii) CutMix <ref type="bibr" target="#b8">[9]</ref>: an image augmentation strategy to generate new data by attaching a random patch extracted from one image to another image, and iv) ClassMix <ref type="bibr" target="#b23">[24]</ref>: an image augmentation strategy by attaching random semantic classes extracted from one image to another image. Our implementations for all baselines obtain performance on par with, and most of the time surpassing, the performance reported in each original publication, giving us a set of strong baselines. Finally, we compare our method with standard supervised learning by training purely on labelled data.</p><p>Training Strategies All baselines and our method are implemented on the same segmentation architecture: DeepLabV3+ <ref type="bibr" target="#b3">[4]</ref> with ResNet-101 backbone <ref type="bibr" target="#b10">[11]</ref>, trained with the same optimisation strategies, and the same labelled and unlabelled data split. Detailed hyperparameters used for each dataset are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Pascal VOC, CityScapes and SUN RGB-D (Full Labels)</head><p>First, we compared our results to baselines (4 semi-supervised and 1 supervised) in a full label setting. For semi-supervised learning, we applied ReCo on top of ClassMix, which consistently outperformed other semi-supervised baselines. In fully supervised learning, we simply applied ReCo on top of standard supervised learning.  <ref type="table">Table 1</ref>: mean IoU validation performance for Pascal VOC, CityScapes and SUN RGB-D datasets. We report the mean and the range over three independent runs for all methods. The number of labelled images shown in first three columns in each dataset are chosen to make sure the least appeared classes have appeared in 5, 15 and 50 images respectively. <ref type="table">Table 1</ref> shows the mean IoU validation performance in three datasets over three individual runs (different labelled and unlabelled data split). We see that for all cases, applying the ReCo loss improves performance in both the semi-supervised and supervised settings. In the fewest label setting in each dataset, applying ReCo with ClassMix can improve results by an especially significant margin, with up to 5 ? 10% relative improvement. We additionally provide quantitative results in Appendix B, evaluated with existing semi-supervised segmentation benchmarks for Cityscapes and Pascal VOC datasets.     <ref type="bibr" target="#b37">[38]</ref>. The percentage and the number of labelled data used are listed in the first row.</p><p>To further justify the effectiveness of ReCo, we also include results on existing benchmarks in <ref type="table" target="#tab_4">Table 2</ref>. Here, all baselines were re-implemented and reported in the PseudoSeg setting <ref type="bibr" target="#b37">[38]</ref>, where the labelled images are sampled from the original PASCAL dataset, with a total of 1.4k images. In both benchmarks, ReCo shows state-of-the-art performance, and specifically is able to reach PseudoSeg's performance, whilst requiring only half the labelled data. Additional results on other data partitions are further shown in Appendix B.</p><p>We presented qualitative results from the semi-supervised setup with fewest labels: 20 labelled CityScapes and 50 labelled SUN RGB-D datasets in <ref type="figure" target="#fig_3">Fig. 5</ref>. The 60 labelled Pascal VOC is further shown in Appendix C. In <ref type="figure" target="#fig_3">Fig. 5</ref>, we can see the clear advantage of ReCo, where the edges and boundaries of small objects are clearly more pronounced such as in the person and bicycle classes in CityScapes, and the lamp and pillow classes in SUN RGB-D. More interestingly, we found that in SUN RGB-D, though all methods may confuse ambiguous class pairs such as table and desk or window and curtain, ReCo still produces consistently sharp and accurate object boundaries compared to the Supervised and ClassMix baselines where labels are noisy near object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Pascal VOC and CityScapes (Partial Labels)</head><p>In the partial label setting, we evaluated on the CityScapes and Pascal VOC datasets. We show qualitative results on Pascal VOC dataset trained on 1 labelled pixel per class per image in <ref type="figure">Fig. 6</ref>. As in the full label setting, we see smoother and more accurate boundary predictions from ReCo. More visualisations from CityScapes are shown in Appendix D. <ref type="table">Table 3</ref> compared ReCo to the two best semi-supervised baselines and a supervised baseline. Again, we see ReCo can improve performance in all cases when applied on top of ClassMix, with around 1 ? 5% relative improvement. We observe less performance improvement than in the full label setting; a very low level of ground-truth annotations could confuse ReCo to provide inaccurate supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablative Analysis</head><p>Next we present an ablative analysis on 20 labelled CityScapes dataset to understand the behaviour of ReCo with respect to different hyper-parameters. We use our default experimental setting from Section 4.2, using ReCo with ClassMix. All results are averaged over three independent runs.  <ref type="table">Table 3</ref>: mean IoU validation performance for Pascal VOC and Cityscapes datasets trained on 1, 1%, 5% and 25% labelled pixels per class per image. We report the mean and the range over three independent runs for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Queries and Keys</head><p>in default. In <ref type="figure" target="#fig_4">Fig. 7a and 7b</ref>, we can observe that performance is better when sampling more number of queries and keys, but after a certain point, the improvements would become marginal. Notably, even in our smallest option having 32 queries per class in a mini-batchconsisting only less than 0.5% among all available pixel space, can still improve performance in a non-trivial margin. Compared to a concurrent work <ref type="bibr" target="#b32">[33]</ref> which requires 10k queries and 40k keys in each training iteration, ReCo can be optimised with ?50 more efficiency in terms of memory footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ratio of Unlabelled Data</head><p>We evaluate how ReCo can generalise across different levels of unlabelled data. In <ref type="figure" target="#fig_4">Fig. 7c</ref>, we show with by only training on 10% unlabelled data in the original setting, we can already surpass the ClassMix baseline. This shows that ReCo can achieve strong generalisation not only in label efficiency but also in data efficiency.  Choice of Semi-Supervised Method Finally, we show that ReCo is robust to the choice of semi-supervised methods. In <ref type="figure" target="#fig_4">Fig. 7d</ref>, we can see ReCo obtains a higher performance from a variety choice of semi-supervised baselines with a similar relative improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Active Sampling</head><p>In <ref type="table">Table 4</ref>, we see that just randomly sampling queries and keys gives much less improvement compared than active sampling in our default setting. Particularly, hard query sampling has a dominant effect on generalisation: if we instead only sample from easy queries, ReCo only marginally improves on the baseline. This further verifies that most queries are redundant.  <ref type="table">Table 4</ref>: mean IoU validation performance on 20 labelled CityScapes dataset based on different query and key sampling strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared to Feature Bank Methods</head><p>We have experimented with ReCo with a stored feature bank framework similar to the design in the concurrent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31]</ref>. We found that just by replacing our batch-wise sampling method with a feature bank sampling method will achieve a similar performance (49.34 mIoU) compared to our original design (49.86 mIoU) on 20 labelled CityScapes, but with a slower training speed. This verifies our assumption that batch-wise sampling is an accurate approximation of class distribution over the entire dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Visualisations and Interpretability of Class Relationships</head><p>In this section, we visualise the pair-wise semantic class relation graph defined in Eq. 3, additionally supported by a semantic class dendrogram using the off-the-shelf hierarchical clustering algorithm in SciPy <ref type="bibr" target="#b29">[30]</ref> for better visualisation. The features for each semantic class used in both visualisations are averaged across all available pixel embeddings in each class from the validation set. In all visualisations, we compared features learned with ReCo on top of supervised learning compared to a standard supervised learning method trained on all labelled data, representing the semantic class relationships of the full dataset.</p><p>Using the same definitions in Section 3.1, we first choose such pixel embedding to be the embedding Z predicted from the encoder network ? which used for pixel-level classification in both supervised and ReCo method. We also show the visualisation for embedding R which is the actual representation we used for ReCo loss and active sampling.</p><p>In <ref type="figure" target="#fig_5">Fig. 8</ref>, we present the semantic class relationship and dendrogram for the CityScapes dataset by embedding R and Z with and without ReCo. We can clearly see that ReCo helps disentangle features compared to supervised learning in which many pairs of semantic classes are similar. In addition, we find that the dendrogram generated by ReCo based on embedding Z is more structured, showing a clear and interpretable semantic tree by grouping semantically similar classes together: for example, all large transportation classes car, truck, bus and train are under the same parent branch. In addition, we find that nearly all classes based on embedding R are perfectly disentangled, except for bus and train, suggesting the CityScapes dataset might not have sufficient bus and train examples to learn a distinctive representation between these two classes.</p><p>The pair-wise relation graph helps us to understand the distribution of semantic classes in each dataset, and clarifies the pattern of incorrect predictions from the trained semantic network. We additionally provide a dendrogram based on embedding R for the SUN RGB-D dataset, clearly showing ambiguous class pairs, such as night stand and dresser; table and desk; floor and floormat, consistent with our results shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. Complete visualisations of these semantic class relationships are shown in Appendix E.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have presented ReCo, a new pixel-level contrastive framework with active sampling, designed specifically for semantic segmentation. ReCo can improve performance in supervised or semi-supervised semantic segmentation methods with minimal additional memory footprint. In particular, ReCo has shown its strongest effect in semi-supervised learning with very few labels, where we improved on the previous state-of-the-art by a large margin. In further work, we aim to design effective contrastive frameworks for video representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>We trained all methods with SGD optimiser with learning rate 2.5 ? 10 ?3 , momentum 0.9, and weight decay 5 ? 10 ?4 . We adopted the polynomial annealing policy to schedule the learning rate, which is multiplied by (1 ? iter total_iter ) power with power = 0.9, and trained for 40k iterations for all datasets.</p><p>For CityScapes, we first downsampled all images in the dataset to half resolution [512 ? 1024] prior to use. We extracted [512 ? 512] random crops and used a batch size of 2 during training.</p><p>For Pascal VOC, we extracted [321 ? 321] random crops, applied a random scale between [0.5, 1.5], and used a batch size of 10 during training.</p><p>For SUN RGB-D, we first rescaled all images to [384 ? 512] resolution, extracted [321 ? 321] random crops, applied a random scale between [0.5, 1.5], and used a batch size of 5. We additionally re-organised the original training and validation split in SUN RGB-D dataset from 5285 and 5050 to 9860 and 475 samples respectively, to increase the amount of training data which we think is more appropriate for semi-supervised task.</p><p>All datasets were additionally augmented with Gaussian blur, colour jittering, and random horizontal flip. The pre-processing for CityScapes and Pascal VOC are consistent with the prior work <ref type="bibr" target="#b23">[24]</ref>.</p><p>In our ReCo framework, we sampled 256 query samples and 512 key samples and used temperature ? = 0.5 for each mini-batch, which we found to work well in all datasets. The dimensionality for pixel-level representation was set to m = 256. The confidence thresholds were set to ? w = 0.7 and ? s = 0.97.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results on Semi-supervised Segmentation Benchmarks</head><p>Here, we present quantitative results for other semi-supervised semantic segmentation benchmarks in CityScapes and Pascal VOC datasets. Note that, this benchmark is much less challenging compared to our proposed benchmark in Section 4.2, evaluted with significantly less number of labelled images. Since some methods applied with different backbones and training strategies, we compared each result with respect to its performance gap compared to its corresponding fully supervised result, as shown in brackets, to ensure fairness following <ref type="bibr" target="#b7">[8]</ref>.</p><p>In <ref type="table">Table 5</ref>, we show results for ReCo applied on top of ClassMix, and trained with both DeepLabv2 and DeepLabv3+. We can observe that ReCo achieved the best performances in most cases in both datasets, showing its robustness to different backbone architectures and number of labelled training images. <ref type="table">Table 5</ref>: mean IoU validation performance in semi-supervsed Pascal VOC and CityScapes datasets. We list the percentage along with the number of labelled training images at the top of each column. The first and second best performances in each setting are coloured in red and orange respectively. * trained images in doubled resolution. All results were taken from the corresponding publications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Visualisation on Pascal VOC (Trained with 60 Labelled Images)</head><p>In the full label setting, the baselines Supervised and ClassMix are very prone to completely misclassifying rare objects such as boat, bottle and table, while our method can predict these rare classes accurately. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Visualisation on CityScapes (Trained with 1% Labelled Pixel)</head><p>In the partial label setting, the performance improvements are less pronounced compared to the full label setting in CityScapes dataset. The improvements typically come from the more accurate predictions in small object boundaries such as in traffic light and traffic sign. Learning semantics with partial labels with minimal boundary information remains an open research question and still has huge scope for improvements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Visualisation on Semantic Class Relationship from Pascal VOC (top) and SUN RGB-D (bottom)</head><p>We show features learned by ReCo are more disentangled compared to the Supervised baseline in all datasets, which helps the segmentation model to learn a better decision boundary. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Easy and hard queries (shown in white) determined from the predicted confidence map in the Cityscapes dataset. Here we set the confidence threshold ? s = 0.97.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Example of training labels for Pascal VOC dataset in Partial Labels Full Dataset setting. (1 Pixel is zoomed 5 times for better visualisation.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualisation of Cityscapes (top) and SUN RGB-D (bottom) validation set trained on 20 and 50 labelled images respectively. Interesting regions are shown in white arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>mean IoU validation performance on 20 labelled CityScapes dataset based on different choices of hyper-parameters. Grey: ClassMix (if not labelled otherwise) in our default setting. Light Blue: ReCo + ClassMix (if not labelled otherwise) in a different hyperparameter setting. Dark Blue: ReCo + ClassMix in our default setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Visualisation of semantic class relation graph (top) and its corresponding semantic class dendrogram (bottom) on CityScapes dataset. Brighter colour represents closer (more confused) relationship. Best viewed in zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>b o o k s b o o k s h e l f s h e l v e s p a p e r b o x p i c t u r e c e i l i n g t o i l e t l a m p m i r r o r t v f r i d g e w h i t e b o a r d w a l l d o o r w i n d o w b l i n d s b e d p i l l o w b a t h t u b c o u n t e r s i n k t a b l e d e s k c a b i n e t d r e s s e r n i g h t s t a n d p e r s o n b a g c l o t h e s t o w e l c u r t a i n s h o w e r c u r t a i n f l o o r f l o o r m a t c h a i r s o fFigure 9 :</head><label>9</label><figDesc>Visualisation of semantic class dendrogram based on embedding R on SUN RGB-D dataset using ReCo + Supervised method. Best viewed in zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>labels 600 labels all labels 20 labels 50 labels 150 labels all labels 50 labels 150 labels 500 labels all labelsSupervised 37.79 ?2.68 53.87 ?1.63 64.04 ?0.55 77.79 ?0.19 38.12 ?0.08 45.42 ?1.13 54.93 ?0.35 70.48 ?0.40 19.79 ?0.26 28.78 ?0.40 37.73 ?1.16 51.06 ?0.73 S4GAN 47.95 ?4.87 61.25 ?0.62 66.21 ?0.94 -37.65 ?0.62 47.08 ?0.77 56.46 ?0.65 -20.53 ?0.61 29.79 ?0.83 38.08 ?0.77 -CutOut 52.96 ?1.90 63.57 ?0.68 69.85 ?0.88 -42.52 ?0.77 50.15 ?0.19 59.42 ?1.02 -25.94 ?2.48 34.45 ?2.75 41.25 ?0.27 -CutMix 53.71 ?2.46 66.95 ?1.44 72.42 ?0.90 -44.02 ?0.06 54.72 ?1.20 62.24 ?0.80 -27.60 ?2.98 37.55 ?0.88 42.69 ?0.40 -ClassMix 49.06 ?8.69 67.95 ?1.91 72.50 ?1.34 -45.61 ?0.69 55.56 ?0.93 63.94 ?0.08 -28.42 ?2.84 37.55 ?1.55 42.46 ?0.56 ClassMix 53.31 ?5.01 69.81 ?1.52 72.75 ?0.13 -49.86 ?0.28 57.69 ?1.26 65.04 ?0.20 -29.65 ?1.61 39.14 ?1.29 44.55 ?0.18 -</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Pascal VOC</cell><cell></cell><cell></cell><cell cols="2">CityScapes</cell><cell></cell><cell></cell><cell cols="2">SUN RGB-D</cell><cell></cell></row><row><cell>Method</cell><cell cols="12">60 labels 200 -</cell></row><row><cell>ReCo + Supervised</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.39 ?0.69</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>71.45 ?0.31</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.01 ?0.54</cell></row><row><cell>ReCo +</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: mean IoU validation performance for Pas-</cell></row><row><cell>cal VOC with data partition and training strategy</cell></row><row><cell>proposed in PseudoSeg</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>?0.14 66.17 ?0.21 69.16 ?0.14 73.75 ?0.48 CutMix 63.50 ?0.89 70.83 ?0.40 73.04 ?0.16 75.64 ?0.20 ClassMix 63.69 ?0.43 71.04 ?0.18 72.90 ?0.41 75.79 ?0.35 ReCo + ClassMix 66.11 ?0.41 72.67 ?0.18 74.09 ?0.04 75.96 ?0.29 ?0.47 52.89 ?0.47 56.65 ?0.22 63.43 ?0.44 CutMix 46.91 ?0.26 54.90 ?0.60 59.69 ?0.33 65.61 ?0.36 ClassMix 47.42 ?0.13 56.68 ?0.36 60.96 ?0.68 66.46 ?0.23 ReCo + ClassMix 49.66 ?0.51 58.97 ?0.24 62.32 ?0.77 66.92 ?0.31</figDesc><table><row><cell cols="2">Background Aeroplane</cell><cell>Bicycle</cell><cell>Bird</cell><cell>Boat</cell><cell>Bottle</cell><cell>Bus</cell><cell>Car</cell><cell>Cat</cell><cell>Chair</cell><cell>Cow</cell></row><row><cell>Table</cell><cell>Dog</cell><cell>Horse</cell><cell>Motorbike</cell><cell>Person</cell><cell>Plant</cell><cell>Sheep</cell><cell>Sofa</cell><cell>Train</cell><cell>Monitor</cell><cell>Undefined</cell></row><row><cell>Method Supervised</cell><cell cols="10">Pascal VOC (Partial) 1 pixel 1% labels 5% labels 25% labels 60.33 CityScapes (Partial) Method 1 pixel 1% labels 5% labels 25% labels Supervised 44.08</cell></row></table><note>We first evaluate the performance by varying different num- ber of queries and keys used in ReCo framework, whilst fixing all other hyper-parametersFigure 6: Visualisation of Pascal validation set with ClassMix (left) vs. with ReCo (right) trained on 1 labelled pixel per class per image. Interesting regions are shown in white arrows.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work has been supported by Dyson Technology Ltd. We thank Zhe Lin for the initial discussion and Zhengyang Feng for his help on the evaluation metric design.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semisupervised semantic segmentation with pixel-level contrastive learning from a classwise memory bank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sabater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Montesano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dmt: Dynamic mutual training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuequan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08514</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation needs strong, high-dimensional perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Ting Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen</forename><forename type="middle">Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guided collaborative training for pixel-wise semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><forename type="middle">W H</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<title level="m">Supervised contrastive learning. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Featmatch: Feature-based augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jo?o Paulo Papa, and Christoph Palm. Semi-supervised segmentation based on error-correcting supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Antonio De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rauber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and low-level consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised learning of dense visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Benmalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Golemo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<meeting>the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weightaveraged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scipy 1.0: fundamental algorithms for scientific computing in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauli</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Gommers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Travis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Oliphant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Haberland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeni</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pearu</forename><surname>Burovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weckesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring cross-image pixel contrast for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Looking beyond single images for contrastive semantic segmentation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Contrastive learning for label efficient semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raviteja</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">Andrew</forename><surname>Mansfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Shapira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via selftraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pseudoseg: Designing pseudo labels for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Jia-Bin Huang, and Tomas Pfister</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
