<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TOOD: Task-aligned One-stage Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjian</forename><surname>Feng</surname></persName>
							<email>feng.chengjian@intellif.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
							<email>zhongyujie@meituan.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
							<email>mscott@malongtech.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malong</forename><surname>Llc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
							<email>weilin.hwl@alibaba-inc.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Intellifusion Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Meituan Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Yu Gao ByteDance Inc</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TOOD: Task-aligned One-stage Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-stage object detection is commonly implemented by optimizing two sub-tasks: object classification and localization, using heads with two parallel branches, which might lead to a certain level of spatial misalignment in predictions between the two tasks. In this work, we propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the two tasks in a learning-based manner. First, we design a novel Task-aligned Head (T-Head) which offers a better balance between learning task-interactive and task-specific features, as well as a greater flexibility to learn the alignment via a task-aligned predictor. Second, we propose Task Alignment Learning (TAL) to explicitly pull closer (or even unify) the optimal anchors for the two tasks during training via a designed sample assignment scheme and a task-aligned loss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a 51.1 AP at single-model single-scale testing. This surpasses the recent one-stage detectors by a large margin, such as ATSS [31] (47.7 AP), GFL [14] (48.2 AP), and PAA [9] (49.0 AP), with fewer parameters and FLOPs. Qualitative results also demonstrate the effectiveness of TOOD for better aligning the tasks of object classification and localization. Code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection aims to localize and recognize objects of interest from natural images, and is a fundamental yet challenging task in computer vision. It is commonly formulated as a multi-task learning problem by jointly optimizing object classification and localization <ref type="bibr">[4,</ref><ref type="bibr">6,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>. The classification task is designed to learn discriminative features that focus on the key or salient part of an object, * Equal contributions. ? Corresponding author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result</head><p>Score IoU <ref type="figure">Figure 1</ref>. Illustration of detection results ('Result') and spatial distributions of classification scores ('Score') and localization scores ('IoU') predicted by ATSS <ref type="bibr" target="#b30">[31]</ref> (top row) and the proposed TOOD (bottom row). Ground-truth is indicated by yellow boxes, and a white arrow means the main direction of the best anchor away from the center of an object. In the 'Result' column, a red/green patch is the location of the best anchor for classification/localization, while a red/green box means an object bounding box predicted from the anchor in the red/green patch (if they coincide, we only show the red patches and boxes).</p><p>while the localization task works on precisely locating the whole object with its boundaries. Due to the divergence of learning mechanisms for classification and localization, spatial distributions of the learned features by the two tasks can be different, causing a certain level of misalignment when predictions are made by using two separate branches.</p><p>Recent one-stage object detectors attempted to predict consistent outputs of the two separate tasks, by focusing on the center of an object <ref type="bibr">[3,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. They assume that an anchor (i.e., an anchor-point for an anchor-free detector, or an anchor-box for an anchor-based detector) at the center of the object is likely to give more accurate predictions for both classification and localization. For example, recent FCOS <ref type="bibr" target="#b26">[27]</ref> and ATSS <ref type="bibr" target="#b30">[31]</ref> both use a centerness branch to enhance classification scores predicted from the anchors near the center of the object, and assign larger weights to the localization loss for the corresponding anchors. Besides, FoveaBox <ref type="bibr">[10]</ref> regards the anchors inside a predefined central region of the object as positive samples. Such heuristic designs have achieved excellent results, but these methods might suffer from two limitations:</p><p>(1) Independence of classification and localization. Recent one-stage detectors perform object classification and localization independently by using two separate branches in parallel (i.e., heads). Such a two-branch design might cause a lack of interaction between the two tasks, leading to an inconsistency in predictions when performing them. As shown in the 'Result' column in <ref type="figure">Figure 1</ref>, an ATSS detector recognizes an object of 'Dining table' (indicated by the anchor shown with a red patch), but localizes another object of 'Pizza' more accurately (red bounding box).</p><p>(2) Task-agnostic sample assignment. Most anchor-free detectors use a geometry-based assignment scheme to select anchor-points near the center of an object for both classification and localization <ref type="bibr">[3,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b30">31]</ref>, while anchor-based detectors often assign anchor-boxes by computing IoUs between the anchor boxes and ground truth <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>. However, the optimal anchors for classification and localization are often inconsistent, and may vary considerably depending on the shape and characteristics of the objects. The widely used sample assignment scheme is task agnostic, and thus may be difficult to make an accurate yet consistent prediction for the two tasks, as demonstrated in 'Score' and 'IoU' distributions of ATSS in <ref type="figure">Figure 1</ref>. The 'Result' column also illustrates that a spatial location of the best localization anchor (green patch) can be not at the center of the object, and it is not well aligned with the best classification anchor (red patch). As a result, a precise bounding box may be suppressed by the less accurate one during Non-Maximum Suppression (NMS).</p><p>To address such limitations, we propose a Task-aligned One-stage Object Detection (TOOD) that aims to align the two tasks more accurately by designing a new head structure with an alignment-oriented learning approach:</p><p>Task-aligned head. In contrast to the conventional head in one-stage object detection where classification and localization are implemented separately by using two branches in parallel, we design a Task-aligned head (T-head) to enhance an interaction between the two tasks. This allows the two tasks to work more collaboratively, which in turn aligns their predictions more accurately. T-head is conceptually simple: it computes task-interactive features, and makes predictions via a novel Task-Aligned Predictor (TAP). Then it aligns spatial distributions of the two predictions according to the learning signals provided by a task alignment learning, as described next.</p><p>Task alignment learning. To further overcome the misalignment problem, we propose a Task Alignment Learning (TAL) to explicitly pull closer the optimal anchors for the two tasks. It is performed by designing a sample assignment scheme and a task-aligned loss. The sample assignment collects training samples (i.e., positives or negatives) by computing a degree of task-alignment at each anchor, whereas the task-aligned loss gradually unifies the best anchors for predicting both classification and localization during the training. Therefore, at inference, a bounding box with the highest classification score and jointly having the most precise localization can be preserved.</p><p>The proposed T-head and learning strategy can work collaboratively towards making predictions with high quality in both classification and localization. The main contributions of this work can be summarized as follows: (1) we design a new T-head to enhance the interaction between classification and localization while maintaining their characteristics, and further align the two tasks at the predictions;</p><p>(2) we propose TAL to explicitly align the two tasks at the identified task-aligned anchors, as well as providing learning signals for the proposed predictor; (3) we conducted extensive experiments on MSCOCO <ref type="bibr" target="#b16">[17]</ref>, where our TOOD achieved a 51.1 AP, surpassing recent one-stage detectors such as ATSS <ref type="bibr" target="#b30">[31]</ref>, GFL <ref type="bibr" target="#b13">[14]</ref> and PAA <ref type="bibr">[9]</ref>, by a large margin. Qualitative results further validate the effectiveness of our task-alignment approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>One-stage detectors. OverFeat <ref type="bibr" target="#b24">[25]</ref> is one of the earliest CNN-based one-stage detectors. Afterward, YOLO <ref type="bibr" target="#b21">[22]</ref> was developed to directly predict bounding boxes and classification scores, without an additional stage to generate region proposals. SSD <ref type="bibr" target="#b17">[18]</ref> introduces anchors with multiscale predictions from multi-layer convolutional features, and Focal loss <ref type="bibr" target="#b15">[16]</ref> was proposed to address the problem of class imbalance for one-stage detectors like RetinaNet. Keypoint-based detection methods, such as <ref type="bibr">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>, address the detection problem by identifying and grouping multiple key points of a bounding box. Recently, FCOS <ref type="bibr" target="#b26">[27]</ref> and FoveaBox <ref type="bibr">[10]</ref> were developed to locate objects of interest via anchor-points and point-to-boundary distances. Most mainstream one-stage detectors are composed of two FCN-based branches for classification and localization, which may lead to the misalignment between the two tasks. In this paper, we enhance the alignment between the two tasks via a new head structure and an alignmentoriented learning approach.</p><p>Training sample assignment. Most anchor-based detectors such as <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref>, collect training samples by computing IoUs between proposals and ground truth, while an anchorfree detector regards the anchors inside the center region of an object as positive samples <ref type="bibr">[3,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b26">27]</ref>. Recent stud-  ies attempted to train the detectors more effectively by collecting more informative training samples using output results. For example, FSAF <ref type="bibr" target="#b35">[36]</ref> selects meaningful samples from feature pyramids based on the computed loss, and similarly, SAPD <ref type="bibr" target="#b34">[35]</ref> provides a soft-selection version of FSAF by designing a meta-selection network. FreeAnchor <ref type="bibr" target="#b31">[32]</ref> and MAL <ref type="bibr">[8]</ref> identify the best anchor-box by computing the losses in an effort to improve the matching between anchors and objects. PAA <ref type="bibr">[9]</ref> adaptively separates the anchors into positive and negative samples by fitting a probability distribution to the anchor scores. Mutual Guidance <ref type="bibr" target="#b28">[29]</ref> improves anchor assignment for one task by considering the prediction quality on the other task. Different from the positive/negative sample assignment, PISA <ref type="bibr">[1]</ref> re-weights the training samples according to the precision rank of the outputs. Noisy Anchor <ref type="bibr" target="#b11">[12]</ref> assigns soft labels to the training samples, and re-weights the anchor-boxes using a cleanliness score to mitigate the noise incurred by binary labels. GFL <ref type="bibr" target="#b13">[14]</ref> replaces the binary classification label with an IoU score to integrate the localization quality into classification. These excellent approaches inspired the current work to develop a new assignment mechanism from a taskalignment point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Task-aligned One-stage Object Detection</head><p>Overview. Similar to recent one-stage detectors such as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>, the proposed TOOD has an overall pipeline of 'backbone-FPN-head'. Moreover, by considering effi-ciency and simplicity, TOOD uses a single anchor per location (same as ATSS <ref type="bibr" target="#b30">[31]</ref>), where the 'anchor' means an anchor point for an anchor-free detector, or an anchor box for an anchor-based detector. As discussed, existing onestage detectors have limitations of task misalignment between classification and localization, due to the divergence of two tasks which are often implemented using two separate head branches. In this work, we propose to align the two tasks more explicitly using a designed Task-aligned head (T-head) with a new Task Alignment Learning (TAL). As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, T-head and TAL can work collaboratively to improve the alignment of two tasks. Specifically, T-head first makes predictions for the classification and localization on the FPN features. Then TAL computes task alignment signals based on a new task alignment metric which measures the degree of alignment between the two predictions. Lastly, T-head automatically adjusts its classification probabilities and localization predictions using learning signals computed from TAL during back propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task-aligned Head</head><p>Our goal is to design an efficient head structure to improve the conventional design of the head in one-stage detectors (as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(a)). In this work, we achieve this by considering two aspects: (1) increasing the interaction between the two tasks, and (2) enhancing the detector ability of learning the alignment. The proposed T-head is shown in <ref type="figure" target="#fig_2">Figure 3</ref>(b), where it has a simple feature extractor with two Task-Aligned Predictors (TAP).</p><p>To enhance the interaction between classification and localization, we use a feature extractor to learn a stack of taskinteractive features from multiple convolutional layers, as shown by the blue part in <ref type="figure" target="#fig_2">Figure 3</ref>(b). This design not only facilitates the task interaction, but also provides multi-level features with multi-scale effective receptive fields for the two tasks. Formally, let X f pn ? R H?W ?C denotes the FPN features, where H, W and C indicate height, width and the number of channels, respectively. The feature extractor uses N consecutive conv layers with activation functions to compute the task-interactive features:</p><formula xml:id="formula_0">X inter k = ?(conv k (X f pn )), k = 1 ?(conv k (X inter k?1 )), k &gt; 1 , ?k ? {1, 2, ..., N },<label>(1)</label></formula><p>where conv k and ? refer to the k-th conv layer and a relu function, respectively. Thus we extract rich multi-scale features from the FPN features using a single branch in the head. Then, the computed task-interactive features will be fed into two TAP for aligning classification and localization.</p><p>Task-aligned Predictor (TAP). We perform both object classification and localization on the computed taskinteractive features, where the two tasks can well perceive the state of each other. However, due to the single branch </p><formula xml:id="formula_1">LAM H ? W ? C Cat &amp; GAP Fc Fc &amp; Sigmoid H ? W ? NC NC C / 8 N Cat H ? W ? C H H H H ? W ? C H ? W ? C C H ? W ? C C Conv ?4 Conv ?4 H ? W ? C fpn X TAP Conv ?N TAP H ? W ? 80 H ? W ? 4 H ? W ? C H ? W ? C Cat GAP Fc Sigmoid Cat Conv A Cat &amp; Conv (Sigmoid) Cat GAP Fc Sigmoid Cat Conv H ? W ? C H ? W ? C H ? W ? C A H ? W ? C H ? W ? C Cat &amp; Conv (Sigmoid)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer attention</head><p>Task-aware prediction</p><formula xml:id="formula_2">H ? W ? 80 /4 H ? W ? 80 /4 H ? W ? 1 /8 H ? W ? C H ? W ? C Prediction alignment Cat GAP Fc Sigmoid A Cat &amp; Conv (Sigmoid)</formula><p>Layer attention </p><formula xml:id="formula_3">H ? W ? 80 /4 H ? W ? 80 /4 H ? W ? C H ? W ? C H ? W ? 1</formula><formula xml:id="formula_4">H ? W ? 4 H ? W ? C H ? W ? C Cat GAP Fc Sigmoid Cat Conv A Cat &amp; Conv (Sigmoid) Cat GAP Fc Sigmoid Cat Conv H ? W ? C H ? W ? C H ? W ? C A H ? W ? C H ? W ? C Cat &amp; Conv (Sigmoid)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer attention</head><p>Task-aware prediction</p><formula xml:id="formula_5">H ? W ? 80 /4 H ? W ? 80 /4 H ? W ? 1 /8 H ? W ? C H ? W ? C Prediction alignment Cat GAP Fc Sigmoid A Cat &amp; Conv (Sigmoid)</formula><p>Layer attention  design, the task-interactive features inevitably introduce a certain level of feature conflicts between two different tasks, which have also been discussed in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>. Intuitively, the tasks of object classification and localization have different targets, and thus focus on different types of features (e.g., different levels or receptive fields). Consequently, we propose a layer attention mechanism to encourage task decomposition by dynamically computing such task-specific features at the layer level. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>(c), the taskspecific features are computed separately for each task of classification or localization:</p><formula xml:id="formula_6">H ? W ? 80 /4 H ? W ? 80 /4 H ? W ? C H ? W ? C H ? W ? 1</formula><formula xml:id="formula_7">X task k = w k ? X inter k , ?k ? {1, 2, ..., N },<label>(2)</label></formula><p>where w k is the k-th element of the learned layer attention w ? R N . w is computed from the cross-layer taskinteractive features, and is able to capture the dependencies between layers:</p><formula xml:id="formula_8">w = ?(f c 2 (?(f c 1 (x inter )))),<label>(3)</label></formula><p>where f c 1 and f c 2 refer to two fully-connected layers. ? is a sigmoid function, and x inter is obtained by applying an average pooling to X inter which are the concatenated features of X inter k . Finally, the results of classification or localization are predicted from each X task :</p><formula xml:id="formula_9">Z task = conv 2 (?(conv 1 (X task ))),<label>(4)</label></formula><p>where X task is the concatenated features of X task k , and conv 1 is a 1?1 conv layer for dimension reduction. Z task is then converted into dense classification scores P ? R H?W ?80 using sigmoid function, or object bounding boxes B ? R H?W ?4 with distance-to-bbox conversion as applied in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Prediction alignment. At the prediction step, we further align the two tasks explicitly by adjusting the spatial distributions of the two predictions: P and B. Different from the previous works using a centerness branch <ref type="bibr" target="#b26">[27]</ref> or an IoU branch <ref type="bibr">[9]</ref> which can only adjust the classification prediction based on either classification features or localization features, we align the two predictions by considering both tasks jointly using the computed task-interactive features. Notably, we perform the alignment method separately on the two tasks. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>(c), we use a spatial probability map M ? R H?W ?1 to adjust the classification prediction:</p><formula xml:id="formula_10">P align = ? P ? M ,<label>(5)</label></formula><p>where M is computed from the interactive features, allowing it to learn a degree of consistency between the two tasks at each spatial location. Meanwhile, to make an alignment on localization prediction, we further learn spatial offset maps O ? R H?W ?8 from the interactive features, which are used to adjust the predicted bounding box at each location. Specifically, the learned spatial offset enables the most aligned anchor point to identify the best boundary predictions around it: <ref type="bibr">(6)</ref> where an index (i, j, c) denotes the (i, j)-th spatial location at the c-th channel in a tensor. Eq.(6) is implemented by bilinear interpolation, and its computational overhead is negligible due to the very small channel dimension of B. Noteworthily, offsets are learned independently for each channel, which means each boundary of the object has its own learned offset. This allows for a more accurate prediction of the four boundaries because each of them can individually learn from the most precise anchor point near it. Therefore, our method not only aligns the two tasks, but also improves the localization accuracy by identifying a precise anchor point for each side.</p><formula xml:id="formula_11">B align (i, j, c) = B(i + O(i, j, 2 ? c), j + O(i, j, 2 ? c + 1), c),</formula><p>The alignment maps M and O are learned automatically from the stack of interactive features: M = ?(conv 2 (?(conv 1 (X inter ))))</p><p>O = conv 4 (?(conv 3 (X inter )))</p><p>where conv 1 and conv 3 are two 1?1 conv layers for dimension reduction. The learning of M and O is performed by using the proposed Task Alignment Learning (TAL) which will be described next. Notice that our T-head is an independent module and can work well without TAL. It can be readily applied to various one-stage object detectors in a plug-and-play manner to improve detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Task Alignment Learning</head><p>We further introduce a Task Alignment Learning (TAL) that further guides our T-head to make task-aligned predictions. TAL differs from previous methods <ref type="bibr">[1,</ref><ref type="bibr">8,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref> in two aspects. First, from the task-alignment point of view, it dynamically selects high-quality anchors based on a designed metric. Second, it considers both anchor assignment and weighting simultaneously. It comprises a sample assignment strategy and new losses designed specifically for aligning the two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Task-aligned Sample Assignment</head><p>To cope with NMS, the anchor assignment for a training instance should satisfy the following rules: (1) a well-aligned anchor should be able to predict a high classification score with a precise localization jointly; (2) a misaligned anchor should have a low classification score and be suppressed subsequently. With the two objectives, we design a new anchor alignment metric to explicitly measure the degree of task-alignment at the anchor level. The alignment metric is integrated into the sample assignment and loss functions to dynamically refine the predictions at each anchor.</p><p>Anchor alignment metric. Considering that a classification score and an IoU between the predicted bounding box and the ground truth indicate the quality of the predictions by the two tasks, we measure the degree of task-alignment using a high-order combination of the classification score and the IoU. To be specific, we design the following metric to compute anchor-level alignment for each instance:</p><formula xml:id="formula_14">t = s ? ? u ? ,<label>(9)</label></formula><p>where s and u denote a classification score and an IoU value, respectively. ? and ? are used to control the impact of the two tasks in the anchor alignment metric. Notably, t plays a critical role in the joint optimization of the two tasks towards the goal of task-alignment. It encourages the networks to dynamically focus on high-quality (i.e., taskaligned) anchors from the perspective of joint optimization.</p><p>Training sample assignment. As discussed in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, training sample assignment is crucial to the training of object detectors. To improve the alignment of two tasks, we focus on the task-aligned anchors, and adopt a simple assignment rule to select the training samples: for each instance, we select m anchors having the largest t values as positive samples, while using the remaining anchors as negative ones. Again, the training is performed by computing new loss functions designed specifically for aligning the tasks of classification and localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Task-aligned Loss</head><p>Classification objective. To explicitly increase classification scores for the aligned anchors, and at the same time, reduce the scores of the misaligned ones (i.e., having a small t), we use t to replace the binary label of a positive anchor during training. However, we found that the network cannot converge when the labels (i.e., t) of the positive anchors become small with the increase of ? and ?. Therefore, we use a normalized t, namelyt, to replace the binary label of the positive anchor, wheret is normalized by the following two properties: (1) to ensure effective learning of hard instances (which usually have a small t for all corresponding positive anchors); <ref type="bibr">(2)</ref> to preserve the rank between instances based on the precision of the predicted bounding boxes. Thus, we adopt a simple instance-level normalization to adjust the scale oft: the maximum oft is equal to the largest IoU value (u) within each instance. Then Binary Cross Entropy (BCE) computed on the positive anchors for the classification task can be rewritten as,</p><formula xml:id="formula_15">L cls pos = Npos i=1 BCE(s i ,t i ),<label>(10)</label></formula><p>where i denotes the i-th anchor from the N pos positive anchors corresponding to one instance. Following <ref type="bibr" target="#b15">[16]</ref>, we employ a focal loss for classification to mitigate the imbalance between the negative and positive samples during training. The focal loss computed on the positive anchors can be reformulated by Eq.(10), and the final loss function for the classification task is defined as follows:</p><formula xml:id="formula_16">L cls = Npos i=1 t i ? s i ? BCE(s i ,t i ) + Nneg j=1 s ? j BCE(s j , 0),<label>(11)</label></formula><p>where j denotes the j-th anchor from the N neg negative anchors, and ? is the focusing parameter <ref type="bibr" target="#b15">[16]</ref>.</p><p>Localization objective. A bounding box predicted by a well-aligned anchor (i.e., having a large t) usually has both a large classification score with a precise localization, and such a bounding box is more likely to be preserved during NMS. In addition, t can be applied for selecting highquality bounding boxes by weighting the loss more carefully to improve the training. As discussed in <ref type="bibr" target="#b20">[21]</ref>, learning from high-quality bounding boxes is beneficial to the performance of a model, while the low-quality ones often have a negative impact on the training by producing a large amount of less informative and redundant signals to update the model. In our case, we apply the t value for measuring the quality of a bounding box. Thus, we improve the task alignment and regression precision by focusing on the wellaligned anchors (with a large t), while reducing the impact of the misaligned anchors (with a small t) during bounding box regression. Similar to the classification objective, we re-weight the loss of bounding box regression computed for each anchor based ont, and a GIoU loss (L GIoU ) <ref type="bibr" target="#b23">[24]</ref> can be reformulated as follows:</p><formula xml:id="formula_17">L reg = Npos i=1t i L GIoU (b i ,b i ),<label>(12)</label></formula><p>where b andb denote the predicted bounding boxes and the corresponding ground-truth boxes. The total training loss for TAL is the sum of L cls and L reg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>Dataset and evaluation protocol. All experiments are implemented on the large-scale detection benchmark MS-COCO 2017 <ref type="bibr" target="#b16">[17]</ref>. Following the standard practice <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, we use the trainval135k set (115K images) for training and minival set (5K images) as validation for our ablation study. We report our main results on the test-dev set for comparison with the state-of-the-art detectors. The performance is measured by COCO Average Precision (AP) <ref type="bibr" target="#b16">[17]</ref>.</p><p>Implementation details. As with most one-stage detectors <ref type="bibr">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref>, we use the detection pipeline of 'backbone-FPN-head', with different backbones including ResNet-50, ResNet-101 and ResNeXt-101-64?4d pretrained on ImageNet <ref type="bibr">[2]</ref>. Similar to ATSS <ref type="bibr" target="#b30">[31]</ref>, TOOD tiles one anchor per location. Unless specified, we report experimental results of an anchor-free TOOD (an anchor-based TOOD can achieve a similar performance as shown in <ref type="table">Table 3</ref>). The number of interactive layers N is set as 6 to make T-head have a similar number of parameters as the conventional parallel head, and the focusing parameter ? is set to 2 as used in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>. More implementation and training details are presented in Supplementary Material (SM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>For an ablation study, we use the ResNet-50 backbone and train the model for 12 epochs unless specified. The performances are reported on COCO minival set. On head structures. We compare our T-head with the conventional parallel head in <ref type="table" target="#tab_3">Table 1</ref>. It can be adopted in various one-stage detectors in a plug-and-play manner, and consistently outperforms the conventional head by 0.7 to 1.9 AP, with fewer parameters and FLOPs. This validates the effectiveness of our design, and demonstrates that T-head can work more efficiently with higher performance, by introducing task interaction and prediction alignment.</p><p>On sample assignments. To demonstrate the effectiveness of TAL, we compare TAL with other learning methods using different sample assignment methods, as shown in <ref type="table" target="#tab_4">Table 2</ref>. Training sample assignment can be divided into the fixed assignment and adaptive assignment according to whether it is a learning-based method. Different from the existing assignment methods, TAL adaptively assigns both positive and negative anchors, and at the same time, computes the weights of positive anchors more carefully, resulting in higher performance. To compare with PAA (+IoU pred.) which has an additional prediction structure, we inte-  grate TAP into TAL, resulting in a higher AP of 42.5. More discussions on the differences between TAL and previous methods are presented in SM.</p><p>TOOD. We evaluate the performance of the complete TOOD (T-head + TAL). As shown in <ref type="table">Table 3</ref>, the anchorfree TOOD and anchor-based TOOD can achieve similar performance, i.e., 42.5 AP and 42.4 AP. Compared with ATSS, TOOD improves the performance of ?3.2 AP. To be more specific, the improvements on AP 75 are significant, which yields ?3.8 points higher AP in TOOD. This validates that aligning the two tasks can improve the detection performance. Notably, TOOD brings a higher improvement (+3.3 AP) than the sum of the individual improvements by T-head + ATSS (+1.9 AP) and Parallel head + TAL (+1.1 AP), as shown in <ref type="table">Table 6</ref>. It suggests that Thead and TAL can compensate strongly to each other.</p><p>On hyper-parameters. We first investigate the performance using different values of ? and ? for TAL, which control the influence of classification confidence and localization precision on anchor alignment metric via t. Through a coarse search shown in <ref type="table">Table 4</ref>, we adopt ? = 1 and ? = 6 for our TAL. We then conduct several experiments to study the robustness of the hyper-parameter m, which is used to select positive anchors. We use different values of m in <ref type="bibr">[5,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref>, and achieve results in a range of 42.0?42.5 AP, which suggests the performance is insensitive to m. Thus, we adopt m = 13 in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State-of-the-Art</head><p>We compare our TOOD with other one-stage detectors on the COCO test-dev in  <ref type="bibr" target="#b36">[37]</ref> more efficiently, by adaptively adjusting the spatial distribution of the learned features for task-alignment. Note that in TOOD, DCN is applied to the first two layers in the head tower. As shown in <ref type="table" target="#tab_6">Table 5</ref>, TOOD achieves a new state-of-the-art result with 51.1 AP in one-stage object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Backbone AP AP50 AP75 APS APM APL  <ref type="table">Table 6</ref>. Analysis for task-alignment of TOOD with backbone ResNet-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Analysis for Task-alignment</head><p>We quantitatively analyze the effect of the proposed methods on the alignment of two tasks. Without NMS, we calculate a Pearson Correlation Coefficient (PCC) between the rankings <ref type="bibr" target="#b19">[20]</ref> of classification and localization by selecting top-50 confident predictions for each instance, and a mean IoU of the top-10 confident predictions, averaged over instances. As shown in <ref type="table">Table 6</ref>, the mean PCC and IoU are improved by using T-head and TAL. Meanwhile, with NMS, the number of the correct boxes (IoU&gt;=0.5) increases while those of the redundant (IoU&gt;=0.5) and error boxes (0.1&lt;IoU&lt;0.5) decrease substantially when applying T-head and TAL. The statistics suggest that TOOD is more compatible with NMS, by preserving more correct boxes, and suppressing the redundant/error boxes significantly. At last, detection performance is improved by 3.3 AP in total.</p><p>Several detection examples are illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we illustrate the misalignment between classification and localization in the existing one-stage detectors, and propose TOOD to align the two tasks. In particular, we design a task-aligned head to enhance the interaction of two tasks, and then improve its ability of learning the alignment. Furthermore, a new task-aligned learning strategy is developed by introducing a sample assignment scheme and new loss functions, both of which are computed via an anchor alignment metric. With these improvements, TOOD achieved a 51.1 AP on MS-COCO, surpassing the state-of-the-art one-stage detectors by a large margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>:</head><label></label><figDesc>The location of the best anchor for classification or localization : The location of the most aligned anchor in the proposed metric : Forward propagation : Back propagation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall learning mechanism of TOOD. First, predictions are made by T-head on the FPN features. Second, the predictions are used to compute a task alignment metric at each anchor point, based on which TAL produces learning signals for T-head . Lastly, T-head adjusts the distributions of classification and localization accordingly. Specifically, the most aligned anchor obtains a higher classification score through 'prob' (probability map), and acquires a more accurate bounding box prediction via a learned 'offset'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ZFigure 3 .</head><label>3</label><figDesc>task (P/B) (c) Task-aligned predictor (TAP) Comparison between the conventional parallel head and the proposed T-Head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of several detection results predicted from the best anchors for classification (in red) and localization (in green). The illustrated patches and bounding boxes correspond to that inFigure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Transfer LAM Conv ?N Conv Conv LAM fpn X</head><label></label><figDesc></figDesc><table><row><cell>Conv ?4</cell><cell>H ? W ? C</cell><cell>Sigmoid Conv</cell><cell>H ? 80 ? W</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>H ? W ? C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conv ?4</cell><cell>H ? W ? C</cell><cell>Conv</cell><cell>H ? 4 ? W</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>H ? W ? C</cell><cell></cell><cell></cell><cell>H ? W ? C</cell><cell></cell><cell>H ? W ? 80</cell><cell></cell><cell></cell><cell>H ? W ? 80</cell><cell cols="2">Conv &amp; Sigmoid</cell><cell cols="2">H ? W ? 80 PAM</cell><cell>NC inter x Cat &amp; GAP</cell><cell>Fc</cell><cell>C / 8</cell><cell cols="2">Fc &amp; Sigmoid</cell><cell>w</cell><cell>N</cell><cell>H ? W ? C</cell><cell>Cat</cell><cell>H ? W ? NC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H ? W ? 4</cell><cell cols="2">Transfer</cell><cell>H ? W ? 4</cell><cell></cell><cell></cell><cell></cell><cell>H ? W ? 4</cell><cell>LAM</cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell>spec</cell></row><row><cell></cell><cell>H ? W ? C</cell><cell>Conv ?N</cell><cell>LAM</cell><cell>H ? W ? C</cell><cell>Conv</cell><cell>H ? W ? 80</cell><cell cols="2">Transfer</cell><cell>H ? W ? 80</cell><cell>Conv Sigmoid Conv</cell><cell>H ? W ? 1 H 8 ? W ?</cell><cell>PAM H ? W ? 80</cell><cell>NC Cat &amp; GAP</cell><cell>Fc</cell><cell cols="2">Fc &amp; Sigmoid NC / 16</cell><cell>N</cell><cell>Cat</cell><cell>H ? NC ? W</cell></row><row><cell></cell><cell></cell><cell></cell><cell>LAM</cell><cell></cell><cell>Conv</cell><cell>H ? W ? 4</cell><cell cols="2">Transfer</cell><cell>H ? W ? 4</cell><cell></cell><cell></cell><cell>H ? W ? 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Transfer LAM Conv ?N Conv Conv LAM fpn X</head><label></label><figDesc></figDesc><table><row><cell>Conv ?4</cell><cell></cell><cell>H ? W ? C</cell><cell>Conv Sigmoid</cell><cell>H ? W ? 80</cell><cell></cell><cell cols="3">Classification supervision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TAP TAP</cell><cell>H ? W ? 80</cell><cell cols="3">Classification supervision</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>/8</cell></row><row><cell>H ? C ? W</cell><cell></cell><cell></cell><cell cols="2">X cls</cell><cell>P</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Conv ?N</cell><cell></cell><cell></cell><cell>P</cell><cell cols="2">Task-alignment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>M/O</cell></row><row><cell cols="2">X fpn Conv ?4</cell><cell>H ? W ? C</cell><cell>Conv</cell><cell>H ? W ? 4</cell><cell></cell><cell cols="3">Localization supervision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X fpn</cell><cell>TAP</cell><cell></cell><cell></cell><cell cols="2">Localization supervision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sigmoid x inter</cell><cell>w</cell><cell>Cat &amp; Conv (Sigmoid)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">X reg (a) Parallel head B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>: Element-wise product X inter 1?N</cell><cell>X task 1?N A : Element-wise product for classification P/B P align /B alig Spatial offset for localization</cell></row><row><cell>H ? W ? C</cell><cell></cell><cell></cell><cell></cell><cell>H ? W ? C</cell><cell></cell><cell>H ? W ? 80 H ? W ? 4</cell><cell cols="2">Transfer</cell><cell>H ? W ? 80 H ? W ? 4</cell><cell cols="2">Conv Conv &amp; Sigmoid</cell><cell></cell><cell cols="2">H ? W ? 80 PAM H 4 ? W ?</cell><cell>Cat &amp; GAP NC inter x LAM</cell><cell>Fc</cell><cell>C / 8</cell><cell cols="2">Fc &amp; Sigmoid</cell><cell>w</cell><cell>N</cell><cell></cell><cell></cell><cell>H ? W ? C</cell><cell>Cat</cell><cell>X</cell><cell>H ? W ? NC spec</cell><cell>X</cell><cell>H ? W ? C fpn</cell><cell>Conv ?4 Conv ?4</cell><cell>H ? W ? C H ? W ? C</cell><cell>Conv Conv</cell><cell>H ? W ? 80 H ? 4 ? W</cell><cell>Tran Tran</cell></row><row><cell></cell><cell></cell><cell>H ? W ? C</cell><cell>Conv ?N</cell><cell>LAM LAM</cell><cell>H ? W ? C</cell><cell>Conv Conv</cell><cell>H ? W ? 80 H ? W ? 4</cell><cell cols="2">Transfer Transfer</cell><cell>H ? W ? 80 H ? W ? 4</cell><cell cols="2">Conv Sigmoid Conv</cell><cell>H ? W ? 1 H 8 ? W ?</cell><cell>PAM H ? W ? 80 4 ? W ? H</cell><cell>NC Cat &amp; LAM GAP</cell><cell>Fc</cell><cell cols="2">Fc &amp; Sigmoid NC / 16</cell><cell>N</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H ? W ? C</cell><cell>Cat</cell><cell>H ? W ? NC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Fc</cell><cell cols="3">Fc &amp; Sigmoid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>NC</cell><cell>C / 8</cell><cell></cell><cell>N</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Cat &amp;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>H ? W ? C</cell><cell cols="3">GAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H ? W ? C</cell><cell cols="2">Cat</cell><cell cols="2">H ? W ? NC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Conv ?4</cell><cell></cell><cell>H ?</cell><cell>Conv</cell><cell>H ?</cell><cell>Transfer</cell><cell>H ?</cell><cell>Classification</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Transfer LAM Conv ?N Conv Conv LAM fpn X</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>Conv ?4</cell><cell>H ? W ? C</cell><cell>Conv Sigmoid</cell><cell cols="2">H ? W ? 80</cell><cell cols="6">Classification supervision Classification supervision</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TAP</cell><cell></cell><cell>H ? W ? 80</cell><cell cols="2">Classification supervision</cell><cell cols="2">Classification supervision</cell><cell>Cat &amp; Conv (Sigmoid)</cell><cell>/8 H ? W 1 ?</cell></row><row><cell>H ? W ? C X fpn</cell><cell>X fpn Conv ?4</cell><cell>H ? W ? C</cell><cell cols="2">X cls Conv</cell><cell>H ? W ? 4</cell><cell cols="3">P Localization supervision</cell><cell cols="3">Localization supervision</cell><cell></cell><cell></cell><cell>H ? W ? C</cell><cell cols="4">X fpn Conv ?N</cell><cell>TAP</cell><cell>H ? W ? C</cell><cell>H ? W ? 4</cell><cell cols="2">P Task-X inter 1?N alignment supervision Localization</cell><cell cols="2">supervision Localization Task-alignment</cell><cell>H ? W ? C</cell><cell>Cat GAP Layer attention Fc</cell><cell>Sigmoid x inter</cell><cell>w</cell><cell>H ? W ? C</cell><cell>/8 M/O Cat &amp; Conv (Sigmoid)</cell><cell>H ? W ? 80 /4</cell><cell>Cat &amp; Conv (Sigmoid) A H ? W ? 80 /4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">X reg</cell><cell cols="2">B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>B</cell><cell></cell><cell>: Element-wise product X inter 1?N : Element-wise product</cell><cell>A : Element-wise product for classification X task 1?N P/B P align /B align A : Element-wise product for classification Spatial offset for localization</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">(b) Task-aligned head (T-Head)</cell><cell>Spatial offset for localization</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Cat &amp; Conv (Sigmoid)</cell><cell></cell><cell></cell><cell></cell><cell>1 ? W ? H</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Prediction alignment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>/8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Cat</cell><cell></cell><cell>Fc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cat &amp; Conv (Sigmoid)</cell><cell>H ? W ? C</cell></row><row><cell>H ? W ? C</cell><cell cols="6">GAP Task-aware prediction Sigmoid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H ? W ? C</cell><cell cols="2">Cat Conv</cell><cell>H ? W ? 80 /4</cell><cell></cell><cell></cell><cell></cell><cell>A</cell><cell>H ? W ? 80 /4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H ? W ? C</cell><cell>Cat GAP</cell><cell>Fc Sigmoid</cell><cell>H ? W ? C</cell><cell>Cat Conv</cell><cell>H ? W ? C</cell><cell>A</cell><cell>C ? W ? H</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Layer attention</cell></row><row><cell></cell><cell>H ? W ? C</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>H ? W ? C</cell><cell></cell><cell></cell><cell>H ? W ? 80 H ? W ? 4</cell><cell cols="2">Transfer</cell><cell>H ? W ? 80 H ? W ? 4</cell><cell cols="2">Conv Conv &amp; Sigmoid</cell><cell></cell><cell></cell><cell cols="2">H ? W ? 80 PAM H 4 ? W ?</cell><cell></cell><cell>Cat &amp; GAP NC inter x LAM</cell><cell></cell><cell>Fc</cell><cell>C / 8</cell><cell cols="2">Fc &amp; Sigmoid</cell><cell>w</cell><cell>N</cell><cell>H ? W ? C</cell><cell>Cat</cell><cell>X</cell><cell>H ? W ? NC spec</cell><cell>X</cell><cell>H ? W ? C fpn</cell><cell>Conv ?4 Conv ?4</cell><cell>H ? W ? C H ? W ? C</cell><cell>Conv Conv</cell><cell>H ? W ? 80 H ? W ? 4</cell><cell>Transfer Transfer</cell><cell>H ? W ? 80 H ? W ? 4</cell></row><row><cell></cell><cell></cell><cell>H ? W ? C</cell><cell>Conv ?N</cell><cell></cell><cell></cell><cell>LAM LAM</cell><cell>H ? W ? C</cell><cell cols="2">Conv Conv</cell><cell>H ? W ? 80 H ? W ? 4</cell><cell cols="2">Transfer Transfer</cell><cell>H ? W ? 80 H ? W ? 4</cell><cell cols="3">Conv Sigmoid Conv</cell><cell>H ? W ? 1 H 8 ? W ?</cell><cell>PAM H ? W ? 80 4 ? W ? H</cell><cell></cell><cell>NC Cat &amp; LAM GAP</cell><cell></cell><cell>Fc</cell><cell cols="2">Fc &amp; Sigmoid NC / 16</cell><cell>N</cell><cell>H ? W ? C</cell><cell>Cat</cell><cell>H ? W ? NC</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Fc</cell><cell></cell><cell></cell><cell cols="4">Fc &amp; Sigmoid</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>NC</cell><cell></cell><cell cols="2">C / 8</cell><cell></cell><cell></cell><cell>N</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Cat &amp;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>H ? W ? C</cell><cell cols="2">GAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">H ? W ? C</cell><cell cols="2">Cat</cell><cell cols="2">H ? W ? NC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">LAM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Conv ?4</cell><cell>H ? W ? C</cell><cell>Conv</cell><cell>H ? W ? 80</cell><cell>Transfer</cell><cell>H ? W ? 80</cell><cell>Classification supervision</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv</cell><cell cols="2">H ? W</cell><cell cols="2">Transfer</cell><cell>H ? W</cell><cell></cell><cell></cell><cell></cell><cell>H ? W</cell><cell></cell><cell cols="3">Classification</cell><cell></cell><cell></cell><cell></cell><cell>H ? W ? C</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Comparisons between different head structures in various detectors. FLOPs are measured on the input image size of 1280?800.</figDesc><table><row><cell>Method</cell><cell cols="3">Head Head/full Params (M) Head/full FLOPs (G)</cell><cell cols="3">AP AP50 AP75</cell></row><row><cell>FoveaBox [10]</cell><cell>Parallel head T-head</cell><cell>4.92/36.20 4.82/36.10</cell><cell cols="2">104.87/206.28 37.3 100.79/202.20 38.0</cell><cell>56.2 56.8</cell><cell>39.7 40.5</cell></row><row><cell>FCOS w/ imprv [27]</cell><cell>Parallel head T-head</cell><cell>4.92/32.02 4.82/31.92</cell><cell cols="2">104.91/200.50 38.6 100.79/196.38 40.5</cell><cell>57.2 58.5</cell><cell>41.7 43.8</cell></row><row><cell cols="2">ATSS (anchor-based) [31] Parallel head T-head</cell><cell>4.92/32.07 4.82/31.98</cell><cell cols="2">104.87/205.21 39.3 100.79/201.13 41.1</cell><cell>57.5 58.6</cell><cell>42.8 44.5</cell></row><row><cell>ATSS (anchor-free) [31]</cell><cell>Parallel head T-head</cell><cell>4.92/32.07 4.82/31.98</cell><cell cols="2">104.87/205.21 39.2 100.79/201.13 41.1</cell><cell>57.4 58.4</cell><cell>42.2 44.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Comparisons between different schemes of training sample assignments. 'Pos/neg': positive/negative anchor assignment. 'Weight': anchor weight assignment. 'fixed': fixed assignment. 'ada': adaptive assignment. Here TAP aligns the predictions based on both classification and localization features from the last head tower. * indicates the model is trained for 18 epochs.</figDesc><table><row><cell>Anchor assignment</cell><cell cols="2">Pos/neg Weight AP AP50 AP75</cell></row><row><cell>IoU-based [16]</cell><cell>fixed</cell><cell>fixed 36.5 55.5 38.7</cell></row><row><cell>Center sampling [10]</cell><cell>fixed</cell><cell>fixed 37.3 56.2 39.3</cell></row><row><cell>Centerness [27]</cell><cell>fixed</cell><cell>fixed 37.4 56.1 40.3</cell></row><row><cell>ATSS [31]</cell><cell>fixed</cell><cell>fixed 39.2 57.4 42.2</cell></row><row><cell>PISA [1]</cell><cell>fixed</cell><cell>ada 37.3 56.5 40.3</cell></row><row><cell>NoisyAnchor [12]</cell><cell>fixed</cell><cell>ada 38.0 56.9 40.6</cell></row><row><cell>ATSS+QFL [14]</cell><cell>fixed</cell><cell>ada 39.9 58.5 43.0</cell></row><row><cell>FreeAnchor [32]</cell><cell>ada</cell><cell>fixed 39.1 58.2 42.1</cell></row><row><cell>MAL [8]</cell><cell>ada</cell><cell>fixed 39.2 58.0 42.3</cell></row><row><cell>PAA [9]  *</cell><cell>ada</cell><cell>fixed 39.9 59.1 42.8</cell></row><row><cell>PAA+IoU pred. [9]  *  TAL TAL  *  TAL + TAP  *</cell><cell>ada ada ada ada</cell><cell>fixed 40.9 59.4 43.9 ada 40.3 58.5 43.8 ada 40.9 59.3 44.3 ada 42.5 60.3 46.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .Table 3 .Table 4 .</head><label>534</label><figDesc>The models are trained with scale jitter (480-800) and for 2? learning schedule (24 Performance of the complete TOOD (T-head + TAL). Analysis of different hyper-parameters for t. epochs) as the most current method<ref type="bibr" target="#b13">[14]</ref>. For a fair comparison, we report results of single model and single testing scale. With ResNet-101 and ResNeXt-101-64?4d, TOOD achieves 46.7 AP and 48.3 AP, outperforming the most current one-stage detectors such as ATSS<ref type="bibr" target="#b30">[31]</ref> (by ?3 AP) and GFL<ref type="bibr" target="#b13">[14]</ref> (by ?2 AP). Furthermore, with ResNet-101-DCN and ResNeXt-101-64?4d-DCN, TOOD brings a larger improvement, comparing to other detectors. For example, it obtains an improvement of 2.8 AP (48.3?51.1 AP) while ATSS has a 2.1 AP (45.6?47.7 AP) improvement. This validates that TOOD can cooperate with Deformable Convolutional Networks (DCN)</figDesc><table><row><cell>Type</cell><cell></cell><cell>Method</cell><cell>AP</cell><cell>AP50 AP75</cell></row><row><cell>Anchor-free</cell><cell cols="3">ATSS [31] 39.2 TOOD 42.5</cell><cell>57.4 59.8</cell><cell>42.2 46.4</cell></row><row><cell cols="4">Anchor-based ATSS [31] 39.3 TOOD 42.4</cell><cell>57.5 59.8</cell><cell>42.8 46.1</cell></row><row><cell>?</cell><cell>?</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell cols="3">0.5 2 42.4 0.5 4 42.3</cell><cell>60.0 59.3</cell><cell>46.1 45.8</cell></row><row><cell cols="3">0.5 6 41.7 1.0 6 42.5 1.0 8 42.2</cell><cell>58.1 59.8 59.0</cell><cell>45.1 46.4 46.0</cell></row><row><cell cols="3">1.5 8 41.5</cell><cell>59.4</cell><cell>44.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Detection results on the COCO test-dev set. ? indicates the concurrent work.</figDesc><table><row><cell>RetinaNet [16]</cell><cell cols="2">ICCV17</cell><cell cols="2">ResNet-101</cell><cell>39.1</cell><cell>59.1</cell><cell>42.3</cell><cell>21.9</cell><cell>42.7</cell><cell>50.2</cell></row><row><cell>FoveaBox [10]</cell><cell>-</cell><cell></cell><cell cols="2">ResNet-101</cell><cell>40.6</cell><cell>60.1</cell><cell>43.5</cell><cell>23.3</cell><cell>45.2</cell><cell>54.5</cell></row><row><cell>FCOS w/ imprv [27]</cell><cell cols="2">ICCV19</cell><cell cols="2">ResNet-101</cell><cell>43.0</cell><cell>61.7</cell><cell>46.3</cell><cell>26.0</cell><cell>46.8</cell><cell>55.0</cell></row><row><cell>Noisy Anchor [12]</cell><cell cols="2">CVPR20</cell><cell cols="2">ResNet-101</cell><cell>41.8</cell><cell>61.1</cell><cell>44.9</cell><cell>23.4</cell><cell>44.9</cell><cell>52.9</cell></row><row><cell>MAL [8]</cell><cell cols="2">CVPR20</cell><cell cols="2">ResNet-101</cell><cell>43.6</cell><cell>62.8</cell><cell>47.1</cell><cell>25.0</cell><cell>46.9</cell><cell>55.8</cell></row><row><cell>SAPD [35]</cell><cell cols="2">CVPR20</cell><cell cols="2">ResNet-101</cell><cell>43.5</cell><cell>63.6</cell><cell>46.5</cell><cell>24.9</cell><cell>46.8</cell><cell>54.6</cell></row><row><cell>ATSS [31]</cell><cell cols="2">CVPR20</cell><cell cols="2">ResNet-101</cell><cell>43.6</cell><cell>62.1</cell><cell>47.4</cell><cell>26.1</cell><cell>47.0</cell><cell>53.6</cell></row><row><cell>PAA [9]</cell><cell cols="2">ECCV20</cell><cell cols="2">ResNet-101</cell><cell>44.8</cell><cell>63.3</cell><cell>48.7</cell><cell>26.5</cell><cell>48.8</cell><cell>56.3</cell></row><row><cell>GFL [14] TOOD (ours)</cell><cell cols="2">NeurIPS20 -</cell><cell cols="2">ResNet-101 ResNet-101</cell><cell>45.0 46.7</cell><cell>63.7 64.6</cell><cell>48.9 50.7</cell><cell>27.2 28.9</cell><cell>48.8 49.6</cell><cell>54.5 57.0</cell></row><row><cell>SAPD [35]</cell><cell cols="2">CVPR20</cell><cell cols="2">ResNeXt-101-64?4d</cell><cell>45.4</cell><cell>65.6</cell><cell>48.9</cell><cell>27.3</cell><cell>48.7</cell><cell>56.8</cell></row><row><cell>ATSS [31]</cell><cell cols="2">CVPR20</cell><cell cols="2">ResNeXt-101-64?4d</cell><cell>45.6</cell><cell>64.6</cell><cell>49.7</cell><cell>28.5</cell><cell>48.9</cell><cell>55.6</cell></row><row><cell>PAA [9]</cell><cell cols="2">ECCV20</cell><cell cols="2">ResNeXt-101-64?4d</cell><cell>46.6</cell><cell>65.6</cell><cell>50.8</cell><cell>28.8</cell><cell>50.4</cell><cell>57.9</cell></row><row><cell>GFL [14] TOOD (ours)</cell><cell cols="2">NeurIPS20 -</cell><cell cols="2">ResNeXt-101-32?4d ResNeXt-101-64?4d</cell><cell>46.0 48.3</cell><cell>65.1 66.5</cell><cell>50.1 52.4</cell><cell>28.2 30.7</cell><cell>49.6 51.3</cell><cell>56.0 58.6</cell></row><row><cell>SAPD [35]</cell><cell cols="2">CVPR20</cell><cell cols="2">ResNet-101-DCN</cell><cell>46.0</cell><cell>65.9</cell><cell>49.6</cell><cell>26.3</cell><cell>49.2</cell><cell>59.6</cell></row><row><cell>ATSS [31]</cell><cell cols="2">CVPR20</cell><cell cols="2">ResNet-101-DCN</cell><cell>46.3</cell><cell>64.7</cell><cell>50.4</cell><cell>27.7</cell><cell>49.8</cell><cell>58.4</cell></row><row><cell>PAA [9]</cell><cell cols="2">ECCV20</cell><cell cols="2">ResNet-101-DCN</cell><cell>47.4</cell><cell>65.7</cell><cell>51.6</cell><cell>27.9</cell><cell>51.3</cell><cell>60.6</cell></row><row><cell>GFL [14] TOOD (ours)</cell><cell cols="2">NeurIPS20 -</cell><cell cols="2">ResNet-101-DCN ResNet-101-DCN</cell><cell>47.3 49.6</cell><cell>66.3 67.4</cell><cell>51.4 54.1</cell><cell>28.0 30.5</cell><cell>51.1 52.7</cell><cell>59.2 62.4</cell></row><row><cell>SAPD [35]</cell><cell cols="2">CVPR20</cell><cell cols="3">ResNeXt-101-64?4d-DCN 47.4</cell><cell>67.4</cell><cell>51.1</cell><cell>28.1</cell><cell>50.3</cell><cell>61.5</cell></row><row><cell>ATSS [31]</cell><cell cols="2">CVPR20</cell><cell cols="3">ResNeXt-101-64?4d-DCN 47.7</cell><cell>66.5</cell><cell>51.9</cell><cell>29.7</cell><cell>50.8</cell><cell>59.4</cell></row><row><cell>PAA [9]</cell><cell cols="2">ECCV20</cell><cell cols="3">ResNeXt-101-64?4d-DCN 49.0</cell><cell>67.8</cell><cell>53.3</cell><cell>30.2</cell><cell>52.8</cell><cell>62.2</cell></row><row><cell>GFL [14]</cell><cell cols="5">NeurIPS20 ResNeXt-101-32?4d-DCN 48.2</cell><cell>67.4</cell><cell>52.6</cell><cell>29.2</cell><cell>51.7</cell><cell>60.2</cell></row><row><cell>GFLV2 [13]  ?</cell><cell cols="2">CVPR21</cell><cell cols="3">ResNeXt-101-32?4d-DCN 49.0</cell><cell>67.6</cell><cell>53.5</cell><cell>29.7</cell><cell>52.4</cell><cell>61.4</cell></row><row><cell>OTA [5]  ?</cell><cell cols="2">CVPR21</cell><cell cols="3">ResNeXt-101-64?4d-DCN 49.2</cell><cell>67.6</cell><cell>53.5</cell><cell>30.0</cell><cell>52.5</cell><cell>62.3</cell></row><row><cell>IQDet [19]  ?</cell><cell cols="2">CVPR21</cell><cell cols="3">ResNeXt-101-64?4d-DCN 49.0</cell><cell>67.5</cell><cell>53.1</cell><cell>30.0</cell><cell>52.3</cell><cell>62.0</cell></row><row><cell>VFNet [30]  ? TOOD (ours)</cell><cell cols="2">CVPR21 -</cell><cell cols="3">ResNeXt-101-64?4d-DCN 49.9 ResNeXt-101-64?4d-DCN 51.1</cell><cell>68.5 69.4</cell><cell>54.3 55.5</cell><cell>30.7 31.9</cell><cell>53.1 54.1</cell><cell>62.8 63.7</cell></row><row><cell>Method</cell><cell>AP</cell><cell cols="8">PCC (top-50) IoU (top-10) #Correct boxes #Redundant boxes #Error boxes</cell></row><row><cell cols="2">Parallel head + ATSS [31] 39.2 T-head + ATSS [31] 41.1 Parallel head + TAL 40.3 T-head + TAL 42.5</cell><cell></cell><cell>0.408 0.440 0.415 0.452</cell><cell>0.637 0.644 0.643 0.661</cell><cell></cell><cell>30,261 30,601 30,506 30,734</cell><cell></cell><cell cols="2">25,428 21,838 15,927 15,242</cell><cell>92,677 79,189 72,320 69,013</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>-Supplementary material -TOOD: Task-aligned One-stage Object Detection <ref type="bibr">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Implementation details</head><p>In this section, we describe the processes of network optimization and inference in more detail.</p><p>Optimization. Our implementations are based on the MMDetection toolbox <ref type="bibr">[2]</ref> and Pytorch <ref type="bibr">[7]</ref>. The models with backbone ResNet-50 are trained with 4 GPUs and a mini-batch of 4 per GPU, while the others are trained with 8 GPUs and a mini-batch of 2 per GPU. We use the Stochastic Gradient Descent (SGD) optimizer with a weight decay of 0.0001 and a momentum of 0.9. Unless specific, the models are trained for 12 epochs (1? learning schedule) and the initial learning rate is set to 0.01 and then reduced by a factor of 10 at the 8-th epoch and the 11-th epoch. The input images are resized to have a shorter side of 800 while the longer side is kept less than 1333. Specifically, if an anchor is assigned to the positive samples of more than one object, we only assign this anchor to the object with the minimal area. For the experiments compared with the state-of-theart detectors, we train the models with scale jitter and for 24 epochs (2? learning schedule) as <ref type="bibr">[6]</ref>.</p><p>Inference. The inference phase is the same as that of ATSS <ref type="bibr">[9]</ref>. Namely, we resize the input image in the same way as the training phase (i.e., the shorter side is resized to 800 while the longer side is kept less than 1333), and then forward it through the detection network to obtain the predicted bounding boxes with a predicted class. Afterward, we use a confidence threshold of 0.05 to filter out the predictions with low confidence, and then select the top 1000 scoring boxes from each feature pyramid. Finally, we adopt the Non-Maximum Suppression (NMS) with the IoU threshold of 0.6 per class to generate the final top 100 confident predictions per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Discussion</head><p>Differences between TAL and previous works. As discussed, the proposed TAL is a learning-based approach for anchor selection and weighting. Here we discuss the differences between our TAL and several recent methods in terms of anchor selection and weighting. As mentioned in the paper, the adaptive methods can be divided into two categories: (1) positive/negative anchor collection such as FreeAnchor <ref type="bibr">[10]</ref>, MAL <ref type="bibr">[3]</ref>, PAA <ref type="bibr">[4]</ref> and Mutual Guidance <ref type="bibr">[8]</ref>; <ref type="bibr">(2)</ref> anchor weighting such as PISA <ref type="bibr">[1]</ref>, NoisyAnchor <ref type="bibr">[5]</ref> and GFL <ref type="bibr">[6]</ref> (e.g., by modifying the loss functions). These methods adaptively perform either anchor collection or anchor weighting. We propose TAL that considers both aspects at the same time, allowing it to measure informative or high-quality anchors more accurately. Specifically, TAL is designed to dynamically collect the positive/negative anchors from a task-alignment point of view, and further weight the positive anchors carefully, according to the degree of task-alignment at each location. Compared with the current assignment methods such as ATSS <ref type="bibr">[6]</ref> and PAA <ref type="bibr">[4]</ref> which first select a set of candidate anchors based on the IoU score, and then analyze the distribution characteristics of the anchors to assign samples, the design of TAL is simpler yet more efficient by directly assigning the samples based on the proposed alignment metric. Particularly, recent Mutual Guidance <ref type="bibr">[8]</ref> tackles the task-misalignment problem by assigning positive/negative anchors for one task according to the predefined anchors and the prediction quality on the other task. Different from Mutual Guidance, TAL assigns positive/negative anchors for each task based on the alignment between both two tasks, and is completely independent of the predefined anchors. Besides, GFL <ref type="bibr">[6]</ref> attempted to align the tasks by replacing a binary classification label with an IoU score, on the basis of ATSS. TAL is different from the GFL, by using the proposed taskalignment metric to design both sample assignment and anchor weighting, which allows it to explicitly learn to refine both classification and localization in a coordinated fashion.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Prime sample attention in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11583" to="11591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exploring classification equilibrium in long-tailed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07507</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ota: Optimal transport assignment for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiple anchor learning for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10206" to="10215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7389" to="7398" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning from noisy anchors for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10588" to="10597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11632" to="11641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Iqdet: Instance-wise quality distribution sampling for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1717" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A ranking-based, balanced loss function unifying classification and localisation in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oksuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Baris Can Cam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Libra r-cnn: Towards balanced learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="821" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
	</analytic>
	<monogr>
		<title level="m">Micha?l Mathieu, Rob Fergus, and Yann LeCun</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting the sibling head in object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11563" to="11572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Double-head rcnn: Rethinking classification and localization for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06493</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Localize to classify and classify to localize: Mutual guidance in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Lef?vre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Avignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feras Dayoub, and Niko Sunderhauf. Varifocalnet: An iou-aware dense object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8514" to="8523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representation sharing for fast object detector search and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Soft anchor-point object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature selective anchor-free module for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9308" to="9316" />
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prime sample attention in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11583" to="11591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiple anchor learning for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10206" to="10215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning from noisy anchors for one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10588" to="10597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Tensors and dynamic neural networks in python with strong gpu acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paskze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://github.com/pytorch" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Localize to classify and classify to localize: Mutual guidance in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Lef?vre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Avignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
