<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revealing Single Frame Bias for Video-and-Language Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
							<email>jielei@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
							<email>tlberg@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revealing Single Frame Bias for Video-and-Language Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-and-language tasks (including text-to-video retrieval and video question answering), we show the surprising result that, with large-scale pre-training and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals the existence of a strong "static appearance bias" in popular video-and-language datasets. Therefore, to allow for a more comprehensive evaluation of video-and-language models, we propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. Our code is available at https://github.com/jayleicn/singularity.</p><p>We start by building a standard image-language model, with a vision encoder and a language encoder for image and text encoding, followed by a multi-modal encoder with cross-attention for cross-modal fusion. We pre-train the model on large-scale image-text and video-text datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4]</ref>. For fine-tuning, we randomly sample a single frame for training, and ensemble multiple uniformly sampled frames per video for making a video-level prediction at inference.</p><p>Single-frame predictions are often noisy and inaccurate, as they are made from incomplete information from single-frames without any context (see examples in <ref type="figure">Figure 5</ref>). Due to this issue, single-frame training typically performs significantly worse than multi-frame training <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b46">47]</ref>. Previous Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video and language are the two primary signals that constitute much of the world we perceive every day -we observe our surrounding environment with our eyes in the form of continuous visual input (video), and communicate with others via language. Intuitively, this leads one to assume that training an effective video-and-language model should require multiple video frames as input. Standard methods <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47]</ref> in this area typically use multiple densely sampled frames for training. Recent work <ref type="bibr" target="#b30">[31]</ref> proposes sparse sampling for video-and-language understanding, where it claims that a few sparsely sampled clips are sufficient for learning due to the high redundancy in videos. This technique has shown <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b67">68]</ref> to be successful in various video-language benchmarks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b31">32]</ref>. However, as demonstrated in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b30">31]</ref>, training with fewer frames (e.g., a single frame) leads to significantly worse performance compared to their multi-frame counterparts. In contrast, in this work, we show that with proper modeling, single-frame models could achieve competitive performance, hence also revealing "static appearance bias" in popular video-and-language datasets. work <ref type="bibr" target="#b21">[22]</ref> suggests that pre-training improves model robustness in the face of label corruption for image recognition. Inspired by this, we hypothesize that large-scale pre-training helps mitigate noise from single-frame training. Our analyses in Section 6 agree with our hypothesis, showing that as we increase pre-training data size, the performance of our single-frame model improves drastically and its gap with a similarly trained multi-frame model is largely eliminated. Besides training, these noisy single-frame predictions also render simple late fusion (e.g., mean-pooling in ClipBERT <ref type="bibr" target="#b30">[31]</ref>) less effective at inference time. To deal with this issue, we propose an early fusion strategy, which takes all frames as model inputs for directly making a more informative video-level prediction. Our analyses show that this early fusion ensemble method outperforms late fusion strategies and also delivers consistently improved performance when more frames are used.</p><p>We compare our approach with existing methods on six datasets across two video-language tasks, including text-to-video retrieval (MSRVTT <ref type="bibr" target="#b61">[62]</ref>, DiDeMo <ref type="bibr" target="#b1">[2]</ref>, and ActivityNet Captions <ref type="bibr" target="#b28">[29]</ref>) and video question answering (MSRVTT-QA <ref type="bibr" target="#b59">[60]</ref>, ActivityNet-QA <ref type="bibr" target="#b65">[66]</ref>, and MSRVTT-MC <ref type="bibr" target="#b64">[65]</ref>). Results show that our approach achieves competitive (mostly better) performance than existing methods that use more training frames and more pre-training data, setting new state-of-the-art for multiple tasks. This conclusion holds for short 15-second videos in MSRVTT to 180-second videos in ActivityNet, demonstrating the effectiveness of our single-frame approach in various scenarios.</p><p>More importantly, this strong single-frame performance reveals that the current evaluation is biased towards still objects, scenes, etc., while the temporal dynamics seem negligible, which in fact should be important for "true" video-language understanding. To address this issue, we next propose two new tasks that are designed to test models' true temporal modeling ability. Based on the videos and annotations from the find-grained action recognition dataset Something-Something v2 (SSv2) <ref type="bibr" target="#b18">[19]</ref>, we create two text-to-video retrieval tasks, one that use SSv2's action template as text queries, e.g., "Throwing [something] in the air and catching it", and another that uses its annotated label as text queries, e.g., "Throwing keys in the air and catching it". See examples in <ref type="figure">Figure 2</ref>. This template task removes the objects and only keeps the actions, enabling an evaluation that focuses almost solely on temporal modeling. The label task, on the other hand, contains both actions and objects, requiring an understanding of both still objects and their motion dynamics. Lastly, we present several baselines on these new tasks and show that temporal modeling is essential in achieving high scores.</p><p>In summary, our contributions are two-fold: (i) We explore single-frame training for video-andlanguage tasks, and show that, with sufficient pre-training data and a proper multi-frame ensemble strategy at inference, our approach can achieve state-of-the-art performance on a range of datasets, including both text-to-video retrieval and video question answering. Importantly, this result reveals the surprising static appearance bias in these existing datasets. (ii) We propose two new tasks specifically designed for testing models' ability for find-grained temporal modeling. These two new tasks complement existing benchmarks for a more comprehensive evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Vision and Language. Vision and language learning considers the problem of learning from both visual and textual signals. Depending on their visual input type, methods in this area can be roughly categorized into two types, one with image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b50">51]</ref> and another with video <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42]</ref>. Standard video-and-language methods <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b46">47]</ref> are typically trained with multiple video frames. This multi-frame training strategy has been the norm and is shown to work well across various datasets <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Unlike previous work that uses multiple frames for training, we explore single-frame training (i.e., similar to training an image-text model) and show it achieves strong performance on existing video-text benchmarks. Concurrent work <ref type="bibr" target="#b6">[7]</ref> proposes a new module, atemporal probe, for selecting the best single-frame as inputs to a trained image-text model during inference; whereas we utilize multiple uniformly sampled frames and study more effective ways of ensembling information from multiple frames.</p><p>Dataset Bias. Biases are prevalent in datasets <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b32">33]</ref>. For example, Zhang et al. <ref type="bibr" target="#b68">[69]</ref> pointed out that blindly answering "yes" to yes/no questions in VQA <ref type="bibr" target="#b2">[3]</ref> without looking at their corresponding images results in an accuracy of 87%; Li et al. <ref type="bibr" target="#b40">[41]</ref> discovered that many video action recognition datasets, such as Kinetics <ref type="bibr" target="#b26">[27]</ref> and UCF-101 <ref type="bibr" target="#b52">[53]</ref>, have a strong static representation, where a linear classifier trained on static appearance (e.g., object, scene, and people) representations achieves much higher performance than chance. In this work, we find similar static appearance bias </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Modal Encoder</head><p>Multi-Modal Encoder Loss <ref type="figure">Figure 1</ref>: SINGULARITY model overview. During training, we randomly sample a single frame as input, and make a video level prediction based on the information from this single frame along with its paired text input. During inference, we uniformly sample multiple frames, and early fuse their encoded image-level representations as input to the multi-modal encoder. See details in Section 3.</p><p>exists in popular video-language datasets <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref>, in which our models trained with single frames could achieve surprisingly good performance, even compared to models that perform explicit temporal modeling. When datasets are biased, they provide incorrect indications of the models' ability. To allow for a more comprehensive evaluation, we propose two new tasks based on an existing action recognition dataset SSv2 <ref type="bibr" target="#b18">[19]</ref> to test the true temporal modeling ability of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>Model Architecture. <ref type="figure">Figure 1</ref> shows an overview of our model (dubbed SINGULARITY). It consists of 3 main components, a vision encoder F v , a language encoder F l , and a multi-modal encoder H. The vision encoder is an image-level visual backbone model, such as ViT <ref type="bibr" target="#b15">[16]</ref>. The language encoder is an arbitrary language model such as BERT <ref type="bibr" target="#b14">[15]</ref>. For the multi-modal encoder, we use a transformer encoder <ref type="bibr" target="#b56">[57]</ref>, in which each layer contains a self-attention, a cross-attention, and a feed-forward network (FFN). The cross-attention layer is used to gather information from encoded visual representations using the text as key, similar to recent work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>We denote a video V contains T frames as V =[f 1 , f 2 , ..., f T ], its paired text as S. During training, we randomly sample a single frame f t from V as model input , where t ? {1, ..., T }. Its encoded representation can be written as F v (f t ) ? R Lv?D . For text, the encoded representation is F l (S) ? R L l ?D . L v and L l are encoded sequence lengths, D is hidden size. We next make a prediction p as:</p><formula xml:id="formula_0">p = H( F l (S) , F v (f t ) ),<label>(1)</label></formula><p>Q, K, V for self-att; Q for cross-att K, V for cross-att where Q, K, V denote the query, key, and value matrices of self-and cross-attention <ref type="bibr" target="#b56">[57]</ref>. We calculate loss based on this prediction. During inference, we uniformly sample T test frames {f ?i } Ttest i=1 . Each frame is encoded separately, and their encoded representations are concatenated as inputs to the multi-modal encoder to get a video-level prediction score:</p><formula xml:id="formula_1">p = H( F l (S) , [F v (f ?1 ); ...; F v (f ? T test )] ),<label>(2)</label></formula><p>where [; ] denotes concatenation, and</p><formula xml:id="formula_2">[F v (f ?1 ); ...; F v (f ? T test )] ? R (Ttest?Lv)?D .</formula><p>This early fusion design allows our model to make an informed prediction given full context. In ClipBERT <ref type="bibr" target="#b30">[31]</ref>, an alternative late fusion design is studied: scores are computed for each frame separately, and the final video-level score is obtained via a manually designed aggregation function G (e.g., mean-pooling):</p><formula xml:id="formula_3">p = G(p ?1 , p ?2 , p ? T test ); p ?i = H( F l (S) , F v (f ?i ) ).<label>(3)</label></formula><p>Since the predictions in late fusion are made with incomplete information from individual frames, they can be quite noisy. In Section 6, we provide a detailed comparison w.r.t. these different frame ensemble methods and show that early fusion consistently outperforms late fusion.</p><p>Pre-Training Objectives. The model is trained with 3 losses: (i) Vision-Text Contrastive: a contrastive loss that aligns the pooled vision and text representations from the vision and language encoders. (ii) Masked Language Modeling (MLM) <ref type="bibr" target="#b14">[15]</ref>: predicting masked tokens from their text and visual context, with multi-modal encoder. (iii) Vision-Text Matching: predicting the matching score of a vision-text pair with multi-modal encoder. These losses have shown to be effective in learning multi-modal representations <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b50">51]</ref>. More details are in Appendix.</p><p>Implementation Details. As our model trains with single frames, in addition to video-text data, it can also utilize image-text data for pre-training. For image-text data, we use a combination of COCO <ref type="bibr" target="#b10">[11]</ref>, Visual Genome (VG) <ref type="bibr" target="#b29">[30]</ref>, SBU Captions <ref type="bibr" target="#b48">[49]</ref>, CC3M <ref type="bibr" target="#b51">[52]</ref>, and CC12M <ref type="bibr" target="#b9">[10]</ref>. For video-text data, we use WebVid <ref type="bibr" target="#b3">[4]</ref>. Note that, even for video-text data, we only sample a single frame from the whole video for training. We pre-train the model on two different subsets of the datasets: (i) 5M corpus that contains 5.44M images and videos from CC3M+WebVid, and (ii) 17M corpus that contains 17.28M images and videos from all the datasets above.</p><p>Our model is implemented in PyTorch <ref type="bibr" target="#b49">[50]</ref>. The vision encoder is initialized using the BEiT BASE <ref type="bibr" target="#b4">[5]</ref> model pre-trained on ImageNet-21K <ref type="bibr" target="#b13">[14]</ref>. The text encoder is initialized from the first 9 layers of BERT BASE <ref type="bibr" target="#b14">[15]</ref>. The multi-modal encoder is initialized from the last 3 layers of the same BERT BASE model, though its cross-attention layers are randomly initialized. We optimize the model for 10 epochs using AdamW <ref type="bibr" target="#b44">[45]</ref> optimizer with an initial learning rate of 1e-4. We warm up the learning rate in the first epoch followed by cosine decay <ref type="bibr" target="#b43">[44]</ref> to 1e-6 during the rest of the training. Mixed precision is used for faster training. The batch size is set to 128 per GPU, and we train the model on 3 NVIDIA A100 GPUs with input image size 224?224. We perform basic augmentations: random resize, crop, and flip to the frames/images during training. This pre-training takes around 1 day on the 5M corpus, and 4 days on the 17M corpus. Our pre-training is quite efficient compared to other similar work, e.g., 10 epochs' pre-training in AlignPrompt <ref type="bibr" target="#b33">[34]</ref> takes 3 days on the same 5M corpus using 16 A100 GPUs, this amounts to 16? computation cost of our pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results on Existing Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Downstream Task Setup</head><p>Text-to-Video Retrieval. Given a text query, the goal of this task is to retrieve relevant videos from a large collection of videos. We evaluate our model on the following datasets: (i) MSRVTT <ref type="bibr" target="#b61">[62]</ref> contains 10K YouTube videos, each paired with 20 captions. We follow <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b30">31]</ref> to use the 7K train+val videos for training, and report results on the 1K test set. (ii) DiDeMo [2] contains 10K Flickr videos with 41K captions. We use standard train/val/test splits. (iii) ActivityNet Captions <ref type="bibr" target="#b28">[29]</ref> contains 20K YouTube videos with 100K captions. We use the train split with 10K videos for training, and we report results on the widely used val1 split, with 4.9K videos. For MSRVTT, we evaluate standard text-to-video retrieval. For DiDeMo and ActivityNet Captions, we evaluate paragraph-tovideo retrieval <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47]</ref>, where the text captions in the same video are concatenated as a single paragraph-level text for retrieval. We report performance using recall at K (R@K).</p><p>For fine-tuning, we use the same architecture as pre-training, except that MLM loss is not used. We use an initial learning rate of 1e-5 with cosine decay to 1e-6. We use a batch size of 32, and train the model for 5 epochs for MSRVTT, 10 epochs for DiDeMo and ActivityNet Captions. During training, we only use a single frame per video. During testing, we use 12 frames per video for MSRVTT and DiDeMo, and 32 frames for ActivityNet Captions since it has longer videos. On a single A100 GPU, this fine-tuning takes around 1.5 hours for MSRVTT, 0.5 hours for ActivityNet Captions or DiDeMo. For open-ended QA tasks, we add an extra multi-modal decoder (initialized from pre-trained multimodal encoder) that takes in multi-modal encoder outputs as cross-attention inputs, and decodes answer text with "[CLS]" as the start token (see details in Appendix). We use an initial learning rate <ref type="table">Table 1</ref>: Comparison to existing methods on text-to-video retrieval. #PT denotes the number of images and or videos used in cross-modal pre-training. #Train Frame denotes the number of frames used at each training step during fine-tuning. For models that use different number of frames for different datasets, we list them together with a separator "/". We gray out methods that use significantly more pre-training data for a fair comparison. The 136M corpus is from HowTo100M <ref type="bibr" target="#b47">[48]</ref>, 0.2M refers to COCO+VG data, 138M is the combination of HowTo100M and WebVid, 400M is the private image-text data used in CLIP <ref type="bibr" target="#b50">[51]</ref>.  of 1e-5, and warm up the learning rate in the first half epoch, followed by cosine decay to 1e-6. We use a batch size of 32, and train the model for 10 epochs. On a single A100 GPU, this fine-tuning takes around 4 hours for MSRVTT-QA, and 1 hour for ActivityNet-QA. We use a single frame per video for training, 12 frames for testing. For MSRVTT-MC, we follow <ref type="bibr" target="#b30">[31]</ref> to use the model trained for the MSRVTT retrieval task, and select the option with the highest retrieval score as the prediction.</p><formula xml:id="formula_4">Method #PT #Train MSRVTT DiDeMo ActivityNet Cap Frame R1 R5 R10 R1 R5 R10 R1 R5</formula><p>For all downstream tasks, we use the same input image size 224?224 and image augmentations as in pre-training. During inference, we resize the input video frames to 224?224.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison to State-of-the-Art on Existing Datasets</head><p>Text-to-Video Retrieval Results. In <ref type="table">Table 1</ref>, we compare SINGULARITY with existing methods on text-to-video retrieval. Across all the datasets, SINGULARITY (5M) achieves better performance compared to methods trained on similar amounts of data, while using only single frames for training. On DiDeMo and ActivityNet Captions, SINGULARITY (5M) outperforms all previous work, including many that pre-train on significantly larger amounts of data, e.g., 400M image-text pairs in CLIP4Clip <ref type="bibr" target="#b46">[47]</ref>, or 136M video-text pairs in VideoCLIP <ref type="bibr" target="#b60">[61]</ref> compared to 5M image-text and video-text pairs in SINGULARITY. We also note that our model is trained with single frames, while previous work uses many more frames, e.g., 64 frames in CLIP4Clip or 8 frames in AlignPrompt <ref type="bibr" target="#b33">[34]</ref>. When trained with a larger amount of data (17M), we notice a further performance boost for our model, demonstrating that SINGULARITY benefits from large-scale pre-training.  <ref type="figure">Figure 2</ref>: SSv2 examples. For each video, we show 3 temporally-ordered frames with their template and label annotations. Based on these annotations, we propose two new retrieval tasks, using "template" and "label" as text queries, respectively.</p><p>Video Question Answering Results. <ref type="table" target="#tab_3">Table 2</ref> compares SINGULARITY with existing methods on video question answering. We notice SINGULARITY (5M) achieves competitive performance with previous work even when using two orders of magnitude smaller pre-training data, e.g., 180M videotext pairs in MERLOT <ref type="bibr" target="#b67">[68]</ref> vs. 5M image-text and video-text pairs. Our method also surpasses the strong video QA model JustAsk <ref type="bibr" target="#b62">[63]</ref>, which is specifically designed for video QA and is pre-trained on 69M video QA pairs. When pre-trained with more data, our model performance further improves. These comparisons show the effectiveness of our single-frame approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">New Tasks that Require Temporal Modeling</head><p>In the previous section, we revealed the interesting observation that popular video-language datasets have strong static appearance biases -enabling our model that uses only a single frame per video at each training step to achieve competitive performance compared to state-of-the-art models that digest multiple temporally-ordered frames. The biased evaluation on these datasets favors models that are strong in recognizing static concepts, and does not provide a good indicator of whether these models are capable of recognizing fine-grained temporal relationships between neighboring video frames.</p><p>Hence, to address this issue, we propose two new datasets that complement existing datasets for a more comprehensive evaluation of video-and-language methods. We draw inspiration from the video action recognition community, and transform the temporally-heavy action recognition dataset Something-Something v2 (SSv2) <ref type="bibr" target="#b18">[19]</ref> into video-and-language datasets. In <ref type="figure">Figure 2</ref>, we show SSv2 examples. A unique property of the SSv2 dataset is that the videos often require fine-grained temporal modeling to correctly predict their action classes. For example, to match the videos and their action classes (template) in <ref type="figure">Figure 2</ref>(a-b), one has to look at multiple temporally ordered frames. Based on SSv2 videos and annotations, we define two text-to-video retrieval tasks:</p><p>? SSv2-Template Retrieval: We use the 174 templates (e.g., "Throwing [something] in the air and catching it") in SSv2 as the text queries to retrieve videos. We use 168,913 SSv2 training videos for training. As ground-truth annotations for test videos are not available, we use validation videos: we sample 12 videos for each template, with a total of 2,088 videos for testing.</p><p>? SSv2-Label Retrieval: We use the annotated labels (e.g., "Throwing keys in the air and catching it") in SSv2 as text queries to retrieve videos. We follow the same split in the template retrieval task, with 168,913 videos for training, and 2,088 videos for testing.</p><p>Since no objects are present in the text queries of the template retrieval task, it requires a deeper understanding of the actions than in the label retrieval task, while the label retrieval task provides a more comprehensive evaluation of both static and temporal understanding.</p><p>Experiments. We use Frozen <ref type="bibr" target="#b3">[4]</ref> and CLIP4Clip (seqTransf version) <ref type="bibr" target="#b46">[47]</ref> as baselines for the new tasks. Frozen uses a space-time transformer for video encoding, CLIP4Clip is an extension based on the CLIP <ref type="bibr" target="#b50">[51]</ref> model with an extra 4-layer temporal transformer encoder. We report performance using standard text-to-video retrieval metrics R@K. For our model, in addition to the single-frame version, we build a multi-frame variant, SINGULARITY-temporal. Specifically, we add a two-layer temporal transformer encoder following the vision encoder, and use its outputs as inputs   ActivityNet-QA acc <ref type="figure">Figure 4</ref>: Impact of frame ensemble strategy. Retrieval performance is shown as avg recall, i.e., average of R@{1,5,10}. We use the same finetuned checkpoint for each task, thus the results difference only comes from inference strategies.</p><p>to the multi-modal encoder (see details in Appendix). From a single-frame pre-trained checkpoint (5M or 17M), we perform a 2nd stage video pre-training with 4 frames using WebVid videos for SINGULARITY-temporal. We use an initial learning rate of 5e-5, and train the model for 5 epochs.</p><p>The results are shown in <ref type="table" target="#tab_5">Table 3</ref>. Compared to Frozen and CLIP4Clip, while SINGULARITY shows competitive performance on existing benchmarks (see <ref type="table">Table 1</ref>), it underperforms these methods on the two temporally-heavy tasks by a large margin. For example, SINGULARITY (5M) underperforms the 4-frame Frozen model by 10.9 for SSv2-template retrieval R1, though it shows a 16.4 improvement for DiDeMo R1, and 5.8 for MSRVTT R1. This is a good sign as it shows that the new tasks cannot be solved by models exploiting static appearance biases. On the other hand, after adding the 2-layer temporal encoder, the 4-frame SINGULARITY-temporal model gets a significant performance boost from the single-frame SINGULARITY model, surpassing the baseline methods. When using more pre-training data (5M?17M), we notice a good performance gain for SSv2-label, while the performance on SSv2-template stays similar. These observations indicate that the SSv2-label task requires both static and temporal modeling, and enhancing either will improve the task performance. For SSv2-template, as no objects exist in its text queries, it requires mostly temporal modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>Frames Ensemble Strategy. Our model is trained with a single-frame regime, and it uses multiple frames covering the full video at inference time. As shown in <ref type="figure" target="#fig_0">Figure 3a</ref> (concat), encoded video frames are concatenated as input to the multi-modal encoder's cross-attention layer for making a video-level prediction. A naive alternative is to compute the prediction score for each frame separately <ref type="figure" target="#fig_0">(Figure 3b</ref>), and then aggregate these frame-level scores together to get a video-level score using an aggregation function, such as LogSumExp (lse), max-pooling and mean-pooling. This simple late fusion strategy has shown to be successful for both video-and-language methods <ref type="bibr" target="#b30">[31]</ref> and video action recognition methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b58">59]</ref>.  <ref type="figure">Figure 5</ref>: Prediction score distribution for a MSRVTT-MC example. We show frame-level score distribution for each frame, and video-level score distribution for late fusion (we use mean as an example) and our early fusion (concat). The highest score for each prediction is indicated by , the correct answer is highlighted in green. Single-frame predictions are often inaccurate, unstable and they fluctuate across the frames. Late fusion can be biased by inaccurate but high confidence frame predictions, e.g., the late fusion prediction is biased towards the 4th frame prediction.  <ref type="figure">Figure 6</ref>: Model performance as a function of pre-training data size, for SINGULARITY (1-frame) and SINGULARITY-temporal (4-frame). The performance differences between the two models in each pre-training setup is also annotated, e.g., the average recall on MSRVTT retrieval for the two models without pre-training are 37.9 and 44.0, respectively, with ?=6.1. In general, as pre-training data size increases, the performance gap between the two models decreases.</p><p>In <ref type="figure">Figure 4</ref>, we compare these different frame ensemble strategies, with varying number of frames at inference. From the comparison, we can draw the following conclusions: (i) Our early fusion strategy (concat) shows a significant gain over the three late fusion strategies (lse, max, mean) for both MSRVTT retrieval and ActivityNet-QA, demonstrating the importance of considering the whole video when making the predictions. (ii) In general, for all ensemble strategies, using more frames at inference improves model performance. However, for the late fusion strategies, sometimes using more frames hurts performance, e.g., for ActivityNet-QA, inference with over 4 frames underperforms that with 4 frames for max-pooling. This observation agrees with the MSRVTT-QA results in ClipBERT <ref type="bibr" target="#b30">[31]</ref>. In contrast, early fusion delivers consistently improved performance when more frames are used. Overall, we hypothesize that the low and unstable performance of late fusion is because its video-level prediction is obtained via aggregating frame-level predictions, while these frame-level predictions can be inaccurate and unstable (see example in <ref type="figure">Figure 5</ref>) -as they are separately predicted using incomplete information within each frame, ignoring their context.</p><p>Pre-Training Data Size. In <ref type="figure">Figure 6</ref>, we study the effect of cross-modal pre-training data size for both the single-frame and the multi-frame model. We show downstream fine-tuning performance under 4 different pre-training data setups: no cross-modal pre-training (0M), pre-train on WebVid (2.49M videos), on 5M corpus (5.44M images+videos), or on 17M corpus (17.28M images+videos).</p><p>We obsereve that both 1-frame and 4-frame model greatly benefit from large-scale pre-training. When comparing the two models, an interesting observation is that, as the pre-training data size increases, the performance gap between the 1-frame and the 4-frame model decreases almost monotonically. This phenomenon suggests that, when pre-trained on a sufficient amount of data, the performance of models trained with single frames might be very close to models trained with multiple frames. Though there can be exceptions for tasks that require fine-grained temporal modeling, such as SSv2-label retrieval, where multi-frame modeling is necessary.</p><p>One possible explanation is that single-frame training is noisier than multi-frame training -due to incomplete context and random sampling, single-frame predictions are often inaccurate and less stable than multi-frame predictions, and pre-training is helpful <ref type="bibr" target="#b21">[22]</ref> in these scenarios. Meanwhile, single-frame training requires the model to extract more information from a single frame while a multi-frame model could rely on rich sources from multiple frames. Therefore, for downstream tasks, it is more essential for the single-frame model to initialize from a strong pre-trained model. Training Efficiency. A core advantage of single-frame training is its training efficiency. In Section 3, we discussed our pre-training cost is only 1/16 of a recent video-language model <ref type="bibr" target="#b33">[34]</ref>. In <ref type="figure" target="#fig_3">Figure 7</ref> we compare the training time and task performance of various models. We note our model <ref type="bibr">(1-</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we explore single-frame training for video-and-language learning. We find that, with sufficient pre-training data and a proper frame ensemble strategy at inference, our model trained with a single frame achieves surprisingly good performance on various video-text tasks, including text-to-video retrieval and video question answering. While these results show the potential of using single-frame training for various video-text tasks, it also reveals that current benchmarks are biased towards static objects and scenes, etc. To address this issue, we propose two new tasks designed to test models' true temporal modeling ability and build several baseline methods for these new tasks. We hope these new tasks can complement existing benchmarks for a more comprehensive video-and-language understanding.</p><p>Acknowledgements. This work is supported by ARO Award W911NF2110220, DARPA KAIROS Grant #FA8750-19-2-1004, DARPA MCS Grant N66001-19-2-4031, and NSF-AI Engage Institute DRL-211263. The views in this article are those of the authors and not of the funding agency.</p><p>Societal Impact. Similar to many data-driven methods, the predictions from our system reflect the distribution of data on which it is trained on, and these predictions can be inaccurate and biased by the data. Therefore, users should not completely rely on the system for making real-world decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>In Section A.1, we show details of our open-ended QA model and SINGULARITY-temporal model, as well as pre-training objectives. In Section A.2, we show more experimental details, such as SINGULARITY-temporal results on existing datasets, SINGULARITY zero-shot results, impact of image size, and results on image-text tasks such as text-to-image retrieval tasks Flickr30K <ref type="bibr" target="#b63">[64]</ref>, COCO <ref type="bibr" target="#b10">[11]</ref> and image question answering task VQA <ref type="bibr" target="#b2">[3]</ref>. In addition, we also show hyper-parameters and more experimental setups in this section. In Section A.3, we show more dataset details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Additional Modeling Details</head><p>Open-ended QA model. <ref type="figure" target="#fig_4">Figure 8a</ref> shows a graphic overview of the model architecture for openended video question answering. Following previous work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref>, we formulate this task as text generation instead of classification. Based on the base model described in main text, we add an extra multi-modal decoder that takes in multi-modal encoder outputs as cross-attention inputs, and decodes answer text with "[CLS]" as the start token. This decoder has the exact same architecture as the multi-modal encoder. We initialize its weight using the pre-trained multi-modal encoder.</p><p>SINGULARITY-temporal. <ref type="figure" target="#fig_4">Figure 8b</ref> shows a graphic overview of the model architecture for temporal modeling, this model is also referred to as SINGULARITY-temporal. Given multiple video frames  </p><formula xml:id="formula_5">{f ?i } Ttrain i=1</formula><p>as input, the model firstly encode each frame into their visual representations</p><formula xml:id="formula_6">{F v (f ?i )} with the vision encoder F v , where F v (f ?i ) ? R Lv?D .</formula><p>Next, we add temporal position encoding to each frame to indicate their temporal order. This temporal position encoding is learned from scratch and is initialized as zeros. For brevity, we omit this encoding in the formulation. These frame-level representations are concatenated together as input to the temporal encoder T , and we feed temporal encoder outputs to the multi-modal encoder's cross-attention layer for making a prediction p:</p><formula xml:id="formula_7">p = H( F l (S) , T ([F v (f ?1 ); ...; F v (f ? T train )]) ),<label>(4)</label></formula><p>Q, K, V for self-att; Q for cross-att K, V for cross-att where [; ] denotes concatenation, and [F v (f ?1 ); ...;</p><p>F v (f ? T train )] ? R (Ttrain?Lv)?D . During inference, when T test frames are used as inputs to the model and T test &gt; T train , we interpolate the temporal position encoding to allow for extended temporal length. This is similar to spatial position encoding interpolation in <ref type="bibr" target="#b55">[56]</ref>.</p><p>Pre-Training Objectives. During pre-training, we optimize the model with three standard visionand-language objectives, Vision-Text Contrastive (VTC), Masked Language Modeling (MLM) <ref type="bibr" target="#b14">[15]</ref>, and Vision-Text Matching. We explain them in detail below.</p><p>(i) Vision-Text Contrastive (VTC) loss aims to aligns paired vision and language embeddings. Given the encoded vision embedding F v (f i,t ), we use a projection head (with pooling) ? v to project the embedding sequence into a vector representation ? v (F v (f i,t )) ? R D . Here f i,t is the t-th frame in the i-th video in the training set, and t is randomly sampled from all available frames in this video. For brevity, we omit the subscript t and use f i to denote a randomly sampled frame from the i-th video during the rest of the discussion. Similarly, we have ? l (F l (S j )) ? R D for the j-th sentence. The similarity score s i,j of the video and text pair is defined as their dot product:</p><formula xml:id="formula_8">s i,j = ? v (F v (f i )) T ? l (F l (S j ))<label>(5)</label></formula><p>We apply a contrastive loss to encourage the alignment between paired vision-language embeddings:</p><formula xml:id="formula_9">p v i = exp(s i,i /? ) j exp(s i,j /? ) , p l i = exp(s i,i /? ) j exp(s j,i /? ) , L vtc = ? n i=1 (logp v i + logp l i ),<label>(6)</label></formula><p>where ? is a learned temperature parameter, and it is initialized as 0.07 following CLIP <ref type="bibr" target="#b50">[51]</ref>. n is the total number of examples in the training set.</p><p>(ii) Masked Language Modeling (MLM) loss, or more precisely, Vision Conditioned Masked Language Modeling loss, aims to predict masked text tokens from their (masked) textual context as well as the visual context. This loss is applied at the last layer of the multi-modal encoder, and we follow the exact formulation in BERT <ref type="bibr" target="#b14">[15]</ref>, except that we add additional vision inputs and use a higher mask ratio of 50%.</p><p>(iii) Vision-Text Matching (VTM) loss works towards the same goal as the VTC loss -encouraging the alignment between paired vision and language inputs. It uses the [CLS] output from the multimodal encoder for binary classification -whether the input vision and language pair match or not. To  make the training more effective, we also leverage hard negative sampling <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12]</ref> to sample more informative negatives within the batch for VTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Additional Experiments</head><p>Analysis Setup. For all ablation studies, we report results on validation splits for the datasets if available. For example, we use validation splits for DiDeMo retrieval and ActivityNet-QA, and we use the test split for MSRVTT retrieval, val1 split for ActivityNet Captions retrieval, and test split for SSv2-label. For retrieval tasks, we use the average recall, which is the average score of R@{1,5,10}) to more holistically compare the model performance. For QA tasks, we use accuracy.</p><p>SINGULARITY-temporal Results on Existing Datasets. In <ref type="table" target="#tab_9">Table 4</ref> and <ref type="table" target="#tab_10">Table 5</ref> we show results of SINGULARITY-temporal on existing text-to-video retrieval and video question answering datasets. In general, the 4-frame model SINGULARITY-temporal improves upon the 1-frame model SINGULAR-ITY, but the performance gap is relatively small, especially considering the greatly increased memory and computation cost (discussed in main text) of using 4 frames.</p><p>Zero-Shot Results. In <ref type="table" target="#tab_11">Table 6</ref> we show zero-shot results of SINGULARITY for text-to-video retrieval. SINGULARITY achieves significantly better results compared to existing methods with a similar amount of pre-training data.</p><p>Performance of Multiple Runs. In <ref type="table" target="#tab_12">Table 7</ref> we show mean and standard deviation of 5 random runs, for text-to-video retrieval.</p><p>Impact of Image Size. In <ref type="figure" target="#fig_4">Figure 8</ref> we study the impact of image size for downstream tasks. In general, a larger image size helps improve model performance, but the performance saturates at a  Comparison on Image-Text tasks. Since our model is pre-trained with single frames, it can be directly used for image-text tasks. In <ref type="table" target="#tab_14">Table 9</ref> we show image-text retrieval results on Flickr30K <ref type="bibr" target="#b63">[64]</ref> and COCO <ref type="bibr" target="#b10">[11]</ref>. In <ref type="table" target="#tab_15">Table 10</ref> we show image question answering results on VQA <ref type="bibr" target="#b2">[3]</ref>. We observe that SINGULARITY demonstrates competitive performance on the image-text tasks. As we still see a gap with state-of-the-art image-text models such as <ref type="bibr" target="#b34">[35]</ref>, one future direction is to adopt improved designs in these methods to further improve video-text task performance.</p><p>Hyper-Parameters. The hyper-parameters for our pre-training and downstream task fine-tuning are listed in <ref type="table" target="#tab_16">Table 11</ref> and <ref type="table" target="#tab_3">Table 12</ref>. Note that we did not do an extensive hyper-parameter search, but mostly use the same hyper-parameters for different datasets under the same task, it is possible that better results can be achieved with more tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Additional Data Details</head><p>Statistics. We show statistics of pre-training datasets in <ref type="table" target="#tab_5">Table 13</ref>, and downstream datasets in <ref type="table" target="#tab_9">Table 14</ref>.</p><p>License. We show dataset licenses in <ref type="table" target="#tab_10">Table 15</ref>.         <ref type="bibr" target="#b29">[30]</ref> CC BY 4.0 SBU <ref type="bibr" target="#b48">[49]</ref> Flickr Terms of Use CC3M <ref type="bibr" target="#b51">[52]</ref> CC3M License CC12M <ref type="bibr" target="#b9">[10]</ref> CC12M License WebVid <ref type="bibr" target="#b3">[4]</ref> Exceptions to Copyright ActivityNet Captions <ref type="bibr" target="#b28">[29]</ref> Fair Use DiDeMo <ref type="bibr" target="#b1">[2]</ref> BSD-2-Clause, Creative Commons MSRVTT <ref type="bibr" target="#b61">[62]</ref> unknown SSV2-Template <ref type="bibr" target="#b18">[19]</ref> SSv2 License SSV2-Label <ref type="bibr" target="#b18">[19]</ref> SSv2 License MSRVTT-QA <ref type="bibr" target="#b59">[60]</ref> MIT ActivityNet-QA <ref type="bibr" target="#b65">[66]</ref> Apache MSRVTT-MC <ref type="bibr" target="#b64">[65]</ref> unknown</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of frame ensemble strategies at inference. concat is our early fusion strategy, lse, max, mean are the late fusion strategies studied in ClipBERT<ref type="bibr" target="#b30">[31]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>a) A white bear is swimming in the water (b) A man is describing the sims games (c) Man singing on talent show (d) A child speaks to the camera at table (e) A sports video of great plays</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Comparison of training time and downstream task performance. The maximum allowed batch size is labeled besides each model as a reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>SINGULARITY model variants for video question answering and temporal modeling (i.e., SINGULARITY-temporal). The horizontal arrows indicate cross-attention inputs, while the vertical arrows indicate self-attention inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Caption:A group of people play kites together on the beach.</figDesc><table><row><cell>Training</cell><cell></cell><cell></cell><cell>Inference</cell><cell>Score</cell></row><row><cell></cell><cell>FFN</cell><cell></cell><cell></cell><cell>FFN</cell></row><row><cell></cell><cell>Cross-Att</cell><cell></cell><cell></cell><cell>Cross-Att</cell></row><row><cell></cell><cell>Self-Att</cell><cell>?N</cell><cell>Concat</cell><cell>Self-Att</cell><cell>?N</cell></row><row><cell>Vision Encoder</cell><cell>Language Encoder</cell><cell></cell><cell>Vision Encoder</cell><cell>Language Encoder</cell></row><row><cell>random</cell><cell>Caption</cell><cell></cell><cell>uniform</cell><cell>Caption</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="5">#PT #Train Frame MSRVTT-QA ActivityNet-QA MSRVTT-MC</cell></row><row><cell>ClipBERT [31]</cell><cell>0.2M</cell><cell>16</cell><cell>37.4</cell><cell>-</cell><cell>88.2</cell></row><row><cell>AlignPrompt [34]</cell><cell>5M</cell><cell>16</cell><cell>42.1</cell><cell>-</cell><cell>-</cell></row><row><cell>JustAsk [63]</cell><cell>69M</cell><cell>640</cell><cell>41.5</cell><cell>38.9</cell><cell>-</cell></row><row><cell>MERLOT [68]</cell><cell>180M</cell><cell>5</cell><cell>43.1</cell><cell>41.4</cell><cell>90.9</cell></row><row><cell cols="2">VideoCLIP [61] 136M</cell><cell>960</cell><cell>-</cell><cell>-</cell><cell>92.1</cell></row><row><cell>All-in-one [58]</cell><cell>138M</cell><cell>9</cell><cell>44.3</cell><cell>-</cell><cell>92.0</cell></row><row><cell>SINGULARITY</cell><cell>5M</cell><cell>1</cell><cell>42.7</cell><cell>41.8</cell><cell>92.0</cell></row><row><cell>SINGULARITY</cell><cell>17M</cell><cell>1</cell><cell>43.5</cell><cell>43.1</cell><cell>92.1</cell></row></table><note>Comparison to existing methods on video question answering. The 69M corpus is the 69M video questions in [63], 180M refers to the 180M YouTube clip-text pairs in YT-Temporal-180M [68].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Throwing [something] in the air and catching it. label: Throwing keys in the air and catching it. template: Throwing [something] in the air and letting it fall. label: Throwing keys in the air and letting it fall. template: Moving [something] away from [something]. label: Moving book away from can.</figDesc><table><row><cell>(a)</cell><cell>(c)</cell></row><row><cell>(b)</cell><cell>(d)</cell></row><row><cell></cell><cell>template: Moving [something] away from [something].</cell></row><row><cell></cell><cell>label: Moving mantis toy away from mini teddy bear.</cell></row></table><note>template:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison to existing methods on SSv2 tasks. * The training of Frozen on the SSv2-label retrieval task fails to converge despite our best efforts in tuning the model.</figDesc><table><row><cell cols="2">Method</cell><cell></cell><cell></cell><cell>#PT</cell><cell cols="2">#Train</cell><cell>SSv2-label</cell><cell>SSv2-template</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Frame R1</cell><cell>R5 R10 R1</cell><cell>R5 R10</cell></row><row><cell cols="2">Frozen [4]*</cell><cell></cell><cell></cell><cell cols="2">5M</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>52.9 94.8 99.4</cell></row><row><cell cols="2">CLIP4Clip [47]</cell><cell></cell><cell></cell><cell cols="2">400M</cell><cell>12</cell><cell>43.1 71.4 80.7 77.0 96.6 98.3</cell></row><row><cell cols="2">SINGULARITY</cell><cell></cell><cell></cell><cell cols="2">5M</cell><cell>1</cell><cell>36.4 64.9 75.4 42.0 86.2 94.3</cell></row><row><cell cols="3">SINGULARITY-temporal</cell><cell></cell><cell cols="2">5M</cell><cell>4</cell><cell>44.1 73.5 82.2 77.0 98.9 99.4</cell></row><row><cell cols="6">SINGULARITY-temporal 17M</cell><cell>4</cell><cell>47.4 75.9 84.0 77.6 96.0 98.9</cell></row><row><cell></cell><cell>Video Score</cell><cell cols="4">Video Score</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">lse/max/mean</cell><cell></cell></row><row><cell></cell><cell>Multi-Modal Enc.</cell><cell>Frame Score</cell><cell cols="2">Frame Score</cell><cell cols="2">Frame Score</cell></row><row><cell>Concat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vision Enc.</cell><cell>Lang. Enc.</cell><cell cols="2">Vision Enc.</cell><cell cols="3">Lang. Enc.</cell></row><row><cell></cell><cell>A group of</cell><cell></cell><cell></cell><cell cols="2">A group of</cell><cell></cell></row><row><cell></cell><cell>people play</cell><cell></cell><cell></cell><cell cols="3">people play</cell></row><row><cell></cell><cell>kites?</cell><cell></cell><cell></cell><cell cols="2">kites?</cell><cell></cell></row><row><cell cols="2">(a) Early fusion: concat</cell><cell cols="5">(b) Late fusion: lse/max/mean</cell></row></table><note>Multi-Modal Enc.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Temporal Enc.</figDesc><table><row><cell></cell><cell>Answer</cell><cell></cell><cell></cell><cell>Score</cell></row><row><cell></cell><cell>Decoder</cell><cell></cell><cell></cell><cell>Multi-Modal Enc.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Position Encoding</cell></row><row><cell>[</cell><cell>]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Multi-Modal Enc.</cell><cell></cell></row><row><cell></cell><cell>Vision Enc.</cell><cell>Lang. Enc.</cell><cell>Vision Enc.</cell><cell>Lang. Enc.</cell></row><row><cell></cell><cell></cell><cell>Q: What are people playing?</cell><cell></cell><cell>A group of people play kites...</cell></row><row><cell></cell><cell cols="2">(a) Question Answering</cell><cell cols="2">(b) Temporal Modeling</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>SINGULARITY-temporal results on text-to-video retrieval.</figDesc><table><row><cell>Method</cell><cell>#PT</cell><cell>#Train</cell><cell>MSRVTT</cell><cell>DiDeMo</cell><cell></cell><cell cols="3">ActivityNet Cap</cell></row><row><cell></cell><cell></cell><cell cols="7">Frame R1 R5 R10 R1 R5 R10 R1 R5 R10</cell></row><row><cell>HERO [37]</cell><cell>136M</cell><cell>310</cell><cell>20.5 47.6 60.9 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMT [18]</cell><cell cols="3">136M 1K/-/3K 26.6 57.1 69.6 -</cell><cell>-</cell><cell cols="4">-28.7 61.4 94.5</cell></row><row><cell>ClipBERT [31]</cell><cell cols="8">0.2M 16/16/8 22.0 46.8 59.9 20.4 48.0 60.8 21.3 49.0 63.5</cell></row><row><cell>VideoCLIP [61]</cell><cell>136M</cell><cell>960</cell><cell>30.9 55.4 66.8 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Frozen [4]</cell><cell>5M</cell><cell>4</cell><cell cols="4">31.0 59.5 70.5 31.0 59.8 72.4 -</cell><cell>-</cell><cell>-</cell></row><row><cell>AlignPrompt [34]</cell><cell>5M</cell><cell>8</cell><cell cols="4">33.9 60.7 73.2 35.9 67.5 78.8 -</cell><cell>-</cell><cell>-</cell></row><row><cell>CLIP4Clip [47]</cell><cell cols="8">400M 12/64/64 42.0 68.6 78.7 42.8 68.5 79.2 40.5 72.4 98.2</cell></row><row><cell>SINGULARITY</cell><cell>5M</cell><cell>1</cell><cell cols="6">36.8 65.9 75.5 47.4 75.2 84.0 43.0 70.6 81.3</cell></row><row><cell>SINGULARITY-temporal</cell><cell>5M</cell><cell>4</cell><cell cols="6">39.9 67.3 76.0 49.2 77.5 85.4 45.9 73.3 83.8</cell></row><row><cell>SINGULARITY</cell><cell>17M</cell><cell>1</cell><cell cols="6">41.5 68.7 77 53.9 79.4 86.9 47.1 75.5 85.5</cell></row><row><cell cols="2">SINGULARITY-temporal 17M</cell><cell>4</cell><cell cols="6">42.7 69.5 78.1 53.1 79.9 88.1 48.9 77.0 86.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>SINGULARITY-temporal results on video question answering.</figDesc><table><row><cell>Method</cell><cell cols="5">#PT #Train Frame MSRVTT-QA ActivityNet-QA MSRVTT-MC</cell></row><row><cell>ClipBERT [31]</cell><cell>0.2M</cell><cell>16</cell><cell>37.4</cell><cell>-</cell><cell>88.2</cell></row><row><cell>AlignPrompt [34]</cell><cell>5M</cell><cell>16</cell><cell>42.1</cell><cell>-</cell><cell>-</cell></row><row><cell>JustAsk [63]</cell><cell>69M</cell><cell>640</cell><cell>41.5</cell><cell>38.9</cell><cell>-</cell></row><row><cell>MERLOT [68]</cell><cell>180M</cell><cell>5</cell><cell>43.1</cell><cell>41.4</cell><cell>90.9</cell></row><row><cell>VideoCLIP [61]</cell><cell>136M</cell><cell>960</cell><cell>-</cell><cell>-</cell><cell>92.1</cell></row><row><cell>SINGULARITY</cell><cell>5M</cell><cell>1</cell><cell>42.7</cell><cell>41.8</cell><cell>92.0</cell></row><row><cell>SINGULARITY-temporal</cell><cell>5M</cell><cell>4</cell><cell>43.3</cell><cell>43.4</cell><cell>92.0</cell></row><row><cell>SINGULARITY</cell><cell>17M</cell><cell>1</cell><cell>43.5</cell><cell>43.1</cell><cell>92.1</cell></row><row><cell cols="2">SINGULARITY-temporal 17M</cell><cell>4</cell><cell>43.9</cell><cell>44.1</cell><cell>93.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>SINGULARITY zero-shot results on text-to-video retrieval.</figDesc><table><row><cell>Method</cell><cell>#PT</cell><cell>#Train</cell><cell></cell><cell>MSRVTT</cell><cell></cell><cell>DiDeMo</cell><cell></cell><cell cols="3">ActivityNet Cap</cell></row><row><cell></cell><cell></cell><cell cols="2">Frame R1</cell><cell cols="2">R5 R10 R1</cell><cell cols="3">R5 R10 R1</cell><cell cols="2">R5 R10</cell></row><row><cell>VideoCLIP [61]</cell><cell>137M</cell><cell>1K</cell><cell cols="4">10.4 22.2 30.0 16.6 46.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Frozen [4]</cell><cell>5M</cell><cell>4</cell><cell cols="5">18.7 39.5 51.6 21.1 46.0 56.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AlignPrompt [34]</cell><cell>5M</cell><cell>8</cell><cell cols="5">24.1 44.7 55.4 23.8 47.3 57.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLIP-straight</cell><cell>400M</cell><cell>1</cell><cell cols="2">31.2 53.7 64.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BLIP</cell><cell>130M</cell><cell>1</cell><cell cols="2">43.3 65.6 74.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SINGULARITY</cell><cell>5M</cell><cell>1</cell><cell cols="8">28.4 50.2 59.5 36.9 61.1 69.3 30.8 55.9 66.3</cell></row><row><cell>SINGULARITY</cell><cell>17M</cell><cell>1</cell><cell cols="8">34.0 56.7 66.7 37.1 61.7 69.9 30.6 55.6 66.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>SINGULARITY results on text-to-video retrieval, with mean/std over 5 random runs. We show the results for the model pre-trained on the 17M corpus. 1?0.5 69.3?0.4 78.1?0.7 53.3?1.0 78.7?1.3 86.3?1.5 47.0?0.5 75.7?0.3 85.3?0.3certain size, e.g., the model performance saturates at around 336?336 for the 3 tasks. Note that our model performance with larger image sizes might suffer from the low resolution of the raw videos we have. For example, we are only able to get videos of resolution 320?240 for MSRVTT.</figDesc><table><row><cell>Method</cell><cell>MSRVTT</cell><cell></cell><cell></cell><cell>DiDeMo</cell><cell></cell><cell></cell><cell>ActivityNet</cell><cell></cell></row><row><cell>R1</cell><cell>R5</cell><cell>R10</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell><cell>R1</cell><cell>R5</cell><cell>R10</cell></row><row><cell>SINGULARITY 42.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Impact of Image Size. We fine-tune models from the same checkpoint, pre-trained with input image size 224?224. We show average recall (average of R@{1,5,10}) for retrieval tasks, and accuracy for the QA task.</figDesc><table><row><cell cols="4">Image size MSRVTT retrieval DiDeMo retrieval ActivityNet QA</cell></row><row><cell>112</cell><cell>58.7</cell><cell>65.9</cell><cell>46.6</cell></row><row><cell>224</cell><cell>62.4</cell><cell>73.4</cell><cell>49.2</cell></row><row><cell>336</cell><cell>65.5</cell><cell>73.4</cell><cell>49.6</cell></row><row><cell>448</cell><cell>64.2</cell><cell>72.9</cell><cell>49.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Comparison to existing methods on image-text retrieval. We show results for both text retrieval (image-to-text retrieval, TR) and image retrieval (IR). 14M 80.6 95.2 97.6 63.1 85.3 91.1 96.6 99.8 100.0 87.2 97.5 98.8 BLIP [35] 129M 81.9 95.4 97.8 64.3 85.7 91.5 97.3 99.9 100.0 87.3 97.6 98.9 ALIGN [26] 1.2B 77.0 93.5 96.9 59.9 83.3 89.8 95.3 99.8 100.0 84.9 97.4 98.6 SINGULARITY 5M 71.9 90.8 95.4 54.6 80.0 87.8 93.3 99.4 99.8 81.4 95.8 97.9 SINGULARITY 17M 77.0 93.7 96.8 59.6 83.4 90.0 96.1 99.8 99.9 84.7 96.8 98.3</figDesc><table><row><cell></cell><cell></cell><cell cols="3">COCO (5K test)</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Flickr30K (1K test)</cell></row><row><cell>Method</cell><cell>#PT</cell><cell>TR</cell><cell></cell><cell cols="2">IR</cell><cell></cell><cell></cell><cell>TR</cell><cell></cell><cell></cell><cell>IR</cell></row><row><cell></cell><cell cols="11">R1 R5 R10 R1 R5 R10 R1 R5 R10 R1 R5 R10</cell></row><row><cell>ViLT [28]</cell><cell cols="11">4M 61.5 86.3 92.7 42.7 72.9 83.1 83.5 96.7 98.6 64.4 88.7 93.8</cell></row><row><cell>UNITER [12]</cell><cell cols="11">4M 65.7 88.6 93.8 52.9 79.9 88.0 87.3 98.0 99.2 75.6 94.1 96.8</cell></row><row><cell>OSCAR [40]</cell><cell cols="7">4M 70.0 91.1 95.5 54.0 80.8 88.5 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Frozen [4]</cell><cell>5M -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">61.0 87.5 92.7</cell></row><row><cell>ALBEF [36]</cell><cell cols="11">4M 73.1 91.4 96.0 56.8 81.5 89.2 94.3 99.4 99.8 82.8 96.7 98.4</cell></row><row><cell>ALBEF [36]</cell><cell cols="11">14M 77.6 94.3 97.2 60.7 84.3 90.5 95.9 99.8 100.0 85.6 97.5 98.9</cell></row><row><cell>BLIP [35]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Comparison to existing methods on VQA.</figDesc><table><row><cell>Method</cell><cell cols="3">#PT test-dev test-std</cell></row><row><cell>ClipBERT [31]</cell><cell>0.2M</cell><cell>69.08</cell><cell>69.43</cell></row><row><cell>ViLT [28]</cell><cell>4M</cell><cell>70.94</cell><cell>-</cell></row><row><cell>VL-BART [13]</cell><cell>0.2M</cell><cell>-</cell><cell>71.30</cell></row><row><cell>LXMERT [55]</cell><cell>4M</cell><cell>72.42</cell><cell>72.54</cell></row><row><cell>UNITER [12]</cell><cell>4M</cell><cell>72.70</cell><cell>72.91</cell></row><row><cell>UNIMO [39]</cell><cell>4M</cell><cell>73.79</cell><cell>74.02</cell></row><row><cell>OSCAR [40]</cell><cell>4M</cell><cell>73.16</cell><cell>73.44</cell></row><row><cell>ALBEF [36]</cell><cell>4M</cell><cell>74.54</cell><cell>74.70</cell></row><row><cell>ALBEF [36]</cell><cell>14M</cell><cell>75.84</cell><cell>76.04</cell></row><row><cell>BLIP [35]</cell><cell>14M</cell><cell>77.54</cell><cell>77.62</cell></row><row><cell>BLIP [35]</cell><cell>129M</cell><cell>78.24</cell><cell>78.17</cell></row><row><cell>SINGULARITY</cell><cell>5M</cell><cell>70.30</cell><cell>70.53</cell></row><row><cell>SINGULARITY</cell><cell>17M</cell><cell>73.13</cell><cell>73.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>SINGULARITY hyper-parameters for pre-training, video QA, image QA and text-to-image retrieval. We only list a single value if all tasks share the same value. For SINGULARITY-temporal, we train with a similar setup, except that we set #training frames to be 4. In addition, for SINGULARITYtemporal 2nd stage pre-training, we also use a smaller batch size of 32 per GPU.</figDesc><table><row><cell>config</cell><cell cols="3">pre-training video QA image QA</cell><cell>text-to-image retrieval</cell></row><row><cell>optimizer</cell><cell></cell><cell></cell><cell cols="2">AdamW [45]</cell></row><row><cell>optimizer momentum</cell><cell></cell><cell></cell><cell cols="2">? 1 , ? 2 =0.9,0.999</cell></row><row><cell>base learning rate</cell><cell>1e-4</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell></row><row><cell>min learning rate</cell><cell>1e-5</cell><cell>1e-6</cell><cell>1e-6</cell><cell>1e-6</cell></row><row><cell>weight decay</cell><cell></cell><cell></cell><cell>0.02</cell><cell></cell></row><row><cell>learning rate schedule</cell><cell></cell><cell></cell><cell cols="2">cosine decay [44]</cell></row><row><cell>image size</cell><cell>224</cell><cell>224</cell><cell>336</cell><cell>336</cell></row><row><cell>image augmentation</cell><cell></cell><cell cols="3">random resize, crop, horizontal flip</cell></row><row><cell>#training epochs</cell><cell>10</cell><cell>10</cell><cell>5</cell><cell>10 (Flickr30K), 5 (COCO)</cell></row><row><cell>#warmup epochs</cell><cell>1</cell><cell>0.5</cell><cell>0.5</cell><cell>0</cell></row><row><cell>batch size x #GPUs</cell><cell>128?3</cell><cell>32?1</cell><cell>64?4</cell><cell>64?2</cell></row><row><cell>#training frames</cell><cell></cell><cell></cell><cell>1</cell><cell></cell></row><row><cell>#inference frames</cell><cell>-</cell><cell>12</cell><cell>1</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 :</head><label>12</label><figDesc>SINGULARITY hyper-parameters for text-to-video retrieval tasks. We only list a single value if all tasks share the same value. For SINGULARITY-temporal, we train it with a similar setup, except that we set #training frames to be 4.</figDesc><table><row><cell>config</cell><cell cols="4">MSRVTT DiDeMo ActivityNet Captions SSv2-template/label</cell></row><row><cell>optimizer</cell><cell></cell><cell></cell><cell>AdamW [45]</cell><cell></cell></row><row><cell>optimizer momentum</cell><cell></cell><cell></cell><cell>? 1 , ? 2 =0.9,0.999</cell><cell></cell></row><row><cell>base learning rate</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-5</cell><cell>1e-4</cell></row><row><cell>min learning rate</cell><cell>1e-6</cell><cell>1e-6</cell><cell>1e-6</cell><cell>1e-5</cell></row><row><cell>weight decay</cell><cell></cell><cell></cell><cell>0.02</cell><cell></cell></row><row><cell>learning rate schedule</cell><cell></cell><cell></cell><cell>cosine decay [44]</cell><cell></cell></row><row><cell>image size</cell><cell></cell><cell></cell><cell>224</cell><cell></cell></row><row><cell>image augmentation</cell><cell></cell><cell cols="2">random resize, crop, horizontal flip</cell><cell></cell></row><row><cell>#training epochs</cell><cell>5</cell><cell>10</cell><cell>10</cell><cell>10</cell></row><row><cell>#warmup epochs</cell><cell></cell><cell></cell><cell>0</cell><cell></cell></row><row><cell>batch size x #GPUs</cell><cell>32x1</cell><cell>32x1</cell><cell>32x1</cell><cell>32x2</cell></row><row><cell>#training frames</cell><cell></cell><cell></cell><cell>1</cell><cell></cell></row><row><cell>#inference frames</cell><cell>12</cell><cell>12</cell><cell>32</cell><cell>12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Statistics of pre-training datasets. The average video length of WebVid is 18 seconds.</figDesc><table><row><cell>Dataset</cell><cell>#image/video</cell><cell>#text</cell><cell>Type</cell></row><row><cell>COCO [11]</cell><cell>113K</cell><cell>567K</cell><cell>image</cell></row><row><cell>VG [30]</cell><cell>100K</cell><cell>768K</cell><cell>image</cell></row><row><cell>SBU [49]</cell><cell>860K</cell><cell>860K</cell><cell>image</cell></row><row><cell>CC3M [52]</cell><cell cols="2">2.95M 2.95M</cell><cell>image</cell></row><row><cell>CC12M [10]</cell><cell cols="2">10.77M 10.77M</cell><cell>image</cell></row><row><cell>WebVid [4]</cell><cell cols="2">2.49M 2.49M</cell><cell>video</cell></row><row><cell>5M corpus = CC3M+WebVid</cell><cell cols="3">5.44M 5.44M video+image</cell></row><row><cell>17M corpus = 5M+COCO+VG+SBU+CC12M</cell><cell cols="3">17.28M 18.41M video+image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 :</head><label>14</label><figDesc>Statistics of downstream datasets.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>#video</cell><cell></cell><cell></cell><cell>#text</cell><cell></cell><cell>Avg Video</cell></row><row><cell></cell><cell>Train</cell><cell>Val</cell><cell>Test</cell><cell>Train</cell><cell>Val</cell><cell cols="2">Test Length (s)</cell></row><row><cell>Text-to-Video Retrieval</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ActivityNet Captions [29] 10,009</cell><cell cols="3">-4,917 10,009</cell><cell cols="2">-4,917</cell><cell>180</cell></row><row><cell>DiDeMo [2]</cell><cell cols="3">8,394 1,065 1,003</cell><cell cols="3">8,394 1,065 1,003</cell><cell>29.3</cell></row><row><cell>MSRVTT [62]</cell><cell>7,010</cell><cell cols="3">-1,000 140,200</cell><cell></cell><cell>1,000</cell><cell>15</cell></row><row><cell>SSV2-Template [19]</cell><cell>168,913</cell><cell cols="2">-2,088</cell><cell>174</cell><cell>-</cell><cell>174</cell><cell>4</cell></row><row><cell>SSV2-Label [19]</cell><cell>168,913</cell><cell cols="3">-2,088 109,968</cell><cell cols="2">-1,989</cell><cell>4</cell></row><row><cell>Video Question Answering</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MSRVTT-QA [60]</cell><cell>6,513</cell><cell cols="5">497 2,990 158,581 12,278 72,821</cell><cell>15</cell></row><row><cell>ActivityNet-QA [66]</cell><cell cols="2">3,200 1,800</cell><cell cols="4">800 32,000 18,000 8,000</cell><cell>180</cell></row><row><cell>MSRVTT-MC [65]</cell><cell>7,010</cell><cell cols="3">-2,990 140,200</cell><cell></cell><cell>14,950</cell><cell>15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 15 :</head><label>15</label><figDesc>Dataset licenses.</figDesc><table><row><cell>Dataset</cell><cell>License</cell></row><row><cell>COCO [11]</cell><cell>CC BY 4.0, Flickr Terms of Use</cell></row><row><cell>VG</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno>ICLR, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamal</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristobal</forename><surname>Eyzaguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01720</idno>
		<title level="m">Revisiting the &quot;video&quot; in video-language understanding</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Uniter: Learning universal image-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Temporal localization of moments in video collections with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Perceiver io: A general architecture for structured inputs &amp; outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno>ICLR, 2022. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densecaptioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tvqa: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">What is more likely to happen next? video-and-language future event prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Align and prompt: Video-and-language pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>with entity prompts. arXiv, 2021. 4, 5, 9</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12086</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hero: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Vx2text: End-to-end learning of video-based text generation from multimodal inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>BMVC, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Im2text: Describing images using 1 million captioned photographs. NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Learning transferable visual models from natural language supervision. arXiv, 2021. 2, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>All in one: Exploring unified video-language pre-training. arXiv, 2022. 5</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Videoclip: Contrastive pre-training for zeroshot video-text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Just ask: Learning to answer questions from millions of narrated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 5</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Activitynet-qa: A dataset for understanding complex web videos via question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Merlot: Multimodal neural script knowledge models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Yin and yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
