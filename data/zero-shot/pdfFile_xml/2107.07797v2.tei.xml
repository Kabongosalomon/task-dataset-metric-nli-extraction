<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Directed Graph Convolution for 3D Human Pose Estimation CCS CONCEPTS ? Computing methodologies ? Motion capture. KEYWORDS 3D human pose, conditional directed graph convolution ACM Reference Format</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date>October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changgong</forename><surname>Zhang</surname></persName>
							<email>changgong.zcg@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
							<email>fnzhan@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
							<email>ttwong@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changgong</forename><surname>Zhang</surname></persName>
							<email>cslzhang@comp.polyu.edu.hk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Tsin</forename><surname>Wong</surname></persName>
						</author>
						<title level="a" type="main">Conditional Directed Graph Convolution for 3D Human Pose Estimation CCS CONCEPTS ? Computing methodologies ? Motion capture. KEYWORDS 3D human pose, conditional directed graph convolution ACM Reference Format</title>
					</analytic>
					<monogr>
						<title level="m">MM &apos;21</title>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published">October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475219</idno>
					<note>Input 2D pose Estimated 3D pose by UGCN Estimated 3D pose by our method * Equal contribution ? Corresponding author 2021. Conditional Directed Graph Convolution for 3D Human Pose Estima-tion. In Proceedings of the 29th ACM International Conference on Multimedia (MM &apos;21), October 20-24, 2021, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Given a sequence of 2D human poses estimated by an off-the-shelf 2D pose estimator, e.g., , our method can produce more precise 3D poses compared with state-of-the-art approach, UGCN [49]. We show the results under three different viewpoints as indicated by the 3D orientation markers. And ground-truth 3D poses are shown in gray as a reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>Graph convolutional networks have significantly improved 3D human pose estimation by representing the human skeleton as an undirected graph. However, this representation fails to reflect the articulated characteristic of human skeletons as the hierarchical orders among the joints are not explicitly presented. In this paper, we propose to represent the human skeleton as a directed graph with the joints as nodes and bones as edges that are directed from parent joints to child joints. By so doing, the directions of edges can explicitly reflect the hierarchical relationships among the nodes. Based on this representation, we further propose a spatial-temporal conditional directed graph convolution to leverage varying nonlocal dependence for different poses by conditioning the graph topology on input poses. Altogether, we form a U-shaped network, named U-shaped Conditional Directed Graph Convolutional Network, for 3D human pose estimation from monocular videos. To evaluate the effectiveness of our method, we conducted extensive experiments on two challenging large-scale benchmarks: Human3.6M and MPI-INF-3DHP. Both quantitative and qualitative results show that our method achieves top performance. Also, ablation studies show that directed graphs can better exploit the hierarchy of articulated human skeletons than undirected graphs, and the conditional connections can yield adaptive graph topologies for different poses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human pose estimation from monocular videos plays a critical role in a wide spectrum of applications, e.g., action recognition <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b52">53]</ref>, athletic training <ref type="bibr" target="#b47">[48]</ref>, data-driven computer animation, and gaming. Compared with the 2D pose in image space, the 3D pose in physical space is more informative. However, estimating 3D poses from monocular videos is much more challenging due to the depth ambiguity, metric inconsistency (i.e., millimeters instead of pixels), and the high non-linearity of human dynamics. Given a monocular video of human motions acquired, for example, from consumer-level cameras, our ultimate goal is to estimate the human pose sequence in the 3D physical space. Following <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49]</ref>, we define the 3D pose as the 3D locations of joints, including "head", "shoulders", "knees", "elbows", and so no, of the human body.</p><p>Thanks to the development of deep learning, we have witnessed remarkable achievements in 3D pose reasoning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>. State-of-the-art approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49]</ref> typically divide the task into two stages: 2D pose estimation to localize the keypoints in the image space, and predicting joint positions in the 3D space from 2D pixel coordinates. We follow this strategy and focus on the second stage, lifting 2D pixel coordinates to 3D positions, while the first stage, 2D pose estimation, is also a popular vision task that is quite well-studied in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b40">41]</ref>. Recent <ref type="figure" target="#fig_2">Figure 2</ref>: The BVH file format (a) is a concrete representation of the hierarchical structure of human skeletons. We adopt the directed graph (b) to represent the articulated human skeleton, with the hierarchy is represented by the directions of edges. The "hip" joint (red dot in (b)) is set as the directed graph's root node. efforts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b48">49]</ref> have explored to represent the human pose as an undirected spatial-temporal graph and thereafter employ graph convolution networks to estimate the 3D pose. Compared with representing human pose as a time sequence of independent joint location vectors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>, the undirected graph representation is more relevant to the inherent nature of human skeletons.</p><p>However, the undirected graph representation does not take into account the hierarchical structure of bones, which is one of the most significant characteristics of human anatomy. For example, when a person moves the shoulder joints, the following joints (i.e. elbows and wrists) are moved as well according to the anatomical articulation while not the other way around. This hierarchy is widely utilized to represent human motion in computer animation as evidenced by the Biovision Hierarchy (BVH) file format, which presents a typical hierarchical structure as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref> (a). To this end, we propose to represent the human skeleton as a directed graph with the joints as nodes and bones as edges that are directed from parent joints to child joints, as shown in <ref type="figure" target="#fig_2">Figure 2</ref> (b). By doing so, the hierarchical relationships among all the joints are explicitly presented by directions of the edges.</p><p>After representing 2D poses as a sequence of directed graphs, we can employ the spatial-temporal directed graph convolution (ST-DGConv) to extract features. However, ST-DGConv shares the same graph topology among all kinds of poses, which may not be optimal since there is non-local dependence among the nodes and the non-local dependence varies a lot for different poses. For example, as shown in <ref type="figure">Figure 3</ref>, the dependence between "left hand" and "right foot" joints is obviously significant when people are walking (since this pose can help keep balance). In contrast, the dependence between "hands" and "head" joints would be high when eating. Inspired by the conditional convolution <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b53">54]</ref> that enables different data samples using different convolution kernels, we propose a spatial-temporal conditional directed graph convolution (ST-CondDGConv) to condition the connections of the directed graph on input poses, such that different kinds of poses can adopt appropriate connections to optimally leverage non-local dependence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Walking</head><p>Eating ! " # <ref type="figure">Figure 3</ref>: Varying non-local dependence among joints. Dependence between "left hand" and "right foot" is significant when people are walking (first row) since this pose can help to keep balance, while dependence between "left hand" and "head" is high when people are eating (second row). 1 , 2 , and 3 are time indices.</p><p>To the best of our knowledge, this is the first attempt to introduce conditional connections to directed graph convolution for 3D human pose estimation. Overall, we composite a U-shaped network with the ST-DGConv and ST-CondDGConv layers, named U-shaped Conditional Directed Graph Convolutional Network (U-CondDGCN), to capture temporal relationship in both short temporal intervals and long temporal ranges. We evaluated our model on two large-scale 3D human pose estimation benchmarks: Human3.6M <ref type="bibr" target="#b16">[17]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b28">[29]</ref>. Both quantitative and qualitative experimental results show our method achieves top performance. Moreover, we conducted extensive ablation studies to demonstrate that the directed graph can better utilize the hierarchy of articulated human skeletons, and the conditional connections can yield adaptive graph topologies for different kinds of poses. Our contributions are summarized below.</p><p>? We argue that the hierarchy of articulated skeletons is beneficial for 3D pose reasoning, and directed graphs are more suitable to model the hierarchy than undirected graphs. ? We propose a novel conditional directed graph convolution to enable adaptive graph topologies for different pose samples at inference time, such that different poses can benefit from appropriate non-local dependence. ? We present a U-shaped Conditional Directed Graph Convolutional Network (U-CondDGCN) for 3D human pose estimation from detected 2D keypoints. Our method achieves top performance on the challenging Human3.6M and MPI-INF-3DHP benchmarks, thus demonstrating its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 3D Human Pose Estimation</head><p>With the development of deep learning, 2D human pose estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref> has shown remarkable progress, while 3D pose estimation remains more challenging due to the depth ambiguity. Several methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b45">46]</ref> propose to relieve this issue  <ref type="figure">Figure 4</ref>: Overview of our framework. Given a sequence of 2D poses estimated by any off-the-shelf 2D pose estimators, we first construct a sequence of directed graphs and then estimate the 3D poses with our U-shaped conditional directed graph convolutional network (U-CondDGCN).</p><p>by adopting multi-view images/videos as input. However, multiview observations are expensive to obtain in daily life scenarios. Thus, 3D human pose estimation from monocular images/videos is highly demanded. Some works explored to directly infer 3D human pose from monocular images/videos with end-to-end deep neural networks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b42">43]</ref>. This end-to-end idea is elegant and free of accumulated errors. However, we need paired data (images/videos and corresponding 3D poses) to supervise the training, while manually labeling 3D human poses for images/videos is impractical. To this end, Martinez et al. <ref type="bibr" target="#b27">[28]</ref> proposed to divide 3D human pose estimation into 2D keypoint detection followed by lifting 2D joint locations to 3D positions, such that the first stage can be trained with manually labeled 2D poses and the training data for the second stage can be obtained by projecting 3D poses, obtained from motion capture devices, to 2D space. This divide-and-conquer strategy benefits from intermediate supervision and outperforms the end-toend counterparts. A family of approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> followed this strategy and focused more on lifting 2D to 3D. Our method can also be categorized into this group.</p><p>Recently, Hossain et al. <ref type="bibr" target="#b15">[16]</ref> proposed to employ LSTM <ref type="bibr" target="#b14">[15]</ref> to leverage temporal information for lifting 2D joint locations to 3D positions. Dilated temporal convolutions <ref type="bibr" target="#b34">[35]</ref> and temporal attention <ref type="bibr" target="#b24">[25]</ref> are further explored for better temporal information aggregation. Besides, some works focus on specific issues in the 3D human pose estimation, e.g., tackling occlusion problems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, relieving the unreliable input issue by kinematics analysis <ref type="bibr" target="#b50">[51]</ref>, or addressing the lack of 3D annotations in the wild and overfitting issues by weakly-supervised learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47]</ref>. Also, Lin et al. <ref type="bibr" target="#b22">[23]</ref> proposed to predict the 3D human pose in trajectory space by factoring the pose sequence into a trajectory base matrix and a coefficient matrix. The above methods represent human poses as a temporal sequence of independent joint location vectors. However, this representation cannot fully express the dependence among highly correlated human joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Convolution Network (GCN)</head><p>GCNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref> generalize conventional convolution operators to graphs, and can be roughly categorized into spectral <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> and spatial <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref> perspective GCNs. Our proposed U-CondDGCN is more related to the latter one. Interested readers are referred to <ref type="bibr" target="#b49">[50]</ref> for a complete survey of GCNs.</p><p>By representing human skeletons as graphs, GCNs have significantly improved a series of human-related reasoning tasks, e.g., action recognition <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b52">53]</ref>, action synthesis <ref type="bibr" target="#b51">[52]</ref> and pose tracking <ref type="bibr" target="#b31">[32]</ref>. Moreover, several works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58]</ref> adopt undirected graphs to represent human skeletons and apply GCN to utilize the prior knowledge of human skeleton. However, the undirected graph representation fails to reflect the articulated characteristic of human skeletons as the hierarchical orders among joints are not explicitly presented. In contrast, directed graph representation explicitly models hierarchical relationships among the nodes by the directions of edges. Recently, Shi et al. <ref type="bibr" target="#b37">[38]</ref> also employed the directed graph representation and proposed a directed graph neural network for action recognition. Their method learns the graph topology from the training data rather than simply defining it based on the natural structure of human skeletons. However, the graph topology is fixed at inference time, which means different data samples still share the same topology. Differently, our conditional directed graph convolution allows different poses to benefit from appropriate non-local dependence at both training and inference time, by conditioning the graph topology on input poses. This mechanism is crucial for 3D human pose estimation since the optimal non-local dependence varies a lot for different poses, as shown in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>Our full pipeline is illustrated in <ref type="figure">Figure 4</ref>. We first construct a temporal sequence of directed graphs from a sequence of human poses in the 2D image space 2D = { , ? R 2 | = 1, 2, 3, ..., ; = 1, 2, 3, ..., }, where and denote the number of frames in the sequence and joints on the human skeleton, respectively. The input 2D pose sequence 2D can be estimated from monocular videos by any off-the-shelf 2D pose estimators, e.g., CPN <ref type="bibr" target="#b4">[5]</ref>, HR-Net <ref type="bibr" target="#b40">[41]</ref>, or OpenPose <ref type="bibr" target="#b2">[3]</ref>. The nodes in the directed graph represent major joints of the human skeleton, while the edges represent the bones among the joints, as shown in <ref type="figure" target="#fig_2">Figure 2</ref> (b). We set the directions of edges following the convention definition in the BVH file format. And the "hip" joint, marked as the red dot in <ref type="figure" target="#fig_2">Figure 2</ref> (b), is set as the root node since it is the gravity center of the human body. The features associated with nodes and edges are initialized as the joints' locations and their first-order derivatives (the difference between the child joint and parent joint), respectively. Formally, a temporal sequence of directed graphs can be formulated as G 2D = { = (N, E) | = 1, 2, 3, ..., }, where N is the set of nodes, and E is the set of directed edges. We then apply our U-shaped Conditional Directed Graph Convolutional Network (U-CondDGCN) to estimate the pose sequence in the 3D physical space 3D = { , ? R 3 | = 1, 2, 3, ..., ; = 1, 2, 3, ..., }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Blocks</head><p>After representing the 2D human pose as a sequence of directed graphs, the problem now lies in how to extract features from them to estimate the 3D pose. To aggregate features both spatially and temporally, we use five types of blocks in our network, i.e., spatial-temporal directed graph convolution (ST-DGConv), spatialtemporal conditional directed graph convolution (ST-CondDGConv), temporal downsampling, temporal upsampling, and FC blocks, as shown in <ref type="figure">Figure 4</ref>.</p><p>ST-DGConv. The ST-DGConv block consists of a directed graph convolution (DGConv) followed by a temporal convolution, as shown in <ref type="figure" target="#fig_0">Figure 5</ref> (a). DGConv exploits the spatial relationship by aggregating features from neighboring edges or nodes. Details of DGConv are to be presented in Section 3.2. On the other hand, to take advantage of the temporal relationship, we employ a temporal convolution, which is a conventional 1D convolution, since the temporal sequence of directed graphs has a regular grid structure along the temporal dimension.</p><p>ST-CondDGConv. The aforementioned ST-DGConv is based on a fixed directed graph connection E, which is defined according to the natural structure of human skeletons. However, the predefined connections can only utilize the local dependence among hierarchically neighboring joints. More importantly, sharing the same connections among all kinds of poses may not be optimal since the non-local dependence varies a lot for different poses, as shown in <ref type="figure">Figure 3</ref>. Inspired by the conditional convolution <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b53">54]</ref> that enables different data samples using different convolution kernels, we propose a spatial-temporal conditional directed graph convolution (ST-CondDGConv) to condition the connections of the directed graph on input poses, such that different poses can adopt appropriate connections to exploit varied non-local dependence. As shown in <ref type="figure" target="#fig_0">Figure 5</ref> (b), the ST-CondDGConv predicts the conditional connections CondE from previous layer's output. Specifically, there are a series of trainable connection matrix bases</p><formula xml:id="formula_0">{E 1 , E 2 , E 3 , ..., E },</formula><p>where E ? R ? and is the number of bases (set to 16 in our experiments), for the network to learn. To encourage the sparsity of the connection matrix bases, we employ the sparse initialization <ref type="bibr" target="#b26">[27]</ref> before the network training. We use a routing function to predict the blending weights for the connection matrix bases from the previous layer's output. The network structure of the routing function is the same as that used in conditional convolution <ref type="bibr" target="#b53">[54]</ref>, which is a global average pooling layer followed by a fully connected layer and a Sigmoid activation. After that, we linearly combine the bases by the predicted blending weights to produce the conditional connections CondE, which are then fed into the CondDGConv to aggregate the spatial information both locally and non-locally. Note that non-local connections here are conditioned on the previous layer's output. Thus, the routing function is able to differentiate input samples to allow different kinds of poses to leverage appropriate non-local dependence at both training and inference time. Finally, a temporal convolution is employed to aggregate the temporal information.</p><formula xml:id="formula_1">PREV LAYER OUTPUT ROUTE FN ? ! ? " ? # COMBINE CondDGConv Cond? ? $ PREV LAYER OUTPUT DGConv TemporalConv TemporalConv (a) ST-DGConv (b) ST-CondDGConv</formula><p>Besides the above two major blocks, the other three types of blocks can be easily derived. The temporal downsampling block is the ST-DGConv with the inside temporal convolution's stride setting to two. It is used to downsample the temporal resolution for the larger receptive field. The temporal upsampling block is the conventional bilinear interpolation along the temporal axis to recover higher temporal resolution. The FC block is the standard fully-connected layer to predict the final 3D pose from the extracted features on directed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional Directed Graph Convolution</head><p>In this section, we first formulate our conditional directed graph convolution (CondDGConv), which is the key operator in the ST-CondDGConv block. The directed graph convolution (DGConv), the key operator in ST-DGConv, can be easily derived from it. As shown in <ref type="figure">Figure 6</ref> (a), the input directed graph for CondDGConv has the predefined connections E = { | = 1, 2, ..., } and the predicted conditional connections CondE = {Cond | = 1, 2, ..., }, where and are the number of predefined connections and conditional connections, respectively. The predefined connections and conditional connections are shown as arrows and blue dash-line arrows in <ref type="figure">Figure 6</ref>, respectively. The CondDGConv can be formulated with three steps, as shown in <ref type="figure">Figure 6</ref> (b), (c), and (d).</p><p>Nodes updating. Connections of the directed graph used in this step are the predefined connections, E = { | = 1, 2, ..., }. For each node , we have the set of incoming edges that are heading into it, E ? , and the set of outgoing edges that are heading out from it, E + . Following the idea of conventional convolution, aggregating neighboring features, the nodes updating is defined as the aggregation of its incoming edge set, its outgoing edge set, and itself. However, the number of elements in the outgoing edge set is varying. For example, as shown in <ref type="figure">Figure 6 (b)</ref>, the incoming edge set of node <ref type="bibr" target="#b3">4</ref> is E ? 4 = { 3 } while the outgoing edge set is E + 4 = { 4 , 5 }. Therefore, we employ a pooling function to summarize features of the outgoing edge set. Mathematically, the nodes updating step can be formulated as:</p><formula xml:id="formula_2">! ! ! " ! # ! $ ! % ! &amp; " ! " % " &amp; " " " # ! ! ! " ! # ! $ ! % ! &amp; " ! " % " &amp; " " " # ! ! ' ! " ' ! # ' ! $ ' ! % ' ! &amp; ' " ! " % " &amp; " " " # ! ! ' ! " ' ! # ' ! $ ' ! % ' ! &amp; ' C o n d !1 C o n d ! 2 ! ! '' ! " '' ! # '' ! $ '' ! % '' ! &amp; '' ! ! '' ! " '' ! $ '' ! &amp; '' " % " &amp; ! # '' ! % '' " ! " " " # ! ! '' ! " '' ! # '' ! $ '' ! % '' ! &amp; '' " ! ' " % ' " &amp; ' " " ' " # '<label>(</label></formula><formula xml:id="formula_3">( ? ) = (w ? [ (E ? ); ( ); P ( (E + ))] T + b),<label>(1)</label></formula><p>where (?) is the mapping from nodes/edges to their associated features; P (?) is the pooling function (average pooling is adopted in our implementation); w and b are the trainable parameters that are similar with the kernel and bias of conventional convolution, respectively; and is the activation function (ReLU is used in our implementation).</p><p>Nodes updating with conditional connections. This step is designed to address the varying non-local dependence for different poses. Thus, connections of the directed graph used in this step are the predicted conditional connections, CondE = {Cond | = 1, 2, ..., }. For each node ? , we have the parent nodes set N p that have edges directed at ? and the child nodes set N c that have edges directed from ? . For example, as shown in <ref type="figure">Figure 6</ref>  Edges updating. Connections of the directed graph used in this step are the predefined connections, E = { | = 1, 2, ..., }. For each edge , we have the source node N and the target node N , e.g., as shown in <ref type="figure">Figure 6 (d)</ref>, the source node and target node of edge <ref type="bibr" target="#b2">3</ref> are ?? 1 and ?? 4 , respectively. Again, we can formulate the edges updating step as:</p><formula xml:id="formula_4">( ? ) = (w ? [ (N ); ( ); (N )] T + b).<label>(3)</label></formula><p>After these three steps, features associated with both nodes and edges are updated. Note that even the edges updating step adopts the predefined connections; the non-local dependence can also be aggregated into the edges' features since they are updated by the source and target nodes' features that have aggregated the non-local information. Removing step (ii), nodes updating with conditional connections, can yield the directed graph convolution (DGConv), which is purely based on the predefined connections. Thus, it can utilize the prior knowledge of the natural structure of human skeletons but fails to leverage the varying non-local dependence for different poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Full Network</head><p>We construct the full network as a U-shaped conditional directed graph convolutional network (UCondDGCN) using the aforementioned blocks to capture temporal relationships in both short temporal intervals and long temporal ranges. As shown in the middle part of <ref type="figure">Figure 4</ref>, the UCondDGCN has three stages: i) downsampling stage to aggregate information at long-time ranges by temporal pooling; ii) upsampling stage to recover the temporal resolution, and there are skip connections between the downsampling and upsampling stages to integrate the low-level details; iii) merging stage to combine multi-scale feature maps to predict the final 3D poses. The numbers shown in the figure indicate the shape of features in the network, where is the batch size, is the number of nodes, is the number of edges, and is the sequence length. As discussed in Section 3.2, the ST-DGConv can better utilize the prior knowledge of the natural structure of human skeletons, whereas ST-CondDGConv allows different kinds of poses to adopt appropriate non-local dependence. To balance the stability and flexibility, we adopt ST-DGConv in the downsampling and upsampling stages while adopting ST-CondDGConv in the merging stage. This configuration is further justified with ablation study as shown in <ref type="table" target="#tab_4">Table 4</ref>. Overall, the input of our UCondDGCN is the 2D human poses represented in directed graphs, and the output is the corresponding 3D poses, as shown in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We formulate the loss function similar to UGCN <ref type="bibr" target="#b48">[49]</ref>,</p><formula xml:id="formula_5">L = L + L ,<label>(4)</label></formula><p>where L is 3D joint position loss that is known as the mean perjoint position error (MPJPE), L is motion loss <ref type="bibr" target="#b48">[49]</ref>, and is a weight to balance them (set to 0.1 in our implementation). The motion loss is originally proposed in UGCN <ref type="bibr" target="#b48">[49]</ref> to supervise the temporal structure of the predicted pose sequence. It first encodes the positions of the same joint at two different temporal instants into pairwise motion encodings for the predicted pose sequence and the ground truth, respectively; and then computes the L1 loss between them. The number of layers, kernel size, and input sequence length are also followed UGCN <ref type="bibr" target="#b48">[49]</ref>. We implemented UCondDGCN on the PyTorch platform <ref type="bibr" target="#b32">[33]</ref> and conducted experiments on a single NVIDIA TITAN V GPU. We optimized the model by the AdaMod optimizer <ref type="bibr" target="#b9">[10]</ref> for 110 epochs with a batch size of 256, in which the learning rate was initially <ref type="table">Table 1</ref>: Quantitative comparisons with state-of-the-art methods on Human3.6M under protocol #1 and protocol #2, where methods marked with ? are video-based; T denotes the number of input frames; and CPN and HR-Net denote the input 2D poses are estimated by <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b40">[41]</ref>, respectively. The best and second-best results are marked in bold and underlined, respectively.   set to 5 ? 10 ?3 and decayed by 0.1 after the 80 th , 90 th , and 100 th epochs. To avoid over-fitting, we set the weight decay factor to 10 ?5 and the dropout rate to 0.3. We followed UGCN <ref type="bibr" target="#b48">[49]</ref> to adopt the sliding window algorithm with a step length of five frames to estimate variable sequence length at inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>Human3.6M. Human3.6M <ref type="bibr" target="#b16">[17]</ref> is the most widely used evaluation benchmark, containing 3.6 million video frames captured from four synchronized cameras with different locations and poses at 50 Hz. There are 11 subjects performing 15 kinds of actions, e.g., "walking", "sitting", and "eating". Following previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49]</ref>, we adopted the 17-joint pose, trained a single model on five subjects (S1, S5, S6, S7, S8) for all kinds of actions, and tested it on the remaining two subjects (S9 and S11). <ref type="bibr" target="#b28">[29]</ref> is a relatively new dataset captured in both indoor and outdoor environments. Similar to Hu-man3.6M, it contains various subjects, actions, and camera settings, and we followed <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref> to split the training and testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP. MPI-INF-3DHP</head><p>Evaluation Metrics. For Human3.6M, we adopt the most widely used two metrics: Protocol 1 is the mean per-joint position error (MPJPE) that is the mean Euclidean distance between the estimated joint positions and ground truth in millimeters; and Protocol 2 is the error after alignment with the ground truth in translation, rotation, and scale (P-MPJPE). For MPI-INF-3DHP, we also report the percentage of correct keypoints (PCK) <ref type="bibr" target="#b28">[29]</ref> score with the threshold of 150 and the area under the curve (AUC) <ref type="bibr" target="#b28">[29]</ref> of the PCK scores with different error thresholds, following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>Results on Human3.6M. To evaluate the effectiveness of our U-CondDGCN, we first quantitatively compared our method with state-of-the-art methods on the Human3.6M benchmark in <ref type="table">Table 1</ref>, including image-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b56">57]</ref> and video-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b50">51]</ref>  estimated from monocular images/videos by either CPN <ref type="bibr" target="#b4">[5]</ref> or HR-Net <ref type="bibr" target="#b40">[41]</ref> (a more powerful 2D pose estimator). We can see that videobased methods generally perform better than image-based methods, which justifies that the temporal information is beneficial to the 3D human pose estimation. More importantly, from both Protocol #1 and Protocol #2 results, we can see our method consistently outperforms all the others by a large margin no matter with CPN or HR-Net input 2D poses (1.7mm and 1.5mm improvements in terms of Protocol #1, respectively). This demonstrates the effectiveness of our proposed conditional directed graph convolution. To further explore the upper bound of our U-CondDGCN for lifting 2D poses to 3D poses, we compared our method with several state-of-the-art methods with ground-truth 2D poses as input since doing so can eliminate the influence of the 2D pose estimators applied. As shown in <ref type="table" target="#tab_2">Table 2</ref>, we can see our method significantly outperforms all the others (? 2.9</p><p>) in terms of MPJPE. It demonstrates if a more powerful 2D pose estimator is available, our U-CondDGCN is able to produce more accurate 3D poses.</p><p>Results on MPI-INF-3DHP. To evaluate the generalization ability, we compared our method with state-of-the-art methods on the MPI-INF-3DHP benchmark, as shown in <ref type="table" target="#tab_3">Table 3</ref>. We followed the experimental setting in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49]</ref> to adopt ground-truth 2D poses as input. Our method achieves significant improvements no matter in terms of PCK (11.0% improvement), AUC (7.4% improvement), or MPJPE <ref type="bibr">(25.6</ref> improvement), which demonstrates our method generalizes well on various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Evaluation</head><p>To qualitatively evaluate our U-CondDGCN, we compared it against the baseline, UGCN <ref type="bibr" target="#b48">[49]</ref>. As shown in <ref type="figure" target="#fig_3">Figure 7</ref>, we show several  <ref type="bibr" target="#b40">[41]</ref>, while that for MPI-INF-3DHP is the ground-truth 2D poses. We can see our results are more consistent with the ground-truth 3D poses, especially for the end-effectors, e.g., the "wrist" and "ankle" joints. It qualitatively demonstrates the effectiveness of our U-CondDCCN for estimating 3D poses. Readers are highly recommended to watch the supplementary video to better explore the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>Effectiveness of CondDGConv. To explore the effectiveness of directed graph representation and our proposed CondDGConv, we  , HR-Net <ref type="bibr" target="#b40">[41]</ref>, and ground-truth, respectively.</p><p>conducted ablation experiments on the Human3.6M dataset by considering the following methods:</p><p>? UGCN <ref type="bibr" target="#b48">[49]</ref> is the baseline, which adopts the undirected graph representation and has the same U-shaped structure; ? UDGCN is a variant of our method that replaces all the CondDGConv with the DGConv; and ? U-CondDGCN is the full version of our method. We adopted the ground-truth 2D poses as input to eliminate the influence of the 2D pose estimator. The results are shown in the top part of Configuration of U-CondDGCN. To explore the best stage to utilize the conditional connections (CondE), we conducted experiments by adopting CondE at various stages, i.e., downsampling stage, upsampling stage, merging stage, and all stages, and show the results in the bottom part of <ref type="table" target="#tab_4">Table 4</ref>. We can see that adopting CondE at the downsampling or upsampling stage would decrease the performance compared with the UDGCN, whereas adopting the CondE at the merging stage can improve the performance. This is because introducing CondE at too early stages would prevent the network from utilizing the prior knowledge of the natural structure of human skeletons while introducing CondE at the late stage, i.e., the merging stage, can both keep the prior knowledge and allow different kinds of poses to adopt optimal non-local dependence. Compared with adopting CondE at all the stages (yields 2.0 improvement), adopting it at the merging stage can better balance the stability and flexibility (yields 2.9 improvement).</p><p>Influence of the input sequence length. To explore it, we measured the MPJPE of estimated 3D poses under various lengths of the input sequence with the input 2D poses from CPN <ref type="bibr" target="#b4">[5]</ref>, HR-Net <ref type="bibr" target="#b40">[41]</ref>, and ground-truth, and plotted curves of the MPJPE against the input sequence length in <ref type="figure" target="#fig_5">Figure 8</ref>. We can see that no matter with what kinds of input 2D poses, the performance always goes better when the sequence length increases. It shows longer sequence can provide more temporal information for the 3D pose reasoning. Visualization of predicted conditional connections. To verify the predicted conditional connections for different input poses, we visualized the connection matrices (from the last layer of U-CondDGCN) predicted for two pose sequences in the "walking dog" and "eating" action classes, respectively. The connection matrix CondE has a shape of ? , where is the number of nodes. The absolute value of entry CondE , means the extent of dependence between node and , while the positive/negative sign stands for node belongs to the child/parent set of node , where and are the row and column indices, respectively. As shown in <ref type="figure" target="#fig_6">Figure 9</ref>, we can see these two connection matrices are very different. For example, the dependency between the left wrist and the right knee/ankle nodes is significant for the "walking dog" sequence, whereas the dependency between the right wrist and the nose/head nodes is significant for the "eating" sequence, as marked with black rectangles. It demonstrates our U-CondDGCN can differentiate input poses to predict appropriate non-local connections at inference time.</p><p>Inference speed. The number of parameters of our method is 3.42M, while that of UGCN [49] is 1.69M. The difference mainly comes from that conventional GCN only considers nodes' features while our CondDGConv considers both nodes' and edges' features. However, such a number of parameters is still far less than that of temporal-convolution-based methods such as VideoPose3D <ref type="bibr" target="#b34">[35]</ref>, which has 16.95M parameters. Under the experimental setting stated in Sec. 4, our method takes around 3.6ms to estimate a 3D pose from the 2D poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We present the conditional directed graph convolution for 3D human pose estimation from monocular videos. The employed directed graph representation can better model the articulated hierarchy of human skeletons than the undirected graph. Moreover, we present a novel conditional connection mechanism for the directed graph convolution, such that different kinds of poses can adopt appropriate non-local dependence to facilitate the 3D pose reasoning. Extensive quantitative and qualitative evaluations demonstrate that our method achieves top performance on two large-scale challenging benchmarks, Human3.6M and MPI-INF-3DHP. Also, we conducted a wide spectrum of analyses to verify our method. We believe the insight behind the conditional directed graph convolution can also benefit other tasks where the articulated structure is involved, like the 3D hand gesture estimation and recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Spatial-temporal directed graph convolution (ST-DGConv) and spatial-temporal conditional directed graph convolution (ST-CondDGConv) blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 C o n d ! 2 Figure 6 :</head><label>126</label><figDesc>a) Input directed graph (b) Nodes updating (c) Nodes updating with conditional connections (d) Edges updating Conditional directed graph convolution (CondDGConv). Part (a) is the input directed graph with predefined connections ( ) and predicted conditional connections (Cond ). Part (b), (c) and (d) are three steps of the CondDGConv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 }</head><label>2</label><figDesc>and the child nodes set N c 4 = { ? 3 }. Similarly, the nodes updating with conditional connections step can be formulated as:( ?? ) = (w ? [P ( (N )); ( ? ); P ( (N ))] T + b).<ref type="bibr" target="#b1">(2)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>(marked with ?). The input 2D poses are Input 2D pose UGCNOurs Qualitative comparison between and the baseline, UGCN<ref type="bibr" target="#b48">[49]</ref> and our method on Human3.6M and MPI-INF-3DHP. To better evaluate 3D poses' quality, we show them under three different viewpoints as indicated by 3D orientation markers. And ground-truth 3D poses are shown in gray as a reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Influence of the input sequence length. Curves show the quality of estimated 3D poses against the input sequence length with the input 2D poses from CPN<ref type="bibr" target="#b4">[5]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Visualization of the predicted conditional connection matrices from two kinds of actions. Two significant dependency examples are marked in black rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Protocol #1 Dir. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg. Disc Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD. Walk WalkT. Avg.</figDesc><table><row><cell>Martinez et al. [28]</cell><cell>(ICCV'17) 51.8 56.2 58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>78.4</cell><cell>55.2</cell><cell>58.1</cell><cell>74.0 94.6</cell><cell>62.3</cell><cell>59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4</cell><cell>62.9</cell></row><row><cell>Fang et al. [12]</cell><cell>(AAAI'18) 50.1 54.3 57.0</cell><cell>57.1</cell><cell>66.6</cell><cell>73.3</cell><cell>53.4</cell><cell>55.7</cell><cell>72.8 88.6</cell><cell>60.3</cell><cell>57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6</cell><cell>60.4</cell></row><row><cell>Zhao et al. [57]</cell><cell>(CVPR'19) 47.3 60.7 51.4</cell><cell>60.5</cell><cell>61.1</cell><cell>49.9</cell><cell>47.3</cell><cell>68.1</cell><cell>86.2 55.0</cell><cell>67.8</cell><cell>61.0</cell><cell>42.1</cell><cell>60.6</cell><cell>45.3</cell><cell>57.6</cell></row><row><cell>Liu et al. [24]</cell><cell>(ECCV'20) 46.3 52.2 47.3</cell><cell>50.7</cell><cell>55.5</cell><cell>67.1</cell><cell>49.2</cell><cell>46.0</cell><cell>60.4 71.1</cell><cell>51.5</cell><cell>50.1</cell><cell>54.5</cell><cell>40.3</cell><cell>43.7</cell><cell>52.4</cell></row><row><cell>Cai et al. [2]  ? (CPN, T=7)</cell><cell>(ICCV'19) 44.6 47.4 45.6</cell><cell>48.8</cell><cell>50.8</cell><cell>59.0</cell><cell>47.2</cell><cell>43.9</cell><cell>57.9 61.9</cell><cell>49.7</cell><cell>46.6</cell><cell>51.3</cell><cell>37.1</cell><cell>39.4</cell><cell>48.8</cell></row><row><cell cols="2">Pavllo et al. [35]  ? (CPN, T=243) (CVPR'19) 45.2 46.7 43.3</cell><cell>45.6</cell><cell>48.1</cell><cell>55.1</cell><cell>44.6</cell><cell>44.3</cell><cell>57.3 65.8</cell><cell>47.1</cell><cell>44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9</cell><cell>46.8</cell></row><row><cell>Xu et al. [51]  ? (CPN, T=9)</cell><cell>(CVPR'20) 37.4 43.5 42.7</cell><cell>42.7</cell><cell>46.6</cell><cell>59.7</cell><cell>41.3</cell><cell>45.1</cell><cell>52.7 60.2</cell><cell>45.8</cell><cell>43.1</cell><cell>47.7</cell><cell>33.7</cell><cell>37.1</cell><cell>45.6</cell></row><row><cell>Liu et al. [25]  ? (CPN, T=243)</cell><cell>(CVPR'20) 41.8 44.8 41.1</cell><cell>44.9</cell><cell>47.4</cell><cell>54.1</cell><cell>43.4</cell><cell>42.2</cell><cell>56.2 63.6</cell><cell>45.3</cell><cell>43.5</cell><cell>45.3</cell><cell>31.3</cell><cell>32.2</cell><cell>45.1</cell></row><row><cell>UGCN [49]  ? (CPN, T=96)</cell><cell>(ECCV'20) 41.3 43.9 44.0</cell><cell>42.2</cell><cell>48.0</cell><cell>57.1</cell><cell>42.2</cell><cell>43.2</cell><cell>57.3 61.3</cell><cell>47.0</cell><cell>43.5</cell><cell>47.0</cell><cell>32.6</cell><cell>31.8</cell><cell>45.6</cell></row><row><cell>UGCN [49]  ? (HR-Net, T=96)</cell><cell>(ECCV'20) 38.2 41.0 45.9</cell><cell>39.7</cell><cell>41.4</cell><cell>51.4</cell><cell>41.6</cell><cell>41.4</cell><cell>52.0 57.4</cell><cell>41.8</cell><cell>44.4</cell><cell>41.6</cell><cell>33.1</cell><cell>30.0</cell><cell>42.6</cell></row><row><cell>Ours  ? (CPN, T=96)</cell><cell>38.0 43.3 39.1</cell><cell>39.4</cell><cell>45.8</cell><cell>53.6</cell><cell>41.4</cell><cell>41.4</cell><cell>55.5 61.9</cell><cell>44.6</cell><cell>41.9</cell><cell>44.5</cell><cell>31.6</cell><cell>29.4</cell><cell>43.4</cell></row><row><cell>Ours  ? (HR-Net, T=96)</cell><cell cols="2">35.5 41.3 36.6 39.1</cell><cell>42.4</cell><cell cols="2">49.0 39.9</cell><cell>37.0</cell><cell>51.9 63.3</cell><cell>40.9</cell><cell>41.3</cell><cell>40.3</cell><cell>29.8</cell><cell>28.9</cell><cell>41.1</cell></row><row><cell cols="2">Protocol #2 Dir. Martinez et al. [28] (ICCV'17) 39.5 43.2 46.4</cell><cell>47.0</cell><cell>51.0</cell><cell>56.0</cell><cell>41.4</cell><cell>40.6</cell><cell>56.5 69.4</cell><cell>49.2</cell><cell>45.0</cell><cell>49.5</cell><cell>38.0</cell><cell>43.1</cell><cell>47.7</cell></row><row><cell>Fang et al. [12]</cell><cell>(AAAI'18) 38.2 41.7 43.7</cell><cell>44.9</cell><cell>48.5</cell><cell>55.3</cell><cell>40.2</cell><cell>38.2</cell><cell>54.5 64.4</cell><cell>47.2</cell><cell>44.3</cell><cell>47.3</cell><cell>36.7</cell><cell>41.7</cell><cell>45.7</cell></row><row><cell>Liu et al. [24]</cell><cell>(ECCV'20) 35.9 40.0 38.0</cell><cell>41.5</cell><cell>42.5</cell><cell>51.4</cell><cell>37.8</cell><cell>36.0</cell><cell>48.6 56.6</cell><cell>41.8</cell><cell>38.3</cell><cell>42.7</cell><cell>31.7</cell><cell>36.2</cell><cell>41.2</cell></row><row><cell>Cai et al. [2]  ? (CPN, T=7)</cell><cell>(ICCV'19) 35.7 37.8 36.9</cell><cell>40.7</cell><cell>39.6</cell><cell>45.2</cell><cell>37.4</cell><cell>34.5</cell><cell>46.9 50.1</cell><cell>40.5</cell><cell>36.1</cell><cell>41.0</cell><cell>29.6</cell><cell>33.2</cell><cell>39.0</cell></row><row><cell cols="2">Pavllo et al. [35]  ? (CPN, T=243) (CVPR'19) 34.1 36.1 34.4</cell><cell>37.2</cell><cell>36.4</cell><cell>42.2</cell><cell>34.4</cell><cell>33.6</cell><cell>45.0 52.5</cell><cell>37.4</cell><cell>33.8</cell><cell>37.8</cell><cell>25.6</cell><cell>27.3</cell><cell>36.5</cell></row><row><cell>Xu et al. [51]  ? (CPN, T=9)</cell><cell>(CVPR'20) 31.0 34.8 34.7</cell><cell>34.4</cell><cell>36.2</cell><cell>43.9</cell><cell>31.6</cell><cell>33.5</cell><cell>42.3 49.0</cell><cell>37.1</cell><cell>33.0</cell><cell>39.1</cell><cell>26.9</cell><cell>31.9</cell><cell>36.2</cell></row><row><cell>Liu et al. [25]  ? (CPN, T=243)</cell><cell>(CVPR'20) 32.3 35.2 33.3</cell><cell>35.8</cell><cell>35.9</cell><cell>41.5</cell><cell>33.2</cell><cell>32.7</cell><cell>44.6 50.9</cell><cell>37.0</cell><cell>32.4</cell><cell>37.0</cell><cell>25.2</cell><cell>27.2</cell><cell>35.6</cell></row><row><cell>UGCN [49]  ? (CPN, T=96)</cell><cell>(ECCV'20) 32.9 35.2 35.6</cell><cell>34.4</cell><cell>36.4</cell><cell>42.7</cell><cell>31.2</cell><cell>32.5</cell><cell>45.6 50.2</cell><cell>37.3</cell><cell>32.8</cell><cell>36.3</cell><cell>26.0</cell><cell>23.9</cell><cell>35.5</cell></row><row><cell>UGCN [49]  ? (HR-Net, T=96)</cell><cell>(ECCV'20) 28.4 32.5 34.4</cell><cell>32.3</cell><cell>32.5</cell><cell>40.9</cell><cell>30.4</cell><cell>29.3</cell><cell>42.6 45.2</cell><cell>33.0</cell><cell>32.0</cell><cell>33.2</cell><cell>24.2</cell><cell>22.9</cell><cell>32.7</cell></row><row><cell>Ours  ? (CPN, T=96)</cell><cell>29.8 34.4 31.9</cell><cell>31.5</cell><cell>35.1</cell><cell>40.0</cell><cell>30.3</cell><cell>30.8</cell><cell>42.6 49.0</cell><cell>35.9</cell><cell>31.8</cell><cell>35.0</cell><cell>25.7</cell><cell>23.6</cell><cell>33.8</cell></row><row><cell>Ours  ? (HR-Net, T=96)</cell><cell cols="2">27.7 32.7 29.4 31.3</cell><cell>32.5</cell><cell cols="2">37.2 29.3</cell><cell>28.5</cell><cell>39.2 50.9</cell><cell>32.9</cell><cell>31.4</cell><cell>32.1</cell><cell>23.6</cell><cell>22.8</cell><cell>32.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on Human3.6M with ground-truth 2D poses as input. Video-based methods are marked with ?.</figDesc><table><row><cell>Method</cell><cell></cell><cell>MPJPE</cell></row><row><cell cols="2">Martinez et al. [28] (ICCV'17)</cell><cell>45.5</cell></row><row><cell>Zhao et al. [57]</cell><cell>(CVPR'19)</cell><cell>43.8</cell></row><row><cell>Liu et al. [24]</cell><cell>(ECCV'20)</cell><cell>37.8</cell></row><row><cell>Cai et al. [2]  ?</cell><cell>(ICCV'19)</cell><cell>37.2</cell></row><row><cell>Pavllo et al. [35]  ?</cell><cell>(CVPR'19)</cell><cell>37.2</cell></row><row><cell>Liu et al. [25]  ?</cell><cell>(CVPR'20)</cell><cell>34.7</cell></row><row><cell>UGCN [49]  ?</cell><cell>(ECCV'20)</cell><cell>25.6</cell></row><row><cell>Ours  ?</cell><cell></cell><cell>22.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on MPI-INF-3DHP with three metrics, where PCK and AUC are the higher, the better, while MPJPE is the lower, the better.</figDesc><table><row><cell>Method</cell><cell cols="3">PCK[?] AUC[?] MPJPE[?]</cell></row><row><cell>Mehta et al. [29]</cell><cell>75.7</cell><cell>39.3</cell><cell>-</cell></row><row><cell>VNect (ResNet50) [30]</cell><cell>77.8</cell><cell>41.0</cell><cell>-</cell></row><row><cell>VNect (ResNet101) [30]</cell><cell>79.4</cell><cell>41.6</cell><cell>-</cell></row><row><cell>TrajectoryPose3D [23]</cell><cell>83.6</cell><cell>51.4</cell><cell>79.8</cell></row><row><cell>UGCN [49]</cell><cell>86.9</cell><cell>62.1</cell><cell>68.1</cell></row><row><cell>Ours</cell><cell>97.9</cell><cell>69.5</cell><cell>42.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation study. We compared results of the baseline (UGCN [49]), the variant of our method (UDGCN), our U-CondDGCN, and different configurations for our U-CondDGCN on Human3.6M. The ? denotes the improvements compared with the baseline. UGCN and ours under various viewpoints in orange and teal, respectively, while the ground truth 3D poses are shown in neutral gray as a reference. The first two examples are from Human3.6M, and the last example is from MPI-INF-3DHP. The input 2D poses for Human3.6M are estimated from monocular videos by the 2D pose estimator, HR-Net</figDesc><table><row><cell>Method</cell><cell>CondE</cell><cell>MPJPE</cell><cell>?</cell></row><row><cell>UGCN</cell><cell>-</cell><cell>25.6</cell><cell>-</cell></row><row><cell>UDGCN</cell><cell>-</cell><cell>23.9</cell><cell>1.7</cell></row><row><cell>U-CondDGCN</cell><cell>Merging stage</cell><cell>22.7</cell><cell>2.9</cell></row><row><cell>U-CondDGCN</cell><cell>Upsampling stage</cell><cell>24.1</cell><cell>1.5</cell></row><row><cell>U-CondDGCN</cell><cell>Downsampling stage</cell><cell>25.3</cell><cell>0.3</cell></row><row><cell>U-CondDGCN</cell><cell>All stages</cell><cell>23.6</cell><cell>2.0</cell></row><row><cell>example results of</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Comparing the results of UGCN and UDGCN, we can see that UDGCN outperforms the UGCN by 1.7 in terms of MPJPE. It shows the directed graph can better model the hierarchy of the articulated human skeleton, and the hierarchy is beneficial to the 3D pose reasoning. Moreover, comparing the results of UDGCN and U-CondDGCN, we can find CondDGConv can further improve the performance by 1.2. It demonstrates adaptive graph topologies for different pose samples can better leverage the varying non-local dependence.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gines</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">2020. 3d human pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Occlusionaware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An Adaptive and Momental Bound Method for Stochastic Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoou-I</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learnable triangulation of human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Iskakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Malkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shape-aware human pose and shape reconstruction using multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Trajectory Space Factorization for Deep Video-Based 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comprehensive study of weight sharing in graph networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sen-ching Cheung, and Vijayan Asari. 2020. Attention mechanism exploits temporal contexts: Real-time 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning via hessian-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lighttrack: A generic framework for online top-down human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">MotioNet: 3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kfir</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Structured Prediction of 3D Human Pose with Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Direct prediction of 3d body poses from motion compensated sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyue</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">AI coach: Deep human pose estimation and analysis for personalized athletic training assistance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Motion guided 3d pose estimation from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Convolutional sequence generation for skeleton-based action synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huahan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">CondConv: Conditionally Parameterized Convolutions for Efficient Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Srnet: Improving generalization in 3d human pose estimation with a split-and-recombine approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deep Monocular 3D Human Pose Estimation via Cascaded Dimension-Lifting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changgong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangneng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03520</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">High-order Graph Convolutional Networks for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
