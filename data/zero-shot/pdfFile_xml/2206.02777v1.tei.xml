<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy (IDEA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
							<email>hzhangcx@connect.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy (IDEA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizhe</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy (IDEA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Dept. of CST</orgName>
								<orgName type="department" key="dep2">Institute for AI</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist Center</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy (IDEA)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>leizhang@idea.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
							<email>ni@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy (IDEA)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
							<email>hshum@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">International Digital Economy Academy (IDEA)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present Mask DINO, a unified object detection and segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising Anchor Boxes) by adding a mask prediction branch which supports all image segmentation tasks (instance, panoptic, and semantic). It makes use of the query embeddings from DINO to dot-product a high-resolution pixel embedding map to predict a set of binary masks. Some key components in DINO are extended for segmentation through a shared architecture and training process. Mask DINO is simple, efficient, scalable, and benefits from joint large-scale detection and segmentation datasets. Our experiments show that Mask DINO significantly outperforms all existing specialized segmentation methods, both on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably, Mask DINO establishes the best results to date on instance segmentation (54.5 AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation (60.8 mIoU on ADE20K). Code will be avaliable at https://github.com/IDEACVR/MaskDINO. * Equal contribution. ? This work was done when Feng Li and Hao Zhang were interns at IDEA. ? Corresponding author.</p><p>Preprint. Under review. generalization ability to address other tasks. The ambition to bridge different tasks gives rise to more advanced methods like HTC [3] for object detection and instance segmentation and Panoptic FPN [16], K-net <ref type="bibr" target="#b37">[38]</ref> for instance, panoptic, and semantic segmentation. Task unification not only helps simplify the algorithm development but also brings in performance improvement in multiple tasks. As an evidence, up to now, an improved HTC (HTC++) is still a widely-used object detection and instance segmentation method used by state-of-the-art (SOTA) models on the COCO object detection and instance segmentation leaderboards [1].</p><p>As we step into the new era of Transformer-based detectors, detection and segmentation tasks diverge into different models. Transformer <ref type="bibr" target="#b31">[32]</ref> was first introduced into object detection by DETR <ref type="bibr" target="#b1">[2]</ref>. DETR is an end-to-end query-based object detector, which adopts a set-prediction objective with bipartite matching. Although DETR addresses both the object detection and panoptic segmentation tasks, its segmentation performance is still inferior to classical segmentation models. To improve the detection and segmentation performance of query-based models, researchers have developed specialized models for object detection <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref>, image segmentation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref>, instance segmentation [10], panoptic segmentation [27], and semantic segmentation [14]. Among the efforts to improve object detection, DINO (DETR with Improved Denoising Anchor Boxes) [37] takes advantage of the dynamic anchor box formulation from DAB-DETR [22] and query denoising training from DN-DETR [18], and further develops contrastive denoising training, mixed query selection, and look forward twice methods to accelerate training and improve the detection performance. As a result, DINO achieves the SOTA result on the COCO object detection leaderboard for the first time as a DETR-like model. Similarly, for improving image segmentation, MaskFormer [6] and Mask2Former [4] propose to unify different image segmentation tasks using query-based Transformer architectures to perform mask classification. Such methods have achieved remarkable performance improvement on multiple segmentation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection and image segmentation are fundamental tasks in computer vision. Both tasks are concerned with localizing objects of interest in an image but have different levels of focus. Object detection is to localize objects of interest and predict their bounding boxes and category labels, whereas image segmentation focuses on pixel-level grouping of different semantics. Moreover, image segmentation encompasses various tasks including instance segmentation, panoptic segmentation, and semantic segmentation with respect to different semantics, e.g, instance or category membership, foreground or background category.</p><p>Remarkable progress has been achieved by classical convolution-based algorithms developed for these tasks with specialized architectures, such as Faster RCNN <ref type="bibr" target="#b27">[28]</ref> for object detection, Mask RCNN <ref type="bibr" target="#b11">[12]</ref> for instance segmentation, and FCN <ref type="bibr" target="#b24">[25]</ref> for semantic segmentation. Although these methods are conceptually simple and effective, they are tailored for specialized tasks and lack the However, detection and segmentation models still diverge significantly, which prevents task and data cooperation between detection and segmentation tasks. For example, the state-of-the-art query-based instance segmentation model Mask2Former still lags behind classical models based on HTC++ with Swin-V2-G <ref type="bibr" target="#b22">[23]</ref>. One reason to account for this performance gap is that HTC-based models are pre-trained on a large-scale detection dataset (i.e Objects365 <ref type="bibr" target="#b30">[31]</ref>) but Mask2Former can not utilize detection data for pre-training. Though we believe detection and segmentation can help each other in a unified architecture, the results of simply using DINO for segmentation and using Mask2Former for detection indicate that they can not do other tasks well. Moreover, trivial multi-task training can even hurt the performance of the original tasks. It naturally leads to two questions: 1) why cannot detection and segmentation tasks help each other in query-based models? and 2) is it possible to develop a unified architecture for all detection and segmentation tasks to replace specialized ones?</p><p>To address these problems, we propose Mask DINO, which extends DINO by adding a mask prediction branch in parallel with the box prediction branch. Inspired by other unified models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref> for image segmentation, we reuse content query embeddings from DINO to perform mask classification for all segmentation tasks. As DINO lacks a high-resolution feature map for mask prediction, we follow MaskFormer and Mask2Former and construct a high-resolution pixel embedding map (1/4 of the input image resolution) obtained from the backbone and Transformer encoder features. The mask branch predicts binary masks by simply dot-producting each content query embedding with the pixel embedding map. Although DINO is not designed for pixel-level alignment, we find it quite effective to reuse its content query embeddings for mask prediction. This can largely attribute to the cross attention-based feature pooling in Transformer decoder, allowing query embeddings to only aggregate related features from the input image. Besides the mask branch, we also extend three key components for box prediction in DINO to improve the segmentation performance. First, we propose unified query selection to initialize mask queries as anchors, which selects masks from encoder dense prior. Second, we propose unified denoising training for masks to accelerate segmentation training. Third, we use a hybrid bipartite matching for more precise matching with both boxes and masks.</p><p>Mask DINO is conceptually simple and easy to implement under the DINO framework. The extension for mask prediction is designed to be as simple as possible so that Mask DINO can reuse all algorithm improvements in DINO as well as its feature representation which can be pre-trained from a much larger detection dataset. We also find that the hard-constrained and dense masked attention in Mask2Former not necessary. Mask DINO indicates simply concatenating multi-scale features from Transformer encoder and using deformable attention can result in a remarkable segmentation performance. In addition, Mask DINO is computationally efficient as we use sparse deformable attention. As an evidence, our FPS is much higher than Mask2Former <ref type="bibr" target="#b3">[4]</ref> (14.8 vs 8.2 in <ref type="table" target="#tab_4">Table 2</ref>).</p><p>To summarize, our contributions are three-fold.</p><p>1. We develop a unified Transformer-based framework for both object detection and segmentation. As the framework is extended from DINO, a DETR-like model, by adding a mask prediction branch, it naturally inherits most algorithm improvements in DINO including anchor box-guided cross attention, query selection, denoising training, and even a better representation pre-trained on a large scale detection dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Detection: Mainstream detection algorithms have been dominated by convolutional neural networkbased frameworks, until recently Transformer-based detectors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37]</ref> achieve great progress. DETR <ref type="bibr" target="#b1">[2]</ref> is the first end-to-end and query-based Transformer object detector, which adopts a setprediction objective with bipartite matching. DAB-DETR <ref type="bibr" target="#b21">[22]</ref> improves DETR by formulating queries as 4D anchor boxes and refining predictions layer by layer. DN-DETR <ref type="bibr" target="#b17">[18]</ref> introduces a denoising training method to accelerate convergence. It takes noised ground-truth objects as input and trains the model to reconstruct the ground-truth objects on the output side. Based on DAB-DETR and DN-DETR, DINO <ref type="bibr" target="#b36">[37]</ref> proposes several new improvements on denoising and anchor refinement and achieves new SOTA results on COCO detection. Despite the inspiring progress, DETR-like detection models are not competitive for segmentation. Vanilla DETR incorporates a segmentation head in its architecture. However, its segmentation performance is inferior to specialized segmentation models and only shows the feasibility of DETR-like detection models to deal with detection and segmentation simultaneously. Segmentation: Segmentation mainly includes three tasks: instance, semantic, and panoptic segmentation. The three tasks are similar but focus on different semantics. Instance segmentation is to predict a mask and its corresponding category for each object instance. Semantic segmentation requires to classify each pixel including the background into different semantic categories. Panoptic segmentation <ref type="bibr" target="#b15">[16]</ref> unifies the instance and semantic segmentation tasks and predicts a mask for each object instance or background segment. In the past few years, researchers have developed specialized architectures for the three tasks. For example, Mask-RCNN <ref type="bibr" target="#b11">[12]</ref> and HTC <ref type="bibr" target="#b2">[3]</ref> can only deal with instance segmentation because they predict the mask of each instance based on its box prediction. FCN <ref type="bibr" target="#b24">[25]</ref> and U-Net <ref type="bibr" target="#b29">[30]</ref> can only perform semantic segmentation since they predict one segmentation map based on pixel-wise classification. Although panoptic segmentation unifies the above two tasks, models designed for panoptic segmentation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref> are usually sub-optimal for instance and semantic segmentation compared with specialized models. Until recently, some image segmentation models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref> are developed to unify the three tasks with a universal architecture. For instance, Mask2Former <ref type="bibr" target="#b3">[4]</ref> improves MaskFormer <ref type="bibr" target="#b5">[6]</ref> by introducing masked-attention to Transformer and achieves SOTA performance on panoptic segmentation. Mask2Former has a similar architecture as DETR to probe image features with learnable queries but differs in using a different segmentation branch and some specialized designs for mask prediction. However, while Mask2Former shows a great success in unifying all segmentation tasks, it leaves object detection untouched and our empirical study shows that its specialized architecture design is not suitable for predicting boxes. Unified Methods: As both object detection and segmentation are concerned with localizing objects, they naturally share common model architectures and visual representations. A unified framework not only helps simplify the algorithm develop effort but also allows to use both detection and segmentation data to improve representation learning. There have been several previous works to unify segmentation and detection tasks, e.g., Mask RCNN <ref type="bibr" target="#b11">[12]</ref>, HTC <ref type="bibr" target="#b2">[3]</ref>, and DETR <ref type="bibr" target="#b1">[2]</ref>. Mask RCNN extends Faster RCNN and pools image features from Region Of Interest (ROI) proposed by RPN. HTC further proposes an interleaved way of predicting boxes and masks to improve the segmentation performance. However, these two models can only perform instance segmentation. DETR predicts boxes and masks together in an end-to-end manner. However, its segmentation performance largely lags behind other models. According to <ref type="table" target="#tab_2">Table 13</ref>, when we use DETR's segmentation head to predict instance segmentation results, the mask AP is 14% lower than the box AP. How to attain mutual assistance between segmentation and detection has long been an important problem to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Mask DINO</head><p>Mask DINO is an extension of DINO <ref type="bibr" target="#b36">[37]</ref>. On top of content query embeddings, DINO has two branches for box prediction and label prediction. The boxes are dynamically updated and used to guide the deformable attention in each Transformer decoder. Mask DINO adds another branch for mask prediction and minimally extends several key components in detection to fit segmentation tasks.</p><p>To better understand Mask DINO, we start by briefly reviewing DINO and then introduce Mask DINO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries: DINO</head><p>DINO is a typical DETR-like model, which is composed of a backbone, a Transformer encoder, and a Transformer decoder. The framework is shown in <ref type="figure" target="#fig_0">Fig. 1</ref> (the blue-shaded part without red lines).</p><p>Following DAB-DETR <ref type="bibr" target="#b21">[22]</ref>, DINO formulates each positional query in DETR as a 4D anchor box, which is dynamically updated through each decoder layer. Note that DINO uses multi-scale features with deformable attention <ref type="bibr" target="#b39">[40]</ref>. Therefore, the updated anchor boxes are also used to constrain deformable attention in a sparse and soft way. Following DN-DETR <ref type="bibr" target="#b17">[18]</ref>, DINO adopts denoising training and further develops contrastive denoising to accelerate training convergence. Moreover, DINO proposes a mixed query selection scheme to initialize positional queries in the decoder and a look-forward-twice method to improve box gradient back-propagation. The key extension is a mask prediction branch which performs dot production between each query embedding with a high resolution (1/4 of the input image resolution) pixel embedding map. In unified query selection, top K features from the last encoder layer are selected to initialize both the positional queries and content queries fed to the Transformer decoder. We also use hybrid matching with masks for more accurate matching and unified DN (denoising) for masks to accelerate segmentation training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Why a universal model has not replaced the specialized models?</head><p>Remarkable progress has been achieved by Transformer-based detectors and segmentation models. For instance, DINO <ref type="bibr" target="#b36">[37]</ref> and Mask2Former <ref type="bibr" target="#b3">[4]</ref> have achieved the best result on COCO detection and panoptic segmentation, respectively. Inspired by such progress, we attempted to simply extend these specialized models for other tasks but found that the performance of other tasks lagged behind the original ones by a large margin. It seems that trivial multi-task training even hurts the performance of the original task. However, in convolution-based models, it has shown effective and mutually beneficial to combine detection and instance segmentation tasks. For example, HTC++ <ref type="bibr" target="#b2">[3]</ref> is still ranked first on the COCO instance segmentation. We are keen to answer two questions in this work: 1) why cannot detection and segmentation tasks help each other in Transformer-based models? and 2) is it possible to develop a unified architecture for all detection and segmentation tasks to replace specialized ones? We will take DINO and Mask2Former as examples to discuss the above questions. ?Why cannot Mask2Former do detection well? The Transformer decoder of Mask2Former is designed for segmentation tasks and does not suit detection for three reasons. First, its queries follow the design in DETR <ref type="bibr" target="#b1">[2]</ref> without being able to utilize better positional priors as studied in Conditional DETR <ref type="bibr" target="#b25">[26]</ref>, Anchor DETR <ref type="bibr" target="#b33">[34]</ref>, and DAB-DETR <ref type="bibr" target="#b21">[22]</ref>. For example, its content queries are semantically aligned with the features from the Transformer encoder, whereas its positional queries are just learnable vectors as in vanilla DETR instead of being associated with a single-mode position <ref type="bibr" target="#b3">4</ref> . If we remove its mask branch, it reduces to a variant of DETR <ref type="bibr" target="#b1">[2]</ref>, whose performance is inferior to recently improved DETR models. Second, Mask2Former adopts masked attention (multi-head attention with attention mask) in Transformer decoders. The attention masks predicted from a previous layer are dense and hard-constrained, which is neither efficient nor flexible for box prediction. Third, Mask2Former cannot explicitly perform box refinement layer by layer. Moreover, its coarse-to-fine mask refinement in decoders fails to use multi-scale features from the encoder. ?Why cannot DETR/DINO do segmentation well? DETR <ref type="bibr" target="#b1">[2]</ref> has incorporated a segmentation head into its architecture to show the potential of extending to segmentation tasks. However, its performance is limited. There are three reasons. First, its segmentation head is not optimal. DETR lets each query embedding dot-product with the smallest feature map to compute attention maps and then upsamples them to get the mask predictions. This design lacks an interaction between queries and larger feature maps from the backbone. Second, DETR cannot use mask auxiliary loss in each decoder layer. DETR's architecture is inefficient for segmentation due to its heavy segmentation head and dense mask loss. Therefore, it only computes segmentation loss in the last decoder layer. Third, It does not support mask refinement as the mask positional prediction from one layer cannot pass to the next layer. ?The motivation of Mask DINO. There has been a trend to unify detection and segmentation tasks using convolution-based models, which not only simplifies model design but also promotes mutual cooperation between detection and segmentation. There are mainly three motivations for us to propose Mask DINO. First, DINO <ref type="bibr" target="#b36">[37]</ref> has achieved SOTA results on object detection. Previous works such as Mask RCNN <ref type="bibr" target="#b11">[12]</ref>, HTC <ref type="bibr" target="#b2">[3]</ref>, and DETR <ref type="bibr" target="#b1">[2]</ref> have shown that a detection model can be extended to do segmentation and help design better segmentation models. Second, detection is a relatively easier task than instance segmentation. As shown in <ref type="table" target="#tab_4">Table 2</ref> (and other previous studies), Box AP is usually 4+ AP higher than mask AP. Therefore, box prediction can guide attention to focus on more meaningful regions and extract better features for mask prediction. Third, the new improvements in DINO and other DETR-like models <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b17">18]</ref> such as query selection and deformable attention can also help segmentation tasks. For example, Mask2Former adopts learnable decoder queries, which cannot take advantage of the position information in the selected top K features from the encoder to guide mask predictions. <ref type="figure">Fig. 2(a)</ref>(b)(c) show that the output of Mask2Former in the 0-th decoder layer is far away from the GT mask while Mask DINO outputs much better masks as region proposals. Mask2Former also adopts specialized masked attention to guide the model to attend to regions of interest. However, masked attention is a hard constraint which ignores features outside a provided mask and may overlook important information for following decoder layers. In addition, deformable attention is also a better substitute for its high efficiency allowing attention to be applied to multi-scale features without too much computational overhead. <ref type="figure">Fig. 2(d)</ref>(e) show a predicted mask of Mask2Former in its 1-st decoder layer and the corresponding output of Mask DINO. The prediction of Mask2Former only covers less than half of the GT mask, which means that the attention can not see the whole instance in the next decoder layer. Moreover, a box can also guide deformable attention to a proper region for background stuff, as shown in <ref type="figure">Fig. 2(f</ref> <ref type="figure">Figure 2</ref>: (a) The green transparent region is the ground truth mask for the girl. (b)(c) The predicted masks of the 0-th decoder layer in Mask2Former and Mask DINO, respectively. Note that we attain the predicted masks by first choosing the query which is finally assigned to the ground truth mask in the last decoder layer. Then we visualize the predicted mask of this query by performing dot production with the pixel embedding map.   <ref type="bibr" target="#b17">[18]</ref>. "Box helps mask" and "Mask helps box" show mutual task cooperation between the two tasks.</p><formula xml:id="formula_0">)(g). (a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Our Method: Mask DINO</head><p>Mask DINO adopts the same architecture design for detection as in DINO with minimal modifications.</p><p>In the Transformer decoder, Mask DINO adds a mask branch for segmentation and extends several key components in DINO for segmentation tasks. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the framework in the blue-shaded part is the original DINO model and the additional design for segmentation is marked with red lines. Segmentation branch: Following other unified models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b3">4]</ref> for image segmentation, we perform mask classification for all segmentation tasks. Note that DINO is not designed for pixel-level alignment as its positional queries are formulated as anchor boxes and its content queries are used to predict box offset and class membership. To perform mask classification, we adopt a key idea from Mask2Former <ref type="bibr" target="#b3">[4]</ref> to construct a pixel embedding map which is obtained from the backbone and Transformer encoder features. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the pixel embedding map is obtained by fusing the 1/4 resolution feature map C b from the backbone with an upsampled 1/8 resolution feature map C e from the Transformer encoder. Then we dot-product each content query embedding q c from the decoder with the pixel embedding map to obtain an output mask m.</p><formula xml:id="formula_1">m = q c ? M(T (C b ) + F(C e )),<label>(1)</label></formula><p>where M is the segmentation head, T is a convolutional layer to map the channel dimension to the Transformer hidden dimension, and F is a simple interpolation function to perform 2x upsampling of C e . This segmentation branch is conceptually simple and easy to implement in the DINO framework, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Unified query selection for mask: We extend the box query selection scheme in DINO to Mask DINO. We predict both boxes and masks in the encoder and select the top-ranked ones to initialize decoder queries. The selected masks and boxes serve as better initial anchors for the decoder. Note that we initialize both the content and anchor box queries in Mask DINO whereas DINO only initializes anchor box queries. Unified denoising for mask: Query denoising in object detection has shown effective <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b17">18]</ref> to accelerate convergence and improve performance. We also extend this technique to Mask DINO, where we feed the noised ground-truth (GT) boxes and their labels to the decoder and train the model to reconstruct both the GT boxes and masks.   Hybrid matching: Mask DINO, as in some traditional models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>, predicts boxes and masks with two parallel heads in a loosely coupled manner. Hence the two heads can predict a pair of box and mask that are inconsistent with each other. To address this issue, we consider both box and mask in bipartite matching to encourage more accurate matching results. Decoupled box prediction: For the panoptic segmentation task, box prediction for "stuff" categories is unnecessary and intuitively inefficient. For example, many "stuff" categories are background like "sky", whose GT mask-derived boxes are highly irregular and often cover the whole image. Therefore, box prediction for these categories can mislead the instance-level ("thing") detection and segmentation. To address this problem, we remove box loss and box matching for "stuff" categories. More specifically, the box prediction pipeline remains the same for "stuff" to locate meaningful regions and extract features with deformable attention. However, we do not count their box prediction loss. In our hybrid matching, the box loss for "stuff" is set to the mean of "thing" categories. This decoupled design can accelerate training and yield additional gains for panoptic segmentation.</p><p>We show a comparison between our model and other mainstream methods in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments and compare with several specialized models for four popular tasks including object detection, instance, panoptic, and semantic segmentation on COCO <ref type="bibr" target="#b20">[21]</ref>, ADE20K <ref type="bibr" target="#b38">[39]</ref>, and Cityscapes <ref type="bibr" target="#b6">[7]</ref>. For all experiments, we use batch size 16 and A100 GPUs with 40GB memory. We use a ResNet-50 <ref type="bibr" target="#b12">[13]</ref> and a SwinL <ref type="bibr" target="#b23">[24]</ref> backbone for our main results and SOTA model. Under ResNet-50, we use 4 A100 GPUs for all tasks. The detailed setting is in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>Instance segmentation and object detection. In <ref type="table" target="#tab_4">Table 2</ref>, we compare Mask DINO with other instance segmentation and object detection models. Mask DINO outperforms both the specialized models such as Mask2Former <ref type="bibr" target="#b3">[4]</ref> and DINO <ref type="bibr" target="#b36">[37]</ref> and hybrid models such as HTC <ref type="bibr" target="#b2">[3]</ref> under the same setting. Especially, the instance segmentation results surpass the strong baseline Mask2Former by a large margin (+2.7 AP and +2.3 AP) on the 12-epoch and 50-epoch settings. In addition, Mask DINO significantly improves the convergence speed, outperforming Mask2Former with less than half training epochs (44.2 AP in 24 epochs). We also observe that the performance of Mask2Former  <ref type="table">Table 4</ref>: Results for Mask DINO and Mask2Former with 100 queries using a ResNet-50 backbone on ADE20K val. We found the performance variance on this dataset is high and run three times to report both the mean and highest results for both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Iterations mIoU (mean) mIoU (high) mIoU (reported) Mask2Former <ref type="bibr" target="#b3">[4]</ref> 90k 78.7 79.0 79.4 Mask DINO (ours) 90k 79.8(+1.1) 80.0(+1.0) 80.0(+0.6) <ref type="table">Table 5</ref>: Results for Mask DINO and Mask2Former with 100 queries using a ResNet-50 backbone on Cityscapes val. We found the performance variance on this dataset is high and run three times to report both the mean and highest results for both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Params  degenerates when using 300 queries, which restricts its scalability. As we use the sparse and softconstrained deformable attention in the decoder, our model is more computationally efficient. For example, the GFLOPS of Mask DINO is comparable with that of Mask2Former when using more queries, multi-scale features, and an additional box prediction branch. Panoptic segmentation. We compare Mask DINO with other models in <ref type="table" target="#tab_5">Table 3</ref>. Mask DINO outperforms all previous best models on both the 12-epoch and 50-epoch settings by 1.0 PQ and 1.1 PQ, respectively. This indicates Mask DINO has the advantages of both faster convergence and superior performance. One interesting observation is that we outperform Mask2Former <ref type="bibr" target="#b3">[4]</ref> in terms of both P Q T h and P Q St . However, instead of using dense and hard-constrained masked attention, we predict boxes and then use them in deformable attention to extract query features. Therefore, our box-oriented deformable attention also works well with "stuff" categories, which makes our unified model simple and efficient. In addition, we improve the mask AP T h pan by 2.6 to 44.3 AP, which is 0.6 higher than the specialized instance segmentation model Mask2Fomer (43.7 AP). Semantic segmentation. In <ref type="table">Table 4</ref> and 5, we show the performance of semantic segmentation with a ResNet-50 backbone. We use 100 queries for these small datasets. We outperform Mask2Former on both ADE20K and Cityscapes by 1.6 and 0.6 mIoU on the reported performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with SOTA Models</head><p>In <ref type="table" target="#tab_8">Table 6</ref>, we compare Mask DINO with SOTA models on three image segmentation tasks to show its scalability. We use the SwinL <ref type="bibr" target="#b23">[24]</ref> backbone and pre-train DINO on the Objects365 <ref type="bibr" target="#b30">[31]</ref> detection dataset. As Mask DINO is an extension of DINO, the pre-trained DINO model can be used to fine-tune Mask DINO for segmentation tasks. After fine-tuning Mask DINO on the corresponding tasks, we achieve the best results on instance (54.5 AP), panoptic (59.4 PQ), and semantic (60.8 mIoU) segmentation. Compared to SwinV2-G <ref type="bibr" target="#b22">[23]</ref>, we significantly reduce the model size to 1/15 and backbone pre-training dataset to 1/5. Our detection pre-training also significantly helps all segmentation tasks including panoptic and semantic with "stuff" categories. However, previous specialized segmentation models such as Mask2Former can not use detection datasets, which severely limits their data scalability. By unifying four tasks in one model, we only need to pre-train one model on a large-scale dataset and finetune on all tasks for 10 to 20 epochs (Mask2Former needs 100 epochs), which is more computationally efficient and simpler in model design.  <ref type="table" target="#tab_4">Table 2</ref> when only training detection) after training segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>We conduct ablation studies using a ResNet-50 backbone to analyze Mask DINO on COCO val2017. Unless otherwise stated, our experiments are based on object detection and instance segmentation. Query selection for masks. <ref type="table">Table 7</ref> shows the results of our query selection for instance segmentation, where we additionally provide the performance of different decoder layers in one single model. Mask2Former also predicts the masks of learnable queries as initial region proposals. However, their performance lags behind Mask DINO by a large margin (-38.5AP ). With our effective query selection scheme, the mask performance achieves 39.6 AP without using the decoder. In addition, our mask performance at layer six is already comparable to the final results with 9 layers. Feature scales. Mask2Former <ref type="bibr" target="#b3">[4]</ref> shows that concatenating multi-scale features as input to Transformer decoder layers does not improve the segmentation performance. However, in <ref type="table">Table 8</ref>, Mask DINO shows that using more feature scales in the decoder consistently improves the performance. Decoder layer number. In DINO, increasing the decoder layer number to nine will decrease the performance of box. In <ref type="table">Table 9</ref>, the result indicates that increasing the number of decoder layers will contribute to both detection and segmentation in Mask DINO. We hypothesize that the multi-task training become more complex and require more decoders to learn the needed mapping function. Object detection and segmentation help each other. To validate task cooperation in Mask DINO, we use the same model but train different tasks and report the 12 epoch and 50 epoch results. As shown in <ref type="table" target="#tab_2">Table 10</ref>, only training one task will lead to a performance drop. Although only training object detection results in faster convergence in the early stage for box prediction, the final performance is still inferior to training both tasks together. Decoupled box prediction. In <ref type="table" target="#tab_2">Table 11</ref>, we show the effectiveness of our decoupled box prediction for panoptic segmentation. This decoupled design of "thing" and "stuff" accelerates training in the early stage (12-epoch setting) and improves the final performance (50-epoch setting). Matching. In <ref type="table" target="#tab_2">Table 12</ref>, we show that only using boxes or masks to perform bipartite matching is not optimal in Mask DINO. A unified matching objective makes the optimization more consistent. Effectiveness of the algorithm components. In <ref type="table" target="#tab_2">Table 13</ref>, we remove each algorithm component at a time and show that each component contributes to the final performance. When we follow DETR <ref type="bibr" target="#b1">[2]</ref> to fine-tune segmentation after finishing training detection with its segmentation head, the mask performance drops by a large margin (-5.7 AP). This result also advocates our analysis in Section 3.2 that using the naive DETR segmentation head can not lead to a favorable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have presented Mask DINO as a unified Transformer-based framework for both object detection and image segmentation. Conceptually, Mask DINO is a natural extension of DINO from detection to segmentation with minimal modifications on some key components. Mask DINO outperforms previous specialized models and achieves the best results on all three segmentation tasks (instance, panoptic, and semantic). Moreover, Mask DINO shows that detection and segmentation can help each other in query-based models. In particular, Mask DINO enables semantic and panoptic segmentation to benefit from a better visual representation pre-trained on a large-scale detection dataset. We hope Mask DINO can provide insights for enabling task cooperation and data cooperation towards designing a universal model for more vision tasks. Limitations: Different segmentation tasks fail to achieve mutual assistance in Mask DINO. For example, in COCO panoptic segmentation, the mask AP still lags behind the model only trained with instances. In addition, under the large-scale setting, we have not achieved a new SOTA detection performance as the segmentation head requires additional GPU memory. To accommodate this memory limitation, for the large-scale setting, we have to use smaller image size and less number of queries compared with DINO, which impacts the final performance of object detection. In the future, we will further optimize the implementation to develop a more universal and efficient model to promote task cooperation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SOTA Results on COCO test-dev</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The framework of Mask DINO, which is based on DINO (the blue-shaded part) with minimal extensions (the red part) for segmentation tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(d)(e) The outputs of the 1-st layer in Mask2Former and Mask DINO. The red masks are predicted masks and the green box is the predicted box by Mask DINO. The blue points are sampled points by deformable attention. Since the 0-th layer of Mask2Former usually outputs unfavorable masks, we avoid using its 0-th layer here. (f)(g) show that Mask DINO can predict correct sampled points, boxes, and masks for background stuffs.</figDesc><table><row><cell>Models</cell><cell>Detection</cell><cell>Segmentation</cell><cell>End-to-End</cell><cell>Feature Extraction</cell><cell>Denoising</cell><cell>Box Helps Mask Mask Helps Box</cell></row><row><cell>Mask-RCNN [12]</cell><cell></cell><cell>Instance</cell><cell></cell><cell>RoI pooling</cell><cell>No</cell><cell></cell></row><row><cell>DETR [2]</cell><cell></cell><cell>Panoptic and instance</cell><cell></cell><cell>Standard attention</cell><cell>No</cell><cell></cell></row><row><cell>DINO [37]</cell><cell></cell><cell>No</cell><cell></cell><cell>Deformable attention</cell><cell>Contrastive DN for box</cell><cell></cell></row><row><cell>HTC [3]</cell><cell></cell><cell>Instance</cell><cell></cell><cell>RoI pooling</cell><cell>No</cell><cell></cell></row><row><cell>Mask2former [4]</cell><cell></cell><cell>Panoptic, instance, and semantic</cell><cell></cell><cell>Masked attention</cell><cell>No</cell><cell></cell></row><row><cell>Mask DINO (ours)</cell><cell></cell><cell>Panoptic, instance, and semantic</cell><cell></cell><cell cols="2">Deformable attention DN for both mask and box</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison of representative related models and Mask DINO. "End-to-end" shows whether the model is end-to-end optimized without handcrafted components such as NMS. "Feature extrac- tion" shows how each model pools features. "Denoising" indicates what kind of denoising scheme is used in DINO and Mask DINO, where DN stands for query denoising</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results for Mask DINO and other object detection and instance segmentation models with a ResNet-50 backbone on COCO val2017. We follow the common practice in DETR-like models to use 300 queries. * Mask2Former using 300 queries is not listed as its performance will degenerate when using 300 queries. ? indicates the box AP is derived from mask prediction. We test the FPS and GFLOPS of Mask2Former and Mask DINO on the A100 GPU using detectron2.</figDesc><table><row><cell>Model</cell><cell>Epochs</cell><cell>Query type</cell><cell>PQ</cell><cell>PQ T h</cell><cell>PQ St</cell><cell>Box AP T h pan</cell><cell>Mask AP T h pan</cell></row><row><cell>DETR [2]</cell><cell cols="2">500 + 25 100 queries</cell><cell>43.4</cell><cell>48.2</cell><cell>36</cell><cell>?</cell><cell>31.1</cell></row><row><cell>Panoptic Segformer [19]</cell><cell>24</cell><cell>353 queries</cell><cell>49.6</cell><cell>54.4</cell><cell>42.4</cell><cell>?</cell><cell>41.7</cell></row><row><cell>Mask2Former  *  [4]</cell><cell>50</cell><cell cols="2">100 queries 51.9/51.5  ?</cell><cell>57.7</cell><cell>43.0</cell><cell>?</cell><cell>41.7</cell></row><row><cell>Mask DINO (ours)</cell><cell>50</cell><cell>100 queries</cell><cell>52.3</cell><cell>58.3</cell><cell>43.2</cell><cell>47.7</cell><cell>43.7</cell></row><row><cell>Mask DINO (ours)</cell><cell>50</cell><cell>300 queries</cell><cell>53.0(+1.1)</cell><cell cols="2">59.1(+1.4) 43.9(+0.9)</cell><cell>48.8</cell><cell>44.3(+2.6)</cell></row><row><cell>Mask DINO (ours)</cell><cell>24</cell><cell>300 queries</cell><cell>51.5</cell><cell>57.3</cell><cell>42.6</cell><cell>46.4</cell><cell>42.8</cell></row><row><cell>Mask2Former [4]</cell><cell>12</cell><cell>100 queries</cell><cell>46.9</cell><cell>52.5</cell><cell>38.4</cell><cell>?</cell><cell>37.2</cell></row><row><cell>Panoptic Segformer [19]</cell><cell>12</cell><cell>353 queries</cell><cell>48.0</cell><cell>52.3</cell><cell>41.5</cell><cell>?</cell><cell>?</cell></row><row><cell>Mask DINO (ours)</cell><cell>12</cell><cell>300 queries</cell><cell>49.0(+1.0)</cell><cell>54.8</cell><cell>40.2</cell><cell>43.2</cell><cell>40.4(+3.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results for Mask DINO and other panoptic segmentation models with a ResNet-50 backbone on COCO val2017.</figDesc><table /><note>* Mask2Former using 300 queries is not listed as its performance will degenerate when using 300 queries.? Our reproduced result.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Comparison of the SOTA models on three segmentation tasks. Mask DINO outperforms all existing models. "TTA" means test-time-augmentation. "O365" denotes the Objects365 [31] dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :Table 9 :Table 13 :</head><label>7913</label><figDesc>Effectiveness of our query selection for mask initialization. We evaluate the instance segmentation performance from different decoder layers in the same model after training for 50 epochs. Decoder layer number comparison under the 12-epoch setting. Mask DINO benefits from more decoders, while DINO's performance will decrease with 9 decoders. Comparison of the proposed components under the 12-epoch setting. * indicates that we use the original DETR [2] segmentation branch in Mask DINO, where we follow DETR to fine-tune segmentation after finishing training detection. ? the performance of detection drops (49.6 AP as shown in</figDesc><table><row><cell cols="4">test layer# Mask DINO Mask2Former</cell><cell>Feature scale</cell><cell cols="2">box AP mask AP</cell><cell cols="4">Decoder layer# Box AP Mask AP</cell></row><row><cell>layer 0</cell><cell cols="2">39.6(+38.5)</cell><cell>1.1</cell><cell>single scale(1/8)</cell><cell>45.8</cell><cell>45.1</cell><cell></cell><cell>3</cell><cell>43.1</cell><cell>40.7</cell></row><row><cell>layer 3 layer 6 layer 9</cell><cell cols="2">44.0 45.9 46.0</cell><cell>42.3 43.3 43.7</cell><cell cols="3">3 scales 4 scales tures for Transformer decoder under the 50.5 45.8 50.5 46.0 Table 8: Comparison of multi-scale fea-</cell><cell></cell><cell>6 9 12</cell><cell>44.3 44.5 44.8</cell><cell>41.1 41.4 41.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">50-epoch setting. Both detection and</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">segmentation benefit from more feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>scales.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Tasks Box Mask 12ep Box AP 50ep</cell><cell>Mask AP</cell><cell>w/o decouple</cell><cell>Epochs 12</cell><cell>PQ 47.9</cell><cell cols="3">PQ thing PQ stf Box AP T h pan 54.0 38.8 42.8</cell><cell>Mask AP T h pan 39.6</cell></row><row><cell></cell><cell>45.1</cell><cell>50.1</cell><cell>-</cell><cell>w/ decouple</cell><cell>12</cell><cell>49.0(+1.1)</cell><cell>54.8</cell><cell>40.2</cell><cell>43.2</cell><cell>40.4</cell></row><row><cell cols="4">-44.5 50.5(+0.4) 46.0(+2.7) 43.3 Table 10: Task comparison under the 50-epoch setting. We train the same Mask DINO with different tasks and validate that box and mask can achieve mutual coopera-</cell><cell cols="7">w/o decouple w/ decouple Table 11: Effectiveness of decoupled box prediction for panoptic segmentation under the 50 52.7 58.8 43.5 48.7 44.1 50 53.0(+0.3) 59.1 43.9 48.8 44.3 12-epoch and 50-epoch settings.</cell></row><row><cell>tion.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Matching Box Mask</cell><cell cols="2">Box AP Mask AP</cell><cell cols="3">Mask DINO (ours)</cell><cell></cell><cell cols="3">Box AP Mask AP 44.5 41.4</cell></row><row><cell cols="4">44.4 40.2 44.5 Table 12: Matching method comparison un-40.5 38.4 41.4 der the 12-epoch setting. We train both tasks</cell><cell cols="4">? DINO Mask branch  *  ? Unified query selection for masks ? Unified denoising for masks ? Hybrid matching</cell><cell>49.5  ? 43.6 44.6 44.4</cell><cell cols="2">35.7 (-5.7) 40.3 (-1.1) 40.7 (-0.7) 40.5 (-0.9)</cell></row><row><cell cols="4">together but use different matching methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">to verify the effectiveness of hybrid matching.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 14 :</head><label>14</label><figDesc>Comparison of SOTA models on COCO test-dev. Mask DINO outperforms all existing models. "TTA" means test-time-augmentation. "O365" denotes the Objects365<ref type="bibr" target="#b30">[31]</ref> dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We refer the interested readers to discussions in Sec. 3 in DAB-DETR<ref type="bibr" target="#b21">[22]</ref> </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Implementation details A.1 General settings Dataset and metrics: We evaluate Mask DINO on two challenging datasets: COCO 2017 <ref type="bibr" target="#b20">[21]</ref> for object detection, instance segmentation, and panoptic segmentation; ADE20K <ref type="bibr" target="#b38">[39]</ref> for semantic segmentation. They both have "thing" and "stuff" categories, therefore we follow the common practice to evaluate object detection and instance segmentation on the "thing" categories and evaluate panoptic and semantic segmentation on the union of the "thing" and "stuff" categories. Unless otherwise stated, all results are trained on the train split and evaluated on the validation split. For object detection and instance segmentation, the results are evaluated with the standard average precision (AP) and mask AP <ref type="bibr" target="#b20">[21]</ref> result. For panoptic segmentation, we evaluate the results with the panoptic quality (PQ) metric <ref type="bibr" target="#b15">[16]</ref>. We also report AP T h pan (AP on the "thing" categories) and AP St pan (AP on the "stuff" categories). For semantic segmentation, the results are evaluated with the mean</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Papers with code -coco test-dev benchmark (instance segmentation)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pointly-supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omkar</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06404</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00057</idno>
		<title level="m">Simple training strategies and model scaling for object detection</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Instances as queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6910" to="6919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2918" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semask: Semantically masked transformers for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anukriti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12782</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6399" to="6408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9404" to="9413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointrend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9799" to="9808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dn-detr: Accelerate detr training by introducing query denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lionel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01305</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03814</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Panoptic segformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="318" to="327" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dab-detr: Dynamic anchor boxes are better queries for detr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbiao</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12329</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09883</idno>
		<title level="m">Swin transformer v2: Scaling up capacity and resolution</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejia</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06152</idno>
		<title level="m">Conditional detr for fast training convergence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pyramid fusion transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.04019</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8430" to="8439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5463" to="5474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Anchor detr: Query design for transformer-based detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07107</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Upsnet: A unified panoptic segmentation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8818" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end semi-supervised object detection with soft teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3060" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lionel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.03605</idno>
		<title level="m">Detr with improved denoising anchor boxes for end-to-end object detection</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">K-net: Towards unified image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021: The Ninth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>Intersection-over-Union (mIoU) metric</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compared to DINO, we increase the number of decoder layers from six to nine and use 300 queries. We follow Mask-RCNN [12] and Mask2Former [4] to setup the training and inference settings for segmentation tasks. We use batch size 16 and train 50 epoch for COCO segmentation tasks (instance and panoptic), 160K iteration for ADE20K semantic segmentation, and 90K iterations for Cityscapes semantic segmentation. We set the initial learning rate (lr) as 1 ? 10 ?4 and adopt a simple lr scheduler, which drops lr by multiplying 0.1 at the 11-th epoch for the 12-epoch setting and the 20-th epoch for the 24-epoch setting. For the other segmentation settings, we drop the lr at 0.9 and 0.95 fractions of the total number of training steps by multiplying 0.1. Under the ResNet-50 backbone, we use 4 A100 GPUs each with 40GB memory for all tasks. We report the frames-per-second (fps) tested on the same A100 NVIDIA GPU for Mask2Former and Mask DINO by taking the average computing time with batch size 1 on the entire validation set. Augmentations and Multi-scale setting: We use the same training augmentations as</title>
		<idno>ResNet-50 [13] and SwinL [24</idno>
	</analytic>
	<monogr>
		<title level="m">To achieve SOTA performance using a large model with the SwinL backbone, we use Objects365 [31] to pre-train an object detection model and then fine-tune the model on the corresponding datasets for all tasks</title>
		<imprint/>
	</monogr>
	<note>For mask loss, we adopt cross-entropy L ce and dice loss L dice . We also follow [17, 5, 4] to use point loss in mask loss for efficiency. Therefore, the total loss is a linear combination of three kinds of losses: ? cls L cls + ? L1 L L1 + ? giou L giou + ? ce L ce + ? dice L dice , where we set ? cls = 4, ? L1 = 5, ? giou = 2, ? ce = 5, and ? dice = 5. Basic hyper-parameters: Mask DINO has the same architecture as DINO [37], which is composed of a backbone, a Transformer encoder, and a Transformer decoder. in Mask2Former [4], where the major difference from DINO [37] on COCO is that we use largescale jittering (LSJ) augmentation [8, 11] and a fixed size crop to 1024 ? 1024, which also works well for detection tasks. We use the same multi-scale setting as in DINO [37] to use 4 scales in ResNet-50-based models and 5 scales in SwinL-based models</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Large models setting For large models with the SwinL backbone, we follow the same setting of DINO [37] to pre-train a model on the Objects365 [31] dataset for object detection. Then we finetune the pre-trained model on COCO instance and panoptic segmentation for 24 epochs and on ADE20K semantic segmentation for 160k iterations. For training settings on instance and panoptic segmentation on COCO, we use 1.2? larger scale (1280 ? 1280) and 16 A100 GPUs. For training settings on ADE20K semantic, we use 3? more queries (900) and 8 A100 GPUs. We also use Exponential Moving Average (EMA) in this setting</title>
		<imprint/>
	</monogr>
	<note>which helps in ADE20K semantic segmentation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
