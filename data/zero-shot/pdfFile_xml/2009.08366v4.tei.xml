<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 GRAPHCODEBERT: PRE-TRAINING CODE REPRESEN- TATIONS WITH DATA FLOW</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Lu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<addrLine>5 Microsoft</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Tufano</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao</forename><forename type="middle">Kun</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Clement</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Drain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 GRAPHCODEBERT: PRE-TRAINING CODE REPRESEN- TATIONS WITH DATA FLOW</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Research Asia, 6 Microsoft Devdiv, 7 Microsoft STCA</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of "wherethe-value-comes-from" between variables. Such a semantic-level structure is less complex and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search. 1</p><p>Published as a conference paper at ICLR 2021</p><p>In this work, we present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we leverage semantic-level information of code, i.e. data flow, for pretraining. Data flow is a graph, in which nodes represent variables and edges represent the relation of "where-the-value-comes-from" between variables. Compared with AST, data flow is less complex and does not bring an unnecessarily deep hierarchy, the property of which makes the model more efficient. In order to learn code representation from source code and code structure, we introduce two new structure-aware pre-training tasks. One is data flow edges prediction for learning representation from code structure, and the other is variable-alignment across source code and data flow for aligning representation between source code and code structure. GraphCodeBERT is based on Transformer neural architecture <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> and we extend it by introducing a graph-guided masked attention function to incorporate the code structure.</p><p>We pre-train GraphCodeBERT on the CodeSearchNet dataset <ref type="bibr" target="#b13">(Husain et al., 2019)</ref>, which includes 2.3M functions of six programming languages paired with natural language documents. We evaluate the model on four downstream tasks: natural language code search, clone detection, code translation, and code refinement. Experiments show that our model achieves state-of-the-art performance on the four tasks. Further analysis shows that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and the model has consistent preference for attending data flow.</p><p>In summary, the contributions of this paper are: (1) GraphCodeBERT is the first pre-trained model that leverages semantic structure of code to learn code representation. (2) We introduce two new structure-aware pre-training tasks for learning representation from source code and data flow. <ref type="formula">(3)</ref> GraphCodeBERT provides significant improvement on four downstream tasks, i.e. code search, clone detection, code translation, and code refinement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pre-trained models such as ELMo <ref type="bibr" target="#b27">(Peters et al., 2018)</ref>, GPT <ref type="bibr" target="#b29">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> have led to strong improvement on numerous natural language processing (NLP) tasks. These pre-trained models are first pre-trained on a large unsupervised text corpus, and then fine-tuned on downstream tasks. The success of pre-trained models in NLP also promotes the development of pre-trained models for programming language. Existing works <ref type="bibr" target="#b15">(Kanade et al., 2019;</ref><ref type="bibr" target="#b17">Karampatsis &amp; Sutton, 2020;</ref><ref type="bibr" target="#b8">Feng et al., 2020;</ref><ref type="bibr" target="#b32">Svyatkovskiy et al., 2020;</ref><ref type="bibr" target="#b4">Buratti et al., 2020</ref>) regard a source code as a sequence of tokens and pre-train models on source code to support code-related tasks such as code search, code completion, code summarization, etc. However, previous works only utilize source code for pre-training, while ignoring the inherent structure of code. Such code structure provides useful semantic information of code, which would benefit the code understanding process. Taking the expression v = max value ? min value as an example, v is computed from max value and min value. Programmers do not always follow the naming conventions so that it's hard to understand the semantic of the variable v only from its name. The semantic structure of code provides a way to understand the semantic of the variable v by leveraging dependency relation between variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Pre-Trained Models for Programming Languages Inspired by the big success of pre-training in NLP <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr">Yang et al., 2019;</ref><ref type="bibr" target="#b30">Raffel et al., 2019)</ref>, pre-trained models for programming languages also promotes the development of code intelligence <ref type="bibr" target="#b15">(Kanade et al., 2019;</ref><ref type="bibr" target="#b8">Feng et al., 2020;</ref><ref type="bibr" target="#b17">Karampatsis &amp; Sutton, 2020;</ref><ref type="bibr" target="#b32">Svyatkovskiy et al., 2020;</ref><ref type="bibr" target="#b4">Buratti et al., 2020)</ref>. <ref type="bibr" target="#b15">Kanade et al. (2019)</ref> pre-train a BERT model on a massive corpus of Python source codes by masked language modeling and next sentence prediction objectives. <ref type="bibr" target="#b8">Feng et al. (2020)</ref> propose CodeBERT, a bimodal pre-trained model for programming and natural languages by masked language modeling and replaced token detection to support text-code tasks such as code search. <ref type="bibr" target="#b17">Karampatsis &amp; Sutton (2020)</ref> pre-train contextual embeddings on a JavaScript corpus using the ELMo framework for program repair task. <ref type="bibr" target="#b32">Svyatkovskiy et al. (2020)</ref> propose GPT-C, which is a variant of the GPT-2 trained from scratch on source code data to support generative tasks like code completion. <ref type="bibr" target="#b4">Buratti et al. (2020)</ref> present C-BERT, a transformer-based language model pre-trained on a collection of repositories written in C language, and achieve high accuracy in the abstract syntax tree (AST) tagging task. Different with previous works, GraphCodeBERT is the first pre-trained model that leverages code structure to learn code representation to improve code understanding. We further introduce a graphguided masked attention function to incorporate the code structure into Transformer and two new structure-aware pre-training tasks to learn representation from source code and code structure.</p><p>Neural Networks with Code Structure In recent years, some neural networks leveraging code structure such as AST have been proposed and achieved strong performance in code-related tasks like code completion <ref type="bibr" target="#b21">(Li et al., 2017;</ref><ref type="bibr" target="#b2">Alon et al., 2019;</ref><ref type="bibr" target="#b18">Kim et al., 2020)</ref>, code generation <ref type="bibr" target="#b28">(Rabinovich et al., 2017;</ref><ref type="bibr">Yin &amp; Neubig, 2017;</ref>, code clone detection <ref type="bibr">(Wei &amp; Li, 2017;</ref><ref type="bibr">Zhang et al., 2019;</ref><ref type="bibr">Wang et al., 2020)</ref>, code summarization <ref type="bibr" target="#b1">(Alon et al., 2018;</ref><ref type="bibr" target="#b12">Hu et al., 2018)</ref> and so on <ref type="bibr" target="#b11">Hellendoorn et al., 2019)</ref>.  propose an AST-based language model to support the detection and suggestion of a syntactic template at the current editing location.  use graphs to represent programs and graph neural network to reason over program structures. <ref type="bibr" target="#b11">Hellendoorn et al. (2019)</ref> propose two different architectures using a gated graph neural network and Transformers for combining local and global information to leverage richly structured representations of source code. However, these works leverage code structure to learn models on specific tasks from scratch without using pre-trained models. In this work, we study how to leverage code structure for pre-training code representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA FLOW</head><p>In this section, we describe the basic concept and extraction of data flow. In next section, we will describe how to use data flow for pre-training.</p><p>Data flow is a graph that represents dependency relation between variables, in which nodes represent variables and edges represent where the value of each variable comes from. Unlike AST, data flow is same under different abstract grammars for the same source code. Such code structure provides crucial code semantic information for code understanding. Taking v = max value ? min value as an example, programmers do not always follow the naming conventions so that it is hard to understand the semantic of the variable. Data flow provides a way to understand the semantic of the variable v to some extent, i.e. the value of v comes from max value and min value in data flow. Besides, data flow supports the model to consider long-range dependencies induced by using the same variable or function in distant locations. Taking <ref type="figure">Figure 1</ref> as an example, there are four variables with same name (i.e. x 3 , x 7 , x 9 and x 11 ) but with different semantic. The graph in the figure shows dependency relation between these variables and supports x 11 to pay more attention to x 7 and x 9 instead of x 3 . Next, we describe how to extract data flow from a source code.  <ref type="figure">Figure 1</ref>: The procedure of extracting data flow given a source code. The graph in the rightmost is data flow that represents the relation of "where-the-value-comes-from" between variables. <ref type="figure">Figure 1</ref> shows the extraction of data flow through a source code. Given a source code C = {c 1 , c 2 , ..., c n }, we first parse the code into an abstract syntax tree (AST) by a standard compiler tool 2 . The AST includes syntax information of the code and terminals (leaves) are used to identify the variable sequence, denoted as V = {v 1 , v 2 , ..., v k }. We take each variable as a node of the graph and an direct edge ? = v i , v j from v i to v j refers that the value of j-th variable comes from i-th variable. Taking x = expr as an example, edges from all variables in expr to x are added into the graph. We denote the set of directed edges as E = {? 1 , ? 2 , ..., ? l } and the graph G(C) = (V, E) is data flow used to represent dependency relation between variables of the source code C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GRAPHCODEBERT</head><p>In this section, we describe GraphCodeBERT, a graph-based pre-trained model based on Transformer for programming language. We introduce model architecture, graph-guided masked attention and pre-training tasks including standard masked language model and newly introduced ones. More details about model pre-training setting are provided in the Appendix A.  <ref type="figure">Figure 2</ref>: An illustration about GraphCodeBERT pre-training. The model takes source code paired with comment and the corresponding data flow as the input, and is pre-trained using standard masked language modeling <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> and two structure-aware tasks. One structure-aware task is to predict where a variable is identified from (marked with orange lines) and the other is data flow edges prediction between variables (marked with blue lines). <ref type="figure">Figure 2</ref> shows the model architecture of GraphCodeBERT. We follow BERT <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> and use the multi-layer bidirectional Transformer <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> as the model backbone. Instead of only using source code, we also utilize paired comments to pre-train the model to support more code-related tasks involving natural language such as natural language code search <ref type="bibr" target="#b8">(Feng et al., 2020)</ref>. We further take data flow, which is a graph, as a part of the input to the model. Given a source code C = {c 1 , c 2 , ..., c n } with its comment W = {w 1 , w 2 , ..., w m }, we can obtain the corresponding data flow G(C) = (V, E) as discussed in the Section 3, where V = {v 1 , v 2 , ..., v k } is a set of variables and E = {? 1 , ? 2 , ..., ? l } is a set of direct edges that represent where the value of each variable comes from. We concatenate the comment, source code and the set of variables as the sequence input X = {[CLS], W, [SEP ], C, [SEP ], V }, where [CLS] is a special token in front of three segments and [SEP ] is a special symbol to split two kinds of data types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MODEL ARCHITECTURE</head><p>GraphCodeBERT takes the sequence X as the input and then converts the sequence into input vectors H 0 . For each token, its input vector is constructed by summing the corresponding token and position embeddings. We use a special position embedding for all variables to indicate that they are nodes of data flow. The model applies N transformer layers over the input vectors to produce contextual representations H n = transf ormer n (H n?1 ), n ? [1, N ]. Each transformer layer contains an architecturally identical transformer that applies a multi-headed self-attention operation <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> followed by a feed forward layer over the input H n?1 in the n-th layer.</p><formula xml:id="formula_0">G n = LN (M ultiAttn(H n?1 ) + H n?1 ) (1) H n = LN (F F N (G n ) + G n )<label>(2)</label></formula><p>where M ultiAttn is a multi-headed self-attention mechanism, F F N is a two layers feed forward network, and LN represents a layer normalization operation. For the n-th transformer layer, the output? n of a multi-headed self-attention is computed via:</p><formula xml:id="formula_1">Q i = H n?1 W Q i , K i = H n?1 W K i , V i = H n?1 W V i (3) head i = softmax( Q i K T i ? d k + M)V i<label>(4)</label></formula><p>G n = [head 1 ; ...; head u ]W O n (5) where the previous layer's output H n?1 ? R |X|?d h is linearly projected to a triplet of queries, keys and values using model parameters</p><formula xml:id="formula_2">W Q i ,W K i ,W V i ? R d h ?d k , respectively. u is the number of heads, d k is the dimension of a head, and W O n ? R d h ?d h is the model parameters. M ? R |X|?|X| is a mask matrix, where M ij is 0 if i-th token is allowed to attend j-th token otherwise ??.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GRAPH-GUIDED MASKED ATTENTION</head><p>To incorporate the graph structure into Transformer, we define a graph-guided masked attention function to filter out irrelevant signals. The attention masking function could avoid the key k i attended by the query q j by adding the attention score q T j k i an infinitely negative value so that the attention weight becomes zero after using a softmax function. To represent dependency relation between variables, a node-query q vi is allowed to attend to a node-key k vj if there is a direct edge from the node v j to the node v i (i.e. v j , v i ? E) or they are the same node (i.e. i = j). Otherwise, the attention is masked by adding an infinitely negative value into the attention score. To represent the relation between source code tokens and nodes of the data flow, we first define a set E , where v i , c j / c j , v i ? E if the variable v i is identified from the source code token c j . We then allow the node q vi and code k cj attend each other if and only if v i , c j / c j , v i ? E . More formally, we use the following graph-guided masked attention matrix as the mask matrix M in the equation 4:</p><formula xml:id="formula_3">M ij = 0 if q i ? {[CLS], [SEP ]} or q i , k j ? W ? C or q i , k j ? E ? E ?? otherwise<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PRE-TRAINING TASKS</head><p>We describe three pre-training tasks used for pre-training GraphCodeBERT in this section. The first task is masked language modeling <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref> for learning representation from the source code. The second task is data flow edge prediction for learning representation from data flow, where we first mask some variables' data flow edges and then let GraphCodeBERT predict those edges. The last task is variable-alignment across source code and data flow for aligning representation between source code and data flow, which predicts where a variable is identified from.</p><p>Masked Language Modeling We follow <ref type="bibr" target="#b7">Devlin et al. (2018)</ref> to apply masked language modeling (MLM) pre-training task. Specially, we sample randomly 15% of the tokens from the source code and paired comment. We replace them with a [MASK] token 80% of the time, with a random token 10% of the time, and leave them unchanged 10% of the time. The MLM objective is to predict original tokens of these sampled tokens, which has proven effective in previous works <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr" target="#b8">Feng et al., 2020)</ref>. In particular, the model can leverage the comment context if the source code context is not sufficient to infer the masked code token, encouraging the model to align the natural language and programming language representations.</p><p>Edge Prediction To learn representation from data flow, we introduce a pre-training task of data flow edges prediction. The motivation is to encourage the model to learn structure-aware representation that encodes the relation of "where-the-value-comes-from" for better code understanding. Specially, we randomly sample 20% of nodes V s in data flow, mask direct edges connecting these sampled nodes by add an infinitely negative value in the mask matrix, and then predict these masked edges E mask . Taking the variable x 11 in <ref type="figure">Figure 2</ref> for an example, we first mask edges x 7 , x 11 and x 9 , x 11 in the graph and then let the model to predict these edges. Formally, the pre-training objective of the task is calculated as</p><formula xml:id="formula_4">Equation 7, where E c = V s ? V ? V ? V s is a set of candidates for edge prediction, ?(e ij ? E) is 1 if v i , v j ? E</formula><p>otherwise 0, and the probability p eij of existing an edge from i-th to j-th node is calculated by dot product following a sigmoid function using representations of two nodes from GraphCodeBERT. To balance positive-negative ratio of examples, we sample negative and positive samples with the same number for E c .</p><formula xml:id="formula_5">loss EdgeP red = ? eij ?Ec [?(e ij ? E mask )logp eij + (1 ? ?(e ij ? E mask ))log(1 ? p eij )] (7)</formula><p>Node Alignment To align representation between source code and data flow, we introduce a pretraining task of node alignment across source code and data flow, which is similar to data flow edge prediction. Instead of predicting edges between nodes, we predict edges between code tokens and nodes. The motivation is to encourage the model to align variables and source code according to data flow. Taking <ref type="figure">Figure 3</ref> for an example, we first mask edges between the variable x 11 in data flow and code tokens, and then predict which code token the variable x 11 in data flow is identified from. As we can see, the model could predict that the variable x 11 is identified form the variable x in the expression "return x" according to data flow information (i.e. the value of x 11 comes from x 7 or x 9 ). Predict which code token the variable 11 in data flow is identified from Mask edges between the variable 11 in data flow and code tokens <ref type="figure">Figure 3</ref>: An example of the Node Alignment task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GraphCodeBERT</head><p>Specially, we randomly sample 20% nodes V s in the graph, mask edges between code tokens and sampled nodes, and then predict masked edges E mask . The pre-training objective of this task is similar to Equation 7, where E c = V s ? C is a set of candidates for node alignment. Similarly, we also sample negative and positive samples with the same number for E c .</p><formula xml:id="formula_6">loss N odeAlign = ? eij ?E c [?(e ij ? E mask )logp eij + (1 ? ?(e ij ? E mask ))log(1 ? p eij )] (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We evaluate our model on four downstream tasks, including code search, clone detection, code translation and code refinement. Detailed experimental settings can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">NATURAL LANGUAGE CODE SEARCH</head><p>Given a natural language as the input, the task aims to find the most semantically related code from a collection of candidate codes. We conduct experiments on the CodeSearchNet code corpus <ref type="bibr" target="#b13">(Husain et al., 2019)</ref>, which includes six programming languages. Different from the dataset and the setting used in the <ref type="bibr" target="#b13">Husain et al. (2019)</ref>, we filter low-quality queries by handcrafted rules and expand 1000 candidates to the whole code corpus, which is closer to the real-life scenario. We use Mean Reciprocal Rank (MRR) as our evaluation metric and report results of existing methods in the <ref type="table" target="#tab_2">Table 1</ref>. We provide more details about the filtered dataset and also give results using the same setting of <ref type="bibr" target="#b13">Husain et al. (2019)</ref>   All models calculate inner product of code and query encodings as relevance scores to rank candidate codes. We follow <ref type="bibr" target="#b13">Husain et al. (2019)</ref> to implement four methods as baselines in the first group to obtain the encodings, including bag-of-words, convolutional neural network, bidirectional recurrent neural network, and multi-head attention. The second group is the results of pre-trained models. Roberta ) is a pre-trained model on text corpus with MLM learning objective, while RoBERTa (code) is pre-trained only on code. CodeBERT <ref type="bibr" target="#b8">(Feng et al., 2020)</ref> is pre-trained on code-text pairs with MLM and replaced token detection learning objectives. As we can see, GraphCodeBERT that leverages code structure for pre-training brings a 2% gain of MRR, achieving the state-of-art performance. We also conducted t-test between our GraphCodeBERT and other baselines, and the results show the improvements are significant with p &lt; 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CODE CLONE DETECTION</head><p>Code clones are multiple code fragments that output similar results when given the same input. The task aims to measure the similarity between two code fragments, which can help reduce the cost of software maintenance and prevent bugs. We conduct experiments on the BigCloneBench dataset <ref type="bibr" target="#b31">(Svajlenko et al., 2014)</ref> and report results in the <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Deckard <ref type="bibr" target="#b14">(Jiang et al., 2007)</ref> is to compute vectors for structural information within ASTs and then a Locality Sensitive Hashing (LSH) <ref type="bibr" target="#b6">(Datar et al., 2004)</ref>    show that our GraphCodeBERT that leverages code structure information significantly outperforms other methods with p &lt; 0.01, which demonstrates the effectiveness of our pre-trained model for the task of code clone detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">CODE TRANSLATION</head><p>Code translation aims to migrate legacy software from one programming language in a platform to another. Following  and <ref type="bibr" target="#b5">Chen et al. (2018)</ref>, we conduct experiments on a dataset crawled from the same several open-source projects as them and report results in the <ref type="table" target="#tab_6">Table 3</ref>.</p><p>The Naive method is directly copying the source code as the translation result. PBSMT is short for phrase-based statistical machine translation <ref type="bibr" target="#b19">(Koehn et al., 2003)</ref>, and has been exploited in previous works <ref type="bibr" target="#b24">(Nguyen et al., 2013;</ref><ref type="bibr" target="#b16">Karaivanov et al., 2014)</ref>. As for the Transformer, we use the  same number of layers and hidden size as pre-trained models. To leverage the pre-trained models for translation, we initialize the encoder with pre-trained models and randomly initialize parameters of the decoder and the source-to-target attention. Results show that the models initialized with pre-trained models (i.e the second group) outperform PBSMT and Transformer models. Among them, GraphCodeBERT achieves state-of-art performance, which demonstrates the effectiveness of our model for code translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">CODE REFINEMENT</head><p>Code refinement aims to automatically fix bugs in the code, which can contribute to reducing the cost of bug-fixes. We use the dataset released by <ref type="bibr" target="#b33">Tufano et al. (2019)</ref> and report results in the <ref type="table" target="#tab_8">Table 4</ref>.</p><p>The Naive method directly copies the buggy code as the refinement result. For the Transformer, we use the same number of layers and hidden size as the pre-trained models. Same as the Section 5.3, we initialize the encoder with pre-trained models and randomly initialize parameters of the decoder  Results in the second group shows that pre-trained models outperform Transformer models further, and GraphCodeBERT achieves better performance than other pre-trained models on both datasets, which shows leveraging code structure information are helpful to the task of code refinement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">MODEL ANALYSIS</head><p>Ablation Study We conduct ablation study on the task of natural language code search to understand various components in our approach impact overall performance. We remove two pre-training tasks and data flow, respectively, to analyze their contribution. <ref type="table">Table 5</ref> shows that the overall performance drops from 71.3% to 70.3%?70.7% when removing Node Alignment and Edge Prediction pre-training tasks, respectively, which reveals the importance of two structure-aware pre-training tasks. After ablating the data flow totally, we can see that the performance drops from 71.3% to 69.3%, which means leveraging data flow to learn code representation could improve GraphCodeBERT.  <ref type="table">Table 5</ref>: Ablation study on natural language code search Node-vs. Token-level Attention <ref type="table">Table 6</ref> shows how frequently a special token [CLS] that is used to calculate probability of correct candidate attends to code tokens (Codes) and variables (Nodes). We see that although the number of nodes account for 5%?20%, attentions over nodes overwhelm node/code ratio (around 10% to 32%) across all programming languages. The results indicate that data flow plays an important role in code understanding process and the model pays more attention to nodes in data flow than code tokens.  <ref type="table">Table 6</ref>: Attention distribution (%) between code tokens (codes) and variables (nodes) across different programming language on natural language code search test sets. The first row is the ratio of the number of code tokens to nodes, and the second row is attention distribution of [CLS] token. <ref type="figure">Figure 4</ref> shows MRR score with respect to input sequence length on the validation dataset of Ruby programming language for the task of code search. AST Pre-order Traversal regards AST as a sequence by linearizing all AST nodes using pre-order traversal algorithm. AST Subtree Masking regards AST as a tree and introduce subtree masking <ref type="bibr" target="#b26">(Nguyen et al., 2019)</ref> for self-attention of the Transformer. In subtree masking, each node-query in AST attends only to its own subtree descendants, and each leaf-query only attends to leaves of AST. Transformer has a self-attention component with O(n 2 ) time and memory complexity where n is the input sequence length, and thus is not efficient to scale to long inputs.  <ref type="figure">Figure 4</ref>: MRR score on the validation dataset of Ruby for code search with varying length of input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison between AST and Data Flow</head><p>We observe that injecting AST even hurts the performance when the sequence length is short (e.g. shorter than 128), while Graph-CodeBERT consistently brings performance boost on varying sequence length and obtains better MRR score than AST-based methods. The main reason is that data flow is less complex and the number of nodes account for 5% ? 20% (see <ref type="table">Table 6</ref>), which does not bring an unnecessarily deep hierarchy of AST and makes the model more accurate and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>We also give a case study to demonstrate that data flow would enhance the code understanding process. Given a source code and a comment, we use GraphCodeBERT with and without data flow to predict whether the comment correctly describes the source code. Results are given in <ref type="figure">Figure 5</ref>. We can see that both models make correct prediction in the original example, where the threshold is 0.5 (left panel). To study the code understanding ability of models, we change the source code (center panel) and the comment (right panel), respectively. Although we make a small change on the source code (return a ? return b) and the comment (sum value ? mean value), the semantic of the source code and the comment are completely different and corresponding gold labels change from 1 to 0. As we can see in the figure, GraphCodeBERT without using data flow fails these tests and still outputs high probability for negative examples. After leveraging data flow, GraphCodeBERT better understands the semantic of source code and makes correct predictions on all tests, which demonstrates that data flow could improve the code understanding ability of the model.   <ref type="figure">Figure 5</ref>: We take a comment and a source code as the input (first row), and use GraphCodeBERT with and without data flow to predict the probability of the source code matching the comment (third row). The label is 1 if the comment correctly describes the source code otherwise 0 (second row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present GraphCodeBERT that leverages data flow to learn code representation.</p><p>To the best of our knowledge, this is the first pre-trained model that considers code structure for pre-training code representations. We introduce two structure-aware pre-training tasks and show that GraphCodeBERT achieves state-of-the-art performance on four code-related downstream tasks, including code search, clone detection, code translation and code refinement. Further analysis shows that code structure and newly introduced pre-training tasks boost the performance. Additionally, case study in the task of code search shows that applying data flow in the pre-trained model improves code understanding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PRE-TRAINING DETAILS</head><p>GraphCodeBERT includes 12 layers Transformer with 768 dimensional hidden states and 12 attention heads. For fair comparison, we use the same dataset as CodeBERT <ref type="bibr" target="#b8">(Feng et al., 2020)</ref> to pretrain our model. The dataset is the CodeSearchNet dataset 3 <ref type="bibr" target="#b13">(Husain et al., 2019)</ref>, which includes 2.3M functions with document pairs for six programming languages. We train the model on two DGX-2 machines, each having 16 NVIDIA Tesla V100 with 32GB memory. We set the max length of sequences and nodes as 512 and 128, respectively. We use the Adam optimizer to update model parameters with 1,024 batch size and 2e-4 learning rate. To accelerate the training process, we adopt the parameters of CodeBERT released by <ref type="bibr" target="#b8">Feng et al. (2020)</ref> to initialize the model. The model is trained with 200K batches and costs about 83 hours.</p><p>At each iteration, we alternate EdgePred and NodeAlign objectives in combination with MLM to pre-train the model. And we follow <ref type="bibr" target="#b20">Lample &amp; Conneau (2019)</ref> to sample each batch from the same programming language according to a multinomial distribution with probabilities {q i } i=1...N , where n i is number of examples for i-th programming language and ?=0.7. Sampling with this distribution could alleviates the bias towards high-resource languages.</p><formula xml:id="formula_7">q i = p ? i j=1 N p ? j with p i = n i k=1 N n k<label>(9)</label></formula><p>B NATURAL LANGUAGE CODE SEARCH Given a natural language as the input, code search aims to find the most semantically related code from a collection of candidate codes. We conduct experiments on the CodeSearchNet code corpus <ref type="bibr" target="#b13">(Husain et al., 2019)</ref> and follow <ref type="bibr" target="#b13">Husain et al. (2019)</ref> to take the first paragraph of the documentation as the query for the corresponding function. However, we observe that some queries contain content unrelated to the code, such as a link "http://..." that refers to external resources. Therefore, we filter following examples to improve the quality of the dataset.</p><p>(1) Examples whose code could not be parsed into abstract syntax tree.</p><p>(2) Examples whose query tokens number is shorter than 3 or larger than 256.</p><p>(3) Examples whose query contains special tokens such as "http://".</p><p>(4) Examples whose query is empty or not written in English.</p><p>Different from the setting of <ref type="bibr" target="#b13">Husain et al. (2019)</ref>, the answer of each query is retrieved from the whole development and testing code corpus instead of 1,000 candidate codes. We list data statistics about the filtered dataset in <ref type="table" target="#tab_15">Table 7</ref>.  We use GraphCodeBERT to separately encode query and source code with data flow, and calculate inner product of their representations of the special token [CLS] as relevance scores to rank candidate codes. In the fine-turning step, we set the learning rate as 2e-5, the batch size as 32, the max sequence length of queries and codes as 128 and 256, and the max number of nodes as 64. We use the Adam optimizer to update model parameters and perform early stopping on the development set.</p><p>We also report the results using the same setting of <ref type="bibr" target="#b13">Husain et al. (2019)</ref> in <ref type="table" target="#tab_17">Table 8</ref>. In this setting, models are required to retrieve an answer for a query from 1000 candidates. The results show that GraphCodeBERT also achieves the state-of-the-art performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C CODE CLONE DETECTION</head><p>Code clone detection aims to measure the similarity between two code fragments. We use Big-CloneBench dataset <ref type="bibr" target="#b31">(Svajlenko et al., 2014)</ref>, which contains over 6,000,000 true clone pairs and 260,000 false clone pairs from 10 different functionalities. We follow the settings in Wei &amp; Li <ref type="formula" target="#formula_0">(2017)</ref>, discarding code fragments without any tagged true and false clone pairs and using 9,134 remaining code fragments. Finally, the dataset provided by <ref type="bibr">Wang et al. (2020) includes 901,724/416,328/416,328</ref> examples for training/validation/testing. We treat the task as a binary classification to fine-tune Graph-CodeBERT, where we use source code and data flow as the input. The probability of true clone is calculated by dot product from the representation of [CLS]. In the fine-turning step, we set the learning rate as 2e-5, the batch size as 16, the max sequence length as 512 the max number of nodes as 128. We use the Adam optimizer to update model parameters and tune hyper-parameters and perform early stopping on the development set.</p><p>We give a case of the GraphCodeBERT output for this task in <ref type="figure">Figure 6</ref>. In this example, two Java source codes both download content from a given URL and convert the type of the content into string type. Therefore, two codes are semantically similar since they output similar results when given the same input. As we can see, our model gives a high score for this case and the pair is classified as true clone pair. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: Two source codes</head><p>Output: Semantically similar (score: 0.983) <ref type="figure">Figure 6</ref>: A case of GraphCodeBERT output for the code clone detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D CODE TRANSLATION</head><p>Code translation aims to migrate legacy software from one programming language in a platform to another. We conduct experiments on a dataset crawled from the same several open-source projects as  and <ref type="bibr" target="#b5">Chen et al. (2018)</ref>, i.e. Lucene 4 , POI 5 , JGit 6 and Antlr 7 . We do not use Itext 8 and JTS 9 as they do because of the license problem. Those projects have both Java and C# implementation. We pair the methods in the two languages based on their file names and method names. After removing duplication and methods with null function body, the total number of method pairs is 11,800, and we split 500 pairs from them as the development set and another 1,000 pairs for test. To demonstrate the effectiveness of GraphCodeBERT on the task of code translation, we adopt various pre-trained models as encoders and stay hyperparameters consistent. We set the learning rate as 1e-4, the batch size as 32, the max sequence length as 256 and the max number of nodes as 64. We use the Adam optimizer to update model parameters and tune hyper-parameters and perform early stopping on the development set.</p><p>We give a case of the GraphCodeBERT output for this task in <ref type="figure" target="#fig_4">Figure 7</ref>. In this example, the model successfully translates a piece of Java code into its C# version. The differences include the type name (from "boolean" to "bool") and the usage of getting a string value of a bool variable (from "String.valueOf(b)" to "b.ToString()"). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E CODE REFINEMENT</head><p>Code refinement aims to automatically fix bugs in the code. We use the dataset released by <ref type="bibr" target="#b33">Tufano et al. (2019)</ref>. The source is buggy Java functions while the target is the according fixed ones. Almost all the names of variables and custom methods are normalized. The dataset contains two subsets based on the code length. For the small dataset, the numbers of training, development and test samples are 46,680, 5,835 and 5,835. For the medium dataset, the numbers are 52,364, 6,545 and 6,545. We also use the sequence-to-sequence Transformer model to conduct the experiments. In the fine-tuning step, we adopt various pre-trained models as encoders. We set the learning rate as 1e-4, the batch size as 32, the max sequence length as 256 and the max number of nodes as 64. We use the Adam optimizer to update model parameters and perform early stopping on the development set.</p><p>We give two cases of the GraphCodeBERT output for this task in <ref type="figure" target="#fig_5">Figure 8</ref>. In the first example, the model successfully fixes the operation bug (from "*" to "+") to match the function name "add". In the second case, the source function and type names are normalized. The return type of this function is "void" but the buggy code gives a return value. Our model successfully removes the "return" word so that the return type of the function matches its declaration. We give a case study to illustrate retrieved results by GraphCodeBERT on the natural language code search task, with a comparison to CodeBERT and RoBERTa (code) models. Two examples are given in <ref type="figure">Figure 9</ref> and we can see that GraphCodeBERT successfully retrieves correct source codes for given queries on both examples. As we can see in the first case, incorporating data flow will help Graph-CodeBERT better understand the complicated expression "[(k, v) for k, v in self.items() if v is not self.EMPTY]" by leveraging dependency relation among variables in data flow graph. In the second case, the terminology "%Y-%m-%d" in Python program language is a format of date time.</p><p>GraphCodeBERT and CodeBERT both successfully search the correct function. Compared with RoBERTa (code), the second case shows that utilizing natural language descriptions for pre-training helps models do better semantic matching between source codes and queries on the code search task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 CODE CLONE DETECTION</head><p>We give a case study to compare GraphCodeBERT with CodeBERT and RoBERTa (code) models on code clone detection task. An example is shown in <ref type="figure">Figure 10</ref>. The first source code is to return the HTML content from a given URL, while the second source code is to return the last line from a fixed URL "http://kmttg.googlecode.com/svn/trunk/version". Their semantics are not similar due to their different outputs. Data flow could help GraphCodeBERT better understand that the return value "pageHTML" in first source code comes from "pageHTML.append(line); pageHTML.append("\r\n");" instead of "bufferedWriter.write(pageHTML.toString());" and the return value "version" in the second source code comes from "version = inputLine" or "version = null;". Although two source codes are highly overlapped (marked in yellow), GraphCodeBERT successfully predict the gold label compared with other models without data flow. <ref type="figure">Figure 10</ref>: An examples on code clone detection task and model prediction from different models. Overlapped code snippets between two source codes are marked in yellow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 CODE TRANSLATION AND CODE REFINEMENT</head><p>We give a case study to compare GraphCodeBERT with Transformer without using data flow on code generation tasks, including code translation and code refinement. We list three cases in <ref type="table">Table  9</ref> and <ref type="table" target="#tab_2">Table 10</ref>, respectively. <ref type="bibr">[src]</ref> represents the source input, <ref type="bibr">[ref]</ref> represents the reference, <ref type="bibr">[sys]</ref> represents Transformer without data flow and <ref type="bibr">[ours]</ref> represents GraphCodeBERT. We can see that the Transformer ([sys]) baseline makes several mistakes, including repeating tokens, logic errors and syntax errors, while GraphCodeBERT ( <ref type="bibr">[ours]</ref>) as a encoder could improve the generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G ERROR ANALYSIS</head><p>We also conduct error analysis and summary two main classes of errors for both code understanding and generation tasks. <ref type="figure">Figure 11</ref> gives three error cases of GraphCodeBERT on the natural language code search task. We observe that GraphCodeBERR mainly fails to retrieve those source code that involves functions of the library like "tf" (Tensorflow) in the first case and " GoogleCloudStorageHook" in the second case. It's difficult for GraphCodeBERR to understand meanings of APIs like "tf.io.read file" and "tf.image.decode image" without relevant information. A potential direction to mitigate the problem is to incorporate definitions of the library. The other major problem is that there are some terminologies like "unistr" in the query (corresponding to "decode('utf-8')" in Python code) in third case.</p><p>Incorporating more text-code pairs for pre-training might alleviate this problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ASTNN</head><label></label><figDesc>Zhang et al. (2019)  uses RNNs to encode AST subtrees for statements, then feed the encodings of all statement trees into an RNN to learn representation for a program. FA-AST-GMN (Wang et al., 2020)   uses GNNs over a flow-augmented AST to leverages explicit control and data flow information for code clone detection. Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>A case of GraphCodeBERT output for the code translation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Two cases of GraphCodeBERT output for the code refinement task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>in the Appendix B.</figDesc><table><row><cell>model</cell><cell cols="2">Ruby Javascript</cell><cell>Go</cell><cell>Python</cell><cell>Java</cell><cell>Php</cell><cell>Overall</cell></row><row><cell>NBow</cell><cell>0.162</cell><cell>0.157</cell><cell>0.330</cell><cell>0.161</cell><cell cols="2">0.171 0.152</cell><cell>0.189</cell></row><row><cell>CNN</cell><cell>0.276</cell><cell>0.224</cell><cell>0.680</cell><cell>0.242</cell><cell cols="2">0.263 0.260</cell><cell>0.324</cell></row><row><cell>BiRNN</cell><cell>0.213</cell><cell>0.193</cell><cell>0.688</cell><cell>0.290</cell><cell cols="2">0.304 0.338</cell><cell>0.338</cell></row><row><cell>selfAtt</cell><cell>0.275</cell><cell>0.287</cell><cell>0.723</cell><cell>0.398</cell><cell cols="2">0.404 0.426</cell><cell>0.419</cell></row><row><cell>RoBERTa</cell><cell>0.587</cell><cell>0.517</cell><cell>0.850</cell><cell>0.587</cell><cell cols="2">0.599 0.560</cell><cell>0.617</cell></row><row><cell>RoBERTa (code)</cell><cell>0.628</cell><cell>0.562</cell><cell>0.859</cell><cell>0.610</cell><cell cols="2">0.620 0.579</cell><cell>0.643</cell></row><row><cell>CodeBERT</cell><cell>0.679</cell><cell>0.620</cell><cell>0.882</cell><cell>0.672</cell><cell cols="2">0.676 0.628</cell><cell>0.693</cell></row><row><cell cols="2">GraphCodeBERT 0.703</cell><cell>0.644</cell><cell>0.897</cell><cell>0.692</cell><cell cols="2">0.691 0.649</cell><cell>0.713</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on code search. GraphCodeBERT outperforms other models significantly (p &lt; 0.01).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>is used to cluster similar vectors for detection. RtvNN (White et al., 2016) trains a recursive autoencoder to learn representations for AST. CDLH (Wei &amp; Li, 2017) learn representations of code fragments via AST-based LSTM and hamming distance is used to optimize the distance between the vector representation of AST pairs.</figDesc><table><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>Deckard</cell><cell>0.93</cell><cell>0.02</cell><cell>0.03</cell></row><row><cell>RtvNN</cell><cell>0.95</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>CDLH</cell><cell>0.92</cell><cell>0.74</cell><cell>0.82</cell></row><row><cell>ASTNN</cell><cell>0.92</cell><cell>0.94</cell><cell>0.93</cell></row><row><cell>FA-AST-GMN</cell><cell>0.96</cell><cell>0.94</cell><cell>0.95</cell></row><row><cell>RoBERTa (code)</cell><cell>0.949</cell><cell cols="2">0.922 0.935</cell></row><row><cell>CodeBERT</cell><cell>0.947</cell><cell cols="2">0.934 0.941</cell></row><row><cell>GraphCodeBERT</cell><cell>0.948</cell><cell cols="2">0.952 0.950</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on code clone detection. Graph- CodeBERT outperforms other pre-trained methods significantly (p &lt; 0.01).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on code translation.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Results on code refinement. and the source-to-target attention. Then we use the training data to fine-tune the whole model. In the table, we see that the Transformer significantly outperforms LSTM.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. Detecting code clones with graph neural networkand flow-augmented abstract syntax tree. arXiv preprint arXiv:2002.08653, 2020.Huihui Wei and Ming Li. Supervised deep features for software functional clone detection by exploiting lexical and syntactical information in source code. In IJCAI, pp. 3034-3040, 2017.</figDesc><table><row><cell>Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk. Deep learning</cell></row><row><cell>code fragments for code clone detection. In 2016 31st IEEE/ACM International Conference on</cell></row><row><cell>Automated Software Engineering (ASE), pp. 87-98. IEEE, 2016.</cell></row><row><cell>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V</cell></row><row><cell>Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint</cell></row><row><cell>arXiv:1906.08237, 2019.</cell></row><row><cell>Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation.</cell></row><row><cell>In The 55th Annual Meeting of the Association for Computational Linguistics (ACL), Vancouver,</cell></row><row><cell>Canada, July 2017. URL https://arxiv.org/abs/1704.01696.</cell></row><row><cell>Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. A novel neural</cell></row><row><cell>source code representation based on abstract syntax tree. In 2019 IEEE/ACM 41st International</cell></row><row><cell>Conference on Software Engineering (ICSE), pp. 783-794. IEEE, 2019.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Data statistics about the filtered dataset. For each query in the development and testing sets, the answer is retrieved from the whole candidate codes (i.e. the last row).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 8 :</head><label>8</label><figDesc>Results on natural language code search using the setting of<ref type="bibr" target="#b13">Husain et al. (2019)</ref>.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tree-sitter/tree-sitter</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/github/CodeSearchNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://lucene.apache.org/ 5 http://poi.apache.org/ 6 https://github.com/eclipse/jgit/ 7 https://github.com/antlr/ 8 http://sourceforge.net/projects/itext/ 9 http://sourceforge.net/projects/jts-topo-suite/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case 1</head><p>Query: Return copy of instance, omitting entries that are EMPTY Gold Source Code: def defined_items(self): return self.__class__ <ref type="bibr">( [(k, v)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoBERTa (code):</head><p>def parse(self, hcl, canonicalize=False): return self.request("parse", json={"JobHCL": hcl, "Canonicalize": canonicalize}, method="post", allow_redirects=True).json() <ref type="figure">Figure 9</ref>: Two examples on code search task and retrieved results from different models. As for the code generation task, <ref type="table">Table 11</ref> shows two cases of GraphCodeBERT on the code translation task. We find that the major problems include semantic errors like identifiers from nowhere in the first case and syntax errors like missing a "}" symbol before "return n" in the second case. This problem might be mitigated by incorporating a dedicated decoder that takes into account grammar of programming languages and different generation paradigm like generating a sequence of production rules <ref type="bibr">(Yin &amp; Neubig, 2017;</ref><ref type="bibr" target="#b9">Guo et al., 2018;</ref> in a context-free grammar manner. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Khademi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaked</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01400</idno>
		<title level="m">Generating sequences from structured representations of code</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Sadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<title level="m">Structural language models of code. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1910</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generative code modeling with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08490</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Exploring software naturalness throughneural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Buratti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Pujar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Bornea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Mccarley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetano</forename><surname>Rossiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Morari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Laredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12641</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tree-to-tree neural networks for program translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2547" to="2557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Locality-sensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayur</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentieth annual symposium on Computational geometry</title>
		<meeting>the twentieth annual symposium on Computational geometry</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08155</idno>
		<title level="m">A pre-trained model for programming and natural languages</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dialog-to-action: Conversational question answering over a large-scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Coupling retrieval and meta-learning for context-dependent semantic parsing. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global relational models of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep code comment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="200" to="20010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Codesearchnet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiferet</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deckard: Scalable and accurate tree-based detection of code clones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Misherghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Glondu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th International Conference on Software Engineering (ICSE&apos;07)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="96" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pre-trained contextual embedding of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gogul</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00059</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Phrase-based statistical translation of programming languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetoslav</forename><surname>Karaivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software</title>
		<meeting>the 2014 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming &amp; Software</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="173" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael-Michael</forename><surname>Karampatsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scelmo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13214</idno>
		<title level="m">Source code embeddings from language models</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Code prediction by feeding trees to transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seohyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinman</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13848</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>UNIVERSITY OF SOUTHERN CALIFORNIA MARINA DEL REY INFORMATION SCIENCES INST</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m">Cross-lingual language model pretraining</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09573</idno>
		<title level="m">Code completion with neural attention and pointer networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph-based statistical language model for code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/ACM 37th IEEE International Conference on Software Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="858" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lexical statistical machine translation for language migration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><forename type="middle">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering</title>
		<meeting>the 2013 9th Joint Meeting on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="651" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Divide-and-conquer approach for multi-phase statistical migration for source code (t)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Tuan Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><forename type="middle">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="585" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tree-structured attention with hierarchical accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan-Phi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07535</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards a big data curated benchmark of inter-project code clones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Svajlenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">F</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iman</forename><surname>Keivanloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chanchal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Mamun</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Software Maintenance and Evolution</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="476" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Intellicode compose: Code generation using transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Shao Kun Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sundaresan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08025</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An empirical study on learning bug-fixing patches in the wild via neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Tufano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cody</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Bavota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><forename type="middle">Di</forename><surname>Penta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denys</forename><surname>Poshyvanyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Software Engineering and Methodology (TOSEM)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Case1: Transformer outputs repeating tokens [src] public static final WeightedTerm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>Attention is all you need. ] getTerms(Query query){return getTerms(query,false</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<idno>sys] public static WeightedTerm</idno>
		<title level="m">GetTerms(Query query){return GetTerms(false, new static static static static static static WeightTerms</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">public static WeightedTerm[] GetTerms(Query query){return GetTerms(query, false)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Transformer outputs codes with severe logic and syntax errors [src] public long skip(long n){int s = (int) Math.min(available(</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">}</forename><surname>Case2</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Math.max(0, n)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">ptr += s;return s;} [ref] public override long Skip(long n){int s = (int)Math.Min(Available(</title>
		<imprint/>
	</monogr>
	<note>Math.Max(0, n)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">ptr += s;return s;} [sys] public override long Skip(long n){int s = Math.Min(n) == 0 ?</title>
		<imprint/>
	</monogr>
	<note>Math.Min(00.0 : Math.Min(n, s.Length);return s</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">long n){int s = (int)Math.Min(Available(</title>
		<imprint/>
	</monogr>
	<note>Math.Max(0, n)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">ptr += s;return s;} Case3: Transformer uses the wrong variable as a parameter</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">public UnbufferedCharStream(int bufferSize){n = 0;data = new int</title>
		<imprint/>
	</monogr>
	<note>bufferSize</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<idno>sys] public UnbufferedCharStream</idno>
		<title level="m">bufferSize){data = new int</title>
		<imprint/>
	</monogr>
	<note>int. data</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">public UnbufferedCharStream(int bufferSize){n = 0;data = new int</title>
		<imprint/>
	</monogr>
	<note>bufferSize</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Three examples that translate from Java to C# programming language on code translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>} Table 9</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>src] represents the source input, [ref] represents the reference, [sys] represents Transformer without data flow and [ours] represents GraphCodeBERT. Case1: Transformer adds redundant parameters (android.view.View view</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Intent VAR 1 = new android.content.Intent ( VAR 2 ) ; METHOD 2 ( VAR 1 , 0 ) ; android.content.Intent i = new android</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">{</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>public void METHOD 1 (. content.Intent ( this , VAR 3 class ) ; METHOD 3 ( i )</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">} [ref] public void METHOD 1 ( ) { android.content.Intent VAR 1 = new android.content.Intent ( VAR 2 )</title>
		<imprint/>
	</monogr>
	<note>METHOD 2 ( VAR 1 , 0 )</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">View view ) { android.content.Intent VAR 1 = new android</title>
		<idno>sys] public void METHOD 1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">content.Intent</note>
	<note>METHOD 2 ( VAR 1 , 0 )</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">public void METHOD 1 ( ) { android.content.Intent VAR 1 = new android</title>
		<imprint/>
	</monogr>
	<note>content.Intent ( VAR 2 ) ; METHOD 2 ( VAR 1 , 0 )</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Transformer outputs codes with severe logic or irrelevant codes [src] public java.util.Date METHOD 1 ( ) { return VAR 1 . METHOD 1 (</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">}</forename><surname>Case2</surname></persName>
		</author>
		<imprint>
			<pubPlace>METHOD 2 (</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">METHOD 2 ( ) ; } else { return null ; } } [sys] public java.util.Date METHOD 1 ( ) { if ( ( VAR 1 ) == null ) { return new java</title>
		<idno>java.util.Date METHOD 1 ( ) { if ( ( VAR 1</idno>
	</analytic>
	<monogr>
		<title level="j">util.Date (</title>
		<imprint/>
	</monogr>
	<note>METHOD 1 ( ) ) != null ) { return VAR 1 . METHOD 1 (. } return VAR 1 . METHOD 1 ( ) . METHOD 2 (</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">METHOD 2 ( ) ; } else { return null ; } } Case3: Transformer makes no change [src] public java.lang.String METHOD 1 ( TYPE 1 VAR 1 ) { if ( VAR 1 == null ) return null</title>
		<idno>java.util.Date METHOD 1 ( ) { if ( ( VAR 1</idno>
		<imprint/>
	</monogr>
	<note>METHOD 1 ( ) ) != null ) { return VAR 1 . METHOD 1 (. return VAR 1 . METHOD 2 (. getText (</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">String METHOD 1 ( TYPE 1 VAR 1 ) { return VAR 1 . METHOD 2 (</title>
		<imprint/>
	</monogr>
	<note>getText (</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">String METHOD 1 ( TYPE 1 VAR 1 ) { if ( VAR 1 == null ) return null</title>
		<imprint/>
	</monogr>
	<note>return VAR 1 . METHOD 2 (. getText (</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">String METHOD 1 ( TYPE 1 VAR 1 ) { return VAR 1 . METHOD 2 ( ) . getText ( ) ; } Table 10: Three examples on code refinement task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>src] represents the source input, [ref] represents the reference, [sys] represents Transformer without data flow and [ours] represents GraphCodeBERT. Figure 11: Error cases of GraphCodeBERT on the natural language code search. Case1: semantic error -identifiers from nowhere</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<title level="m">public String toString</title>
		<imprint/>
	</monogr>
	<note>{return getKey() + &quot;: &quot; + getValue(</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">public override string ToString(){return Name + &quot;: &quot; + GetValue(</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">}</forename><surname>Case2</surname></persName>
		</author>
		<title level="m">syntax errors -missing a &quot;}&quot; before &quot;return n</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Object o : data) {if ( o!=null ) n++;}return n;} [ref] public static int NumNonnull(object[] data){int n = 0;if (data == null){return n;} foreach (object o in data){if (o != null){n++;}}return n;} [ours] public static int NumNonNull(object[] data){int n = 0;if (data == null){return n;} foreach (object o in data){if (o != null){n++</title>
	</analytic>
	<monogr>
		<title level="m">public static int numNonnull(Object[] data) {int n = 0;if ( data == null ) return n; for</title>
		<imprint/>
	</monogr>
	<note>}return n;} Table 11: Error cases of GraphCodeBERT on the code translation task. [src] represents the source input, [ref] represents the reference and [ours] represents GraphCodeBERT</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
