<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ELSD: Efficient Line Segment Detector and Descriptor</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Zhang</surname></persName>
							<email>zhanghaotian@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megvii</forename><surname>Technology</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicheng</forename><surname>Luo</surname></persName>
							<email>luoyicheng@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbo</forename><surname>Qin</surname></persName>
							<email>qinfangbo2013@ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megvii</forename><surname>Technology</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation, CAS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Yijia He</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ELSD: Efficient Line Segment Detector and Descriptor</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the novel Efficient Line Segment Detector and Descriptor (ELSD) to simultaneously detect line segments and extract their descriptors in an image. Unlike the traditional pipelines that conduct detection and description separately, ELSD utilizes a shared feature extractor for both detection and description, to provide the essential line features to the higher-level tasks like SLAM and image matching in real time. First, we design the one-stage compact model, and propose to use the mid-point, angle and length as the minimal representation of line segment, which also guarantees the center-symmetry. The non-centerness suppression is proposed to filter out the fragmented line segments caused by lines' intersections. The fine offset prediction is designed to refine the mid-point localization. Second, the line descriptor branch is integrated with the detector branch, and the two branches are jointly trained in an end-to-end manner. In the experiments, the proposed ELSD achieves the state-of-the-art performance on the Wireframe dataset and YorkUrban dataset, in both accuracy and efficiency. The line description ability of ELSD also outperforms the previous works on the line matching task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image representation is an essential issue for many computer vision tasks such as SLAM, Structure-from-Motion (SfM), and image matching. Local point features <ref type="bibr">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> are widely used in these tasks, and recently the researchers have been exploring the usage of structural features for the better geometric representation <ref type="bibr">[9,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref>. Line segments are the most widely seen structural features in manmade environments. The reliable extraction of line segments and the matching across frames are important for the aforementioned tasks.</p><p>Recently, the convolutional neural networks (CNN) based line segment detection models have significantly outperformed the traditional methods. The models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref> consist of two stages. They first detect junctions and then generate line segment proposals and finally feed the embedding of each line segment into a classifier. Although these two-stage methods can achieve high performance, their running speed cannot satisfy real-time applications. TP-LSD <ref type="bibr">[12]</ref> first realizes the compact one-stage detection by introducing the Tri-points representation of line segment. However, TP-LSD predicts the two end-points separately and does not leverage the center-symmetric characteristics of the line segment. Thus, the predicted root-point might not be the mid-point of the two predicted end-points, and even the three points might be not co-linear. Moreover, the prediction of the root point is ambiguous especially when the lines intersect with each other so that many false rootpoints belonging to the fragmented line segments are detected. Besides, TP-LSD does not differentiate hard and easy examples during training. Some hard root points of line segments may not be properly detected.</p><p>Line segment descriptor is required to represent the line segment in a high-dimensional metric space, and the same line in two adjacent frames should be close in this metric space. There exist some CNN-based line descriptors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref>. However, these line descriptors are designed individually, and not yet tightly coupled with the line segment detector. It is also time-consuming to execute detection and description separately.</p><p>To this end, we propose ELSD that simultaneously predicts line segments and inferences line descriptors in an end-to-end fashion. 1) We introduce the one-stage architecture that utilizes the Center-Angle-Length (CAL) representation to vectorize a line segment. Our line detector consists of two module: (i) localization module and (ii) regression module. 2) Since the mid-points might be ambiguous for detection when lines intersect, as shown in <ref type="figure" target="#fig_2">Figure 3b</ref>, we introduce the line-centerness to filter the false mid-points belonging to fragmented line segments and adopt modified focal loss <ref type="bibr" target="#b18">[19]</ref> to focus more on the mid-points of hard cases. <ref type="bibr">3)</ref> In the regression module, the geometric maps are predicted to provide the rotation angles and lengths. Moreover, we refine the position of the midpoints by predicting the fine offsets to compensate for the localization accuracy. 4) In the line descriptor branch, we obtain the descriptor of each predicted line segment by line pooling. The descriptor is trained learned by random homography-based selfsupervision. The pipeline of ELSD is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>In summary, the main contributions are as follows:</p><p>? We present a pipeline that simultaneously detects line segments and inferences line descriptors in an end-toend fashion. To the best of our knowledge, this is the first work that unifies line detector and descriptor in a compact neural network. The major computation in the backbone is shared by the two tasks, and the two task branches can be jointly training, with negligible loss on detection performance.</p><p>? We utilize the Center-Angle-Length (CAL) representation to encode a line segment that has only four parameters to predict. To overcome the detection ambiguity when lines intersect, we proposed the non-centerness suppression mechanism to remove the mid-points of fragmented line segments. The midpoint position is further refined by using the offset regression so that the line segment localization is more precise.</p><p>? Our proposed ELSD obtains state-of-the-art performance in both accuracy and efficiency on the Wireframe and YorkUrban datasets. Moreover, the light version of our model achieves the speed of 107.5 FPS on a single GPU (RTX2080Ti) with comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Line Segment Detection</head><p>Deep learning-based line segment detection methods has attracted great attention due to the remarkable performances <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. AFM <ref type="bibr" target="#b31">[32]</ref> presented regional partition maps and attraction field maps of line segment maps, followed by a squeeze module to generate line segments. L-CNN <ref type="bibr" target="#b37">[38]</ref> first proposed a two-stage pipeline for wireframe parser. It predicts junction map to generate line proposals and utilize the LoI-pooling to gather feature of the proposals. Then a line verification network classifies proposals and removes false lines. PPGNet <ref type="bibr" target="#b35">[36]</ref> used a graph formulation to represent the relation between junctions. HAWP <ref type="bibr" target="#b32">[33]</ref> proposed a 4-D holistic attraction field map for generating line proposals and refine the proposals with junction heat maps. HT-HAWP <ref type="bibr" target="#b19">[20]</ref> combined Hough transform and HAWP model, obtaining excellent results in line segment detection. As the first one-stage line segment detector, TP-LSD <ref type="bibr">[12]</ref> proposed a Tri-Points representation to encode line segments and predicted two endpoints of each line segment in an end-to-end manner. LETR <ref type="bibr" target="#b30">[31]</ref> applied transformers for line segment detection from coarseto-fine grained. Our ELSD has a similar pipeline with TP-LSD. We encode a line segment by CAL representation, and can directly detect possible semantic line segments in the image without additional classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Object Detection</head><p>The recent surge of some keypoint-based object detectors has achieved remarkable performance. CornerNet <ref type="bibr" target="#b16">[17]</ref> formulated each object by a pair of corner keypoints and grouped all the detected corner keypoints to form the final detected bounding box, which requires more complicated post-processing. CenterNet <ref type="bibr" target="#b36">[37]</ref> models an object by the center point of its bounding box, and uses keypoint estimation method to find center points and regresses to its size. FCOS <ref type="bibr" target="#b25">[26]</ref> treats all the pixels with an object as candidate position and proposed center-ness to represent the importance of all the candidate positions. PolarNet <ref type="bibr" target="#b28">[29]</ref> learns corner pairs based on polar coordinates and avoids the large variance of learned offsets in Cartesian coordinate. Such keypoint-based methods have good detection capabilities with a fast speed and brief structure. Motivated by these, we proposed a new line segment representation and further designed a keypoint-based line segment detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Line Description</head><p>Like descriptor-based keypoint matching <ref type="bibr">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref>, line matching is also based on comparing the descriptors of the same line segments in two frames. MSLD <ref type="bibr" target="#b27">[28]</ref> constructs the line descriptors by counting the mean and variance of the gradients of pixels in the neighbor region of a line seg- ment. LBD <ref type="bibr" target="#b34">[35]</ref> proposes a line-band descriptor that computes gradient histograms over bands with more robustness and efficiency. Recently, some deep learning-based methods are used in learning line descriptors. LLD <ref type="bibr" target="#b26">[27]</ref> and DLD <ref type="bibr" target="#b15">[16]</ref> use the convolution neural network to learn the line descriptors and achieve remarkable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Line Representation</head><p>Line segments have two characteristics: 1) Due to the center-symmetry, the mid-point determines the location of the line segment, then the geometric feature is determined by the angle and length. 2) Since a line segment is straight, its direction can be consistently measured from a local part of it, which is easier to learn and requires a small receptive field. Therefore, we propose the Center-Angle-Length (CAL) representation to vectorize a line segment, which only has four parameters: 2D coordinates, rotation angle, and total length. In comparison, the Tri-points representation in TP-LSD <ref type="bibr">[12]</ref> has six parameters to predict, which is redundant, and the prediction results might not satisfy the center-symmetry.</p><p>With angle ?, length ?, and center point</p><p>x c y c , the two endpoints of the line segment are given by,</p><formula xml:id="formula_0">xs ys = xc yc + ? 2 cos ? sin ? xe ye = xc yc ? ? 2 cos ? sin ?<label>(1)</label></formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overall Network Architecture</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, our proposed ELSD consists of a backbone, a line detector branch, and a line descriptor branch. Our backbone is a U-shape network that consists of an encoder and two decoder blocks. The backbone takes an image of size 3 ? 512 ? 512 as input and outputs the shared feature with a size of 128 ? 128 ? 128. After the backbone, the architecture splits into two parts: one for line detector and the other for line descriptor. The line detector branch can predict line segments from an image. We can further obtain line descriptors by feeding both shared feature and predicted line segments into the line descriptor branch. ELSD can produce line segments and further extract fixed dimensional descriptors of the line segments in a single forward pass. Moreover, unlike the traditional pipeline that first detects line segments, then computes line descriptors, ELSD shares most of the parameters between these two tasks, which reduces the computation cost and improves the compactness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Line Detector Branch</head><p>Our line detector branch takes the shared feature from the backbone as input and splits into two modules: 1) Localization module, which consists of a line-midpoint detection head and a line-centerness detection head. In Non-Centerness-Suppression (NCS), the two heads are combined to get a more accurate center detection; 2) Regression module, which contains a geometrics regression head and a fine offset regression head. The outputs of the regression module are a pair of geometrics maps that consists of (?, ?) and a pair of fine offset maps. Finally, the outputs of two modules are combined together to generate the mid-points with two symmetrical endpoints as the line segment detec- tion results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Localization Module</head><p>Similar to TP-LSD <ref type="bibr">[12]</ref>, we use a deformable convolution, two atrous convolution (dilation rate=2) and a standard convolution layers to obtain the adaptive spatial sampling and a large receptive field, to predict the mid-point map. Furthermore, we leverage the line-centerness, i.e. how close an on-line point lies to the mid-point, to distinct the midpoints of the entire lines and the fragmented lines. The linecenterness is calculated by,</p><formula xml:id="formula_1">Pcenterness = min(d1, d2) max(d1, d2)<label>(2)</label></formula><p>where d 1 , d 2 are the distances from a point on the line segment to the two end-points, respectively. Apparently, P centerness equals 1 when the point is midpoint and decreases to 0 when the point approximates the end-points. The line-centerness module has the same architecture as the localization module. Denote the predicted line-midpoint map and line-centerness map as P mid and P centerness , respectively. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we propose the Non-Centerness Suppression (NCS) to filter false local midpoints belonging to fragmented line segments, and obtain a more accurate center confidence map P , as given by,</p><formula xml:id="formula_2">P = P mid ? P 0.5 centerness (3)</formula><p>The effectiveness of NCS is explained as follows. The midpoint detection is to obtain the exact positions but is prone to false detection caused by lines intersection. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, when a line segment is intersected with another line, its two endpoints and the intersection point form two shorter fragmented line segments. Although the mid-points of these fragmented line segments are not annotated as ground truths and are not expected to be detected, the detector tends to detect them because the fragmented line segments satisfy the definition of line segment. Differently, as visualized in <ref type="figure" target="#fig_2">Figure 3</ref>, line-centerness is not exact but provides a non-local distribution along the global line segment. The non-local distribution is more significant to inference and contains the global structure information of the potentially intersected lines. Namely, the midpoints can only mark a line segment without the awareness of the global structure, and the line-centerness map can further encode the global structure information with a non-local nonlinear multi-peak 2D distribution. Therefore, the line midpoint map and line-centerness map are fused by Eq 3 to suppress the false detection and get the final mid-points. Thus the ambiguity problem met by TP-LSD is effectively alleviated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Regression Module</head><p>Our regression module consists of two heads: a fine offset regression head and a geometrics regression head. The fine offset regression head is used to predict the offset of the center caused by the downsampling ratio. The refined sub-pixel mid-point can be obtained by just add the corresponding offset to the position of the predicted mid-point. The geometrics regression head can predict angle and length with respect to the midpoint. Both of our regression heads contain two 3 ? 3, a 1 ? 1 convolutional layers, and a deconvolutional layer. The deconvolutional layer is used to restore the size of the output map to 256 ? 256. We can index the related angle ? and length ? by the center position (x c , y c ) on the output map. Then a line segment can be obtained by Eq 1.</p><p>We utilize the CAL representation rather than Cartesian coordinates representation because the angle belongs to the geometric attributes of the line segment itself. Since the angle information can be perceived from a local part of the line segment, it is easier and more precise to predict the angle than the coordinates. We have done experiments to compare CAL representation and Cartesian coordinate representation under the same settings in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Line Descriptor Branch</head><p>Given a set of line segments, the purpose of the line descriptor branch is to learn a fixed-length descriptor for each line segment, which is used to distinguish different line segments according to the distance between their descriptors. We first apply two 3 ? 3 stride-1 convolution on the shared feature map from backbone. Then this intermediate feature map is resized to 256 ? 256 by bilinear interpolation. The resulting feature map named dense descriptor map is used in the following Line Pooling.</p><p>Line Pooling: Similar to RoIPool <ref type="bibr">[6]</ref> and RoIAlign <ref type="bibr">[7]</ref> used in object detection, the Line Pooling is used to squeeze the rotated narrow ROI to a descriptor vector. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the RoI of a line segment is defined as a rotated bounding box centered at the line segment, with the same length and angle as the line segment. The width of the RoI is a hyperparameter that depends on the desired size of the receptive field. Then we crop a fixed-size line feature map by sampling from the dense descriptor map using bilinear interpolation. Assuming there exist N candidate line segments and each line feature map has the size of C?H ?W , in which C is the channel dimension of the dense descriptor map and H , W represent the height and width of the line feature map respectively. We further apply a 1?W stride-1 depth-wise convolution as well as a stride-S max pooling to the line feature map. Finally, the resulting feature vector is flattened and fed into a fully connected layer and then normalized, producing the final descriptor with a fixed length denoted as d.</p><p>Self-supervised learning: Similar to <ref type="bibr">[4]</ref>, we apply random homographies on an image to produce a paired image with different views of the same scene, assuming that planar scenes or distant scenes are common in the real environment. The homography transformation that we used is composed of a set of transformations such as translation, scale, rotation, and perspective distortion, covering most of the viewpoint change caused by camera motion. After applying random homography on the input image, we can obtain the exact image-to-image transformation. So we can label matched or unmatched line segments just by transforming the endpoint of the line segment from one image to another and checking whether the distance of two corresponding endpoints is close enough.</p><p>When training from scratch, inspired by the Line Sampling Module of L-CNN <ref type="bibr" target="#b37">[38]</ref> that adopts static line sampler and dynamic line sampler to train the classifier, we use static line segments and dynamic line segments to train the de-scriptors. In the training stage, the static line segments are the annotated ground-truths, and the dynamic line segments are those predicted by the detection branch which changed as the model training proceeds. Because the line segment detection are not confident at the early training stage, we only use the detected line segments that are close enough to ground-truths as the dynamic line segments. Note that for training line descriptor branch, the proposed ELSD is trained on mini-batches of image pairs. We can obtain the ground truth correspondence of a pair of image's static line segments set during data preparation. The ground truth correspondence of a pair of dynamic line segments can be given by its closest static line segments. If the closest static line segments of a pair of dynamic line segments are matched, we then label this dynamic pair as a match and otherwise non-match. The whole training process of ELSD is shown in <ref type="figure" target="#fig_4">Figure 5</ref>. To sum up, the training with the static line segments helps to cold-start the training of descriptors at the beginning. The training with dynamic line segments helps to couple the descriptor with the actual prediction of the detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Functions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Total Loss</head><p>The total loss to train ELSD is composed of the line detector loss L p and line descriptor loss L d . Note that the input of ELSD is a pair of images with random homographies, which have both ground truth line segments, as well as the ground truth correspondence of ground truth line segments and predicted line segments. This allows us to optimize the two losses simultaneously. Given a pair of image, (I A , I B ), and the total loss can be represented as:</p><formula xml:id="formula_3">L(A, B) = ? p (L p (A) + L p (B)) + ? d L d (A, B)<label>(4)</label></formula><p>We empirically set ? p = 0.9, ? d = 0.1 in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Line Detector Loss</head><p>In the training stage of line detector branch, the outputs of four heads include line midpoint map, line-centerness map, geometrics maps, and fine offset maps. The ground truth of these maps is generated from the raw line segments label. The total loss of line segment detection is shown in Eq (5)</p><formula xml:id="formula_4">L p (A) =? mid L mid (A) + ? cen L cen (A)+ ? geo L geo (A) + ? of f L of f (A)<label>(5)</label></formula><p>where weights ? mid,cen,geo,of f = {25, 10, 1, 3} Localization loss: Given an image I A , for each ground truth midpoint p with continuous value, we construct the midpoint confidence map P ? [0, 1] H?W ?1 with four pixels near the midpoint by flooring and ceiling and we denote the selected pixels set by v. The 2D Gaussian kernel G xy =exp(? (x?px) 2 +(y?py) 2 2? 2 ) is then used to compute each confidence of the pixels in v. Then we normalize these confidence by dividing the max value of v. If the confidence of a pixel is assigned more than one time, we keep the max value of it. The overall process is described by,</p><formula xml:id="formula_5">P xy = max( G xy max (i,j)?v G ij , P xy )<label>(6)</label></formula><p>Then we followed CornerNet <ref type="bibr" target="#b16">[17]</ref> to use a variant of focal loss:</p><formula xml:id="formula_6">L mid (A) = ?1 N HW xy ? ? ? ? ? (1 ? Pxy) ? log( Pxy), if Pxy = 1 (1 ? Pxy) ? ( Pxy) ? log(1 ? Yxy), otherwise<label>(7)</label></formula><p>where ? and ? are hyper-parameters and N is the number of midpoints in an image. We set ? = 2 and ? = 4. According to Eq (2), we can obtain ground truth centerness map. Then we use weighted Binary Cross Entropy (BCE) loss denoted as L cen to supervise the learning process of the centerness.</p><p>Regression loss: Suppose the ground truth angle, length is (?, ?) and the corresponding predicted angle, length is ( ?, ?). We use L1 loss and smooth L1 loss as the geometrics regression loss which is defined as</p><formula xml:id="formula_7">L geo (A) = ? ang L1(?, ?) + ? len SmoothL1(?, ?) (8)</formula><p>where ? ang,len = {300, 10}. Besides, to recover the discretization error of midpoint coordinate caused by downsampling with ratio s, we additionally predict the fine offset maps O for each midpoint. The offset is trained with loss.</p><formula xml:id="formula_8">L of f (A) = 1 N N k=1 | O ? (p ? p s )|<label>(9)</label></formula><p>Note that only the midpoints where the confidence score of the ground truth equals 1 are involved in regression loss calculation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Line Descriptor Loss</head><p>We utilize the triplet loss proposed in Facenet <ref type="bibr" target="#b24">[25]</ref> to learn a line descriptor. Since the descriptors are regularized by l 2 normalization, the cosine similarity of two descriptors can be represented as cos </p><formula xml:id="formula_9">(d i , d j ) = d T i d j , where d i , d j</formula><formula xml:id="formula_10">T (A, B) = 1 N N i=1 [m ? d T i d + i + d T i d ? i ]+<label>(10)</label></formula><p>where [x] + = max{0, x}. N is the number of line segments in A, m is the margin that simultaneously enhances the consistency of matched line segments and the discrepancy of unmatched line segments. As mentioned in Section 3.4, we have both static and dynamic line segments, so the overall loss of descriptor loss is:</p><formula xml:id="formula_11">L d (A, B) =?D(T D (A, B) + T D (B, A))+ ?S(T S (A, B) + T S (B, A))<label>(11)</label></formula><p>where T D , T S represent the dynamic and static descriptor loss according to Eq <ref type="bibr">(10)</ref>. We set ? D = e E and ? S = 1 ? e E in this paper, where the E denotes the total epochs of entire training process and e denotes the current epoch. Briefly, we expect to rely more on static loss at the early training stage, and rely more on dynamic loss after the detector is well trained to adapt the descriptor to the actual detection result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setting</head><p>Implementation details: We use ResNet34 <ref type="bibr">[8]</ref> and optionally Hourglass Network <ref type="bibr" target="#b22">[23]</ref> and as the backbone, respectively. We conduct standard data augmentation for the training set, including horizontal/vertical flip and random rotate. Input images are resize to 512 ? 512. Our model is trained using ADAM <ref type="bibr" target="#b13">[14]</ref> optimizer with a total of 170 epochs on four NVIDIA RTX 2080Ti GPUs and an Inter Xeon Gold 6130 2.10 GHz CPU. The initial learning rate, weight decay, and batch size are set to 1e?3, 1e?5, and 16 respectively. The learning rate is divided by 10 at the 100th and 150th epoch.</p><p>Datasets: We train and evaluate our model on Wireframe Dataset <ref type="bibr">[11]</ref>, which contains 5000 images for training and 462 images for testing. We further evaluate on YorkUrban dataset <ref type="bibr">[3]</ref> with 102 test images from both indoor scenes and outdoor scenes to validate the generalization ability.</p><p>Structural Average Precision Metric <ref type="bibr" target="#b37">[38]</ref>: The structural average precision (sAP) of the line segment is based on the L2-distance between the predicted end-points and the ground truths. The predicted line segments will be counted as True Positive (TP) if the distance is less than a certain threshold ? and otherwise False Positives (FP). We set the threshold ? = 5, 10, 15 and report the corresponding results, denote by sAP 5 , sAP 10 , sAP <ref type="bibr" target="#b14">15</ref> . For more details see <ref type="bibr" target="#b37">[38]</ref>.</p><p>Heatmap based Metric <ref type="bibr" target="#b37">[38]</ref>: Heatmap-based F-score and average precision, F H and AP H are typical metrics used in wireframe parsing and line segment detection. We first convert the predicted line and ground truth line to two heatmaps by rasterizing the lines respectively. Then we can calculate the pixel-level precision and recall (PR) curves. Finally, we can compute F H and AP H with the PR curves.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison Experiments on Line Detection</head><p>We compare our proposed ELSD with line segment detection methods and wireframe parsing methods. Our model use ResNet34 as backbone and for a fair comparison with other methods, we also alter the backbone with Hourglass denote by Ours-HG. Ours-Lite is a faster version of our model. In Ours-Lite, we resize the input image to 256 ? 256 and add a decoder in backbone. Therefore the outputs maps of each head is 256 ? 256. <ref type="table" target="#tab_0">Table 1</ref> shows quantitative results based on sAP, AP H , F H , and FPS of line segment detection.</p><p>Ours-Res34 model achieves the best sAP on two datasets at a FPS of 42.6. It outperforms HAWP by 2.3% and 1.8% in msAP(mean of sAP) metric on Wireframe and YorkUrban respectively. Besides, when we replace the backbone with an Hourglass network(Ours-HG), it stills reaches a comparable sAP results on Wireframe. Since the HAWP and L-CNN are two stage methods, their inference speeds are limited. Moreover, their line segments rely on a pair of junctions, where junctions are usually local features that contain less global information. On the other hand, benefiting from more accurate midpoint detection and a more compact line representation method, our method is superior to TP-LSD. For further comparison, we evaluate the AP of mid-points similar to Junction AP proposed in L-CNN <ref type="bibr" target="#b37">[38]</ref>. The mean AP of mid-point of ELSD is 2.9% higher than TP-LSD, which means the mid-points are predicted more accurately in ELSD.</p><p>In terms of the heatmap based metric, ELSD shows significant results in AP H =87.2 on wireframe dataset and achieves comparable results on F H . Since our model predicts the line's angle, the angle prediction error of only one line segment could produce a lot of incorrect pixels, and but has the less influence to sAP. Therefore, the improvement of our model in pixel-based metric is not as obvious as that of sAP.</p><p>Our lightweight model can reach 107.5 FPS, which is 1.4 ? 48.9 times faster than other learning-based methods while the accuracy drop is limited. We use Ours-Res34 as the representative model and depicted the precision and recall curves on both datasets in <ref type="figure" target="#fig_6">Figure 6</ref>. Our ELSD outperforms other line segment detection methods especially in sAP metric on Wireframe dataset. Besides, ELSD achieved better generalization ability on YorkUrban dataset than other two-stage methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study for Line Detection</head><p>We run ablation experiments on the Wireframe dataset, as reported in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>NCS: NCS is to suppress the midpoints of fragmented line segments and remain the midpoints of entire segments. It improves the sAP 10 from 0.680 to 0.689 according to No.3 and No.1 .</p><p>Descriptor: The multi-task learning of detection and description leads to very small reduction on the detection accuracy sAP 10 from 0.689 to 0.685, according to No.1 and No.2 .</p><p>Upsample: For detecting line segments in real time, we use the shared feature map with 128 resolution, which is the same setting as L-CNN and HAWP. However, the prediction of the center in 128 resolution is much difficult than higher resolution. We solve this problem by upsampling the midpoint map, centerness map, geometrics maps and fine offset maps to 256 resolution. The sAP 10 is thus improved from 0.658 to 0.689 according to No.4 and No.1. Since we only upsample once by bilinear interpolation or deconvolution, it has almost no extra cost on inference speed.</p><p>Focal loss: We use a variant focal loss instead of standard Binary Cross Entropy (BCE) loss for training the midpoint map. Since we treat the prediction of the midpoints as a binary classification problem, the focal loss that we used can have the ability to focus on the hard classified examples of midpoints. By introducing the focal loss, the sAP 10 is improved from 0.660 to 0.689 according to No.5 and No.1 .</p><p>CAL: The proposed CAL representation is compared to the Tri-points representation in TP-LSD. The sAP 10 is improved from 0.679 to 0.689 according to No.6 and No.1 by replacing Tri-points with the CAL representation. This is because the Tri-points need to regress more parameters than the CAL representation (4 vs 2), and the angle is easier to learn than the displacements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison Experiments on Line Description</head><p>To evaluate the line descriptor performance, we compare our method with LBD <ref type="bibr" target="#b34">[35]</ref> and LLD <ref type="bibr" target="#b26">[27]</ref>. The methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref> etc, are not involved to be compared, because they leverage the additional geometric characters of lines, other than local appearances. We test all of the algorithms on a subset of ScanNet dataset <ref type="bibr">[2]</ref> which is an RGB-D video dataset annotated with 3D camera poses. We select  about 1000 image pairs with large viewpoint change, rotation change, and scale change for quantitative evaluation. We further compute the corresponding line descriptors of line segments detected by our model. We then obtain the ground truth line matches of the image pairs by checking if the reprojection error of corresponding lines less than a certain threshold. We find the nearest neighbors to match across descriptors and perform cross-checking then we can get predicted corresponds of line segments. We report the recall, precision and F-score to evaluate different descriptors. In our experiments, we use the OpenCV implementation of 72-dimensional LBD descriptors and the pre-trained model of LLD descriptors provided by the author. Meanwhile, our model is trained with setting the length of the descriptor to 256, 64 and 36 respectively. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. Our descriptors outperform the LBD and LLD significantly, especially in Recall. The LBD descriptor is designed by the human priority that might not be the optimal solution. The LLD descriptor and the similar learning-based descriptors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> are trained with the line segments given by the line detectors such as Edlines <ref type="bibr">[1]</ref>. However, there is a gap between those detected line segments and the annotated line segments in the datasets <ref type="bibr">[3,</ref><ref type="bibr">11]</ref>. In comparison, our descriptors cooperate well with our line detector since they share most of the parameters and representation, and their training is coupled, which can further reduce computation cost. The overall inference speed of ELSD (ResNet34 as backbone) with both line detector and line descriptor can achieve 38 FPS. Moreover, the 64-dimensional descriptor presents the same result as the 256-dimensional descriptor, and is better than the 36dimensional descriptor in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a fast and accurate model ELSD that simultaneously detects line segments and their descriptors in a single forward pass, allowing share computation and representation in the two tasks. To detect line segments, We first utilize the Center-Angle-Length (CAL) representation to encode a line segment that fully exploits the geometric characters of lines. Furthermore, a centerness map is introduced to filter the false line segments by Non-Centerness-Suppression (NCS). Our proposed line detector achieves state-of-the-art performance on two benchmarks in both accuracy and efficiency. Moreover, our model also achieves real-time speed with a single GPU. The lite model can reach the high speed of 107.5 FPS while keeping a comparable performance, and thus is useful for many higher-level tasks such as SLAM and SfM that require high real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Implementation details</head><p>Our ELSD uses the backbone of U-shape network that adopts ResNet34 <ref type="bibr">[4]</ref> as the encoder and optionally Hourglass Network <ref type="bibr">[8]</ref> as the backbone. We conduct standard data augmentation for the training set, including horizontal/vertical flip and random rotate. Input images are resized to 512 ? 512. Our model is trained using ADAM <ref type="bibr">[6]</ref> optimizer with a total of 170 epochs on four NVIDIA RTX 2080Ti GPUs and an Inter Xeon Gold 6130 2.10 GHz CPU. The initial learning rate, weight decay, and batch size are set to 1e ? 3, 1e ? 5, and 16 respectively. The learning rate is divided by 10 at the 100th and 150th epoch. It is recommended to train line detector firstly and then jointly train with descriptors, since the line descriptor branch is easier to learn compared to the line detector branch. See the source code in the supplementary file for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Qualitative Results on Line Segment Detector</head><p>We show more visualization results on the Wireframe dataset <ref type="bibr">[5]</ref> and YorkUrban dataset <ref type="bibr">[2]</ref> in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="figure" target="#fig_1">Figure  2</ref>. The configurations for visualization of different methods are as follows:</p><p>? The a-contrario validation of LSD <ref type="bibr">[3]</ref> is set to ?log = 0.01 ? 1.75 8 . ? The thresholds in line verification of L-CNN <ref type="bibr">[12]</ref>, HAWP <ref type="bibr">[10]</ref> and HT-HAWP <ref type="bibr">[7]</ref> are set to 0.98, 0.95 and 0.99 respectively, where the PR curve of sAP 10 achieves maximum F-score on Wireframe dataset. ? The threshold of root-point detection in TP-LSD is set as 0.43, where the PR curve of sAP 10 also achieves maximum F-score. ? For Our ELSD, the threshold of mid-point's score after Non-Centerness Suppression is set to 0.22 for the same purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Qualitative Results on Line Descriptor</head><p>To perform the quantitative and qualitative evaluation for line matching using different descriptors, we select about 1000 image pairs from ScanNet <ref type="bibr">[1]</ref> dataset that includes large viewpoint change, rotation change, and scale change. We further visualize the line matching results of LBD <ref type="bibr">[11]</ref>, LLD <ref type="bibr">[9]</ref> and our 64-dimensional descriptor. We use the OpenCV implementation of 72-dimensional LBD descriptors and the official model of LLD descriptors. Note that we find the nearest neighbors to match lines across descriptors and perform cross-checking. The results are shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Inference speed (FPS) and accuracy (sAP 10 ) on Wireframe dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the architecture of our proposed ELSD. It consists of three components: backbone, line detector branch and line descriptor branch. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of Non-Centerness-Suppression (NCS). (b) and (c) show the predicted mid-point map and centerness map, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of Line Pooling. See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Training framework. The random homography is used to realize self-supervised training. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>are two descriptors. Given image pair (I A , I B ) and their line segments set L A , L B , let L A i , d A i be the i-th line segment of image I A and its corresponding descriptor, d + i be the descriptor of its matched line segment in image I B , d ? i be the descriptor of its unmatched line segment in image I B with the maximal cosine similarity. Then the hard-negative triplet loss from image I A to I B can be represented as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>PR curves of sAP10  and AP H on Wireframe datasets (the left two figures) and YorkUrban datasets (the right two figures). The curve of our model is depicted in red. The results of DWP, AFM, and LSD on YorkUrban datasets are not displayed since they are slightly lower than the current methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>sAP 10 sAP 15 AP H F H sAP 5 sAP 10 sAP 15 AP H F Comparison experiments on line segment detection. '/' means the values are not reported in the related paper. ' ? ' means the post-processing scheme proposed in L-CNN[38] is used.</figDesc><table><row><cell>Method</cell><cell>Input size</cell><cell>Backbone</cell><cell cols="10">Wireframe sAP 5 H YorkUrban</cell><cell>FPS</cell></row><row><cell>LSD[5]</cell><cell>320</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>55.2</cell><cell>62.5</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>50.9</cell><cell>60.1</cell><cell>100</cell></row><row><cell>AFM[32]</cell><cell>320</cell><cell>U-Net</cell><cell>18.5</cell><cell>24.4</cell><cell>27.5</cell><cell>69.2</cell><cell>77.2</cell><cell>7.3</cell><cell>9.4</cell><cell>11.1</cell><cell>48.2</cell><cell>63.3</cell><cell>12.8</cell></row><row><cell>DWP[11] LETR[31] TP-LSD-Lite[12] TP-LSD[12]</cell><cell>512 512 320 512</cell><cell>Hourglass ResNet101 ResNet34 ResNet34</cell><cell>3.7 / 56.4 57.6</cell><cell>5.1 65.2 59.7 57.2</cell><cell>5.9 67.7 / /</cell><cell>67.8 86.3 / /</cell><cell>72.2 83.3 80.4 80.6</cell><cell>1.5 / 24.8 27.6</cell><cell>2.1 29.4 26.8 27.7</cell><cell>2.6 31.7 / /</cell><cell>51 62.7 / /</cell><cell>61.6 66.9 68.1 67.2</cell><cell>2.2 / 78.2 18.1</cell></row><row><cell>L-CNN[38]</cell><cell>512</cell><cell>Hourglass</cell><cell>58.9</cell><cell>62.9</cell><cell>64.9</cell><cell>80.3 82.8  ?</cell><cell>76.9 81.3  ?</cell><cell>24.3</cell><cell>26.4</cell><cell>27.5</cell><cell>58.5 59.6  ?</cell><cell>63.8 65.3  ?</cell><cell>11.1</cell></row><row><cell>HT-HAWP[20]</cell><cell>512</cell><cell>Hourglass</cell><cell>62.9</cell><cell>66.6</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>25</cell><cell>27.4</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>8.9</cell></row><row><cell>HAWP[33]</cell><cell>512</cell><cell>Hourglass</cell><cell>62.5</cell><cell>66.5</cell><cell>68.2</cell><cell>84.5 86.1  ?</cell><cell>80.3 83.1  ?</cell><cell>26.1</cell><cell>28.5</cell><cell>29.7</cell><cell>60.6 61.2  ?</cell><cell>64.8 66.3  ?</cell><cell>32.1</cell></row><row><cell>Ours-Lite Ours-HG Ours-Res34</cell><cell>256 512 512</cell><cell>ResNet34 Hourglass ResNet34</cell><cell>57.4 62.7 64.3</cell><cell>63.1 67.2 68.9</cell><cell>65.5 69.0 70.9</cell><cell>85.6 84.7 87.2 87.3  ?</cell><cell>80.2 80.3 82.3 83.1  ?</cell><cell>24.3 23.9 27.6</cell><cell>27.4 26.3 30.2</cell><cell>29.3 27.9 31.8</cell><cell>63.2 57.8 62.0 62.6  ?</cell><cell>63.3 62.1 63.6 64.8  ?</cell><cell>107.5 47 42.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>No. NCS Upsample Focal loss CAL Descriptor sAP 5 sAP 10 sAP 15 Ablation study of ELSD. See text for details.</figDesc><table><row><cell>1 2</cell><cell>64.3 64.2</cell><cell>68.9 68.5</cell><cell>70.9 70.3</cell></row><row><cell>3</cell><cell>63.6</cell><cell>68.0</cell><cell>70.0</cell></row><row><cell>4</cell><cell>60.3</cell><cell>65.8</cell><cell>68.2</cell></row><row><cell>5</cell><cell>61.8</cell><cell>66.0</cell><cell>68.0</cell></row><row><cell>6</cell><cell>62.3</cell><cell>67.9</cell><cell>70.3</cell></row><row><cell>7</cell><cell>58.0</cell><cell>62.8</cell><cell>64.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>The precision, recall and F-Score for LBD, LLD, and Ours with different dimension.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Edlines: Real-time line segment detection by edge drawing (ed)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 18th IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2837" to="2840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
		<editor>David Forsyth, Philip Torr, and Andrew Zisserman</editor>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Berlin Heidelberg</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Superpoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="224" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lsd: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="722" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Plvio: Tightly-coupled monocular visual-inertial odometry using point and line features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1159</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Keyframe-based dense planar slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Westman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5110" to="5117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TP-LSD: tri-points based line segment detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangbo</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Proceedings</note>
	<note>Part XXVII</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Linear rgb-d slam for planar environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pyojin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Coltin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H Jin</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="333" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wld: A wavelet and learning based line descriptor for line feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Raisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VMV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dld: A deep learning based line descriptor for line feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schweinfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5910" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical line matching based on line-junction-line structure descriptor and local homography estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page" from="1" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollar. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep hough-transform line priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><forename type="middle">L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh IEEE international conference on computer vision</title>
		<meeting>the seventh IEEE international conference on computer vision</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Robust line segments matching via graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanmeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianzhi</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (8)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">FCOS: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Computer Vision (ICCV)</title>
		<meeting>Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learnable line segment descriptor for visual slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakhitov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="39923" to="39934" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Msld: A robust descriptor for line matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanyi</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="941" to="953" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Polarnet: Learning to optimize polar keypoints for keypoint based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Xiongwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoi</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pose estimation from line correspondences: A complete analysis and a series of solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1209" to="1222" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Line segment detection using transformers without edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Holisticallyattracted wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Building a 3-d line-based map using stereo slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1364" to="1377" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An efficient and robust line segment matching approach based on lbd descriptor and pairwise geometric consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="794" to="805" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ppgnet: Learning point-pair graph for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d manhattan wireframes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Efficient edge-based methods for estimating manhattan frames in urban imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Elder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
		<idno>berg. 1</idno>
		<editor>David Forsyth, Philip Torr, and Andrew Zisserman</editor>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="197" to="210" />
			<pubPlace>Berlin, Heidelberg; Berlin Heidel</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lsd: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="722" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep hough-transform line priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><forename type="middle">L</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (8)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learnable line segment descriptor for visual slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vakhitov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39923" to="39934" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Holisticallyattracted wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14205v1[cs.CV]29</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2021-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An efficient and robust line segment matching approach based on lbd descriptor and pairwise geometric consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="794" to="805" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
