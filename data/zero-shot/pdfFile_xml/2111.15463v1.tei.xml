<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Consensus Synergizes with Memory: A Simple Approach for Anomaly Segmentation in Urban Scenes Image Segmentation Result SML MulMem CosMe</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiazhong</forename><surname>Cen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenkun</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moe</forename><surname>Key</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Consensus Synergizes with Memory: A Simple Approach for Anomaly Segmentation in Urban Scenes Image Segmentation Result SML MulMem CosMe</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. An overview of anomaly segmentation. When encountered with anomalies, segmentation models may make mistakes, e.g., anomalous samples (marked by yellow boxes) are recognized as road, people and their mixture, which may cause accidents in autonomous driving. We show the anomaly segmentation results of standardized max logit (SML) <ref type="bibr" target="#b21">[22]</ref>, our memory baseline MulMem,and CosMe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Anomaly segmentation is a crucial task for safety-critical applications, such as autonomous driving in urban scenes, where the goal is to detect out-of-distribution (OOD) objects with categories which are unseen during training. The core challenge of this task is how to distinguish hard indistribution samples from OOD samples, which has not been explicitly discussed yet. In this paper, we propose a novel and simple approach named Consensus Synergizes with Memory (CosMe) to address this challenge, inspired by the psychology finding that groups perform better than individuals on memory tasks. The main idea is 1) building a memory bank which consists of seen prototypes extracted from multiple layers of the pre-trained segmentation model and 2) training an auxiliary model that mimics the behavior of the pre-trained model, and then measuring the consensus of their mid-level features as complementary cues that synergize with the memory bank. CosMe is good at distinguishing between hard in-distribution examples and OOD samples. Experimental results on several urban scene anomaly segmentation datasets show that CosMe outperforms previ-Corresponding Author: wei.shen@sjtu.edu.cn ous approaches by large margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. An overview of anomaly segmentation. When encountered with anomalies, segmentation models may make mistakes, e.g., anomalous samples (marked by yellow boxes) are recognized as road, people and their mixture, which may cause accidents in autonomous driving. We show the anomaly segmentation results of standardized max logit (SML) <ref type="bibr" target="#b21">[22]</ref>, our memory baseline MulMem,and CosMe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Anomaly segmentation is a crucial task for safety-critical applications, such as autonomous driving in urban scenes, where the goal is to detect out-of-distribution (OOD) objects with categories which are unseen during training. The core challenge of this task is how to distinguish hard indistribution samples from OOD samples, which has not been explicitly discussed yet. In this paper, we propose a novel and simple approach named Consensus Synergizes with Memory (CosMe) to address this challenge, inspired by the psychology finding that groups perform better than individuals on memory tasks. The main idea is 1) building a memory bank which consists of seen prototypes extracted from multiple layers of the pre-trained segmentation model and 2) training an auxiliary model that mimics the behavior of the pre-trained model, and then measuring the consensus of their mid-level features as complementary cues that synergize with the memory bank. CosMe is good at distinguishing between hard in-distribution examples and OOD samples. Experimental results on several urban scene anomaly segmentation datasets show that CosMe outperforms previ-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation in urban scenes is an important technology for many vision-based applications. Current studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36]</ref> on semantic segmentation mainly focused on designing complex segmentation networks with higher segmentation capacities on indistribution samples, while they paid less attention to outof-distribution (OOD) samples, a.k.a, anomalous objects, whose categories are unseen during training. A commonlynoticed shortcoming of current segmentation networks is that they are incapable of identifying anomalous objects. Instead, they can only predict an anomalous object as one seen category. This issue greatly impedes their uses in safetycritical applications such as autonomous driving in urban scenes. For example, the anomalous objects (marked by yellow boxes) in <ref type="figure">Fig. 1</ref> are predicted as a road by a segmentation network, which may lead to accidents. To address this issue, anomaly segmentation, a task to detect and segment out unseen anomalous objects, is attracting more and more attention. and "OOD" denote in-distribution samples and out-of-distribution samples, respectively. Normal in-distribution samples and hard in-distribution samples are distinguished by thresholding with the MulMem anomaly score at 95% TPR. We can observe that the ability of MulMem to differentiate hard in-distribution samples from OOD samples is hardly guaranteed since the averaged Mul-Mem anomaly score over hard in-distribution samples is even higher than that over OOD samples, while AuxCon shows a favorable ability to address this issue.</p><p>Previous approaches attempted to address this task by revealing anomalies according to prediction incorrectness of the segmentation networks, e.g., uncertainties over categories <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> and re-synthesis errors caused by segmentation failures <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b39">39]</ref>. However, they lack a mechanism to distinguish hard in-distribution samples from anomalies, and thus suffer from high false positive rates.</p><p>Differentiating between hard in-distribution samples and anomalies is the core challenge of anomaly segmentation. In this paper, we devote to addressing this challenge inspired by how humans deal with normal and hard examples: For a normal example, a human can confidently confirm his/her opinion about it is correct according to his/her memory, e.g., experience or knowledge; For a hard example, the human might be uncertain about it, but he/she can check with others and know his/her opinion is probably correct if others have consistent opinions. This is known as a general psychology finding that groups perform better than individuals on memory tasks <ref type="bibr" target="#b19">[20]</ref>.</p><p>Based on the above intuition, we introduce a novel and simple approach named Consensus Synergizes with Memory (CosMe) for anomaly segmentation. First, we present a strong memory-based baseline, named Multi-layer Memory (MulMem), which leverages a feature bank consisting of seen prototypes extracted from multiple layers of the pre-trained segmentation model to memorize seen objects with different scales. Then, we present a consensusbased module, named auxiliary consensus (AuxCon), in which an auxiliary network is trained to keep reaching a consensus with the pre-trained segmentation model in a self-supervised manner. This is achieved by explicitly maintaining hierarchical consistency between the auxiliary network and the pre-trained segmentation model on the feature representations of samples from seen categories.</p><p>Intuitively, whether an in-distribution sample is normal or hard can be determined by its distance to the prototypes in the MulMem feature bank. By this means, we divide the in-distribution samples into normal and hard sets. Then we compute the averaged anomaly scores for normal in-distribution samples, hard in-distribution samples and OOD samples according to MulMem (the minimum distance to prototypes) and AuxCon (the feature inconsistency), respectively. As illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, the memorybased module can easily differentiate normal in-distribution samples from OOD samples, but its ability to distinguish hard in-distribution samples is hardly guaranteed. In contrast, the consensus-based module shows a clearly better differentiation ability between hard in-distribution samples and OOD samples than the memory-based module, while its overall discrimination between in-distribution samples and OOD samples is relatively smaller. These observations suggest good complementarity between MulMem and Aux-Con. And thus, a simple combination of them, i.e., CosMe, has a strong ability for anomaly segmentation, especially can favorably distinguish hard in-distribution samples from anomalies.</p><p>Experimental results show that CosMe achieves consistent and substantial improvements over the state-of-the-art anomaly segmentation approaches on several urban scene datasets, and its performance is even on par with those methods using extra OOD samples for re-training.</p><p>In summary, our contributions are as follows:</p><p>? We are the first to explicitly design a mechanism to tackle the challenge of hard in-distribution samples in anomaly segmentation.</p><p>? We propose CosMe, a novel approach which enjoys the benefits from the complementarity between memorybased prototype-level distance and feature-level inconsistency, with a good ability in differentiating between hard in-distribution samples and OOD samples.</p><p>? CosMe significantly outperforms the current state-ofthe-art anomaly segmentation approach and even is comparable with the methods using extra OOD samples for re-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The problem of detecting and segmenting unseen anomalous objects gradually attracts more and more attention. It is also related to out-of-distribution (OOD) detection. In this section, we first give a brief review of OOD detection, then describe previous approaches for anomaly segmentation in urban scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Out of Distribution (OOD) detection</head><p>OOD detection is a broad concept, which is critical to ensuring the reliability of machine learning systems. There are many sub-tasks under this task, such as open set recognition <ref type="bibr" target="#b37">[37]</ref>, novelty detection <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>, etc. Since Hendryck and Gimpel <ref type="bibr" target="#b18">[19]</ref> proposed the first OOD detection baseline in 2017, a plethora of approaches were developed, which can be categorized into clustering-based <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b31">31]</ref>, uncertainty-based <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">25]</ref>, reconstruction-based <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b39">39]</ref> and density-based <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>, etc. As clustering-based is one type of most straightforward OOD detection method, we revisit it for anomaly segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anomaly Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Uncertainty-based</head><p>Detecting anomalies based on model uncertainties is intuitive, since they are unseen during model training. Hendryck and Gimpel <ref type="bibr" target="#b18">[19]</ref> proposed a baseline for OOD detection named "maximum softmax probability" (MSP), which measures anomaly scores by the maximum softmax probability outputted by the softmax classifier. Then they proposed "maxlogit" <ref type="bibr" target="#b17">[18]</ref>. In maxlogit, the logits, i.e., the inputs of the softmax classifier are used as the anomaly scores instead. Jung et al. <ref type="bibr" target="#b21">[22]</ref> proposed "standardized maximum logit" (SML) by improving "maxlogit". They used the statistics of the training set to standardize the "maxlogit" scores for each seen category, leading to a large improvement in anomaly segmentation results. However, these approaches lack a mechanism to distinguish hard indistribution samples from anomalies. To address this challenge, some other approaches tried to first make the segmentation networks more sensitive to anomalous samples by either re-training them with a new loss function <ref type="bibr" target="#b4">[5]</ref> or re-designing them by a new network architecture <ref type="bibr" target="#b1">[2]</ref>, then applied the uncertainty measures. However, the segmentation networks modified by these strategies sacrifice their performance on seen categories. Chan et al. <ref type="bibr" target="#b5">[6]</ref> utilized samples from the COCO dataset <ref type="bibr" target="#b26">[26]</ref> as OOD proxy for urban scenes and introduced an extra training objective to maximize the uncertainty on these samples. However, in practice, the categories of anomalous samples are unknown and inaccessible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Synthesis-based</head><p>Recently, thanks to the rapid development of generative adversarial networks (GANs) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">34]</ref>, one can reconstruct complex urban scenes from segmentation results. Some recent studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b39">39]</ref> re-synthesized an image from a segmentation result and then compare it to the original input image to localize the anomalous instances. Synthesisbased approaches are unable to differentiate between hard in-distribution samples and OOD samples since segmentation results of hard samples are usually incorrect. In addition, their anomaly segmentation performance is heavily dependent on the generation quality of GANs and may suffer from degradation due to artifacts and style shifts in the synthesized images. Last, the serialized processing, i.e., segment-synthesize-compare, make this kind of approach difficult to be applied in practical real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Hybrid</head><p>To achieve better anomaly segmentation results, Giancarlo et al. <ref type="bibr" target="#b8">[9]</ref> proposed a hybrid approach, which combines both uncertainty-based approach and synthesis-based approach. Nevertheless, their hybrid approach still suffers from the own issues of each component mentioned above. Besides, they implicitly made use of OOD samples to train a discriminator in their approach, which limits the generalization ability of their approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Consensus Synergizes with Memory</head><p>In this section, we first formulate the problem of anomaly segmentation, and then sketch out the overall framework of our method CosMe. Next, we describe the details of the two key components of CosMe, including the memory-based baseline Multi-layer Memory (MulMem) (Sec. 3.3) and the consensus-based module Auxiliary Consensus (AuxCon) (Sec. 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Setup</head><p>We first set up the problem of anomaly segmentation. Given a training set T = {(x (n) , y (n) ) n=1 N for semantic segmentation with a category set C, where (x (n) , y (n) ) denotes a pair of training image and its corresponding segmentation ground-truth and y (n) i,j ? C denotes the category label for the pixel at location (i, j), a segmentation model M is pre-trained on T , parameterized by ?. Given a testing image x with categories from an unseen set U ? C = ?, the goal of anomaly segmentation is to segment out pixels that belong to unseen categories, based on M and T . This can be achieved by assigning an anomaly score ? i,j (x) to each pixel (i, j), so that</p><formula xml:id="formula_0">min ? i,j (x), if y i,j ? C; max ? i,j (x), if y i,j ? U.</formula><p>Some state-of-the-art anomaly segmentation approaches either retrain the pre-trained segmentation model or even make use of OOD data. However, we argue that these approaches have limitations, since 1) retraining may lead  to negative effects on in-distribution segmentation performance; 2) the categories of OOD data are unknown and inaccessible in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overall Framework</head><p>The overall framework of CosMe is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. Except for the pre-trained segmentation model, there are mainly two parts in CosMe: one is the Multi-layer Memory (MulMem), the other is the Auxiliary Consensus (AuxCon). These two modules are combined to tackle the problem of hard in-distribution samples while fully utilizing the memories embedded in the segmentation model. We give a brief introduction to them as follows:</p><p>? The Multi-layer Memory (MulMem) is a feature bank consisting of several feature sub-branches, each of which stores representative features, i.e., prototypes, outputted from a specific layer of the segmentation network. Whether a sample is anomalous is determined by the feature distance of the sample to the prototypes in sub-branches.</p><p>? The Auxiliary Consensus (AuxCon) is an auxiliary model sharing information from the pre-trained model. Its task is to mimic the behavior of the pre-trained model on in-distribution data. When encountered with anomalies, the auxiliary model will show relatively big inconsistency with the pre-trained model. So the mimicking error of the auxiliary model is a kind of anomaly score naturally.</p><p>CosMe was built on the observation that hard indistribution samples often lead to segmentation failures and are easily confused with anomalies. We are inspired by how humans distinguish uncertain samples. Humans tend to query others for suggestions when they cannot draw clear conclusions with their own memory. Thus, Mul-Mem in CosMe imitates human memory, while AuxCon imitates someone else with similar experiences. For hard in-distribution samples, AuxCon can show relatively higher consistency than totally unseen anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-layer Memory</head><p>Let f (l) (x; ?) be the feature map for an input image x, outputted from layer l of the pre-trained segmentation model M, and let f (l) i,j (x; ?) denote the feature vector at the location (i, j) of this feature map. Our goal is to build a memory bank M = {S (l) |l ? L} to store prototype features of seen samples from the training set T , where S (l) is a sub-branch of the bank to store prototype features outputted from layer l, and L is the set of layer of interests.</p><p>To memorize seen samples, a straightforward way is performing clustering on the training set to generate prototypes. Since the segmentation network processes data samples batch-wise, we propose a batch-based clustering algorithm to generate the prototypes. Without loss of generality, we describe our batch-based clustering algorithm by taking prototype generation for one feature sub-branch S (l) as an example. We first set the feature sub-branch as an empty set: S (l) = {?}, then we initialize this set by iteratively adding elements into it to form the prototypes. The Randomly select one f (l) i,j (x; ?) from its features <ref type="bibr">5:</ref> if max{? p, f</p><formula xml:id="formula_1">(l) i,j (x; ?) |?p ? S (l) } &lt; ? then 6: S (l) ? S (l) ? {f (l) i,j (x; ?)} 7:</formula><p>end if 8: end while elements are features from some randomly selected training images. Given a new element, e.g., f (l) i,j (x; ?), we first compute the cosine similarity ?(?, ?) between the element f (l) i,j (x; ?) and each prototype p ? S (l) in current feature sub-branch, if this element is not similar to all the prototypes (determined by a similarity threshold ? ), then we add it to the feature sub-branch as a new prototype, i.e.,</p><formula xml:id="formula_2">S (l) ? S (l) ? {f (l)</formula><p>i,j (x; ?)}; otherwise, this element is not considered. This element adding process for prototype initialization is ended until the size of the feature sub-branch reaches a pre-set number K. The algorithm of prototype initialization for sub-branch S (l) is shown in Algorithm 1.</p><p>After prototype initialization, we learn the prototypes in S (l) by a momentum update, given each training batch B. Specifically, for each prototype p in S (l) , we update p by the features that are closest to p, i.e., the highest cosine similarity. This can be achieved by maintaining a set S (l) p to store such features for prototype p:</p><formula xml:id="formula_3">S (l) p ? {f ? F B |p = arg max p ?S (l) Q S (l) ,F B },<label>(1)</label></formula><p>where</p><formula xml:id="formula_4">F B ? {f (l)</formula><p>i,j (x; ?)|x ? B} is the set containing all the features of the images in batch B and Q S (l) ,F B ? {?(p, f )|?p ? S (l) , ?f ? F B } is the set containing all cosine similarities between each prototype p ? S (l) and each feature F B . Finally, p is computed by a momentum update:</p><formula xml:id="formula_5">p ? m ? p + (1 ? m) ? 1 |S (l) p | s (l) p ?S (l) p s (l) p ,<label>(2)</label></formula><p>where m is a pre-defined momentum coefficient. The algorithm of prototype learning by the momentum update for sub-branch S (l) is given in Algorithm 2.</p><p>With the learned feature sub-branch S (l) , given an input image x, an anomaly score map ? (l) (x) for this input image is computed by</p><formula xml:id="formula_6">? (l) i,j = 1 ? max{? p, f (l) i,j (x; ?) |?p ? S (l) },<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Memory Learning for Sub-branch S (l)</head><p>Input:</p><formula xml:id="formula_7">Training batch B, coefficient m, initialized S (l) Output: Updated K prototypes S (l) = {p k } K k=1 1: F B ? {f (l) i,j (x; ?)|x ? B} 2: Q S (l) ,F B ? {?(p, f )|?p ? S (l) , ?f ? F B } 3: for p ? S (l) do 4: S (l) p ? {f ? F B |p = arg max p ?S (l) Q S (l) ,F B } 5: p ? m ? p + (1 ? m) ? 1 |S (l) p | s (l) p ?S (l) p s (l) p 6: end for where ? (l)</formula><p>i,j (x) denotes the anomaly score at the location (i, j) of the anomaly score map ? (l) (x).</p><p>Since several sub-branches form the feature bank M = {S (l) |l ? L} of MulMem, we compute the anomaly score map ?(x) given by the feature bank M by a simple combination of the anomaly score maps given by each sub-branch:</p><formula xml:id="formula_8">? i,j (x) = ? l?L ? (l) i,j (x),<label>(4)</label></formula><p>where ? i,j (x) is the MulMem anomaly score at the location (i, j) of the anomaly score map ?(x). We additionally adopt the standardization strategy in <ref type="bibr" target="#b21">[22]</ref> to normalize the MulMem anomaly scores in ?(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Auxiliary Consensus</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, the auxiliary consensus module explicitly ensures feature consistency between the pretrained segmentation model and an auxiliary model. This is achieved by self-supervised learning without using any segmentation annotations of the training set T . Given the pretrained segmentation model M, we build an auxiliary model M parameterized by ? which has the same down-sampling schedule as M, so that for a layer l in M we can find its corresponding layer l in M . For example, ResNet50 <ref type="bibr" target="#b16">[17]</ref> can be the backbone of an auxiliary model for a segmentation model with ResNet101 as the backbone.</p><p>Let L s be a set of layers of M, (e.g., for ResNet, L s can be the last Conv layers of the five Conv blocks L s = {C1, C2, C3, C4, C5}) which is used to supervise the corresponding layers of M . For each layer l ? L s , let s (l) be the size of the feature map f (l) (x; ?), and l is its corresponding layer in M , our learning purpose is to enforce the feature map g (l ) (x; ? ) outputted by layer l of M to approach f (l) (x; ?).</p><p>Towards this end, we fix ?, and minimize the following loss function on the training set T : </p><formula xml:id="formula_9">L = x?T l?Ls 1 s (l) ||f (l) (x, ?) ? g (l ) (x; ? )|| 2 F ,<label>(</label></formula><formula xml:id="formula_10">L ? x?B l?Ls 1 s (l) ||f (l) (x, ?) ? g (l ) (x, ? )|| 2 F 4: ? ? ? ? ? ? ?L ?? 5: end for</formula><p>where || ? || F means the Frobenius norm of a matrix. The overall training algorithm is shown by Algorithm 3.</p><p>During inference, to compute anomaly scores, we select a subset L e from L s as an evaluation set. Given a testing image x, the anomaly score ?</p><formula xml:id="formula_11">(l) i,j at each location (i, j) is computed by: ? (l) i,j = 1 C (l) ||f (l) i,j (x, ?) ? g (l ) i,j (x, ? )|| 2 2 ,<label>(6)</label></formula><p>where || ? || 2 means 2 norm and C (l) denotes the dimension of the feature channel. Then AuxCon anomaly score at each location (i, j) is:</p><formula xml:id="formula_12">? i,j (x) = ? l?Le ? (l) i,j (x).<label>(7)</label></formula><p>Finally, the CosMe anomaly score ? i,j (x) is calculated by:</p><formula xml:id="formula_13">? i,j (x) = ? i,j (x) ? ? i,j (x).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we describe the datasets used for our experiments, implementation details, evaluation metrics and experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We conduct our experiments on four widely-used anomaly segmentation datasets: Fishyscapes Lost &amp; Found <ref type="bibr" target="#b2">[3]</ref>, Fishyscapes Static <ref type="bibr" target="#b2">[3]</ref>, Road Anomaly <ref type="bibr" target="#b27">[27]</ref> and Streethazards <ref type="bibr" target="#b17">[18]</ref>. Fishyscapes Lost &amp; Found. Fishyscapes Lost (FS) &amp; Found <ref type="bibr" target="#b2">[3]</ref> is a high-quality image dataset containing real obstacles on roads. Based on the original Lost &amp; Found <ref type="bibr" target="#b35">[35]</ref> dataset, the FS Lost &amp; Found dataset also follows the same setup as Cityscapes <ref type="bibr" target="#b30">[30]</ref>, which is a widely used dataset in urban-scene segmentation. It contains real urban images with 37 types of unexpected road obstacles and 13 different street scenarios (e.g., different road surface appearances, strong illumination changes, etc). FS Lost &amp; Found includes a public validation set of 100 images and a hidden test set of 275 images for the benchmarking. Fishyscapes Static. Fishyscapes (FS) Static <ref type="bibr" target="#b2">[3]</ref> is built based on validation set of Cityscapes <ref type="bibr" target="#b30">[30]</ref>. Anomalous objects collected from PASCAL VOC <ref type="bibr" target="#b10">[11]</ref> are superimposed on Cityscapes seamlessly to ensure matching with the style of Cityscapes. This dataset contains a publicly available validation set with 30 images and a test set hidden for benchmarking with 1,000 images. Road Anomaly. Road Anomaly <ref type="bibr" target="#b27">[27]</ref> captures dangerous scenes where vehicles encounter on roads. It consists of 60 images collected from the Internet, including strange objects on roads (e.g., animals, rocks, etc.), with a resolution of 1280 ? 720. Since this dataset is not collected under conditions similar to Cityscapes, there is a big domain gap between them. This dataset can be used to verify the generalization ability of an anomaly segmentation approach. Streethazards. Streethazards <ref type="bibr" target="#b17">[18]</ref> is an anomaly segmentation dataset created by using the Unreal Engine along with the CARLA simulation environment. This dataset contains 5125 image and semantic segmentation ground-truth pairs for training, 1,031 pairs without anomalies for validation, and 1,500 test pairs with anomalies. There are 250 unique anomaly models of diverse types in total and 12 classes of objects used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Detail</head><p>Pre-trained model. For fair comparison, we follow <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">39]</ref> to adopt PSPnet <ref type="bibr" target="#b40">[40]</ref> with ResNet101 <ref type="bibr" target="#b16">[17]</ref> as the segmentation model on Streethazards and follow <ref type="bibr" target="#b8">[9]</ref> [22] to adopt DeepLabV3+ <ref type="bibr" target="#b6">[7]</ref> with ResNet101 as the segmentation model on the other three datasets. Multi-layer Memory. We maintain three memory subbranches, in which prototypes are extracted from the outputs of C4 layer and C5 layer of the ResNet101 backbone as well as the last hidden layer (LH) of the segmentation model. The similarity threshold ? = 0.85. Auxiliary Consensus. We use ResNet50 as the backbone of the auxiliary model. The supervision layer set is L s = {C2, C3, C4, C5, LH, O}, where O is the output layer of the segmentation model, i.e., the last 1 ? 1 Conv layer to compute the outputted logits over categories. The evaluation layer set for computing AucCon anomaly score is L e = {C5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">39]</ref>, three metrics are used for evaluation: area under receiver operating curve (AUROC), false positive rate at 95% true positive rate (FPR95) and average precision (AP). Since anomalous samples are much less than in-distribution samples, the data imbalance suggests FPR95 and AP are major evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with Previous Approaches</head><p>Fishyscapes test sets. We first compare CosMe with other anomaly segmentation approaches on Fishyscapes (FS Lost  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>We have shown ablation results w.r.t. the two submodules of CosMe in Tab. 2. Now, we conduct our ablation study w.r.t. the design in each sub-module on the FS Lost &amp; Found validation set. Ablation on layer set L for sub-branches. L is introduced in Sec. 3.3, which contains the layers used for prototype learning in the memory bank. The ablation result is shown in the upper part of Tab. 4. Note that this ablation is done solely on MulMem, without the help of AuxCon. It shows that when L = {C4, C5, LH}, MulMem achieves the best performance. This evidences there are rich memories embedded in the segmentation network, which are not fully exploited by the previous approaches. Ablation on evaluation layer set L e . By fixing L = {C4, C5, LH}, we then conduct ablation on evaluation layer set L e . As shown in the lower part of Tab. 4, when L e = {C5}, CosMe reaches its best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Compared with previous classic OOD detection approaches, maxlogit <ref type="bibr" target="#b17">[18]</ref> and MSP <ref type="bibr" target="#b18">[19]</ref>, we measure anoma- lies in a white box manner, with a requirement to access the internal structure of the pre-trained segmentation model. But, here we give an intuitive explanation for the necessity of the access: According to Information Bottleneck (IB) theory <ref type="bibr" target="#b38">[38]</ref>, the calculation process of neural networks can be seen as a kind of filtering process. Redundant information is filtered by the multi-layer architectures of deep networks. Since the optimization goal of training a segmentation model is to maximize the model's prediction toward ground-truth in-distribution categories, information for OOD detection is filtered to a certain extent. In summary, information only from the final prediction is not sufficient for anomaly segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we pointed out the core challenge of anomaly segmentation is the existence of hard indistribution samples. Based on the psychology finding of consensus processes in group recognition memory performance, we proposed "Consensus Synergizes with Mem-ory" (CosMe), which utilizes inconsistency with an auxiliary model to complement the memory-based prototypelevel distance for anomaly segmentation. Our approach was verified on various datasets and achieved superior results on all of them. Note that, our approach has no constraint on the segmentation network and can be parallelized. This merit shows its potential in practical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Statistical analysis of MulMem anomaly scores and AuxCon anomaly scores on Fishyscapes Lost &amp; Found. "ID"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The overall framework of CosMe. It consists of two main modules: Multi-layer Memory (MulMem) and Auxiliary Consensus (AuxCon). Given a fixed pre-trained segmentation model, MulMem stores prototypes extracted from multiple layers (C2 ? C5 are Conv layers of the backbone, LH and O are the last hidden layer and the last 1 ? 1 Conv layer to compute the outputted logits over categories of the segmentation model, respectively) of the pre-trained segmentation model and AuxCon is an auxiliary model which is trained to maintain consistency with the segmentation model on in-distribution data. For the former, the distance to prototypes in MulMem is used as the MulMem anomaly score; For the latter, the inconsistency between the pre-trained model and the auxiliary model is used as the AuxCon anomaly score. These two kinds of anomaly scores are combined to give the final anomaly prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 3 :</head><label>13</label><figDesc>Memory Initialization for Sub-branch S (l) Input: Training batch B, threshold ? , sub-branch size K Output: Initialized K prototypes S (l) ? {p k } K k=1 1: S (l) = {?} 2: while |S (l) | &lt; K do Randomly select an image x from B 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>MulMem Output Supervision Network Data Flow AuxCon Output Pre-trained Model Auxiliary Consensus Multi-layer Memory Inconsistency Score</head><label></label><figDesc></figDesc><table><row><cell>Branch</cell><cell></cell><cell>Branch</cell><cell>Branch</cell><cell></cell><cell></cell></row><row><cell>C4</cell><cell></cell><cell>C5</cell><cell>LH</cell><cell></cell><cell></cell></row><row><cell>C2</cell><cell>C3</cell><cell>C4</cell><cell>C5</cell><cell>LH</cell><cell>O</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Auxiliary Model Learning Input: Training set T , layer set L s , learning rate ? Output: The auxiliary model parameterized ? 1: Initialize ? randomly 2: for each image batch B ? T do</figDesc><table><row><cell>Algorithm 3 3:</cell></row><row><cell>5)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison with previous approaches reported in Fishyscapes Leaderboard . The top part of the table shows the approaches with the same setting as our CosMe, i.e., no re-training and no extra OOD data. The bottom part shows the approaches which require retraining or extra OOD data. Our method outperforms the approaches with the same setting and even most of the approaches with re-training or extra OOD data, by large margins. Found comes from AuxCon, while the improvement on FS Static is mainly thanks to MulMem. This phenomenon further confirms the improvement in CosMe are mainly comes from tackling hard in-distribution samples, since there are more hard in-distribution samples in FS lost &amp; Found than FS Static: As we introduced in Sec. 4.1, the images in FS Lost &amp; Found come from real driving scenes, while the anomalous samples in FS Static are cut and pasted from other datasets, such as PASCAL VOC. The domain gap between normal background (Cityscapes) and anomalous foreground (PASCAL VOC) in FS Static can benefit memory-based anomaly segmentation, which relatively reduces the difficulty of segmentation.Road Anomaly. As shown in Tab. 2, our results on Road Anomaly are significantly better than others and on par with SML<ref type="bibr" target="#b21">[22]</ref>. Since there is a big domain gap between Road Anomaly and Cityscapes, there are massive hard indistribution samples in Road Anomaly. In this case, Aux-Con helps to improve performance much more than Mul-Mem, which further evidences that AuxCon is adept at detecting hard in-distribution samples.Streethazards. Tab. 3 shows the results on the test set of Streethazards. CosMe outperforms the previous approaches without re-training or extra OOD data by large margins and is on par with DML and LDN BIN which require re-training. Note that when we replace PSPnet with DeepLabV3+ as our pre-trained segmentation model, the performance is further improved and outperforms LDN BIN. This result implies the memories embedded in more powerful models might be stronger. AUROC ? AP ? FPR95 ? AUROC ? AP ? FPR95 ? AUROC ? AP ? Comparison on Fishyscapes validation sets and Road Anomaly. ? and indicate re-training and extra OOD data, respectively.</figDesc><table><row><cell>https://fishyscapes.com/results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison results on Streethazards. ? and indicate re-training and extra OOD data, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation results for the selection of L (upper part) and Le (lower part).</figDesc><table><row><cell></cell><cell>Layers in Set</cell><cell cols="3">FPR95 ? AUROC ? AP ?</cell></row><row><cell></cell><cell>{C4}</cell><cell>34.55</cell><cell>93.17</cell><cell>9.81</cell></row><row><cell></cell><cell>{C5}</cell><cell>36.49</cell><cell>92.97</cell><cell>22.11</cell></row><row><cell>L</cell><cell>{LH}</cell><cell>20.84</cell><cell>95.74</cell><cell>12.32</cell></row><row><cell></cell><cell>{C4, C5}</cell><cell>23.94</cell><cell>95.34</cell><cell>27.38</cell></row><row><cell></cell><cell>{C4, C5, LH}</cell><cell>14.47</cell><cell>97.39</cell><cell>41.73</cell></row><row><cell></cell><cell>{C4}</cell><cell>10.58</cell><cell>98.13</cell><cell>44.99</cell></row><row><cell></cell><cell>{C5}</cell><cell>11.65</cell><cell>98.11</cell><cell>50.22</cell></row><row><cell></cell><cell>{LH}</cell><cell>15.45</cell><cell>97.04</cell><cell>35.91</cell></row><row><cell>L e</cell><cell>{O}</cell><cell>16.90</cell><cell>96.50</cell><cell>27.95</cell></row><row><cell></cell><cell>{C4, C5}</cell><cell>10.39</cell><cell>98.24</cell><cell>49.96</cell></row><row><cell></cell><cell>{C4, C5, LH}</cell><cell>13.09</cell><cell>97.67</cell><cell>42.74</cell></row><row><cell></cell><cell>{C4, C5, LH, O}</cell><cell>15.07</cell><cell>97.00</cell><cell>32.28</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clustering and unsupervised anomaly detection with l2 normalized deep auto-encoder representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Aytekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Cricri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Aksu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dense outlier detection and open-set recognition based on training with noisy negative images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sini?a?egvi?</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09193</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The fishyscapes benchmark: Measuring blind spots in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lof: identifying density-based local outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2000 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep metric learning for open world semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="15333" to="15342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Entropy maximization and meta classification for out-ofdistribution detection in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Gottschalk</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE international conference on computer vision</title>
		<meeting>IEEE international conference on computer vision</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cars can&apos;t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanne</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9373" to="9383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pixel-wise anomaly detection in complex driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><forename type="middle">Di</forename><surname>Biase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time object classification and novelty detection for collaborative video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hampshire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN&apos;02 (Cat. No.02CH37290)</title>
		<meeting>the 2002 International Joint Conference on Neural Networks. IJCNN&apos;02 (Cat. No.02CH37290)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2620" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial networks. Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">This is not what i imagined: Error detection for semantic segmentation through visual dissimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haldimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00676</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition(CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scaling out-of-distribution detection for real-world settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cognitive and consensus processes in group recognition memory performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Verlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinsz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social psychology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">705</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhancing camera surveillance using computer vision: a research note</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haroon</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Surette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Policing: An International Journal</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Standardized max logits: A simple yet effective approach for identifying unexpected road obstacles in urban-scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghun</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungsoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehoon</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="15425" to="15434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting multivariate outliers: Use a robust variant of the mahalanobis distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Leys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Dominicy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Ley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="150" to="156" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Foveanet: Perspective-aware urban scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="784" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting the unexpected via image resynthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Nakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE international conference on computer vision (ICCV)</title>
		<meeting>IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordts</forename><surname>Marius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omran</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramos</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rehfeld</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enzweiler</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benenson</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franke</forename><surname>Uwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schiele</forename><surname>Bernt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Clustering-based anomaly detection approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kishan G Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chilukuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaming</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anomaly Detection Principles and Algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="41" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Evaluating bayesian deep learning methods for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jishnu</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12709</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient semantic segmentation with pyramidal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sini?a?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2337" to="2346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lost and found: detecting small road hazards for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04597[cs.CV]).1</idno>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Toward open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archana</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bialek</surname></persName>
		</author>
		<title level="m">The information bottleneck method. arXiv preprint physics/0004057</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Synthesize then compare: Detecting failures and anomalies for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yingda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Fengze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename><surname>Alan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition(CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition(CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
