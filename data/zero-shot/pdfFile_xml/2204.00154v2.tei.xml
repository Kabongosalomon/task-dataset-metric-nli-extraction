<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An End-to-end Supervised Domain Adaptation Framework for Cross- Domain Change Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Graphic Communication, Printing and Packaging</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Xuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center for Multimedia Software</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Hubei Key Laboratory of Multimedia and Network Communication Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Gan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center for Multimedia Software</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Hubei Key Laboratory of Multimedia and Network Communication Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Land Satellite Remote Sensing Application Center</orgName>
								<orgName type="institution" key="instit2">MNR</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Liu</surname></persName>
							<email>liujuhua@whu.edu.cn.</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Graphic Communication, Printing and Packaging</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Juhua Liu, Research Center for Graphic Communication, Printing and Packaging</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Engineering Research Center for Multimedia Software</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Hubei Key Laboratory of Multimedia and Network Communication Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An End-to-end Supervised Domain Adaptation Framework for Cross- Domain Change Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 Address all correspondence to:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Change Detection</term>
					<term>Supervised Domain Adaptation</term>
					<term>Image Adaptation</term>
					<term>Feature Adaptation ? Jia Liu and Wenjie Xuan contributed equally to this work</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Change detection is a crucial but extremely challenging task in remote sensing image analysis, and much progress has been made with the rapid development of deep learning. However, most existing deep learning-based change detection methods try to elaborately design complicated neural networks with powerful feature representations. However, they ignore the universal domain shift induced by time-varying land cover changes, including luminance fluctuations and seasonal changes between preevent and post-event images, thereby producing suboptimal results. In this paper, we propose an end-to-end supervised domain adaptation framework for cross-domain change detection named SDACD, to effectively alleviate the domain shift between bitemporal images for better change predictions. Specifically, our SDACD presents collaborative adaptations from both image and feature perspectives with supervised learning. Image adaptation exploits generative adversarial learning with cycle-consistency constraints to perform cross-domain style transformation, which effectively narrows the domain gap in a two-side generation fashion. As for feature adaptation, we extract domain-invariant features to align different feature distributions in the feature space, which could further reduce the domain gap of cross-domain images. To further improve the performance, we combine three types of bi-temporal images for the final change prediction, including the initial input bi-temporal images and two generated bi-temporal images from the pre-event and post-event domains. Extensive experiments and analyses conducted on two benchmarks demonstrate the effectiveness and generalizability of our proposed framework. Notably, our framework pushes several representative baseline models up to new State-Of-The-Art records, achieving 97.34% and 92.36% on the CDD and WHU building datasets, respectively.</p><p>The source code and models are publicly available at https://github.com/Perfect-You/SDACD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Change detection (CD) aims to identify significant differences in geographical elements between bitemporal images of the same geographic area. It takes registered bi-temporal images as the input and outputs pixel-wise change maps. This fundamental but important remote sensing task has gradually become an active topic in the computer vision community due to its wide applications in urbanization monitoring <ref type="bibr" target="#b0">[1]</ref>, resource and environment monitoring <ref type="bibr" target="#b1">[2]</ref>, disaster assessment <ref type="bibr" target="#b2">[3]</ref>, etc. Many excellent methods have been proposed recently due to the easy acquisition of high-resolution remote sensing images and the success of deep learning.</p><p>However, many issues in this task remain open and challenging due to the complex and heterogeneous appearance of geographical elements at different times.</p><p>During the past decades, numerous kinds of conventional change detection methods have been proposed, which can be classified into four categories: 1) Algebra-based methods perform channel-wise algebraic operations on the registered bi-temporal images directly, including image differencing, image regression, and change vector analysis <ref type="bibr" target="#b3">[4]</ref>. However, it is scene-dependent and time-consuming to find a suitable threshold to distinguish changed pixels from unchanged ones. 2) Transformation-based methods cast bi-temporal images into specific feature spaces capable of narrowing the pixel differences of unchanged regions and highlighting the changed information. However, methods such as <ref type="bibr" target="#b4">[5]</ref> <ref type="bibr" target="#b5">[6]</ref> struggle to handle high-resolution images because they depend on empirically designed features. 3) Classification-based methods such as <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b7">[8]</ref> identify change regions by comparing the pre-generated land cover labels of geographic elements between bi-temporal images. However, the classification errors of the pre-generated labels would accumulate on the final change maps and inevitably reduce the accuracy of change predictions. 4) Machine learningbased methods employ traditional machine learning algorithms, such as random forest regression <ref type="bibr" target="#b8">[9]</ref> and support vector machine <ref type="bibr" target="#b9">[10]</ref>, to determine whether the specified area has changed. Although most of these conventional methods are simple and explainable, they show poor robustness in real scenarios because their performance is sensitive to noise and limited by handcrafted features. Recently, with the rapid development of deep learning, great progress has been made in the computer vision community <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b14">[15]</ref>, and many deep learning-based change detection methods have also been proposed.</p><p>Due to the powerful ability of automatic high-level feature extraction, they have demonstrated superior performance and robustness to conventional methods. Deep learning-based methods usually first extract discriminative features from bi-temporal images using convolutional neural networks similar to Siamese networks, and then utilize fully convolutional networks, such as FC-Siam-diff <ref type="bibr" target="#b15">[16]</ref> and SNUNet <ref type="bibr" target="#b16">[17]</ref>, or metric-based methods, such as STANet <ref type="bibr" target="#b17">[18]</ref> and DASNet <ref type="bibr" target="#b18">[19]</ref>, to predict the final change results. For accurately predicting change results from the feature space, the features extracted from bi-temporal images should satisfy the following constraints: feature vectors associated with changed pixel pairs are farther apart from each other, while invariant pixel pairs are close. However, this assumption is difficult to satisfy in reality.</p><p>Since bi-temporal images are usually acquired at different times, their imaging sensors, atmospheric and luminance conditions would be different. This inevitably leads to completely different appearances in the bitemporal images regardless of whether the geographical elements have changed. As shown in <ref type="figure">Fig. 1</ref>, the time-varying changes introduced by seasons and climates are more obvious than the real ones caused by <ref type="bibr" target="#b3">4</ref> human factors, such as the construction or destruction of buildings and roads. These pseudo changes make it difficult for existing deep learning-based methods, which usually focus on designing complicated neural networks with powerful feature representation while ignoring the domain shift between bi-temporal images, to extract features satisfying the above constraints from the bi-temporal images.</p><p>In this paper, we propose a supervised domain adaptation framework for cross-domain change detection, named SDACD, to effectively alleviate the severe domain shift between the pre-event and post-event images.</p><p>Our proposed SDACD presents collaborative adaptations from both image and feature perspectives and comprises two key modules: the image adaptation (IA) module and the feature adaptation (FA) module. The image adaptation module addresses domain shift by aligning the image appearance between bi-temporal domains via image-to-image transformation. Specifically, we transform the appearances of pre-event images and post-event images to each other by using bi-directional generative adversarial networks with cycleconsistency constraints, i.e., we transform pre-event images to the appearance of post-event images and postevent images to pre-event images. Afterward, we employ two types of bi-temporal images from the pre-event domain and post-event domain as well as the original bi-temporal images to train the change detection model in the feature adaptation module, where we integrate feature adaptation into the framework to further narrow the remaining domain shift. Specifically, we first input three bi-temporal images, i.e., the pre-event and postevent images, pre-event and post-event stylized as pre-event images, and post-event and pre-event stylized as post-event images, into the feature adaptation module and predict the change map for each pair of bitemporal images. Then, we design a feature domain-invariance discriminator, which connects the change predictions and the bi-temporal images, to differentiate the predictions generated from the bi-temporal images.</p><p>If this discriminator fails to distinguish, it means the extracted features for predicting change maps are domain-invariant. Finally, to fully utilize the image information from different domains, we make the final change prediction by fusing the above three features from different bi-temporal images. Our proposed framework combines the image and feature adaptation procedures in an end-to-end trainable manner, which makes them benefit from each other and achieve better performance. To the best of our knowledge, this is <ref type="bibr" target="#b4">5</ref> the first work that fully studies domain adaptation with supervised learning for change detection and provides a simple and effective solution that can be easily plugged in any non-domain adaptation model to further improve their performance. This is especially friendly for real-world applications. The architecture of our proposed framework is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>We implement the SDACD framework based on three representative baseline models, STANet <ref type="bibr" target="#b15">[16]</ref>, DASNet <ref type="bibr" target="#b18">[19]</ref>, and SNUNet <ref type="bibr" target="#b16">[17]</ref>, and validate its effectiveness and generalizability on the CDD <ref type="bibr" target="#b20">[21]</ref> and WHU building datasets <ref type="bibr" target="#b21">[22]</ref>. Our framework shows consistent improvements on all three baseline models.</p><p>For the CDD dataset, the F1-scores of the STANet, DASNet, and SNUNet-based frameworks improve by 0.70%, 0.78%, and 1.66%, respectively, while for the WHU building dataset, the scores improve by 8.81%, 2.59%, and 6.85%, respectively. Moreover, when combined with the cutting-edge SNUNet, our framework achieves state-of-the-art performance on both datasets. The visualized results also demonstrate the superiority of our framework in tackling bi-temporal change detection under severe domain shift, as illustrated in Section 4.</p><p>Our contributions can be summarized as follows:</p><p>? We propose a novel supervised domain adaptation framework SDACD for cross-domain change detection. It unifies image adaptation and feature adaptation in an end-to-end trainable manner to alleviate the domain shift between the pre-event and post-event images; ? Our framework is compatible with existing change detection networks that do not take into account domain shift. Furthermore, it can handle cross-domain change detection and consistently improve the performance as an easy-to-plug-in module;</p><p>? Experimental results on two benchmark datasets demonstrate the effectiveness and generalizability of our SDACD. Most importantly, our SNUNet-based framework sets new state-of-the-art performance with an F1-score of 97.34% on the CDD dataset and 92.36% on the WHU building dataset.</p><p>The rest of the paper is organized as follows. In Section 2, we briefly review the related works of change detection and domain adaptation. In Section 3, we introduce our proposed framework before analyzing each <ref type="bibr" target="#b5">6</ref> module. Section 4 reports and discusses the experimental results. Finally, we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Change detection</head><p>In recent years, due to the powerful feature extraction capabilities of deep neural networks, many deep learning-based change detection methods have been proposed. They can be divided into two main categories, namely, patch-based methods and image-based methods.</p><p>Patch-based methods [23]-[25] regard patches as the smallest processing unit for change detection. These methods first split bi-temporal images into patches and then decide whether each patch has changed or not.</p><p>Zhang et al. <ref type="bibr" target="#b22">[23]</ref> employed a deep belief network to transform multi-spectral image patches into specified feature spaces and relieve noise for change detection. To facilitate the utilization of spectral-spatial-temporal representations, Mou et al. <ref type="bibr" target="#b23">[24]</ref> designed a ReCNN, which combined CNNs with RNNs to explore spectralspatial representations and temporal dependencies between the patches of bi-temporal images. Although patch-based methods have made remarkable progress compared with traditional methods, they suffer from high memory consumption and low computational efficiency. Furthermore, patch size restricts the receptive field. Thus, these methods cannot exploit global information, which also greatly limits their performance.</p><p>Image-based methods <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b19">[20]</ref> directly employ the whole image to make pixel-wise predictions. This is computationally efficient and leads to good performance. FC-EF, FC-Siam-conc, and FC-Siam-diff <ref type="bibr" target="#b15">[16]</ref> are some earlier image-based methods. These methods employed UNet as the backbone network of feature extractors and explored three different feature fusion strategies, which greatly improved the performance and processing speed compared to patch-based methods. Later, Chen et al. <ref type="bibr" target="#b17">[18]</ref> proposed an influential STANet, which designed a spatial-temporal self-attention module in the Siamese network to explore spatial-temporal dependencies between bi-temporal images. Recently, Fang et al. <ref type="bibr" target="#b16">[17]</ref> proposed a densely connected SNUNet and achieved SOTA on the CDD dataset. SNUNet adopted a Siamese UNet++ <ref type="bibr" target="#b25">[26]</ref> architecture as the feature extractor and utilized an ensemble channel attention module to aggregate multilevel features to enhance discriminative feature representations. <ref type="bibr" target="#b6">7</ref> Although the aforementioned methods have achieved promising results by using powerful feature extractors and elaborately designed structures, they do not consider the universal domain shift between bitemporal images caused by different seasons, various imaging conditions, etc. This produces suboptimal results. DLSF <ref type="bibr" target="#b26">[27]</ref> and PDA <ref type="bibr" target="#b27">[28]</ref> are recent representative methods for cross-domain change detection that employ GAN with complex constraints to preserve semantic information and bridge the style gaps between bi-temporal images. Our method differs from DLSF and PDA in two ways. First, these two methods only employ image adaptation with complex constraints. In contrast, we alleviate the domain shift from both the image and feature perspectives and validate the effectiveness of each component. Second, they are specifically designed for metric-based predictors, while our SDACD is compatible with most existing deep learning-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Domain adaptation</head><p>Domain adaptation intends to address performance degradation caused by domain shift by transferring knowledge between domains, which benefits the generalization of models. Given the source domain and target domain, most of the existing methods consider unsupervised or semi-supervised situations, where no or only limited labels in the target domain are available. This topic has aroused great interest in recent years and many influential approaches have been proposed <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref>. To the best of our knowledge, domain adaptation mainly focuses on two aspects, namely, image adaptation and feature adaptation.</p><p>Image adaptation attempts to minimize appearance differences for images of different modals or styles.</p><p>The intuitive idea is to transform images from one style to another through generation models. The famous CycleGAN <ref type="bibr" target="#b28">[29]</ref> first employed bi-directional generation with cycle-consistency constraints in GAN and realized desirable unpaired image-to-image transformation. This provided a reference paradigm for image adaptation. Afterward, many works applied the ideas of CycleGAN to deal with domain shift via image-toimage transformation. For example, CyCADA <ref type="bibr" target="#b29">[30]</ref> first employed a cycle-consistent GAN to transform source domain images to the target domain and then trained the model with the synthetic data in the target domain. In contrast, Song et al. <ref type="bibr" target="#b30">[31]</ref> transformed target domain images into the source domain, then generated <ref type="bibr" target="#b7">8</ref> pseudo labels using the source domain model, and finally utilized them to fine-tune the source domain model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature adaptation aligns the feature distributions between the source and target domains by exploiting</head><p>domain-invariant features through adversarial training. DANN <ref type="bibr" target="#b31">[32]</ref> was the earliest general framework that employed adversarial training to learn domain-invariant features for domain adaptation. To further improve the discrimination of domain-invariance features, Tzeng et al. <ref type="bibr" target="#b32">[33]</ref> combined discriminative models with an untied weight sharing strategy. Recently, Li et al. <ref type="bibr" target="#b33">[34]</ref> proposed a novel loss function called maximum density divergence (MDD), which can be combined with adversarial learning to minimize the domain distribution gaps and align the source and target domain on the feature space. Afterward, they also proposed a faster domain adaptation <ref type="bibr" target="#b34">[35]</ref> protocol, especially for scenarios with limited training data and energy-sensitive platforms, which executed different number of network layers for different samples according to the adaptation difficulty.</p><p>As mentioned above, most existing domain adaptation methods are designed for unsupervised or semisupervised scenes. We, on the other hand, treat cross-domain change detection as a supervised domain adaptation problem, where there are supervised annotations for bi-temporal images with severe domain shift.</p><p>Although current methods provide valuable solutions to bridge domain gaps, they suffer from underutilization or loss of image information in cross-domain change detection. Our work is closely related to SIFA <ref type="bibr" target="#b36">[37]</ref>, which also combines image adaptation and feature adaptation for domain adaptation. SIFA solves the performance degradation by synergistic learning between CT-to-MR transformation and CT segmentation, which mainly focuses on the segmentation accuracy in the CT domain. However, given the pre-event and post-event images share the annotation in change detection, we highlight our supervised domain adaptation framework explores information from all available domains to improve the robustness and precision of the model. This is because change detection model distinguishes changes through feature differences, but these features would suffer more or less information loss during image transformation. In addition, unlike SIFA, we separate image adaptation and feature adaptation as two separate modules, which enables our framework to easily plugged in any non-domain adaptation change detection model. The main <ref type="bibr" target="#b8">9</ref> contribution of our work is providing a general and flexible change detection framework, helping existing methods handle cross-domain change detection with few adjustments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first introduce the architecture of our proposed framework and then illustrate details of two main modules: the image adaptation module and the feature adaptation module. Finally, we define the full loss function of our framework.  Therefore, we obtain two bi-temporal images in the same domain, where ( ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><formula xml:id="formula_0">? ) ? , ( ? , ) ? . Afterward, we input the two generated bi-temporal images ( , ? ), ( ? ,</formula><p>) with the original ( , ) into the feature adaptation module, where we further reduce the domain gap by aligning feature distributions in the feature space via a feature domain-invariant discriminator. Finally, to fully utilize the image information from different domains, we predict the final change map by fusing the aligned features extracted from the above three bi-temporal images. More details of the IA and FA modules will be presented in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image Adaptation Module for Appearance Alignment</head><p>Since bi-temporal images are usually captured under different imaging conditions, we specifically design an image adaptation (IA) module to minimize the appearance variations between bi-temporal images, while the original contents with geographical semantics remain unaffected. The goal of the IA module is to learn mapping functions between two domains and given training samples ( , ).</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>  </p><formula xml:id="formula_1">? ? , pre , , ? =~? ( )? + ~[ log (1 ? ( ( )))]<label>(1)</label></formula><p>where aims to generate images ( ) that have a similar appearance to images from the preevent domain , while pre tries to distinguish real images from transformed images ( ).</p><p>aims to minimize this objective against pre that tries to maximize it, i.e.,</p><formula xml:id="formula_2">? ( , pre , , )</formula><p>. Similarly, the adversarial loss for the generator G and its discriminator can be defined as:</p><formula xml:id="formula_3">? ( , post , , ) =~[ ( )] + ~[ log (1 ? ( ( )))]<label>(2)</label></formula><p>Similar to CycleGAN <ref type="bibr" target="#b28">[29]</ref>, we employ a reverse generator to impose pixel-wise cycle-consistency to preserve the original geographical elements in the transformed images. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>  </p><formula xml:id="formula_4">? ? , ? =~[|| ( ? ?) ? || 1 ] + ~[ || ( ( )) ? || 1 ]<label>(3)</label></formula><p>Since our image transformation procedures are cyclical, we also introduce two reverse adversarial losses . The reverse adversarial losses ? ? and ? ? can be computed as follows:</p><formula xml:id="formula_5">? ? ? , pre , , ? =~? ? ?? + ?~[ log (1 ? ( ( ? )))]<label>(4)</label></formula><formula xml:id="formula_6">? ? ? , , , ? =~? ? ?? + ?~[ log (1 ? ( ( ? )))]<label>(5)</label></formula><p>Therefore, the full adversarial loss of image adaptation ? can be formulated as:</p><formula xml:id="formula_7">? ? , pre , , , , ? = ? ? , pre , , ? + ? ( , post , , ) + ? ? ? , pre , , ? + ? ? ( , , , )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Adaptation Module for Feature Alignment</head><p>After the original bi-temporal images ( Thus motivated, we design a feature adaptation (FA) module to further reduce the remaining domain gap <ref type="bibr" target="#b12">13</ref> and fully utilize the image information from different domains. Specifically, we impose an additional feature domain-invariance discriminator to align the feature distributions in the feature space and contribute from the perspective of feature adaptation. Generally, feature adaptation aims to extract domain-invariant features without considering the appearance difference between input domains. The most intuitive way is to use a discriminator to distinguish which features come from which domain via adversarial learning. However, since the feature space is high-dimensional, it is difficult to directly align using adversarial learning. Inspired by output space adaptation <ref type="bibr" target="#b37">[38]</ref>, we enhance the domain invariance of feature distribution via adversarial learning in different lower-dimensional spaces. Specifically, we add adversarial losses between the change detection prediction space and the input bi-temporal image spaces.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 2 (FA)</ref>, we adopt three bi-temporal images from different domains, including ( , )</p><p>?PrD, ( , ). If the features extracted from different domains are aligned, discriminator would fail to differentiate between their corresponding predictions. Otherwise, the adversarial gradients are backpropagated into the feature extractor E of bi-temporal images to minimize the distance among the feature distributions from different domains. Since there are three input bi-temporal images, there are three adversarial losses for feature adaptation, which can be computed as: <ref type="bibr" target="#b13">14</ref> where C represents the classifier for change prediction.</p><formula xml:id="formula_8">? ? , , , , ? = [log ( ( ( ), ( )))] + [log (1 ? ( ( ( ), ( )))]<label>(7)</label></formula><formula xml:id="formula_9">? ? ? , , , , ? = [ ( ( ( ? ), ( )))] + [log (1 ? ( ( ( ? ), ( )))]<label>(8)</label></formula><formula xml:id="formula_10">? ? , ? , , , ? = [ ( ( ( ), ( ? ))] + [log (1 ? ( ( ( ), ( ? )))]<label>(9)</label></formula><p>Combined with the above three adversarial losses, the full adversarial loss of feature adaptation ? can be formulated as:</p><formula xml:id="formula_11">? ? , , ? = ? ( , , , , ) + ? ( ? , , , , ) + ? ( , ? , , , )<label>(10)</label></formula><p>As mentioned above, the generated images from image adaptation suffer more or less information loss, which results in a suboptimal model trained with bi-temporal image samples from only one domain.</p><p>Therefore, for fully utilizing the image information from different domains and keeping training samples diverse, we integrate the aligned features, which are extracted from ( , ), ( , During the feature adaptation procedure, we make change predictions for each input bi-temporal image and then fuse the aligned features to predict the final change map. Therefore, the full change detection loss ? can be computed as follows:</p><formula xml:id="formula_12">?(O, Y) = ? ( , ) + ? ( , )<label>(11)</label></formula><formula xml:id="formula_13">? = ? ?( ? , Y) =0,1,2 + ?( , Y)<label>(12)</label></formula><p>where i is the index of the input bi-temporal images, ? is the predicted result of the i-th bi-temporal images for the feature adaptation, O is the final prediction of our framework, and Y is the ground truth. (?) is a hybrid loss function for SNUNet <ref type="bibr" target="#b16">[17]</ref>, which consists of the weighted cross-entropy loss ? (?) and the dice loss ? (?). Note that any loss function can be used in our framework, and the loss function (11) is only employed in SNUNet-SDACD. For a fair comparison, we set the same loss function as their corresponding baselines in our experiments. <ref type="bibr" target="#b14">15</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Full Objective</head><p>The key characteristic of SDACD is that both image and feature adaptations are unified into an end-to-end trainable framework. In each training iteration, the key modules are sequentially updated in the following order: </p><formula xml:id="formula_14">? G ? ? ? ? ? ? . Specifically,</formula><formula xml:id="formula_15">? = ? + ? + ? + ?<label>(13)</label></formula><p>Where we conducted the grid search for the hyperparameters and empirically set =10, = 1, = 0.1, and = 1 in all experiments.</p><p>Furthermore, another characteristic of our SDACD is its universality, as we can integrate most non-domain adaptation networks into our framework with few modifications. Given a non-domain adaptation change detection network consisting of a feature extractor ? and a classifier ?, we can directly replace E and C in our pipeline, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, which enables the original network to tackle the domain shift between bitemporal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we implement the SDACD framework based on STANet (2020), DASNet (2021), and SNUNet (2021) and conduct experiments on the CDD and WHU building datasets, which contain time- <ref type="bibr" target="#b15">16</ref> varying changes caused by seasons and luminance. We compare our method with the baseline as well as state-of-the-art methods and carefully analyze the functions of each proposed module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>The CDD <ref type="bibr" target="#b20">[21]</ref>  The WHU Building Dataset <ref type="bibr" target="#b21">[22]</ref> focuses on the changes in buildings and contains one pair of very highresolution images sized 32507?15354 pixels with a spatial resolution of 0.075 m per pixel. <ref type="bibr" target="#b21">[22]</ref> divided the original image pair into a training set of 21243?15354 pixels and a testing set of 11265?15354 pixels.</p><p>Because of memory constraints, we further cropped the images into patches of 256?256 pixels without overlap thus producing 4980 bi-temporal images for training and 2700 bi-temporal images for testing. Note that images were taken in 2012 and 2016 separately. Thus, bi-temporal images showed varied appearances caused by luminance fluctuations, as shown in <ref type="figure">Fig. 1</ref>. The annotations only consist of changes caused by the construction and destruction of buildings.</p><p>We adopt three metrics to evaluate the performance of cross-domain change detection, including Precision (P), Recall (R), and F1-score. While Precision reflects the accuracy of detection, Recall represents the completeness of the predicted changed regions. Generally, the F1-score is of more concern to researchers because it takes both precision and recall into account and reflects the comprehensive performance of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>Our experiments were conducted on a high-performance computing server with NVIDIA Tesla V100 (16G)</p><p>GPUs. All models were trained with 4 GPUs and evaluated with 1 GPU. We implemented our SDACD framework based on three baseline models, denoted as STANet-SDACD, DASNet-SDACD, and SNUNet-SDACD. Following their original settings in <ref type="bibr" target="#b16">[17]</ref>[18] <ref type="bibr" target="#b18">[19]</ref>, we initialized STANet and DASNet with publicly available pretrained ImageNet parameters, while SNUNet was initialized by kaiming normalization. The other parameters were all initialized by kaiming normalization. For data augmentation, we employed random flips, random rotation, and normalization. In addition, since the proportion of changed areas is relatively small in the WHU building dataset, we further augmented the training set with 3442 bi-temporal images generated by randomly cropping several 256?256 patches around each instance of changed areas. We adopted the Adam optimizer, and the initial learning rates were set to 5e-4 in all experiments. We trained STANet and STANet-SDACD for 200 epochs, DASNet and DASNet-SDACD for 60 epochs, and SNUNet and SNUNet-SDACD for 120 epochs. Note that all networks were trained in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with State-of-the-Art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison Methods</head><p>To demonstrate the superiority of our framework, we compared the performance of our framework with several representative and state-of-the-art deep learning-based change detection methods. <ref type="bibr" target="#b15">[16]</ref> are the earliest FCN-based methods, which integrated UNet into the Siamese architecture for change detection. These three methods explored three different fusion strategies, including bi-temporal image fusion, multilevel feature concatenation, and multilevel feature difference, which are the main strategies for bi-temporal feature fusion in existing change detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FC-EF, FC-Siam-conc and FC-Siam-diff</head><p>STANet <ref type="bibr" target="#b17">[18]</ref> is an impressive work that utilizes a pyramid transformer structure to learn multilevel spatialtemporal dependencies. This structure effectively eliminates incorrect detections caused by luminance differences and misregistration.</p><p>DASNet <ref type="bibr" target="#b18">[19]</ref> employs a Siamese convolutional module that obtains local features of the images at different times and then builds connections between local features by a dual attention module, which exploits global contextual information to distinguish the changed areas from the unchanged ones.</p><p>SNUNet <ref type="bibr" target="#b16">[17]</ref> is the current state-of-the-art method on the CDD dataset. It employs a UNet++ architecture to extract multilevel representations and an elaborate ensemble channel attention module (ECAM) to integrate multilevel features, which boosts the performance to a higher level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Performance on CDD Dataset</head><p>The second column of <ref type="table">Table 1</ref>   <ref type="table">Table 1</ref> also imply that the performance of other deep learning-based models could be further enhanced by combining them with our SDACD framework. Therefore, our framework shows great potential in cross-domain change detection as an easy-to-plug-in module, which provides a simple but effective way to tackle the performance degradation caused by different seasons and climates for existing non-domain adaptation models. <ref type="bibr" target="#b18">19</ref> As shown in <ref type="figure">Fig. 3</ref>, since bi-temporal images have significantly different appearances (summer vs. winter), this poses great challenges for the baseline models, which do not consider domain shift, to discriminate real changes from false-positives, especially for changed regions with delicate structures. However, our SDACD can minimize the impact of time-varying land cover changes and accurately detect fine boundaries of roads and small-scale object changes. This explicitly demonstrates the superiority of our framework for crossdomain change detection. <ref type="table">Table 1</ref>. Performance comparison with the state-of-the-art methods on the CDD and WHU building datasets. The red and blue arrows indicate the increase and decrease in metrics compared with the baseline model, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CDD WHU building P (%) R (%) F (%) P (%) R (%) F (%)</head><p>FC   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Performance on the WHU Building Dataset</head><p>Different from CDD, which is a class-agnostic change detection dataset, the WHU building dataset focuses on building changes. Therefore, the WHU building dataset is considered to be more challenging than CDD.</p><p>We further conducted experiments on the WHU building dataset to see if our framework can work under this more challenging scenario. We report the experimental results in the third column of <ref type="table">Table 1</ref>. Similar to CDD, our framework outperforms the baseline model by a large margin, bringing an increase in the F1-score of 8.81% on STANet, 2.59% on DASNet, and 6.85% on SNUNet. Moreover, our SDACD-SNUNet also achieves new state-of-the-art performance on the WHU building dataset with an F1-score of 92.36%. These results further confirm that our framework can not only effectively bridge the appearance difference by image adaptation but also provide domain-invariant and discriminative representations, which are significant for the precision of change detection.</p><p>Visual comparison results are provided in <ref type="figure">Fig. 4</ref>. We notice that without domain adaptation, the baseline models are sensitive to pseudo changes caused by luminance fluctuations. In contrast, since our framework provides domain adaptation ability to the baseline model, it helps to avoid false-positives and false-negatives, thus generating more precise change predictions with clear boundaries and enhancing the performance of cross-domain change detection.</p><p>In addition, from the performance comparison results in <ref type="table">Table 1</ref>, we note that although our SDACD implementations consistently achieves higher F1-score, in some cases, the precision and recall of SDACD-STANet and SDACD-DASNet decline. For the change detection task, we usually focus more on the performance metric of F1-score, and this degradations of precision or recall can be regarded as a trade-off.</p><p>Because higher precision would lead to lower recall as the threshold increases. Specifically, the degradations of SDACD-DASNet on both CDD and WHU building datasets are rather small and negligible, and the decrease of recall of SDACD-STANet on CDD is mainly caused by false negatives of small-scale changes with higher threshold settings. <ref type="bibr" target="#b22">23</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Model Size and Computational Complexity</head><p>We further compare the number of parameters and computational efficiency among three baseline models and our SDACD implementations in <ref type="table" target="#tab_2">Table 2</ref>. Since we introduce an IA module and an FA module to alleviate the domain gap, the efficiency is sacrificed for better performance under cross-domain scenarios. Compared with the baseline models, i.e., STANet, DASNet, and SNUNet, the parameters of our SDACD implementations increase by 23.12M, 32.72M, and 25.85M, respectively. The parameter increments are relatively fixed. However, the increment of FLOPs is highly related to the computational complexity of the baseline model. This is because we repeatedly employ the feature extractor of the baseline model to extract feature maps from three pairs of bi-temporal images, and then integrate these features from different domains to learn more discriminative representations, as illustrated in <ref type="figure" target="#fig_1">Fig. 2 (FA)</ref>. Note that the main purpose of this paper is to propose an effective solution for the challenging cross-domain change detection, so we leave designing a more efficient framework as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this subsection, we report the results of ablation studies on the WHU building dataset to evaluate the effectiveness of each component proposed in SDACD based on SNUNet. In addition, we also reveal insights into the framework design, i.e., why we employ three bi-temporal images for feature extraction and which strategy is the best for fusing feature maps from different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Effectiveness of IA and FA Modules</head><p>We first conduct experiments to verify the effectiveness of the proposed IA and FA modules. The experimental results are reported in <ref type="table" target="#tab_3">Table 3</ref> and demonstrate that both the IA module and FA module improve the performance of the baseline model by a large margin. Specifically, with the help of the IA module, our framework significantly improves the precision by 9.73%, which contributes most to the improvement in the F1-score. As shown in <ref type="figure">Fig. 5</ref>, this is because our IA module can relieve the appearance difference between bi-temporal images and effectively avoid most false positives produced by the baseline model. Moreover, it can be observed that incorporating the FA module with the IA module achieves the best performance with a <ref type="bibr" target="#b23">24</ref> competitive recall value and an improvement of 2% in precision. The reason is that, as described in Section 3.3, image adaptation alone is insufficient for the optimal results due to severe domain shift between bitemporal images and more or less information loss during image-to-image transformation. Thus, the FA module further improves the performance by exploiting domain-invariance features to alleviate the domain gap from another perspective. The visualized results in <ref type="figure">Fig. 6</ref> also reveal similar observations. Therefore, we can conclude that our proposed adaptation modules, i.e., IA and FA, are both effective in boosting the crossdomain change detection performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Impact of Different Combinations of Bi-temporal Images</head><p>Different from general domain adaptation tasks <ref type="bibr" target="#b28">[29]</ref>[30] <ref type="bibr" target="#b30">[31]</ref> where only source-domain labels are available, for cross-domain change detection, the transformed bi-temporal images are still considered to share the same annotations of changed regions. This makes it possible to adopt information from different domains for predictions. On the other hand, the generated bi-temporal images suffer from information loss more or less. Thus, using only one domain bi-temporal images to train the change detection model would underutilize the image information. Therefore, we further explore the impact of different combinations of bi-temporal images.</p><p>Intuitively, we experiment on three pairs of bi-temporal images, i.e., the original ( , ),</p><formula xml:id="formula_16">( , ? ) ? and ( ? ,</formula><p>) ? , which results in seven different combinations in total.</p><p>We present the results in <ref type="table" target="#tab_4">Table 4</ref> and visualize the change maps in <ref type="figure" target="#fig_14">Fig. 7</ref> for comparison. As observed, when only one pair of bi-temporal images is used, the performance is ( ,</p><formula xml:id="formula_17">) &lt; ( , ? ) &lt; ( ? ,</formula><p>), which proves that relieving appearance differences contributes to cross-domain change detection. If we combine two or more pairs of bi-temporal images to extract feature maps, the performance shows consistent improvements. When exploiting all three pairs of bi-temporal images, we obtain the best performance on the WHU building dataset with an F1-score of 92.36%, which is 6.85% higher than the baseline. Here, we emphasize that incorporating the original bi-temporal images ( , ) can also benefit change detection because they contain all available information. Another interesting observation is that if one pair of bi-temporal images obtains higher scores alone, such bi-temporal images can bring more obvious improvements when combined with other pairs. This provides guidance to select proper combinations of bi-temporal images.</p><p>Moreover, although we achieve the best performance with the aforementioned combination of three bitemporal images, we highlight that not all pairs of bi-temporal images as well as generated images can constantly boost the performance. We also conducted experiments combining ( The reason could be that those bi-temporal images suffer from information loss after iterative image-to-image transformation, which is harmful for the final change prediction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Discussion of Different Fusion Strategies</head><p>Since we obtain three groups of domain-invariant feature maps after feeding three pairs of bi-temporal images into the feature adaptation module, we are curious as to how to integrate these feature maps to produce better results. Here, we explore two different fusion strategies, i.e., output fusion and feature fusion, which are widely used in the computer vision community [39] <ref type="bibr" target="#b39">[40]</ref>. Output fusion refers to first predicting the change map for each feature of bi-temporal images and then integrating the results, while feature fusion integrates all feature maps for the final prediction. As shown in <ref type="table" target="#tab_5">Table 5</ref>, feature fusion yields an F1-score of 92.36%, which is 0.5% higher than the output fusion. As shown in <ref type="figure" target="#fig_15">Fig. 8</ref>, we observe that feature fusion can discriminate detailed changes in buildings and effectively prevent false-positive predictions. We consider that feature fusion can better utilize the image information from different domains and compensate for each other, which formulates more expressive representations. Therefore, we employ feature fusion in our FA <ref type="bibr" target="#b27">28</ref> module, where three groups of feature maps from different domains are concatenated first and then fused to predict the final change map.  to be more powerful in capturing semantic information than output fusion, producing complete change regions with accurate boundaries owing to integrated representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel end-to-end supervised domain adaptation framework called SDACD, which can be easily incorporated into non-domain adaptation networks to tackle challenging cross-domain change detection on very high-resolution remote sensing images. The proposed IA module leverages generative adversarial networks to realize cross-domain image-to-image transformation, which aligns the appearance of bi-temporal images. To further reduce the domain gap, we introduce an FA module, which extracts domain-invariant features from different domains for feature alignment. And we integrate feature <ref type="bibr" target="#b28">29</ref> maps from all domains for the final change prediction. Our experiments show the superiority of our framework in overcoming domain shift. It achieves new state-of-the-art F1-scores of 97.34% and 92.36% on the CDD and WHU building datasets, respectively. More importantly, our method provides a general solution with great potential that is capable of bringing consistent increments to existing change detection methods and friendly to real-world applications for change detection.</p><p>Since we add an image adaptation (IA) module and a feature adaptation (FA) module to solve the domain shift of cross-domain change detection, our proposed framework sacrifices some efficiency compared with the baseline. In future work, it would be promising to consider compressing our framework through knowledge distillation to improve efficiency, which can further benefit real-world applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 Fig. 1</head><label>31</label><figDesc>The cross-domain bi-temporal images in the CDD and WHU building datasets. Images from top to bottom are (a) pre-event images, (b) post-event images, and (c) corresponding ground truths, respectively. The bi-temporal images of the CDD dataset were acquired in summer and winter, while the WHU building dataset was taken under varied luminance conditions in 2012 and 2016, respectively. Both datasets exhibit significant appearance variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>The overall architecture of our SDACD framework for cross-domain change detection. Given bi-temporal images ( , ) with severe domain shift, we first utilize an image adaptation module (IA) to alleviate the domain shift by aligning the image appearance. Then, we input two bi-temporal images from the pre-event domain and post-event domain, i.e., ( , well as the original bi-temporal images ( , ) into the feature adaptation module (FA) and extract domain-invariant features via a feature domain-invariant discriminator. The feature adaptation module can further reduce the domain gap by aligning feature distributions from different domains in the feature space. Finally, we integrate the extracted features from three bi-temporal images to predict the final change map by the classifier C. We show the training order of key modules in the SDADC framework at the bottom. Better viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>depicts the overall architecture of our proposed framework. Given the pre-event image ? ? ? and the post-event image ? ? ? , we aim to predict a binary change map ? ? , { ? | ? {0, 1}, = 1, 2, ? , } , where 1 represents changed pixels and 0 means no change. To highlight the cross-domain change detection problem, we denote the pre-event image as ? and the post-event image as ? ( ? ), where and represent the pre-event domain and post-event domain, respectively. To narrow the domain gap, we design an image adaptation (IA) module and a feature adaptation (FA) module to implement domain adaptation from different perspectives. Accordingly, the pipeline can be divided into two main phases. First, we transform the pre-event image to the post-event domain to obtain the pre-event stylized as post-event image ? . Similarly, we generate the post-event stylized as pre-event image ? by transforming the post-event image to the pre-event domain .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>? and ? ? for the two reverse image transformations, i.e.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, ) are transformed by the above image adaptation module, we obtain several bi-temporal images. Ideally, we can train a change detection model with excellent performance by using bi-temporal images ( , for training a cross-domain change detection model with optimal performance, several challenges remain to be solved. First, for bi-temporal images of cross-domain remote sensing, due to the severe domain shift between bi-temporal images, only utilizing image adaptation would be insufficient to achieve the expected domain adaptation results. Second, during the image adaptation procedure, compared with the original image, the generated image has the issue of information loss. Therefore, using only one domain of bi-temporal images to train a change detection model would suffer from underutilization or even loss of image information, thus producing suboptimal results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?)</head><label></label><figDesc>?PoD and the initial bi-temporal images ( ? ,), to train the change detection model. We make change predictions for each bi-temporal image and construct a feature domaininvariant discriminator Df to determine whether the predictions come from ( ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>predict the final change map O. We also later conduct ablation studies to investigate the selection of bi-temporal images for training the change detection model and fusion strategies for the prediction final change map, which will be discussed in Section 4.4.2 and Section 4.4.3, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>in the image adaptation module, the generators and are updated first to transform and into generated pre-event and post-event images, i.e., ? and ? , respectively. Then, the discriminators and are updated to distinguish the real images from the generated images. Next, the feature extractor E in the feature adaptation module is updated to extract features from the three input bi-temporal images, and then the change predictor C is updated to predict change maps for each bi-temporal image based on the extracted features. Afterward, the feature domain-invariance discriminator Df is updated to distinguish the input domain of the three prediction maps. Finally, the change predictor C is updated to predict the final change map based on the fused features. Therefore, combined with the cycle-consistency loss and adversarial loss of image adaptation in Eq. (3) and Eq. (6), the full adversarial loss of feature adaptation in Eq. (10) and the change detection loss in Eq. (12), the full multitask loss function can be defined as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>is a widely used dataset for change detection proposed by M. A et al., and it contains 11 pairs of bi-temporal images obtained from Google Earth in different seasons with a spatial resolution ranging from 3 to 100 cm per pixel. Since the original bi-temporal images included seven 4725?2700 image pairs and four 1900?1000 image pairs, which were too large to process directly, [21] cropped the original image pairs into the same size of 256?256 pixels, thus generating 10000 image pairs for training, 3000 image pairs for validation, and 3000 image pairs for testing. As shown in Fig. 1, the appearances of bi-temporal images vary greatly due to different seasons. To conform to the assumption ? , ? where ? , we partially adjusted the order of bi-temporal image pairs to obtain similar appearances in each domain, forming summer-styled pre-event images and winter-styled post-event images. The annotations of changed regions consist of objects varied in size and category, including cars, roads, constructions, etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>list the experimental results on the CDD dataset. Although STANet, DASNet, and SNUNet have already exhibited impressive performance, our SDACD-STANet, SDACD-DASNet, and SDACD-SNUNet can bring consistent improvements on the F1-score by 0.7%, 0.78%, and 1.66%, respectively. Notably, based on the cutting-edge work SNUNet, our method achieves new state-ofthe-art performance on CDD with an F1-score of 97.34%. Our framework outperforms the baseline model and mainly benefits from the consideration of domain adaptation from two perspectives. First, the IA module aligns the image appearance by image-to-image transformation, which effectively relieves the effect of different seasons and climates on the land cover changes. Second, the FA module further reduces the remaining domain shift by exploiting domain-invariance features to predict the final change map. In addition, the results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 3 21 Fig. 4 .</head><label>3214</label><figDesc>Visualized experimental results on CDD. Images from left to right are (a) pre-event images, (b) post-event images, (c) preevent stylized as post-event images, (d) post-event stylized as pre-event images, (e) ground truth, (f) the results of baseline, and (g) the results of baseline + SDACD. Compared with the corresponding baselines, our method can effectively avoid false-positive predictions introduced by varied seasons and climates and preserve detailed changes with clearer boundaries. Visualized experimental results on the WHU Building Dataset. Images from left to right are (a) pre-event images, (b) postevent images, (c) pre-event stylized as post-event images, (d) post-event stylized as pre-event images, (e) ground truth, (f) the results of baseline, (g) the results of baseline + SDACD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>22</head><label>22</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>The results of cross-domain image-to-image transformation through the IA module. Images from top to bottom are (a) preevent images, (b) pre-event stylized as post-event images, (c) post-event images, and (d) post-event stylized as pre-event images. Note that images of (a) and (d) belong to the pre-event domain, while (b) and (c) belong to the post-event domain. The transformed images in the same domain share a similar appearance, which validates the effectiveness of the IA module. 25 Visualized change maps of the ablation studies for IA and FA modules. Images from left to right are (a) pre-event images, (b) post-event images, (c) ground truth, (d) the results of SNUNet, (e) the results of SNUNet + IA, (f) the results of SNUNet + IA + FA. Both IA and FA help to reduce false-positive predictions and generate fine details for the changed regions. The best performance is achieved when combining the baseline model with IA and FA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>the performance of the change detection model was worse than that of even the original bi-temporal images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 7 .</head><label>7</label><figDesc>Comparison of visualized change maps produced by different combinations of bi-temporal images. For simplicity, we denote the bi-temporal images of ( , , ? and ?, respectively. Images from left to right are (a) pre-event images, (b) post-event images, (c) ground truth, the results of (d) ?, (e) ?, (f) ?, (g) ? + ?, (h) ? + ?, (i) ? + ?, and (j) ? + ? + ?, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison of the change maps with different fusion strategies. Images from top to bottom are (a) pre-event images, (b) post-event images, (c) ground truth, (d) the results of output fusion, and (e) the results of feature fusion. Feature fusion is shown</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>(IA), we employ generative adversarial networks, which have made wide success in image-to-image transformation, by building two generators G : corresponding generators to correctly distinguish the real images from the transformed images. Therefore, the generator G and its discriminator can be optimized via adversarial learning:</figDesc><table><row><cell>discriminators</cell><cell>and</cell><cell cols="2">compete with their</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>and G</cell><cell>:</cell><cell>?</cell><cell>and</cell></row><row><cell cols="2">two adversarial discriminators</cell><cell>and</cell><cell cols="2">. The generator G</cell><cell>aims to transform images in the post-</cell></row><row><cell cols="4">event domain to the pre-event domain, while G</cell><cell cols="2">performs the reverse image transformation. The</cell></row><row><cell></cell><cell></cell><cell></cell><cell>11</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of model size and computational complexity with baseline models.</figDesc><table><row><cell>-EF</cell><cell>84.68</cell><cell>65.13</cell><cell>73.63</cell><cell>80.75</cell><cell>67.29</cell><cell>73.40</cell></row><row><cell>FC-Siam-diff</cell><cell>87.57</cell><cell>66.69</cell><cell>75.07</cell><cell>48.84</cell><cell>88.96</cell><cell>63.06</cell></row><row><cell>FC-Siam-conc</cell><cell>88.81</cell><cell>62.20</cell><cell>73.16</cell><cell>54.20</cell><cell>81.34</cell><cell>65.05</cell></row><row><cell>STANet</cell><cell>83.17</cell><cell>92.76</cell><cell>87.70</cell><cell>82.12</cell><cell>89.19</cell><cell>83.40</cell></row><row><cell>SDACD-STANet</cell><cell>87.40 ?4.23</cell><cell>89.50 ?3.26</cell><cell>88.40 ?0.70</cell><cell>90.90 ?8.78</cell><cell>93.50 ?4.31</cell><cell>92.21 ?8.81</cell></row><row><cell>DASNet</cell><cell>93.28</cell><cell>89.91</cell><cell>91.57</cell><cell>83.77</cell><cell>91.02</cell><cell>87.24</cell></row><row><cell>SDACD-DASNet</cell><cell>92.85 ?0.43</cell><cell>91.87 ?1.96</cell><cell>92.35 ?0.78</cell><cell>89.21 ?5.44</cell><cell>90.46 ?0.56</cell><cell>89.83 ?2.59</cell></row><row><cell>SNUNet</cell><cell>96.60</cell><cell>94.77</cell><cell>95.68</cell><cell>82.12</cell><cell>89.19</cell><cell>85.51</cell></row><row><cell>SDACD-SNUNet</cell><cell>97.13 ?0.53</cell><cell>97.56 ?2.79</cell><cell>97.34 ?1.66</cell><cell>93.85 ?11.73</cell><cell>90.91 ?1.72</cell><cell>92.36 ?6.85</cell></row><row><cell></cell><cell>Models</cell><cell></cell><cell>Params/M</cell><cell>FLOPs/G</cell><cell></cell><cell></cell></row><row><cell></cell><cell>STANet</cell><cell></cell><cell>16.93</cell><cell>26.33</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SDACD-STANet</cell><cell></cell><cell>40.05</cell><cell>286.68</cell><cell></cell><cell></cell></row><row><cell></cell><cell>DASNet</cell><cell></cell><cell>48.22</cell><cell>201.44</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SDACD-DASNet</cell><cell></cell><cell>80.94</cell><cell>752.82</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SNUNet</cell><cell></cell><cell>27.07</cell><cell>246.32</cell><cell></cell><cell></cell></row><row><cell></cell><cell>SDACD-SNUNet</cell><cell></cell><cell>52.92</cell><cell>1368.48</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Impact of the proposed IA and FA modules on the WHU building dataset.</figDesc><table><row><cell>Baseline</cell><cell>IA</cell><cell>FA</cell><cell>P (%)</cell><cell>R (%)</cell><cell>F (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>82.12</cell><cell>89.19</cell><cell>85.51</cell></row><row><cell>SNUNet</cell><cell>?</cell><cell></cell><cell>91.85</cell><cell>91.10</cell><cell>91.47</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>93.85</cell><cell>90.91</cell><cell>92.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of different combinations of bi-temporal images on the WHU building dataset.</figDesc><table><row><cell>I pre + I post</cell><cell>I pre + I post-pre</cell><cell>I pre-post + I post</cell><cell>P (%)</cell><cell>R (%)</cell><cell>F (%)</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>82.12</cell><cell>89.19</cell><cell>85.51</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell>87.62</cell><cell>86.94</cell><cell>87.28</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>92.10</cell><cell>90.19</cell><cell>91.14</cell></row><row><cell>?</cell><cell>?</cell><cell></cell><cell>87.28</cell><cell>88.09</cell><cell>87.68</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>92.35</cell><cell>90.76</cell><cell>91.55</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>91.91</cell><cell>90.62</cell><cell>91.26</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>93.85</cell><cell>90.91</cell><cell>92.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Comparison of different fusion strategies on the WHU building dataset.</figDesc><table><row><cell>Fusion Strategy</cell><cell>P (%)</cell><cell>R (%)</cell><cell>F (%)</cell></row><row><cell>output fusion</cell><cell>93.21</cell><cell>90.55</cell><cell>91.86</cell></row><row><cell>feature fusion</cell><cell>93.85</cell><cell>90.91</cell><cell>92.36</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Long-term annual mapping of four cities on different continents by applying a deep information learning method to landsat data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haobo</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">471</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transferred deep learning for sea ice change detection from synthetic-aperture radar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1655" to="1659" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Destroyed-buildings detection from VHR SAR images using deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipan</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Bovolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Signal Processing for Remote Sensing XXIV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10789</biblScope>
			<biblScope unit="page">107890</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Change vector analysis: an approach for detecting forest changes with Landsat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LARS symposia</title>
		<imprint>
			<date type="published" when="1980" />
			<biblScope unit="page">385</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PCA-based land-use change detection and analysis using multitemporal and multisensor satellite data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Js Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4823" to="4838" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving change detection results of IR-MAD by eliminating strong changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Prashanth R Marpu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morton J</forename><surname>Gamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Canty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="799" to="803" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A post-classification change detection method based on iterative slow feature analysis and Bayesian soft fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing of Environment</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="page" from="241" to="255" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fuzzy clustering algorithms for unsupervised change detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niladri</forename><surname>Shekhar Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susmita</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="699" to="715" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rapid land cover map updates using change detection and robust random forest classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frans</forename><surname>Wessels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">P</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">C</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Steenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derick</forename><surname>Macalister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debbie</forename><surname>Swanepoel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">888</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple support vector machines for land cover change detection: An application for mapping urban extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassiba</forename><surname>Nemmour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youcef</forename><surname>Chibani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="133" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to Rank using User Clicks and Visual Features for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="767" to="779" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal Deep Autoencoder for Human Pose Recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5659" to="5670" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal face-pose estimation with multitask manifold deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongnan</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyong-Ho</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3952" to="3961" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical deep click feature prediction for finegrained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="563" to="578" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FCL-Net: Towards Accurate Edge Detection via Fine-scale Corrective Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="248" to="259" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese networks for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertr</forename><forename type="middle">Le</forename><surname>Rodrigo Caye Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4063" to="4067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SNUNet-CD: A densely connected siamese network for change detection of VHR images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1662</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DASNet: Dual attentive fully convolutional siamese networks for change detection in high-resolution satellite images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1194" to="1206" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HFA-Net: High frequency attention siamese network for building change detection in VHR remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoguo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenlong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page">108717</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CHANGE DETECTION IN REMOTE SENSING IMAGES USING CONDITIONAL ADVERSARIAL NETWORKS. International Archives of the Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vizilter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vygolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Va Knyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu Rubis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunping</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="574" to="586" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Feature-level change detection using deep representation and feature change analysis for multispectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoguo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puzhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linzhi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1666" to="1670" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Xiang</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="924" to="935" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonlocal patch similarity based heterogeneous remote sensing change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuli</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gangyao</forename><surname>Kuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">107598</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Mahfuzur Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dual learning-based siamese framework for change detection using bi-temporal VHR optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Kou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1292</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Progressive domain adaptation for change detection using seasonvarying remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">3815</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from synthetic images via active pseudo-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lefei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="6452" to="6465" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Maximum density divergence for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3918" to="3930" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster domain adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2021.3060473</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Divergence-agnostic Unsupervised Domain Adaptation by Adversarial Attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3109287</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="845" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
