<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FINE Samples for Learning with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehyeon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Ko</surname></persName>
							<email>jongwoo.ko@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwook</forename><surname>Cho</surname></persName>
							<email>sangwookcho@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhwan</forename><surname>Choi</surname></persName>
							<email>jinhwanchoi@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se-Young</forename><surname>Yun</surname></persName>
							<email>yunseyoung@kaist.ac.kr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">KAIST AI KAIST</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">KAIST AI KAIST Daejeon</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">KAIST AI KAIST Daejeon</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">KAIST AI KAIST Daejeon</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">KAIST AI KAIST Daejeon</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FINE Samples for Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern deep neural networks (DNNs) become weak when the datasets contain noisy (incorrect) class labels. Robust techniques in the presence of noisy labels can be categorized into two types: developing noise-robust functions or using noisecleansing methods by detecting the noisy data. Recently, noise-cleansing methods have been considered as the most competitive noisy-label learning algorithms. Despite their success, their noisy label detectors are often based on heuristics more than a theory, requiring a robust classifier to predict the noisy data with loss values. In this paper, we propose a novel detector for filtering label noise. Unlike most existing methods, we focus on each data point's latent representation dynamics and measure the alignment between the latent distribution and each representation using the eigen decomposition of the data gram matrix. Our framework, coined as filtering noisy instances via their eigenvectors (FINE), provides a robust detector using derivative-free simple methods with theoretical guarantees. Under our framework, we propose three applications of the FINE: sample-selection approach, semi-supervised learning (SSL) approach, and collaboration with noiserobust loss functions. Experimental results show that the proposed methods consistently outperform corresponding baselines for all three applications on various benchmark datasets 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have achieved remarkable success in numerous tasks as the amount of accessible data has dramatically increased <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15]</ref>. On the other hand, accumulated datasets are typically labeled by a human, a labor-intensive job or through web crawling <ref type="bibr">[48]</ref> so that they may be easily corrupted (label noise) in real-world situations. Recent studies have shown that deep neural networks have the capacity to memorize essentially any labeling of the data <ref type="bibr">[49]</ref>. Even a small amount of such noisy data can hinder the generalization of DNNs owing to their strong memorization of noisy labels <ref type="bibr">[49,</ref><ref type="bibr" target="#b28">29]</ref>. Hence, it becomes crucial to train DNNs that are robust to corrupted labels. As label noise problems may appear anywhere, such robustness increases reliability in many applications such as the e-commerce market <ref type="bibr" target="#b8">[9]</ref>, medical fields <ref type="bibr" target="#b44">[45]</ref>, on-device AI <ref type="bibr" target="#b45">[46]</ref>, and autonomous driving systems <ref type="bibr" target="#b10">[11]</ref>.</p><p>To improve the robustness against noisy data, the methods for learning with noisy labels (LNL) have been evolving in two main directions <ref type="bibr" target="#b17">[18]</ref>: <ref type="bibr" target="#b0">(1)</ref> designing noise-robust objective functions or regular- Noise-cleansing learning generally separates clean data from the original dataset by using prediction outputs. We propose a novel derivative-free detector based on an unsupervised clustering algorithm on the high-order topological space. FINE measures the alignment of pre-logits (i.e., penultimate layer representation vectors) toward the class-representative vector that is extracted through the eigen decomposition of the gram matrix of data representations.</p><p>izations and (2) detecting and cleansing the noisy data. In general, the former noise-robust direction uses explicit regularization techniques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">52,</ref><ref type="bibr">50]</ref> or robust loss functions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr">51]</ref>, but their performance is far from state-of-the-art <ref type="bibr">[49,</ref><ref type="bibr" target="#b25">26]</ref> on datasets with severe noise rates. Recently, researchers have designed noise-cleansing algorithms focused on segregating the clean data (i.e., samples with uncorrupted labels) from the corrupted data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42]</ref>. One of the popular criteria for the segregation process is the loss value between the prediction of the noisy classifier and its noisy label, where it is generally assumed that the noisy data have a large loss <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b17">18]</ref> or the magnitude of the gradient during training <ref type="bibr">[51,</ref><ref type="bibr" target="#b39">40]</ref>. However, these methods may still be biased by the corrupted linear classifier towards label noise because their criterion (e.g., loss values or weight gradient) uses the posterior information of such a linear classifier <ref type="bibr" target="#b23">[24]</ref>. Maennel et al. <ref type="bibr" target="#b30">[31]</ref> analytically showed that the principal components of the weights of a neural network align with the randomly labeled data; this phenomenon can yield more negative effects on the classifier as the number of randomly labeled classes increases. Recently, Wu et al. <ref type="bibr" target="#b41">[42]</ref> used an inherent geometric structure induced by nearest neighbors (NN) in latent space and filtered out isolated data in such topology, and its quality was sensitive to its hyperparameters regarding NN clustering in the presence of severe noise rates.</p><p>To mitigate such issues for label noise detectors, we provide a novel yet simple detector framework, filtering noisy labels via their eigenvectors (FINE) with theoretical guarantees to provide a high-quality splitting of clean and corrupted examples (without the need to estimate noise rates). Instead of using the neural network's linear classifier, FINE utilizes the principal components of latent representations made by eigen decomposition which is one of the most widely used unsupervised learning algorithms and separates clean data and noisy data by these components <ref type="figure" target="#fig_0">(Figure 1a</ref>). To motivate our approach, as <ref type="figure" target="#fig_0">Figure 1b</ref> shows, we find that the clean data (blue points) are mainly aligned on the principal component (black dotted line), whereas the noisy data (orange points) are not; thus, the dataset is well clustered with the alignment of representations toward the principal component by fitting them into Gaussian mixture models (GMM). We apply our framework to various LNL methods: the sample selection approach, a semi-supervised learning (SSL) approach, and collaboration with noise-robust loss functions. The key contributions of this work are summarized as follows:</p><p>? We propose a novel framework, termed FINE (filtering noisy labels via their eigenvectors), for detecting clean instances from noisy datasets. FINE makes robust decision boundary for the high-order topological information of data in latent space by using eigen decomposition of their gram matrix.</p><p>? We provide provable evidence that FINE allows a meaningful decision boundary made by eigenvectors in latent space. We support our theoretical analysis with various experimental results regarding the characteristics of the principal components extracted by our FINE detector.</p><p>? We develop a simple sample-selection method by replacing the existing detector method with FINE. We empirically validate that a sample-selection learning with FINE provides consistently superior detection quality and higher test accuracy than other existing alternative methods such as the Co-teaching family <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">47]</ref>, TopoFilter <ref type="bibr" target="#b41">[42]</ref>, and CRUST <ref type="bibr" target="#b31">[32]</ref>.</p><p>? We experimentally show that our detection framework can be applied in various ways to existing LNL methods and validate that ours consistently improves the generalization in the presence of noisy data: sample-selection approach <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">47]</ref>, SSL approach <ref type="bibr" target="#b24">[25]</ref>, and collaboration with noise-robust loss functions [51, <ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Organization. The remainder of this paper is organized as follows. In Section 2, we discuss the recent literature on LNL solutions and meaningful detectors. In Section 3, we address our motivation for creating a noisy label detector with theoretical insights and provide our main method, filtering the noisy labels via their eigenvectors (FINE). In Section 4, we present the experimental results. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Zhang et al.</p><p>[49] empirically showed that any convolutional neural networks trained using stochastic gradient methods easily fit a random labeling of the training data. To tackle this issue, numerous works have examined the classification task with noisy labels. We do not consider the works that assumed the availability of small subsets of training data with clean labels <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr">53,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Noise-Cleansing-based Approaches. Noise-cleansing methods have evolved following the improvement of noisy detectors. Han et al. <ref type="bibr" target="#b13">[14]</ref> suggested a noisy detection approach, named coteaching, that utilizes two networks, extracts subsets of instances with small losses from each network, and trains each network with subsets of instances filtered by another network.  <ref type="bibr" target="#b41">[42]</ref> proposed a method called TopoFilter that filters noisy data by utilizing the k-nearest neighborhood algorithm and Euclidean distance between pre-logits. Mirzasoleiman et al. <ref type="bibr" target="#b31">[32]</ref> introduced an algorithm that selects subsets of clean instances that provide an approximately low-rank Jacobian matrix and proved that gradient descent applied to the subsets prevents overfitting to noisy labels. Pleiss et al. <ref type="bibr" target="#b33">[34]</ref> proposed an area under margin (AUM) statistic that measures the average difference between the logit values of the assigned class and its highest non-assigned class to divide clean and noisy samples. Cheng et. al <ref type="bibr" target="#b7">[8]</ref> progressively filtered out corrupted instances using a novel confidence regularization term. The noise-cleansing method was also developed in a semi-supervised learning (SSL) manner. Li et al. <ref type="bibr" target="#b24">[25]</ref> modeled the per-sample loss distribution and divide it into a labeled set with clean samples and an unlabeled set with noisy samples, and they leverage the noisy samples through the well-known SSL technique MixMatch <ref type="bibr" target="#b3">[4]</ref>.</p><p>Noise-Robust Models. Noise-robust models have been studied in the following directions: robustloss functions, regularizations, and strategies. First, for robust-loss functions, Ghosh et al. <ref type="bibr" target="#b12">[13]</ref> showed that the mean absolute error (MAE) might be robust against noisy labels. Zhang &amp; Sabuncu et al. <ref type="bibr">[51]</ref> argued that MAE performed poorly with DNNs and proposed a GCE loss function, which can be seen as a generalization of MAE and cross-entropy (CE). Wang et al. <ref type="bibr" target="#b39">[40]</ref> introduced the reverse version of the cross-entropy term (RCE) and suggested that the SCE loss function is a weighted sum of the CE and RCE. Some studies have stated that the early-stopped model can prevent the memorization phenomenon for noisy labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">49]</ref> and theoretically analyzed it <ref type="bibr" target="#b25">[26]</ref>. Based on this hypothesis, Liu et al. <ref type="bibr" target="#b28">[29]</ref> proposed an early-learning regularization (ELR) loss function to prohibit memorizing noisy data by leveraging the semi-supervised learning techniques. Xia et al. <ref type="bibr" target="#b42">[43]</ref> clarified which neural network parameters cause memorization and proposed a robust training strategy for these parameters. Efforts have been made to develop regularizations on the prediction level by smoothing the one-hot vector <ref type="bibr" target="#b29">[30]</ref>, using linear interpolation between data instances [50], and distilling the rescaled prediction of other models <ref type="bibr" target="#b19">[20]</ref>. However, these works have limitations in terms of performance as the noise rate of the dataset increases.</p><p>Dataset Resampling. Label-noise detection may be a category of data resampling which is a common technique in the machine learning community that extracts a "helpful" dataset from the distribution of the original dataset to remove the dataset bias. In class-imbalance tasks, numerous studies have conducted over-sampling of minority classes <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1]</ref> or undersampling the majority classes <ref type="bibr" target="#b4">[5]</ref> to balance the amount of data per class. Li &amp; Vasconcelos et al. <ref type="bibr" target="#b26">[27]</ref> proposed a resampling procedure to reduce the representation bias of the data by learning a weight distribution that favors difficult instances for a given feature representation. Le Bras et al. <ref type="bibr" target="#b21">[22]</ref> suggested an adversarial filteringbased approach to remove spurious artifacts in a dataset. Analogously, in anomaly detection and INPUT : Noisy training data D, feature extractor g, number of classes K, clean probability threshold ?, set of FINE scores for class k F k OUTPUT : Collected clean data C 1: Initialize C ? ?,D ? D, ? k ? 0 for all k = 1, . . . , K /* Update the convariance matrices for all classes */ 2: for (x i , y i ) ? D do 3:</p><formula xml:id="formula_0">z i ? g(x i ) 4:</formula><p>Update the gram matrix ? yi ? ? yi + z i z i 5: end for /* Generate the principal component with eigen decomposition */ 6: for k = 1, . . . , K do 7:</p><formula xml:id="formula_1">U k , ? k ? EIGEN DECOMPOSITION OF ? k 8:</formula><p>u k ? THE FIRST COLUMN OF U k 9: end for /* Compute the alignment score and get clean subset C */ 10: for (x i , y i ) ? D do <ref type="bibr">11:</ref> Compute the FINE score f i = u yi , z i 2 and F yi ? F yi ? {f i } 12: end for /* Finding the samples whose clean probability is larger than ? */ 13: C ? C? GMM (F k , ?) for all k = 1, . . . , K out-of-distribution detection problems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23]</ref>, the malicious data are usually detected by examining the loss value or negative behavior in the feature representation space. While our research is motivated by such previous works, this paper focuses on the noisy image classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we present our detector framework and the theoretical motivation behind using the detector in high-dimensional classification. To segregate the clean data, we utilize the degree of alignment between the representations and the eigenvector of the representations' gram matrices for all classes, called FINE (FIltering Noisy instnaces via their Eigenvectors). Our algorithm is as follows (Algorithm 1). FINE first creates a gram matrix of the representation in the noisy training dataset for each class and conducts the eigen decomposition for those gram matrices. Then, FINE finds clean and noisy instances using the square of inner product values between the representations and the first eigenvector having the largest eigenvalue. In this manner, we treat the data as clean if aligned onto the first eigenvector, while most of the noisy instances are not. Here, we formally define 'alignment' and 'alignment clusterability' in Definition 1 and Definition 2, respectively. Definition 1. (Alignment) Let x be the data labeled as class k and z is the output of feature extractor with input x, and the gram matrix ? k of all features labeled as class k in dataset D (i.e., ? k = z?{class=k} zz ). Then, we estimate the alignment of the data x via u 1 , z 2 where u 1 is the first column of U k from the eigen decomposition of ? k , i.e., ? k = U k ? k U k when the eigenvalues are in descending order.</p><p>Definition 2. (Alignment Clusterability) For all features labeled as class k in dataset D, let fit a Gaussian Mixture Model (GMM) on their alignment (Definition 1) distribution to divide current samples into a clean set and a noisy set; the set having larger mean value is treated as a clean set, and another one is a noisy set. Then, we say a dataset D satisfies alignment clusterability if the representation z labeled as the same true class belongs to the clean set.</p><p>As an empirical evaluation, the quality of our detector for noisy data is measured with the F-score, a widely used criterion in noisy label detection, anomaly detection and out-of-distribution detection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23]</ref>. We treat the selected clean samples as the positive class and the noisy samples as negative class. The F-score is the harmonic mean of the precision and the recall; the precision indicates the fraction of clean samples among all samples that are predicted as clean, and the recall indicates the portion of clean samples that are identified correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Alignment Analysis for Noisy Label Detector</head><p>Perturbation Noisy Clean <ref type="figure" target="#fig_1">Figure 2</ref>: Illustration for the problem settings and Theorem 1. The perturbation (green shade) is the angle between the first eigenvector of clean instances (blue line) and the estimated first eigenvector (green line) which is perturbed by that of noisy instances (orange line). Note that blue and orange points are clean instances and noisy instances, respectively.</p><p>To design a robust label noise filtering framework, we explore the linear nature of the topological space of feature vectors for data resampling techniques and deal with the classifier contamination due to random labels. Recent studies on the distribution of latent representations in DNNs provide insight regarding how correctly the outlier samples can be filtered with the hidden space's geometrical information. For instance, in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, the authors proposed frameworks for novelty detection using the topological information of pre-logit based on the Mahalanobis distance, and, in <ref type="bibr" target="#b41">[42]</ref>, the authors filtered the noisy data based on the Euclidean distance between prelogits. Maennel et al. <ref type="bibr" target="#b30">[31]</ref> analytically showed that an alignment between the principal components of network parameters and those of data takes place when training with random labels. This finding points out that random labels can corrupt a classifier, and thus building a robust classifier is required.</p><p>Motivated by these works, we aim to design a novel detector using the principal components of latent features to satisfy Definition 2. However, it is intractable to find the optimal classifier to maximize the separation of alignment clusterability because clean data distribution and noisy data distribution are inaccessible. To handle this issue, we attempt to approximate the clean eigenvector to maximize the alignment values of clean data rather than to maximize the separation; the algorithm utilizes the eigenvector of the data for each class <ref type="figure" target="#fig_1">(Figure 2</ref>). Below, we provide the upper bound for the perturbation toward the clean data's eigenvector under simple problem settings with noisy labels referred to in other studies <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b41">42]</ref>. We first introduce notations. Next, we establish the theoretical evidence that our FINE algorithm approximates the clean data's eigenvectors under some assumptions for its analytic tractability (Theorem 1). We mainly present the theorem and its interpretation; details of the proofs can be found in the Appendix.</p><p>Notations. Consider a binary classification task. Assume that the data points and labels lie in X ? Y, where the feature space X ? R d and label space Y = {?1, +1}. A single data point x and its true label y follow a distribution (x, y) ? P X ?Y . Denote by? the observed label (potentially corrupted). Without loss of generality, we focus on the set of data points whose observed label is y = +1.</p><p>Let X ? X be the finite set of features with clean instances whose true label is y = +1. Similarly, letX ? X be the set of noisy instances whose true label is y = ?1. To establish our theorem, we assume the following reasonable conditions referred to other works using linear discriminant analysis (LDA) assumptions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12]</ref>: Assumption 1. The feature distribution is comprised of two Gaussians, each identified as a clean cluster and a noisy cluster. For any ? ? (0, 1), its perturbation towards the v in assumption 2 (i.e., 2-norm for difference of projection matrices; left hand side of Eq. (1)) holds the following with probability 1 ? ?: where w is the first eigenvector of noisy instances, ? is the fraction between noisy and clean in-</p><formula xml:id="formula_2">uu ? vv 2 ? 3? cos ? + O(? 2 d+log(4/?) N+ ) 1 ? ? (sin ? + 3 cos ?) ? O(? 2 d+log(4/?) N+ ) (1) (a) CIFAR10 (b) CIFAR100 (d) Scalability (c) Perturbation</formula><formula xml:id="formula_3">stances ( N? N+ ), ? is ?(w, v)</formula><p>, and ? 2 is a variance of white noise.</p><p>Theorem 1 states that the upper bound for the perturbation toward v are dependent on both the ratio ? and the angle ? between w and v; small upper bound can be guaranteed as the number of clean data increases, ? decreases, and ? approaches ? 2 . We also derive the lower bound for the precision and the recall when using the eigenvector u in Appendix. In this theoretical analysis, we can ensure that such lower bound values become larger as v and w become orthogonal to each other. To verify these assumptions, we provide various experimental results for the separation of alignment clusterability, the perturbation values, the scalability to the number of samples, the quality of our detector in the application of sample selection approach, and the comparison with an alternative clustering-based estimator <ref type="bibr" target="#b23">[24]</ref>.</p><p>Validation for our Estimated Eigenvector. To validate our FINE's principal components, we first propose a simple visualization scheme based on the following steps: (1) Pick the first eigenvector (u) extracted by FINE algorithm, (2) Generate a random hyperplane spanned by such eigenvector (u) and a random vector, (3) Calculate the value of the following Eq. (2) on any unit vectors (a) in such hyperplane and plot a heatmap with them:</p><formula xml:id="formula_4">1 |X| xi?X a, x i 2 ? 1 |X| xj ?X a, x j 2<label>(2)</label></formula><p>Eq. <ref type="formula" target="#formula_4">(2)</ref> is maximized when the unit vector a not only maximizes the FINE scores of clean data for the first term in Eq. <ref type="formula" target="#formula_4">(2)</ref>, but also minimizes those of noisy data for the second term in Eq. Scalability to Number of Samples. Despite FINE's superiority, it may require high computational costs if the whole dataset is used for eigen decomposition. To address this issue, we approximate the eigenvector with a small portion of the dataset and measure the cosine similarity values between the approximated term and the original one (u) <ref type="figure" target="#fig_2">(Figure 3d</ref>). Interestingly, we verify that far accurate eigenvector is computed even using 1% data (i.e., a cosine similarity value is 0.99), and thus the eigenvector can be accurately estimated with little computation time.</p><p>Validation for Dynamics of Sample-selection Approach. We evaluate the F-score dynamics of every training epoch on the symmetric and the asymmetric label noise in <ref type="figure" target="#fig_11">Figure 4</ref>. We compare FINE with the following sample-selection approaches: Co-teaching <ref type="bibr" target="#b13">[14]</ref> and TopoFilter <ref type="bibr" target="#b41">[42]</ref>. In <ref type="figure">Figure</ref>  CIFAR 100 Asym 40%.</p><p>(c) Asym 40% (C100) <ref type="figure" target="#fig_11">Figure 4</ref>: Comparisons of F-scores on CIFAR10 and CIFAR100 under symmetric and asymmetric label noise. C10 and C100 denote CIFAR-10 and CIFAR-100, respectively. noise and asymmetric noise settings, while Co-teaching and TopoFilter achieve lower quality. Unlike TopoFilter and FINE, Co-teaching even performs the sample-selection with the access of noise rate. This evidence show that FINE is also applicable to the naive sample-selection approaches. Comparison for Mahalanobis Distance Estimator Under similar conditions, Lee et al. <ref type="bibr" target="#b23">[24]</ref> measured the Mahalanobis distance of pre-logits using the minimum covariance determinant (MCD) estimator and selected clean samples based on this distance. While they also utilized the LDA assumptions on pre-logits, FINE consistently outperforms MCD in both precision and recall, thus yielding better Fscore ( <ref type="figure" target="#fig_4">Figure 5</ref>). The experimental results justify our proposed detector, in comparison with a similar alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we demonstrate the effectiveness of our FINE detector for three applications: sample selection approach, SSL, and collaboration with noise-robust loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Noisy Benchmark Dataset. Following the previous setups <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29]</ref>, we artificially generate two types of random noisy labels: injecting uniform randomness into a fraction of labels (symmetric) and corrupting a label only to a specific class (asymmetric). For example, we generate noise by mapping TRUCK ? AUTOMOBILE, BIRD ? AIRPLANE, DEER ? HORSE, CAT ? DOG to make asymmetric noise for CIFAR-10. For CIFAR-100, we create 20 five-size super-classes and generate asymmetric noise by changing each class to the next class within super-classes circularly. For a real-world dataset, Clothing1M <ref type="bibr" target="#b43">[44]</ref> containing inherent noisy labels is used. This dataset contains 1 million clothing images obtained from online shopping websites with 14 classes 2 . The dataset provides 50k, 14k, and 10k verified as clean data for training, validation, and testing. Instead of using the 50k clean training data, we use a randomly sampled pseudo-balanced subset as a training set with 120k images. For evaluation, we compute the classification accuracy on the 10k clean dataset. Networks and Hyperparameter Settings. We use the architectures and hyperparameter settings for all baseline experiments following the setup of Liu et al. <ref type="bibr" target="#b28">[29]</ref> except with SSL approaches. For SSL approaches, we follow the setup of Li et al. <ref type="bibr" target="#b24">[25]</ref>. We set the threshold ? as 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Application of FINE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sample Selection-Based Approaches</head><p>We apply our FINE detector for various sample selection algorithms. In detail, after warmup training, at every epoch, FINE selects the clean data with the eigenvectors generated from the gram matrices of data predicted to be clean in the previous round, and then the neural networks are trained with them. We compare our proposed method with the following sample selection approaches: (1) <ref type="table">Table 1</ref>: Test accuracies (%) on CIFAR-10 and CIFAR-100 under different noisy types and fractions. All comparison methods are reproduced with publicly available code, while the results for Bootstrap <ref type="bibr" target="#b34">[35]</ref> and Forward <ref type="bibr" target="#b32">[33]</ref> are taken from <ref type="bibr" target="#b28">[29]</ref>. For CRUST <ref type="bibr" target="#b31">[32]</ref>, we experiment without mixup to compare the intrinsic sample selection effect of each method. The average accuracies and standard deviations over three trials are reported. Here, we substitute the sample selection method of Co-teaching <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">47]</ref> with FINE (i.e., F-Co-teaching). The best results sharing the noisy fraction and method are highlighted in bold. . We evaluate these algorithms three times and report error bars. <ref type="table">Table 1</ref> summarizes the performances of different sample selection approaches on various noise distribution and datasets. We observe that our FINE method consistently outperforms the competitive methods over the various noise rates. Our FINE methods can filter the clean instances without losing essential information, leading to training the robust network.</p><p>To go further, we improve the performance of Co-teaching <ref type="bibr" target="#b13">[14]</ref> by substituting its sample selection state with our FINE algorithm. To combine FINE and the Co-teaching family, unlike the original methods that utilize the small loss instances to train with clean labels, we train one model with extracted samples by conducting FINE on another model. The results of the experiments are shown in the eighth and ninth rows of <ref type="table">Table 1</ref>. SSL approaches <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">41]</ref> divide the training data into clean instances as labeled instances and noisy instances as unlabeled instances and use both the labeled and unlabeled samples to train the networks in SSL. Recently, methods belonging to this category have shown the best performance among the various LNL methods, and these methods can train robust networks for even extremely high noise rates. We compare the performances of the existing semi-supervised approaches and that in which the sample selection state of DivideMix <ref type="bibr" target="#b24">[25]</ref> is substituted with our FINE algorithm (i.e., F-DivideMix). The results of the experiments are shown in <ref type="table" target="#tab_3">Table 2</ref>. We achieve consistently higher performance than DivideMix by utilizing FINE instead of its loss-based filtering method and show comparable performance to the state-of-the-art SSL methods such as DST <ref type="bibr" target="#b40">[41]</ref> and LongReMix <ref type="bibr" target="#b9">[10]</ref>. Interestingly, as <ref type="figure" target="#fig_5">Figure 6</ref> shows, clean and noisy data are well classified in F-DivideMix under extreme noise cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">SSL-Based Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Collaboration with Noise-Robust Loss Functions</head><p>The goal of the noise-robust loss function is to achieve a small risk for unseen clean data even when noisy labels exist in the training data. There have been few collaboration studies of the noise-robust loss function methodology and dynamic sample selection. Most studies have selected clean and noisy data based on cross-entropy loss.    Here, we state the collaboration effects of FINE with various noise-robust loss functions: generalized cross entropy (GCE) [51], symmetric cross entropy (SCE) <ref type="bibr" target="#b39">[40]</ref>, and early-learning regularization (ELR) <ref type="bibr" target="#b28">[29]</ref>. <ref type="figure" target="#fig_8">Figure 7</ref> shows that FINE facilitates generalization in the application of noise-robust loss functions on severe noise rate settings. The detailed results are reported in the Appendix. Unlike other methods, it is still theoretically supported because FINE extracts clean data with a robust classifier using representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Real-World Dataset</head><p>As <ref type="table" target="#tab_4">Table 3</ref> shows, FINE and F-DivideMix work fairly well on the Clothing 1M dataset compared to other approaches when we reproduce the experimental results under the same settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper introduces FINE for detecting label noise by designing a robust noise detector. Our main idea is utilizing the principal components of latent representations made by eigen decomposition. Most existing detection methods are dependent on the loss values, while such losses may be biased by corrupted classifier <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref>. Our methodology alleviates this issue by extracting key information from representations without using explicit knowledge of the noise rates. We show that the FINE detector has an excellent ability to detect noisy labels in theoretical and experimental results. We propose three applications of the FINE detector: sample-selection approach, SSL approach, and collaboration with noise-robust loss functions. FINE yields strong results on standard benchmarks and a real-world dataset for various LNL approaches.</p><p>We believe that our work opens the door to detecting samples having noisy labels with explainable results. It is a non-trivial task and of social significance, and thus, our work will have a substantial social impact on DL practitioners because it avoids the a labor-intensive job of checking data label quality. As future work, we hope that our work will trigger interest in the design of new labelnoise detectors and bring a fresh perspective for other data-resampling approaches (e.g., anomaly detection and novelty detection). The development of robustness against label noise even leads to an improvement in the performance of network trained with data collected through web crawling. We believe that our contribution will lower the barriers to entry for developing robust models for DL practitioners and greatly impact the internet industry. On the other hand, we are concerned that it can be exploited to train robust models using data collected illegally and indiscriminately on the dark web (e.g., web crawling), and thus it may raise privacy concerns (e.g., copyright).</p><p>[ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Theoretical Guarantees for FINE Algorithm</head><p>This section provides the detailed proof for Theorem 1 and the lower bounds of the precision and recall. We derive such theorems with the concentration inequalities in probabilistic theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Preliminaries</head><p>Spectral Norm. In this section, we frequently use the spectral norm. For any matrix A ? R m?n , the spectral norm are defined as follows:</p><formula xml:id="formula_5">A 2 = sup x?R n : x =1 Ax ,</formula><p>where a ij is the (i, j) element of A.</p><p>Singular Value Decomposition (SVD). Let A ? R m?n . There exist orthogonal matrices that satisfy the following:</p><formula xml:id="formula_6">U = [u 1 , u 2 , ? ? ? , u m ] ? R m?m and V = [v 1 , v 2 , ? ? ? , v n ] ? R n?n such that U AV = diag[? 1 , ? 2 , ? ? ? , ? min{m,n} ]<label>(3)</label></formula><p>where ? 1 ? ? 2 ? ? ? ? ? ? min{m,n} which are called singular values and diag[?] is a diagonal matrix whose diagonal consists of the vector in the bracket <ref type="bibr">[?]</ref>. (Note that UU = U U = I when U is an orthogonal matrix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Theorem 1</head><p>We deal with some require lemmas which are used for the proof of Theorem 1. Lemma 1. Let V and W be orthogonal matrices and</p><formula xml:id="formula_7">V = [V 1 , V 2 ] and W = [W 1 , W 2 ] with V 1 , W 1 ? R N ?N . Then, we have V 1 V 1 ? W 1 V 1 2 = V 1 W 2 2 = V 2 W 1 2 .</formula><p>Proof. From the orthogonal invariance property,</p><formula xml:id="formula_8">V 1 V 1 ? W 1 W 1 2 = V (V 1 V 1 ? W 1 W 1 )W 2 = 0 V 1 W 2 ?V 2 W 1 0 2 = max{ V 1 W 2 2 , V 2 W 1 2 },</formula><p>where the last line can be obtained from A 2 2 = max x?R N :</p><formula xml:id="formula_9">x 2 =1 Ax 2 2</formula><p>. Thus, to conclude this proof, it suffices to show that V 1 W 2 2 = V 2 W 1 2 .</p><formula xml:id="formula_10">Since W 1 W 1 + W 2 W 2 = I, V 1 W 2 2 2 = max x?R K : x 2 =1 x V 1 W 2 W 2 V 1 x = max x?R K : x 2 =1 V V 1 (I ? W 1 W 1 )V 1 x = max x?R K : x 2 =1 1 ? x V 1 W 1 W 1 V 1 x = 1 ? max x?R K : x 2 =1 x V 1 W 1 W 1 V 1 x = 1 ? ? k (V 1 W 1 W 1 V 1 ) = 1 ? ? k (V 1 W 1 ) 2 ,</formula><p>where ? k (V 1 W 1 ) is the k-th singular value of V 1 W 1 . Analogously, we can show that</p><formula xml:id="formula_11">W 1 V 2 2 2 = 1 ? ? k (V 1 W 1 ) 2 .</formula><p>Thus, we have </p><formula xml:id="formula_12">V 1 V 1 ? W 1 W 1 2 = V 1 W 2 2 = V 2 W 1 2 = 1 ? ? k (V 1 W 1 ) 2 .</formula><formula xml:id="formula_13">? i (A + B) ? ? i (A) + ? 1 (B).</formula><p>Proof. From the definition of the SVD, for any given matrix X ? R m?n . </p><formula xml:id="formula_14">? i (X) = sup V:dim(V)=i inf v?V: v 2 =1 v (A + B) 2 ? sup V:dim(V)=i inf v?V: v 2 =1 v A 2 + v B 2 ? sup V:dim(V)=i inf v?V: v 2 =1 v A 2 + B 2 ? ? i (A) + ? 1 (B).</formula><formula xml:id="formula_15">U 1:k (U 1:k ) ?? 1:k (? 1:k ) 2 ? B 2 ? k (A) ? ? k+1 (A) ? B 2 ,</formula><p>where U 1:k and? 1:k denote the first k columns of U and?, respectively.</p><p>Proof. Assume that A and A + B have non-negative eigenvalues. If not, there exists a large enough constant c to make? + cI so that? and? + B become positive semi-definite matrices. Note that A (resp. A + B) and? (resp.? + B) share the same eigenvectors and eigenvalue gaps ? i (A) ? ? i+1 (A).</p><p>From the Lemma 2, we have</p><formula xml:id="formula_16">? i (A) ? B 2 ? ? i (A + B) ? ? i (A) + B 2 .<label>(4)</label></formula><p>Thus,</p><formula xml:id="formula_17">(? k+1 (A) + B 2 )||(? k+1:n ) U k+1:n || 2 ? ||(? k+1:n ) (A + B)U 1:k || 2 (5) ? ||(? k+1:n ) AU 1:k || 2 ? B 2 (6) ? ? k (A)||(? k+1:n ) U 1:k || 2 ? B 2<label>(7)</label></formula><p>From <ref type="formula" target="#formula_17">(7)</ref>, we have</p><formula xml:id="formula_18">||U 1:k (U 1:k ) ?? 1:k (? 1:k ) || 2 ? B 2 ? k (A) ? ? k+1 (A) ? B 2 .</formula><p>Proof of (5) : Since the columns of? k+1:n are singular vectors of A + B, (? k+1:n ) (A + B)U 1:k =? k+1:n (? k+1:n ) U 1:k .</p><p>Therefore,</p><formula xml:id="formula_19">(? k+1:n ) (A + B)U 1:k 2 ? ? k+1:n 2 (? k+1:n ) U 1:k 2 = ? k+1 (A+B) (? k+1:n ) U 1:k 2 From (4), we have ? k+1 (A + B) ? ? k+1 (A) + B 2</formula><p>Proof of (6) : From the triangle inequality,</p><formula xml:id="formula_20">(? k+1:n ) AU 1:k 2 = (? k+1:n ) (A + B)U 1:k + (?? k+1:n ) BU 1:k 2 ? ? k+1:n ) (A + B)U 1:k 2 + (?? k+1:n ) BU 1:k 2 .</formula><p>We have</p><formula xml:id="formula_21">(?? k+1:n ) BU 1:k 2 ? (?? k+1:n ) 2 B 2 U 1:k 2 = B 2 .</formula><p>Proof of <ref type="formula" target="#formula_17">(7)</ref> : Since the columns of U 1:k are singular vectors of A,</p><formula xml:id="formula_22">(? k+1:n ) AU 1:k = (? k+1:n ) U 1:k ? 1:k</formula><p>Therefore, (? k+1:n ) AU 1:k 2 = (? k+1:n ) U 1:k ? 1:k 2 ? (? k+1:n ) U 1:k 2 ? k (A). Proof. Let M ? S d?d and let N be an -net of S d?1 . Furthermore, we define y ? N satisfy x ? y 2 ? . Then,</p><formula xml:id="formula_23">|x M x ? y My| = |x M(x ? y) + y M(x ? y)| ? |x M(x ? y)| + |y M(x ? y)|<label>(8)</label></formula><p>Looking at |x M(x ? y)| we have</p><formula xml:id="formula_24">|x M(x ? y)| ? M(x ? y) 2 x 2 ? M 2 x ? y 2 x 2 ? M 2<label>(9)</label></formula><p>Applying the same argument to y M(x ? y)| gives us |x M x ? y My| ? 2 M 2 . To complete the proof, we see that M 2 = max x?S d?1 x Mx ? 2 M 2 + max y?N y My. Rearranging the equation gives M 2 ? 1 1?2 max y?N y My as desired. x i x i be the empirical gram matrix. Then, there exists a universal constant C &gt; 0 such that, for ? ? (0, 1), with probability at least 1 ? ?</p><formula xml:id="formula_25">? n ? ? 2 ? 2 ? C max{ d + log(2/?) n , d + log(2/?) n }</formula><p>Proof. Applying Lemma 4 on? n ? ? with = 1/4 we have</p><formula xml:id="formula_26">? n ? ? 2 ? 2 max v?N 1/4 |v ? n ? ?v|</formula><p>Additionally, we know that N 1/4 ? 9 d . From here, we can apply standard concentration tools as follows:</p><formula xml:id="formula_27">P( ? n ? ? 2 ? t) ? P( max v?N 1/4 |v (? n ? ?)v| ? t/2) ? |N 1/4 |P(|v i (? n ? ?)v i | ? t/2)<label>(10)</label></formula><p>We</p><formula xml:id="formula_28">rewrite v i (? n ? ?)v i as follows: v i (? n ? ?)v i = 1 n n i=1 (v i x j ) 2 ? E (v i x j ) 2 = 1 n n i=1 z j ? E[z j ]</formula><p>where z j 's are independent and by assumption v i x j ? SG(? 2 ) so that z j ? E[z j ] ? SE((16? 2 ) 2 , 16? 2 ). Applying the sub-exponential tail bound gives us</p><formula xml:id="formula_29">P(v i (? n ? ?)v i | ? t/2) ? 2 exp ? n 2 min n (32? 2 ) 2 , n 32? 2 so that P( ? n ? ? 2 ? t) ? 2 ? 9 d 2 exp ? n 2 min n (32? 2 ) 2 , n 32? 2</formula><p>Inverting the bound gives the desired result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Proof of Theorem 1</head><p>Proof. Let v ? be the unit vector, which is orthogonal to v. Then, w can be expressed by v ? and v (i.e. w = cos ? ? v + sin ? ? v ? with ??/2 ? ? ? ?/2).</p><formula xml:id="formula_30">Since vv + v ? v ? = I, we have w = vv w + v ? v ? w.</formula><p>Then, we have</p><formula xml:id="formula_31">ww = (vv w + v ? v ? w)(vv w + v ? w ? w) = vv ww vv + v ? v ? ww vv + vv ww v ? v ? + v ? v ? ww v ? v ?<label>(11)</label></formula><p>Let A and B be the projection matrices for clean instances and whole instances for using the David-Kahan sin Theorem as followings:</p><formula xml:id="formula_32">A = E ? ? N+ i=1 (v + i )(v + i ) ? ? + v ? v ? ww v ? v ? + ? 2 I (12) A + B = N+ i=1 (v + i )(v + i ) + N? j=1 (w + j )(w + j )<label>(13)</label></formula><p>The difference between first eigenvalue and second eigenvalue of gram matrix A is equal to</p><formula xml:id="formula_33">? 1 (A) ? ? 2 (A) = N + ? N ? sin 2 ? ? N + ? N ? sin ?<label>(14)</label></formula><p>By triangular inequality, we have</p><formula xml:id="formula_34">B 2 ? N+ i=1 (v + i )(v + i ) ? vv ? ? 2 I 2 + N? i=1 (w + i )(w + i ) ? ww ? ? 2 I 2 + N? j=1 ww ? v ? ww v ? v ? 2<label>(15)</label></formula><p>For the first and the second terms of RHS in Eq. (15), using Lemma 5, with probability at least 1 ? ?/2, we can derive each term as followings:</p><formula xml:id="formula_35">N+ i=1 (v + i )(v + i ) ? vv ? ? 2 I 2 ? N + C? 2 max d + log(4/?) N + , d + log(4/?) N + , N? j=1 (w + j )(w + j ) ? ww ? ? 2 I 2 ? N ? C? 2 max d + log(4/?) N ? , d + log(4/?) N ? As N + , N ? ? ?, with probability at least 1 ? ? 1 N + ? ? N+ i=1 (v + i )(v + i ) ? vv ? ? 2 I 2 + N? j=1 (w + j )(w + j ) ? ww ? ? 2 I 2 ? ? ? C? 2 d + log(4/?) N + + N ? N + d + log(4/?) N ? = O ? 2 d + log(4/?) N + + ? 2 ? d + log(4/?) N ?<label>(16)</label></formula><p>For the third term of RHS in Eq. (15), we have  .</p><formula xml:id="formula_36">N? j=1 ww ? v ? ww v ? v ? 2 = N ? ? vv ww vv + v ? v ? ww vv + vv ww v ? v ? 2 ? N ? ? 3 cos ?<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Additional Theorem</head><p>After projecting the features on the principal component of FINE detector, we aim to guarantee the lower bounds for the precision and recall of such values with high probability. Since the feature distribution comprises two Gaussian distributions, the projected distribution is also a mixture of two Gaussian distributions. Here, by LDA assumptions, its decision boundary with ? = 0.5 is the same as the average of mean of two clusters. In this situation, we provide the lower bounds for the precision and recall of our FINE detector in Theorem 2.</p><p>Theorem 2. Let ? be the cumulative distribution function (CDF) of N (0, 1). Additionally, we define the z i as linear projection of x i on arbitrary vector. For the decision boundary b =</p><formula xml:id="formula_38">1 2 ( N i=1 1 {y i =1} zi N+ + N i=1 1 {y i =?1} zi N? )</formula><p>, with probability 1-?, the lower bounds for the precision and recall can be derived as Eq. <ref type="formula" target="#formula_4">(21)</ref>  </p><formula xml:id="formula_39">N i=1 1 {Y i =1} Zi N+ ? N ((u v) 2 , ? 2 N+ ) and N i=1 1 {Y i =?1} Zi N? ? N ((u w) 2 , ? 2 N? ). Furthermore, we can get N i=1 1 {Y i =1} Zi N+ + N i=1 1 {Y i =?1} Zi N? ? N ((u v) 2 + (u w) 2 , ( 1 N+ + 1 N? )? 2 )</formula><p>By the concentration inequality on standard Gaussian distribution, we have</p><formula xml:id="formula_40">P N i=1 1 {Yi=1} Z i N + + N i=1 1 {Yi=?1} Z i N ? ? ((u v) 2 + (u w) 2 ) &gt; ? &lt; 2 exp (? ? 2 2 ? 1 ? 2 N+ + ? 2 N? )<label>(19)</label></formula><p>Therefore, with probability 1 ? ?,</p><formula xml:id="formula_41">(u v) 2 + (u w) 2 2 ? C 1 N + + 1 N + log(2/?) ? b b ? (u v) 2 + (u w) 2 2 + C 1 N + + 1 N + log(2/?)<label>(20)</label></formula><p>where C &gt; 0 is a constant. Then, by using the Eq. (20), we can derive the lower bound for the recall as follows:</p><formula xml:id="formula_42">RECALL = P(Z &gt; b|Y = +1) = P (Z &gt; b|Y = +1) ? P(Z &gt; ? + + ? ? 2 + C 1 N + + 1 N + log(2/?)|Y = +1) = P( Z ? ? + ? &gt; ?? + + ? ? 2? + C 1 N+ + 1 N+ log(2/?) ? ) = P(N (0, 1) &gt; ?? + 2C 1 N+ + 1 N+ log(2/?) 2? ) = 1 ? P(N (0, 1) ? ?? + 2C 1 N+ + 1 N+ log(2/?) 2? ) = 1 ? ?( ?? + 2C 1 N+ + 1 N+ log(2/?) 2? ) = ?( ? ? 2C 1 N+ + 1 N+ log(2/?) 2? )<label>(21)</label></formula><p>Furthermore, we have lower bound for precision as follows:</p><formula xml:id="formula_43">PRECISION = P(Y = +1|Z &gt; b) = P(Z &gt; b|Y = +1)P (Y = +1) i?{?1,+1} P(Z &gt; b|Y = i)P (Y = i) ? P(Z &gt; ?++?? 2 + C 1 N+ + 1 N+ log(2/?)|Y = +1)P (Y = +1) i?{?1,+1} P(Z &gt; ?++?? 2 ? C 1 N+ + 1 N+ log(2/?)|Y = i)P (Y = i) = P(Z &gt; ?++?? 2 + C 1 N+ + 1 N+ log(2/?)|Y = +1)P (Y = +1) i?{?1,+1} P(Z &gt; ?++?? 2 ? C 1 N+ + 1 N+ log(2/?)|Y = i)P (Y = i) ? 1 1 + P(Z&gt; ? + +? ? 2 +C 1 N + + 1 N + log(2/?)|Y =?1)P (Y =?1) P(Z&gt; ? + +? ? 2 ?C 1 N + + 1 N + log(2/?)|Y =+1)P (Y =+1) = 1 1 + p???( ???2C 1 N + + 1 N + log(2/?) 2? ) p+??( ??2C 1 N + + 1 N + log(2/?) 2? )<label>(22)</label></formula><p>where ? := u v ? u w, and p + and p ? are the noise distribution for clean instances and noisy instances, respectively. Additionally, we can find that the difference of mean between two Gaussian distribution, ? &gt; 0 is an important factor of computing both lower bounds. As ? become larger, we have larger lower bounds for both recall and precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details for section 4</head><p>In section 4, there are two reporting styles regarding the test accuracy: (1) reporting the accuracy with statistics and (2) reporting the best and last test accuracy. For the first one, we leverage an additional validation set to select the best model <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>, and thus the reported accuracy is computed with this selected model. In the next case, we report both the best and last test accuracy without the usage of validation set <ref type="bibr" target="#b24">[25]</ref>. We reproduce all experimental results referring to other official repositories <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5</ref> .</p><p>Dataset. In our expereiments, we compare the methods regarding image classification on three benchmark datasets: CIFAR-10, CIFAR-100, and Clothing-1M [44] 6 . Because CIFAR-10, CIFAR-100 do not have predefined validation sets, we retain 10% of the training sets to perform validation <ref type="bibr" target="#b28">[29]</ref>.</p><p>Data Preprocessing We use the same settings in <ref type="bibr" target="#b28">[29]</ref>. We apply normalization and simple data augmentation techniques (random crop and horizontal flip) on the training sets of all datasets. The size of the random crop is set to 32 for the CIFAR datasets and 224 for Clothing1M referred to previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr">51,</ref><ref type="bibr" target="#b18">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Sample-Selection Approaches</head><p>As an extension on the experiments in original papers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">47,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32]</ref>, we conduct experiments on various noise settings. We use the same hyperparameter settings written in each paper (algorithm 2). Therefore, we unify the hyperparameter settings. In this experiment, we use ResNet-34 models and reported their accuracy. For using FINE detector, we substitute the Topofilter <ref type="bibr" target="#b41">[42]</ref> with FINE. Specifically, we use 40 epochs for warmup stage, and the data selection using FINE detector is performed every 10 epochs for computational efficiency referred to the alternative method <ref type="bibr" target="#b41">[42]</ref>. The other settings are the same with them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Semi-Superivsed Approaches</head><p>DivideMix <ref type="bibr" target="#b24">[25]</ref> solves a noisy classification challenge as semi-supervised approach. It trains two separated networks to avoid confirmation errors. The training pipeleine consists of co-divide phase and semi-supervised learning (SSL) phase. Firstly, in co-divide phase, two networks divide the whole training set into clean and noisy subset and provide them to each other. In SSL phase, each network utilizes clean and noisy subset as labeled and unlabeled training set, respectively, and do the Mix-Match <ref type="bibr" target="#b3">[4]</ref> after processing label adjustment, co-refinement and co-guessing. It adjusts the labels of given samples with each model's prediction, and this adjustment can be thought as a label smoothing for robust training.</p><p>In co-divide phase, each network calculates cross-entropy (CE) loss value of each training sample and fits them into Gaussian Mixture Model (GMM) with two components which indicate the distribution of clean and noisy subsets. From this process, each sample has clean probability which means how close the sample is to the 'clean' components of GMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Collaboration with Noise-Robust Loss Functions</head><p>We conduct experiments with CE, GCE, SCE, ELR mentioned in subsubsection 4.2.3. We follow all experiments settings presented in the <ref type="bibr" target="#b28">[29]</ref> except for the GCE on CIFAR-100 dataset. We use ResNet-34 models and trained them using a standard Pytorch SGD optimizer with a momentum of 0.9. We set a batch size of 128 for all experiments. We utilize weight decay of 0.001 and set the initial learning rate as 0.02, and reduce it by a factor of 100 after 40 and 80 epochs for CIFAR-10 (total 120 epochs) and after 80 and 120 epochs for CIFAR-100 (total 150 epochs). For noise-robust loss functions, we train the network naively for 50 epochs, and conduct the FINE for every 10 epochs.</p><p>C More Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Degree of Alignment</head><p>We additionally explain the vital role of the first eigenvector compared to other eigenvectors and the mean vector.</p><p>Comparison to other eigenvectors. We provide robustness by means of the way the first eigenvector is robust to noisy vectors so that FINE can fix the noisy classifier by using segregated clean data. Unlike the first eigenvector, the other eigenvectors can be significantly affected by noisy data (  Comparison to the mean vector. The mean vector can be a nice ad-hoc solution as a decision boundary. This is because the first eigenvector of the gram matrix and the mean vector of the cluster become similar under a low noise ratio. However, because the gram matrix of the cluster becomes larger in a high noise ratio scenario, naive averaging can cause a lot of perturbation. On the other side, because the first eigenvector arises from the principal component of the representation vectors, FINE is more robust to noisy representations so that it has less perturbation and provides better performance.</p><p>To support this explanation, we performed additional experiments by changing the anchor point with the first eigenvector and the mean vector. As  <ref type="table" target="#tab_10">Table 5</ref>: Comparison of test accuracies on the CIFAR-10 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Detailed values for Figure 7</head><p>We provide the detailed values for <ref type="figure" target="#fig_8">Figure 7</ref> in <ref type="table" target="#tab_11">Table 6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Hyperparameter sensitivity towards ?</head><p>We perform additional experiments; we report the test accuracy and f1-score on CIFAR-10 with a symmetric noise ratio of 80% across the value of the hyperparameter ( <ref type="table">Table 7</ref>). We can observe that the performance change is small in the acceptable range from 0.4 to 0.6.  <ref type="table">Table 7</ref>: Sensitivity analysis for hyperparameter zeta on CIFAR-10 with symmetric noise 80%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Feature-dependent Label Noise</head><p>We additionally conducted experiments with our FINE methods on the feature-dependent noise labels dataset (noise rates of 20% and 40% by following the experimental settings of CORES <ref type="bibr" target="#b31">[32]</ref>) <ref type="table" target="#tab_13">(Table 8</ref>). To compare our FINE to CORES 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Filtering Time Analysis</head><p>We compare the training times per one epoch of FINE with other filtering based methods, using a single Nvidia GeForce RTX 2080. We also report the computational time when the different number of data is used for eigen decomposition. We discover that there remain little difference as the number of instances is differently used for eigen decomposition. As <ref type="table" target="#tab_14">Table 9</ref> shows, the computational efficiency for FINE can be obtained without any degradation issues.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Other Real-world Noisy Datasets</head><p>We conduct additional experiments with our FINE method on the mini Webvision dataset for comparison with state-of-the-art methods ( <ref type="table" target="#tab_16">Table 10</ref>). In the comparison with CRUST <ref type="bibr" target="#b31">[32]</ref>, which is the state-of-the-art sample selection method, our method achieved 75.24% while CRUST achieved 72.40% on the test dataset of (mini) Webvision. Looking at the results, the difference between Dividemix <ref type="bibr" target="#b24">[25]</ref> and F-Dividemix is marginal <ref type="table" target="#tab_16">(Table 10</ref>). However, the reason for this is that we have to reduce the batch size due to the limitation of our current GPU, and we cannot do hyperparameter tuning (e.g. weight decay, learning rate). The final version will be able to run experiments by supplementing this issue, and it is expected that the performance will be improved.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of (a) basic concept of this work and (b) proposed detection framework, FINE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Assumption 2 .</head><label>2</label><figDesc>The features of all instances with y = +1 are aligned on the unit vector v with the white noise, i.e., E x?X [x] = v. Similarly, features of all instances with y = ?1 are aligned on the unit vector w, i.e., E x?X [x] = w. Theorem 1. (Upper bound for the perturbation towards the clean data's eigenvector v) Let N + and N ? be the number of clean instances and noisy instances, respectively, and u be the FINE's eigenvector which is the first column of U from the eigen decomposition of the whole data's matrix ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a), (b): Heatmaps of Eq. (2) values on unit circle in random hyperplane. We evaluate this visualization on the ResNet34 model trained with common cross-entropy loss on CIFAR-10 with asymmetric noise 40% and CIFAR-100 with symmetric noise 80%, respectively. Colors closer to yellow indicate larger the values; (c): comparison of perturbations of Eq. (1) on CIFAR-10 with symmetric noise 20%; (d): comparison of cosine similarity values between FINE's principal components and approximated principal components using fraction of data on CIFAR-10 with symmetric noise 80%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc><ref type="bibr" target="#b1">(2)</ref>. This visualization shows in 2-D how FINE's first eigenvector (u) optimizes such values in the presence of noisy instances(Figure 3a and 3b). As the figures show, the FINE's eigenvector u (red dotted line) has almost maximum value of Eq. (2). Furthermore, we empirically evaluate the perturbation values in Theorem 1 as the noise rate changes(Figure 3c); FINE has small perturbation values even in a severe noise rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Comparisons of F-scores on CIFAR-10 and CIFAR-100 under symmetric (S) and asymmetric noise (A) settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Comparisons of F-scores on CIFAR-10 under symmetric 90% noise. Blue line indicates the error bar of two networks' F-score used in Dividemix<ref type="bibr" target="#b24">[25]</ref>, and Orange line indicates those replaced by our FINE detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Test accuracies (%) on CIFAR-10 and CIFAR-100 under different noisy types and fractions for noise-robust loss approaches. Note that the blue and orange bars are results for without and with FINE, respectively. The average accuracies and standard deviations over three trials are reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Lemma 2 .</head><label>2</label><figDesc>(Weyl's Theorem) For any real matrices A, B ? R m?n ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Lemma 3 .</head><label>3</label><figDesc>(David-Kahan sin Theorem) For given symmetric matrices A, B ? R n?n , let A = U?U and A + B =??? be eigenvalue decomposition of A and A + B. Then,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Lemma 4 .</head><label>4</label><figDesc>Let M ? S d?d and let N be an -net of S d?1 . Then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Lemma 5 .</head><label>5</label><figDesc>Let x 1 , . . . , x n be an i.i.d sequence of ? sub-gaussian random vectors such that V[x 1 ] = ? and let? n := 1 N n i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>and Eq. (22). Proof. N + and N ? are equal to N i=1 1 {Yi=1} and N i=1 1 {Yi=?1} , respectively. With definition 2, the mean of the projection values of clean instances is (u v) 2 and that of noisy instances is (u w) 2 . By the central limit theorem (CLT), we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>4, during the training process, F-scores of FINE becomes consistently higher on both symmetric</figDesc><table><row><cell>0.0 0.2 0.4 0.6 0.8</cell><cell>0</cell><cell>20 CIFAR 10 Sym 80%. 40 60 80 100 120 FINE Co-teaching TopoFilter</cell><cell>0.0 0.1 0.2 0.3 0.4 0.5 0.6</cell><cell>0 20 40 60 80 100 120 140 CIFAR 100 Sym 80%.</cell><cell>0.5 0.6 0.7 0.8 0.9</cell><cell>0 20 40 60 80 100 120 140</cell></row><row><cell></cell><cell></cell><cell>(a) Sym 80% (C10)</cell><cell></cell><cell>(b) Sym 80% (C100)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of test accuracies (%) for FINE collaborating with DivideMix and existing semi-supervised approaches on CIFAR-10 and CIFAR-100 under different noisy types and fractions. The results for all comparison methods are taken from their original works.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell></row><row><cell></cell><cell>Noisy Type</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sym</cell><cell></cell><cell>Asym</cell><cell></cell><cell cols="2">Sym</cell></row><row><cell></cell><cell>Noise Ratio</cell><cell></cell><cell></cell><cell>20</cell><cell>50</cell><cell>80</cell><cell>90</cell><cell>40</cell><cell>20</cell><cell>50</cell><cell>80</cell><cell>90</cell></row><row><cell></cell><cell>DivideMix [25]</cell><cell cols="2">Best Last</cell><cell>96.1 95.7</cell><cell>94.6 94.4</cell><cell>93.2 92.9</cell><cell>76.0 75.4</cell><cell>93.4 92.1</cell><cell>77.3 76.9</cell><cell>74.6 74.2</cell><cell>60.2 59.6</cell><cell>31.5 31.0</cell></row><row><cell></cell><cell>DST [41]</cell><cell cols="2">Best Last</cell><cell>96.1 95.9</cell><cell>95.2 94.7</cell><cell>92.9 92.6</cell><cell>--</cell><cell>94.3 92.3</cell><cell>78.0 77.4</cell><cell>74.3 73.6</cell><cell>57.8 55.3</cell><cell>--</cell></row><row><cell></cell><cell>LongReMix [10]</cell><cell cols="2">Best Last</cell><cell>96.2 96.0</cell><cell>95.0 94.7</cell><cell>93.9 93.4</cell><cell>82.0 81.3</cell><cell>94.7 94.3</cell><cell>77.8 77.5</cell><cell>75.6 75.1</cell><cell>62.9 62.3</cell><cell>33.8 33.2</cell></row><row><cell></cell><cell>F-DivideMix</cell><cell cols="2">Best Last</cell><cell>96.1 96.0</cell><cell>94.9 94.5</cell><cell>93.5 93.2</cell><cell>90.5 89.6</cell><cell>93.8 92.8</cell><cell>79.1 78.8</cell><cell>74.6 74.3</cell><cell>61.0 60.1</cell><cell>34.3 31.2</cell></row><row><cell>80</cell><cell>CIFAR10 Sym 80%.</cell><cell cols="4">95 CIFAR10 Asym 40%.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>55 60 65 70 75</cell><cell></cell><cell>75 80 85 90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50</cell><cell>CE GCE SCE ELR</cell><cell>70</cell><cell cols="4">CE GCE SCE ELR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a) Sym 80% (C10)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy on Clothing1M dataset</figDesc><table><row><cell>Method</cell><cell>Standard</cell><cell>GCE [51]</cell><cell>SCE [40]</cell><cell>ELR [29]</cell><cell>DivideMix [25]</cell><cell>CORES 2 [8]</cell><cell>FINE</cell><cell>F-DivideMix</cell></row><row><cell>Accuracy</cell><cell>68.94</cell><cell>69.75</cell><cell>71.02</cell><cell>72.87</cell><cell>74.30</cell><cell>73.24</cell><cell>72.91</cell><cell>74.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>47] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? arXiv preprint arXiv:1901.04215, 2019. [48] Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary labels. In Proceedings of the European conference on computer vision (ECCV), pages 68-83, 2018. [49] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.</figDesc><table><row><cell>[50] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond</cell></row><row><cell>empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.</cell></row><row><cell>[51] Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural</cell></row><row><cell>networks with noisy labels. arXiv preprint arXiv:1805.07836, 2018.</cell></row><row><cell>[52] Zizhao Zhang, Han Zhang, Sercan O Arik, Honglak Lee, and Tomas Pfister. Distilling effective</cell></row><row><cell>supervision from severe label noise. In Proceedings of the IEEE/CVF Conference on Computer</cell></row><row><cell>Vision and Pattern Recognition, pages 9294-9303, 2020.</cell></row></table><note>[53] Zizhao Zhang, Han Zhang, Sercan O. Arik, Honglak Lee, and Tomas Pfister. Distilling effective supervision from severe label noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Hence, by using Eq. (14), Eq. (16), and Eq. (17) for Lemma 3, when ? is sufficiently small, we have</figDesc><table><row><cell>uu ? vv 2 ?</cell><cell>3? cos ? + O(? 2 d+log(4/?) N+ 1 ? ? (sin ? + 3 cos ?) ? O(? 2 d+log(4/?) ) N+</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>).</cell><cell></cell></row><row><cell>Noise</cell><cell cols="2">1st eigenvector 2nd eigenvector</cell></row><row><cell cols="2">sym 20 0.015 ? 0.009</cell><cell>0.043 ? 0.021</cell></row><row><cell cols="2">sym 50 0.029 ? 0.019</cell><cell>0.078 ? 0.044</cell></row><row><cell cols="2">sym 80 0.057 ? 0.038</cell><cell>0.135 ? 0.052</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the perturbations of Eq. (1) ( uu ? vv ) on CIFAR-10 with symmetric noise. The values in the table are written as mean (std) of the perturbations between u and v obtained for each class.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5</head><label>5</label><figDesc>shows, the performance degradation occurs as the noise ratio increases by replacing the first eigenvector with the mean vector</figDesc><table><row><cell>Noise</cell><cell cols="2">sym 20</cell><cell cols="2">sym 50</cell><cell cols="2">sym 80</cell></row><row><cell></cell><cell>mean</cell><cell>eigen</cell><cell>mean</cell><cell>eigen</cell><cell>mean</cell><cell>eigen</cell></row><row><cell cols="2">Acc (%) 90.32</cell><cell>91.42</cell><cell>86.03</cell><cell>87.20</cell><cell>67.78</cell><cell>71.55</cell></row><row><cell cols="7">F-score 0.8814 0.9217 0.4879 0.8626 0.6593 0.7339</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Test accuracies (%) on CIFAR-10 and CIFAR-100 under different noisy types and fractions for noise-robust loss approaches. The average accuracies and standard deviations over three trials are reported.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">CIFAR-10</cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Noisy Type</cell><cell></cell><cell>Sym</cell><cell></cell><cell>Asym</cell><cell></cell><cell>Sym</cell><cell></cell><cell>Asym</cell></row><row><cell>Noise Ratio</cell><cell>20</cell><cell>50</cell><cell>80</cell><cell>40</cell><cell>20</cell><cell>50</cell><cell>80</cell><cell>40</cell></row><row><cell>Standard</cell><cell>87.0 ? 0.1</cell><cell>78.2 ? 0.8</cell><cell>53.8 ? 1.0</cell><cell>80.1 ? 1.4</cell><cell>58.7 ? 0.3</cell><cell>42.5 ? 0.3</cell><cell>18.1 ? 0.8</cell><cell>42.7 ? 0.6</cell></row><row><cell>GCE</cell><cell>89.8 ? 0.2</cell><cell>86.5 ? 0.2</cell><cell>64.1 ? 1.4</cell><cell>76.7 ? 0.6</cell><cell>66.8 ? 0.4</cell><cell>57.3 ? 0.3</cell><cell>29.2 ? 0.7</cell><cell>47.2 ? 1.2</cell></row><row><cell>SCE*</cell><cell>89.8 ? 0.3</cell><cell>84.7 ? 0.3</cell><cell>68.1 ? 0.8</cell><cell>82.5 ? 0.5</cell><cell>70.4 ? 0.1</cell><cell>48.8 ? 1.3</cell><cell>25.9 ? 0.4</cell><cell>48.4 ? 0.9</cell></row><row><cell>ELR*</cell><cell>91.2 ? 0.1</cell><cell>88.2 ? 0.1</cell><cell>72.9 ? 0.6</cell><cell>90.1 ? 0.5</cell><cell>74.2 ? 0.2</cell><cell>59.1 ? 0.8</cell><cell>29.8 ? 0.6</cell><cell>73.3 ? 0.6</cell></row><row><cell>FINE</cell><cell>91.0 ? 0.1</cell><cell>87.3 ? 0.2</cell><cell>69.4 ? 1.1</cell><cell>89.5 ? 0.1</cell><cell>70.3 ? 0.2</cell><cell>64.2 ? 0.5</cell><cell>25.6 ? 1.2</cell><cell>61.7 ? 1.0</cell></row><row><cell>GCE + FINE</cell><cell>91.4 ? 0.1</cell><cell>86.9 ? 0.1</cell><cell>75.3 ? 1.2</cell><cell>88.9 ? 0.3</cell><cell>70.5 ? 0.1</cell><cell>61.5 ? 0.5</cell><cell>37.0 ? 2.1</cell><cell>62.4 ? 0.5</cell></row><row><cell>SCE + FINE</cell><cell>90.4 ? 0.2</cell><cell>85.1 ? 0.2</cell><cell>70.5 ? 0.8</cell><cell>86.9 ? 0.3</cell><cell>70.9 ? 0.3</cell><cell>64.1 ? 0.7</cell><cell>29.9 ? 0.8</cell><cell>64.3 ? 0.3</cell></row><row><cell>ELR + FINE</cell><cell>91.5 ? 0.1</cell><cell>88.5 ? 0.1</cell><cell>74.7 ? 0.5</cell><cell>91.1 ? 0.2</cell><cell>74.9 ? 0.2</cell><cell>66.7 ? 0.4</cell><cell>32.5 ? 0.5</cell><cell>73.8 ? 0.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Comparison of test accuracies on clean datasets under feature-based label noise.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Filtering Time Analysis on CIFAR-10 dataset</figDesc><table><row><cell>DivideMix [25]</cell><cell>FINE</cell><cell>FINE using 1% dataset</cell><cell>F-Dividemix</cell><cell>F-Dividemix using 1% dataset</cell></row><row><cell>2.2s</cell><cell>20.1s</cell><cell>1.1s</cell><cell>40.2s</cell><cell>2.1s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Comparison with state-of-the-art methods trained on (mini) WebVision dataset. Numbers denote top-1 (top-5) accuracy (%) on the WebVision validation set and the ImageNet ILSVRC12 validation set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">T-shirt, Shirt, Knitwear, Chiffon, Sweater, Hoodie, Windbreaker, Jacket, Down Coat, Suit, Shawl, Dress, Vest, and Underwear. The labels in this dataset are extremely noisy (estimated noisy level is 38.5%)<ref type="bibr" target="#b36">[37]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/bhanML/Co-teaching 4 https://github.com/LiJunnan1992/DivideMix 5 https://github.com/shengliu66/ELR 6 This dataset is not public, and thus we contact the main owner of this dataset to access this dataset. Related procedures are in https://github.com/Cysu/noisy_label.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We demonstrate that FINE detector may be a substitute for the noisy detector in co-divide phase (algorithm 3). In every training epoch in DivideMix, the noisy instances are filtered through our FINE detector. algorithm 3 represents the details about the modified algorithm, written based on Dividemix original paper. All hyper-parameters settings are the same with <ref type="bibr" target="#b24">[25]</ref>, even for the clean probability threshold ?.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for (x i , y i ) ? D do 4:</p><p>Update the gram matrix ? yi ? ? yi + z i z i 6:</p><p>end for /* Generate the principal component with eigen decomposition */ 7:</p><p>for k = 1, . . . , K do 8:</p><p>u k ? THE FIRST COLUMN OF U k 10:</p><p>end for /* Compute the alignment score and get clean subset C */ <ref type="bibr">11:</ref> for (x i , y i ) ? C e?1 do 12:</p><p>Compute the FINE score</p><p>end for /* Finding the samples whose clean probability is larger than ? */ <ref type="bibr" target="#b13">14</ref>:</p><p>Train network ? using loss function L on C e 16: end while for k = 1, 2 do 6:</p><p>for b = 1 to B do 9:</p><p>for m = 1 to M do 10:</p><p>end for 13: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep over-sampling framework for classifying imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun Yuan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="770" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep k-nn for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="540" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="259" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15766</idno>
		<title level="m">Nikos Arechiga, Adrien Gaidon, and Tengyu Ma. Heteroskedastic and imbalanced deep learning with adaptive regularization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning with instance-dependent label noise: A sample sieve approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02347</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Leveraging weakly annotated data for fashion image retrieval and label prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Corbiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Ram?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Ollion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2268" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Longremix: Robust learning with high confidence samples in a noisy label environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ragav</forename><surname>Filipe R Cordeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Sachdeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carneiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04173</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Haase-Schutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinz</forename><surname>Hertlein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudius</forename><surname>Glaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Wiesbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Dietmayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1341" to="1360" />
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The use of multiple measurements in taxonomic problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A baseline for detecting misclassified and out-ofdistribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02136</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05300</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">O2u-net: A simple noisy label detection approach for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lie</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongfei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3326" to="3334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nakyil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangwook</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se-Young</forename><surname>Yun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial filters of dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1078" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3763" to="3772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dividemix: Learning with noisy labels as semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4313" to="4324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Repair: Removing representation bias by dataset resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9572" to="9581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Enhancing the reliability of out-ofdistribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayadurgam</forename><surname>Srikant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02690</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00151</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Does label smoothing mitigate label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6448" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Maennel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Baldock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keysers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10455</idno>
		<title level="m">What do neural networks learn when trained with random labels? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Baharan Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.07451</idno>
		<title level="m">Coresets for robust training of neural networks against noisy labels</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Identifying mislabeled data using the area under the margin ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10528</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Prestopping: How does early stopping help generalization against label noise?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning with symmetric label noise: The importance of being unhinged</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Van Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Williamson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07634</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from noisy large-scale datasets with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="839" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dst: Data selection and joint training for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00813</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A topological filter for learning with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songzhu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04835</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust early-learning: Hindering the memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">L_dmi: A novel information-theoretic loss function for training deep nets robust to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6222" to="6233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Robust federated learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoungseob</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Byun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
