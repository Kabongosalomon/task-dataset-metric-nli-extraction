<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Receptive Field Analysis of Temporal Convolutional Networks for Monaural Speech Dereverberation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Ravenscroft</surname></persName>
							<email>jwravenscroft1@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Goetze</surname></persName>
							<email>s.goetze@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hain</surname></persName>
							<email>t.hain@sheffield.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Sheffield</orgName>
								<address>
									<settlement>Sheffield</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Receptive Field Analysis of Temporal Convolutional Networks for Monaural Speech Dereverberation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speech</term>
					<term>dereverberation</term>
					<term>temporal convolutional network</term>
					<term>enhancement</term>
					<term>sequence modelling</term>
					<term>tasnet</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech dereverberation is often an important requirement in robust speech processing tasks. Supervised deep learning (DL) models give state-of-the-art performance for singlechannel speech dereverberation. Temporal convolutional networks (TCNs) are commonly used for sequence modelling in speech enhancement tasks. A feature of TCNs is that they have a receptive field (RF) dependent on the specific model configuration which determines the number of input frames that can be observed to produce an individual output frame. It has been shown that TCNs are capable of performing dereverberation of simulated speech data, however a thorough analysis, especially with focus on the RF is yet lacking in the literature. This paper analyses dereverberation performance depending on the model size and the RF of TCNs. Experiments using the WHAMR corpus which is extended to include room impulse responses (RIRs) with larger T60 values demonstrate that a larger RF can have significant improvement in performance when training smaller TCN models. It is also demonstrated that TCNs benefit from a wider RF when dereverberating RIRs with larger RT60 values.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Speech dereverberation is often an important requirement in robust speech processing tasks. Supervised deep learning (DL) models give state-of-the-art performance for singlechannel speech dereverberation. Temporal convolutional networks (TCNs) are commonly used for sequence modelling in speech enhancement tasks. A feature of TCNs is that they have a receptive field (RF) dependent on the specific model configuration which determines the number of input frames that can be observed to produce an individual output frame. It has been shown that TCNs are capable of performing dereverberation of simulated speech data, however a thorough analysis, especially with focus on the RF is yet lacking in the literature. This paper analyses dereverberation performance depending on the model size and the RF of TCNs. Experiments using the WHAMR corpus which is extended to include room impulse responses (RIRs) with larger T60 values demonstrate that a larger RF can have significant improvement in performance when training smaller TCN models. It is also demonstrated that TCNs benefit from a wider RF when dereverberating RIRs with larger RT60 values.</p><p>Index Terms-speech, dereverberation, temporal convolutional network, enhancement, sequence modelling, tasnet</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In far-field recording environments, reverberation affects the quality and intelligibility of the recorded audio signal <ref type="bibr" target="#b0">[1]</ref>. This remains a problem for many domains in speech technology <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Dereverberation of speech signals has been studied thoroughly over the past decades <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> based on machine learning models and signal processing (SP) techniques <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Most SP approaches model reverberant speech as a mixture of the anechoic speech signal summed with delayed, exponentially decaying weighted sums of itself. The sequence of weights used in this summation is commonly referred to as the RIR, which is typically modelled in three parts: the direct path, the early reflections (ERs) and the late reflections (LRs) <ref type="bibr" target="#b5">[6]</ref>. ERs in speech are typically assumed to occur within the first 50 ms after the direct path. SP methodologies for suppressing reverberant content in speech signals range from a number of techniques with the most prominent approaches in recent work using spectral suppression or linear predictive modelling <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>DL models have mostly surpassed pure SP approaches for enhancing reverberant speech signals on objective measures such as word error rate (WER) or perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Time-domain audio separation networks (TasNets) <ref type="bibr" target="#b11">[12]</ref> were proposed for speech separation which were later applied also to dereverberation <ref type="bibr" target="#b12">[13]</ref>. Convolutional TasNets (Conv-TasNets) <ref type="bibr" target="#b13">[14]</ref> replace the BLSTM network of TasNets with a fully convolutional model using a TCN <ref type="bibr" target="#b14">[15]</ref>. TCNs have also shown to be effective at more general speech enhancement tasks including dereverberation <ref type="bibr" target="#b15">[16]</ref>. A dereverberation network using a TCN with self attention was proposed in <ref type="bibr" target="#b10">[11]</ref> which demonstrated that TCN models give competitive results with other state-of-theart techniques such as deep neural network (DNN) weighted prediction error (WPE) models.</p><p>In this work, Conv-TasNets are analysed for application to monaural dereverbation of speech. The main focus is to analyse the interplay between RF, model size and RIR length on the capability of Conv-TasNets to dereverb speech.</p><p>The remainder of the paper proceeds as follows, Section II describes the signal model, in Section III Conv-TasNet is formulated as a denoising autoencoder (DAE), in Section IV the data and experimental setup are discussed, in Section V results are presented and Section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SIGNAL MODEL</head><p>A discrete single channel reverberant speech signal</p><formula xml:id="formula_0">x[i] = h[i] * s[i] = s dir [i] + s rev [i]<label>(1)</label></formula><p>for discrete time index i can be decomposed into direct-path signal s dir [i] = ?s[i ? i 0 ], with a delay i 0 and possible attenuation by a factor ?, and reverberant part s rev [i]. In (1), h[i] is the RIR and * denotes the convolution operation. The signal of length L s can be split into L x blocks of length L BL with a 50% overlap and block index defined as</p><formula xml:id="formula_1">x = [x[0.5( ? 1)L BL ], . . . , x[0.5(1 + )L BL ? 1]] (2)</formula><p>The aim of this paper is to estimate the values of</p><formula xml:id="formula_2">s dir = [s dir [0], . . . , s dir [L s ? 1]] denoted as? = [?[0], . . . ,?[L s ? 1]].</formula><p>III. DEREVERBERATION NETWORK The dereverberation network is based on reformulating Conv-TasNet as a DAE composed of an encoder, a mask estimation network and a decoder [?], <ref type="bibr" target="#b13">[14]</ref>. The audio blocks x are encoded into feature vectors w . The mask estimation network produces a sequence of masks from the encoded signal. The masks m are then multiplied with the encoded features w to produce a sequence of output features that are decoded back into the time domain signal by the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2204.06439v3 [cs.SD] 1 Jul 2022</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Encoder</head><p>The input signal blocks x ? R 1?LBL are encoded by a 1D convolutional layer with a rectified linear unit (ReLU) activation function, H enc , such that</p><formula xml:id="formula_3">w = H enc (x B) ,<label>(3)</label></formula><p>where B ? R LBL?N is a matrix of trainable weights and w is the encoded feature vector for the th signal block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. TCN Mask Estimation Network</head><p>The mask estimation network produces a mask m for every block . The encoded features w are first normalized using channelwise layer normalization <ref type="bibr" target="#b16">[17]</ref>. The normalized features are transformed by a pointwise convolutional layer <ref type="bibr" target="#b13">[14]</ref> which reduces the feature dimension from N to B. The sequence of features is then processed by a stack of X convolutional blocks (CBs) with increasing the dilation f of a factor of two per CB, i.e. f ? {1, . . . , 2 X?1 }. Each CB is comprised of a pointwise convolutional layer, a parametric ReLU (PReLU) activation function, global layer normalization (gLN) <ref type="bibr" target="#b13">[14]</ref>, and a depthwise separable convolutional layer <ref type="bibr" target="#b13">[14]</ref>. The pointwise convolutional layer has B input channels and H output channels. The depthwise separable convolutional layer has H input channels and B output channels. The X CBs of increasing dilation is repeated R times. This repetition widens the RF of the network to a lower degree than continuing to increase the dilation whilst also producing a deeper layered network with more parameters per second of the RF. The RF of the TCN, measured in seconds, is defined as</p><formula xml:id="formula_4">R(L BL , R, X, P ) = L BL 2f s 1+R(P ?1) X i=1 2 X?i (4)</formula><p>where P is the kernel size in the CB and f s defines the sampling rate in Hz. Proceeding the CBs is a PReLU activation function, followed by a pointwise convolutional layer which transforms the feature dimension from B to N . A ReLU activation function is used to produce a set of non negative masks defined as m ? R 1?N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Decoder</head><p>The decoder is a transposed 1D convolutional layer that decodes the masked encoded mixture v = m w back into the time domain. The operator denotes the Hadamard product. The transposed convolutional operation is defined a?</p><formula xml:id="formula_5">s = (m w )U = v U<label>(5)</label></formula><p>where U ? R N ?LBL and? is the decoded time domain block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Objective Function</head><p>The scale-invariant signal-to-distortion ratio (SISDR) objective function <ref type="bibr" target="#b17">[18]</ref> is used for training the DAE network. To use SISDR as an objective function, the negative SISDR is computed such that the network is optimized to maximize the SISDR of the estimated speech signal. The SISDR loss function is defined as</p><formula xml:id="formula_6">L SISDR (?, s dir ) := ?10 log 10 ?,s dir s dir s dir 2 2 ? ? ?,s dir s dir s dir 2 2 .<label>(6)</label></formula><p>IV. DATA AND EXPERIMENTS A. Data WHAMR <ref type="bibr" target="#b15">[16]</ref>, a monaural noisy reverberant two speaker speech corpus, and an extension of WHAMR, denoted as WHAMR ext in the following, are used for all experiments. Only the first speakers' audio clips are used since the focus is on single speaker dereverberation. The RIRs are generated using the pyroomacoustics <ref type="bibr" target="#b18">[19]</ref> software framework. RT60 values for the RIRs are randomly generated between 0.1s and 1s in WHAMR. To create WHAMR ext, reverberant speech with larger RT60 values between 1s and 3s were simulated following the same routine as for WHAMR. Scripts to recreate WHAMR ext can be found on github <ref type="bibr" target="#b0">1</ref> .</p><p>An 8kHz sample rate is used for all audio. For each corpus, WHAMR and WHAMR ext, the training set is comprised of 20,000 speech examples. For training, audio clips are truncated or padded to 4 seconds, resulting in a total of 22.22 hours of data being used. This approach is used to address signal length mismatches in batches during training <ref type="bibr" target="#b15">[16]</ref>. For validation 5000 audio examples are used resulting in 14.65 hours of speech and for testing 3000 audio examples are used, i.e. 9 hours of speech. All models are evaluated on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Configuration</head><p>All experiments are done using the speechbrain speech processing software framework <ref type="bibr" target="#b19">[20]</ref>. Training is performed over 100 epochs with an initial learning rate set to 10 ?3 that is halved if there is no improvement in the average SISDR over the validation set after 3 epochs.</p><p>The number of blocks, X, in the dilated stack of the TCN was varied from 1 to 10 and the number of repeats, R, of the stack itself was varied from 1 to 8. The rest of the network's configuration is fixed. The encoder has L BL = 16 input channels and N = 512 output channels. The TCN is configured such that there are B = 128 output channels from the bottleneck layer and each CB has H = 512 internal convolutional channels and a kernel size P = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>A number of metrics were considered for evaluating the dereverberation performance objectively <ref type="bibr" target="#b20">[21]</ref>. SISDR is reported for all experiments. In addition PESQ <ref type="bibr" target="#b21">[22]</ref>, extended short-time objective intelligibility (ESTOI) <ref type="bibr" target="#b22">[23]</ref> and speechto-reverberation modulation energy ratio (SRMR) <ref type="bibr" target="#b23">[24]</ref> are reported for some models. PESQ is an objective measure of speech quality. ESTOI is an objective measures of speech intelligibility. SRMR is a non-intrusive measure of reverberation energy. ?-measures show the improvement over the reverberant speech, x[i].</p><p>V. RESULTS a) ? SISDR on WHAMR and WHAMR ext: The ? SISDR results for the models trained and evaluated on the WHAMR dataset can be seen in <ref type="table" target="#tab_0">Table I</ref> for varying X and R. These parameters are varied such that they change the receptive field and model size of TCNs where X has more effect on the RF (cf. Eq. (4)) and both increase the number of layers in the network but R has more effect on the temporal parameter density, i.e. number of parameters per second of the receptive field. Note that one CB has BH + H + HP + HB = 133, 120 parameters.  <ref type="table" target="#tab_0">Table I</ref> show that for smaller models ( 2M parameters) it is preferable to have a larger RF, i.e. a higher X value, than a deeper network per the RF, i.e. a higher R value. For example, for a model with 12 CBs the best performing model configuration is (X, R) = (6, 2) where X = 6 is the largest possible value for the 4 possible TCN configurations, {(X, R)} = {(4, 3), (3, 4), (6, 2), (2, 6)}. The importance of widening the receptive field is also apparent in the first row of <ref type="table" target="#tab_0">Table I</ref> where R = 1 remains constant for best performance for the first 9 CBs. This importance of having X &gt; R disappears as the number of CBs surpasses 36 (X = 6, R = 6) at which point the best performance is gained by models with R &gt; X, i.e. more importance is given to a deeper network than a wider receptive field. Comparing the best performing models for increasing the number of CBs (X ? R) trained on WHAMR (cf <ref type="table" target="#tab_0">. Table I)</ref> and WHAMR ext (cf <ref type="table" target="#tab_0">. Table II)</ref> indicates that for a dataset containing only larger RT60 values between 1s and 3s it is more preferable to increase the model's RF as the number of CBs in the TCN increases up to 42, i.e. (X, R) = <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b5">6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results in</head><p>For RT60 values between 0.1s and 1s (WHAMR corpus, <ref type="table" target="#tab_0">Table I</ref>) the model only benefits from putting more emphasis when expanding the RF up to 36 CBs. <ref type="table" target="#tab_0">Table III</ref> shows the best performing models in terms of SISDR, trained on each training set and evaluated on each test set. These results show that performance improvements in SISDR are similarly replicated in SRMR PESQ and ESTOI measures and thus correspond to an objective improvement in perceived reverberation, quality and intelligibility of speech. <ref type="table" target="#tab_0">Tables I and II</ref> have been calculated also for SRMR, PESQ and ESTOI, which show similar trends. <ref type="table" target="#tab_0">Table III also</ref> shows that when evaluated on WHAMR the model that was trained on WHAMR ext gives better dereverberation performance (in SRMR) but was more distorted (in SISDR), c.f. SISDR and SRMR results in rows 1 &amp; 3. It is however expected that training on both WHAMR and WHAMR ext jointly would lead to further generalisation improvements on the WHAMR test set.  b) RF and model size: <ref type="figure" target="#fig_2">Figure 1</ref> shows the results for models trained on WHAMR (upper panel) and WHAMR ext (lower panel) depending on the RF and model size. Note that the SISDR measure is used here, as opposed to the ?   <ref type="table" target="#tab_0">Table III</ref> indicate it may benefit from the larger model size when having to dereverb larger reverberation times especially when trained only with smaller RT60s. <ref type="figure" target="#fig_2">Figure 1</ref> further illustrates that the SISDR performance saturates before the RF reaches 1s for models trained on WHAMR, i.e. the highest occurring RT60 in WHAMR. The RFs of the best performing models can be seen in <ref type="table" target="#tab_0">Table III</ref>. The best model trained and evaluated on WHAMR in terms of SISDR has an RF of 1.02s. Analysing the same models but evaluated on WHAMR ext the best SISDR performance is attained with an RF of 4.09s. For models trained on WHAMR ext the optimal model evaluated on WHAMR has an RF of 1.02s and an RF of 2.04s when evaluated on WHAMR ext. <ref type="figure">Figure 2</ref> analyses the best performing models on each of the two datasets by their RFs depending on model size (i.e. number of CBs). This indicates that for larger RT60 values TCNs benefit from having a larger RF as most of the best performing models evaluated on WHAMR ext have a larger RF than the best performing models evaluated on WHAMR. The SISDR of the estimated signal improves even with RT60s larger than the RF as can be seen in <ref type="figure" target="#fig_2">Figure 1</ref>. This implies the network learns how to estimate masks that suppress the characteristic of reverberation, as opposed to trying to perform a blind convolution operation based on an estimated RIR representation in the network. As such, it should be possible to dereverb RIRs of any RT60 value regardless of RF but <ref type="table" target="#tab_0">Table III</ref> indicates that having an RF close to the maximum RT60 value is optimal.</p><p>(a) Trained on WHAMR.</p><p>(b) Trained on WHAMR ext.  <ref type="figure" target="#fig_3">Figure 3)</ref> indicates that with sufficiently large model sizes ( 4M parameters) much of the residual distortions in the signal are artifacts introduced by the network and not reverberation itself. This also indicates that the larger the reverberation time, the more residual distortions are present in the estimate clean speech signal. Audibly, the distortions present in RT60s 1 were not particularly noticeable however as the RT60 approaches 3s they become more noticeable as informal listening tests showed. The performance of SRMR also saturates slightly earlier than that of SISDR similarly implying that some of the gain from increasing the model size has more correlation to reducing artefact distortions in?[i] than any residual reverberant effects. This is an argument in favour of using the SISDR function over a pure reverberation based measure like SRMR as the loss function because SRMR is more agnostic to general distortions in the signal.  <ref type="figure" target="#fig_4">Figure 4</ref> demonstrate that greater reverberation improvement can be attained on the more reverberant WHAMR ext dataset but that larger model sizes ( 4M parameters) are required to fully capitalize on this. Also for both WHAMR ext evaluation there is much broader distribution of values as the receptive field increases. This is related to the R variable in the TCN, in other words using a deeper network leads to a more significant improvement when evaluating SRMR improvement on larger RT60s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper TCNs were analysed for their application in dereverberation tasks. It was found that for smaller models more emphasis should be put on widening the RF of the network than using more layers in a network. The model performance in both SISDR and SRMR starts to saturate around a model size of 4M parameters. It was shown that the for larger RT60 values there is strong evidence that having a wider receptive field is important for achieving optimal performance. This is especially true when the model is trained on smaller RT60s. It was found that SRMR performance was fairly agnostic to the variation in RT60 values but SISDR performance was significantly impacted. This indicates that much of the distortion remaining in the signal maybe modelling errors as opposed to reverberant effects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]. This work was also funded in part by 3M Health Information Systems, Inc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Trained on WHAMR.(b) Trained on WHAMR ext.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>SISDR depending on logarithmic RF. Circles and squares indicate evaluation on WHAMR and WHAMR ext test sets, respectively. Maximum RT60s of WHAMR and WHAMR ext are shown by dashed lines. Colour scale indicates the number of model parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>SRMR depending on logarithmic RF. Circles and squares indicate evaluation on WHAMR and WHAMR ext test sets, respectively. Maximum RT60s of WHAMR and WHAMR ext are shown by dashed lines. Colour scale indicates the number of model parameters. c) SRMR vs SISDR: Comparing results of SISDR (Figure 1) with SRMR (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>? SRMR for model trained on WHAMR depending on logarithmic RF. Circles and squares indicate evaluation on WHAMR and WHAMR ext test sets, respectively. Maximum RT60s of WHAMR and WHAMR ext are shown by dashed lines. Colour scale indicates the number of model parameters. d) Improvement on WHAMR vs WHAMR ext: The ? SRMR results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>? SISDR in dB for all TCN configurations trained and evaluated on WHAMR, best performing model for number of CBs (X ? R) in TCN shown in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell></cell><cell cols="10">1 1.88 2.61 3.32 4.05 4.66 5.09 5.41 5.65 5.67 5.68</cell></row><row><cell></cell><cell cols="10">2 2.48 3.58 4.45 5.25 5.92 6.26 6.47 6.45 6.60 6.63</cell></row><row><cell>R</cell><cell cols="10">3 2.95 4.08 4.94 5.94 6.43 6.80 6.88 6.94 7.02 7.01 4 3.28 4.46 5.47 6.53 6.97 7.01 7.16 7.23 7.14 7.11</cell></row><row><cell></cell><cell cols="10">5 3.54 4.82 5.86 6.70 7.06 7.31 7.29 7.32 7.42 7.44</cell></row><row><cell></cell><cell cols="10">6 3.74 4.99 6.16 6.87 7.25 7.37 7.45 7.51 7.47 7.40</cell></row><row><cell></cell><cell cols="10">8 4.09 5.55 6.44 7.12 7.44 7.63 7.59 7.54 7.48 7.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>? SISDR in dB for all TCN configurations trained and evaluated on WHAMR ext, best performing model for number of CBs (X ? R) in TCN shown in bold. .04 3.85 4.67 5.79 6.84 7.68 8.09 8.55 8.69 8.69 2 3.65 4.76 6.11 7.44 8.56 9.19 9.52 9.64 9.76 9.79 3 4.06 5.44 6.98 8.42 9.29 9.83 10.13 10.19 10.21 10.15 4 4.45 6.10 7.62 8.96 9.68 10.11 10.41 10.42 10.42 10.47 5 4.70 6.51 8.21 9.36 10.01 10.37 10.60 10.62 10.54 10.50 6 4.96 6.85 8.48 9.63 10.15 10.49 10.74 10.77 10.67 10.60 7 5.29 7.14 8.75 9.71 10.34 10.61 10.72 10.68 10.76 10.70 R 8 5.45 7.44 9.03 10.02 10.49 10.80 10.81 10.67 10.68 10.57</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell>1 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Best performing models for models trained on WHAMR and WHAMR ext evaluated on each test set.</figDesc><table><row><cell>Fig. 2: RFs for the best performing models in Tables I and</cell></row><row><cell>II shown by increasing model size measured in number of</cell></row><row><cell>CBs (one CB is 133,120 parameters). Line colour and style</cell></row><row><cell>indicates the training and test set used.</cell></row><row><cell>SISDR measure, because it is later compared with SRMR</cell></row><row><cell>in this section. In terms of model size, the SISDR perfor-</cell></row><row><cell>mance that can be achieved by TCNs alone saturates as the</cell></row><row><cell>number of parameters approaches 6M, for both WHAMR and</cell></row><row><cell>WHAMR ext when evaluated on WHAMR. For evaluation</cell></row><row><cell>on WHAMR ext, the SISDR performance also saturates as it</cell></row><row><cell>approaches 6M parameters but the results for the model trained</cell></row><row><cell>on WHAMR in</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Mixing script available online at https://github.com/jwr1995/WHAMR ext</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Naylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Gaubitch</surname></persName>
		</author>
		<title level="m">Speech Dereverberation</title>
		<imprint>
			<publisher>Springer Publishing</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>1st ed</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Far-Field Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="148" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The AMI Meeting Transcription System: Progress and Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vepa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Machine Learning for Multimodal Interaction, ser. MLMI&apos;06</title>
		<meeting>the Third International Conference on Machine Learning for Multimodal Interaction, ser. MLMI&apos;06<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="419" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combination of MVDR beamforming and single-channel spectral processing for enhancing noisy and reverberant speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cauchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kodrasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gerlach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jukic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doclo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goetze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonlinear filtering of multiplied and convolved signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oppenheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stockham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio and Electroacoustics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="437" to="466" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Single-and multi-microphone speech dereverberation using spectral enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Linear predictionbased dereverberation with very deep convolutional neural networks for reverberant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-04" />
			<publisher>January. Institute of Electrical and Electronics Engineers Inc</publisher>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On end-to-end multichannel time domain speech separation in reverberant environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zorila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doddipatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6389" to="6393" />
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning based dereverberation of temporal envelopes for robust speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Purushothaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sreeram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganapathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2020</title>
		<imprint>
			<date type="published" when="2020-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Neural network-based spectrum estimation for online WPE dereverberation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kinoshita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delcroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Monaural speech dereverberation using temporal convolutional networks with self attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1598" to="1607" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">TasNet: Time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time single-channel dereverberation and separation with time-domain audio separation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks: A unified approach to action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 Workshops</title>
		<editor>G. Hua and H. J?gou</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WHAMR!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04624</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SDR -halfbaked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019</title>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pyroomacoustics: A python package for audio room simulation and array processing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Scheibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bezzam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dokmanic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">SpeechBrain: A generalpurpose speech toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Plantinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rouhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dawalatabad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rastorgueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Aris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04624</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A study on speech quality and speech intelligibility measures for quality assessment of single-channel dereverberation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goetze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warzybok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kodrasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename><surname>Jungmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cauchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rennies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mertins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doclo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kollmeier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="233" to="237" />
			<pubPlace>Antibes, France</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
			<date type="published" when="2001-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An algorithm for predicting the intelligibility of speech masked by modulated noise maskers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2009" to="2022" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An improved non-intrusive intelligibility metric for noisy and reverberant speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Senoussaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWAENC 2014</title>
		<imprint>
			<date type="published" when="2014-09" />
			<biblScope unit="page" from="55" to="59" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
