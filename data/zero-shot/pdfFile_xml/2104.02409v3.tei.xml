<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Estimate Hidden Motions with Global Motion Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ACRV</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ACRV</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ACRV</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ACRV</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Estimate Hidden Motions with Global Motion Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Global motion aggregation helps resolve ambiguities caused by occlusions. Occlusions-a term we extend to include any parts of a scene that disappear in the next frame-cause large ambiguities in the optical flow estimation problem that cannot be resolved by local approaches. Based on the assumption that points on an object have homogeneous motions, which often holds approximately, we propose to globally aggregate motion features of pixels that are likely to belong to the same object. In this example, most pixels on the blade move out-of-frame from frame 2 to frame 3. When only these two frames are provided, global aggregation allows motion information to be passed from non-occluded pixels to occluded pixels, which helps resolve ambiguities caused by occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Occlusions pose a significant challenge to optical flow algorithms that rely on local evidences. We consider an occluded point to be one that is imaged in the reference frame but not in the next, a slight overloading of the standard definition since it also includes points that move out-of-frame. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work relies on CNNs to learn occlusions, without much success, or requires multiple frames to reason about occlusions using temporal smoothness. In this paper, we argue that the occlusion problem can be better solved in the two-frame case by modelling image self-similarities. We introduce a global motion aggregation module, a transformer-based approach to find long-range dependencies between pixels in the first image, and perform global aggregation on the corresponding motion features. We demonstrate that the optical flow estimates in the occluded regions can be significantly improved without damaging the performance in non-occluded regions. This approach obtains new state-ofthe-art results on the challenging Sintel dataset, improving the average end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At the time of submission, our method ranks first on these benchmarks among all published and unpublished approaches. Code is available at https://github.com/zacjiang/GMA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Global motion aggregation helps resolve ambiguities caused by occlusions. Occlusions-a term we extend to include any parts of a scene that disappear in the next frame-cause large ambiguities in the optical flow estimation problem that cannot be resolved by local approaches. Based on the assumption that points on an object have homogeneous motions, which often holds approximately, we propose to globally aggregate motion features of pixels that are likely to belong to the same object. In this example, most pixels on the blade move out-of-frame from frame 2 to frame 3. When only these two frames are provided, global aggregation allows motion information to be passed from non-occluded pixels to occluded pixels, which helps resolve ambiguities caused by occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Occlusions pose a significant challenge to optical flow algorithms that rely on local evidences. We consider an occluded point to be one that is imaged in the reference frame but not in the next, a slight overloading of the standard definition since it also includes points that move out-of-frame. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work relies on CNNs to learn occlusions, without much success, or requires multiple frames to reason about occlusions using temporal smoothness. In this paper, we argue that the occlusion problem can be better solved in the two-frame case by modelling image self-similarities. We introduce a global motion aggregation module, a transformer-based approach to find long-range dependencies between pixels in the first image, and perform global aggregation on the corresponding motion features. We demonstrate that the optical flow estimates in the occluded regions can be significantly improved without damaging the performance in non-occluded regions. This approach obtains new state-ofthe-art results on the challenging Sintel dataset, improving the average end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At the time of submission, our method ranks first on these benchmarks among all published and unpublished approaches. Code is available at https://github.com/zacjiang/GMA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>How can we estimate the 2D motion of a point we only see once? This is the problem faced by optical flow algorithms for points that become occluded between frames. Estimating the optical flow, that is, the apparent motion of pixels in an image as the camera and scene move, is a classic problem in computer vision studied since the seminal work of Horn and Schunck <ref type="bibr" target="#b13">[14]</ref>. There are many factors that make optical flow prediction a hard problem, including large motions, motion and defocus blur, and featureless regions. Among these challenges, occlusion is one of the most difficult and under-explored. In this paper, we propose an approach that specifically targets the occlusion problem in the case of two-frame optical flow prediction.</p><p>We first define what we mean by occlusion in the context of optical flow estimation. In this paper, an occluded point is defined as a 3D point that is imaged in the reference frame but is not visible in the matching frame. This definition incorporates several different scenarios, such as the query point moving out-of-frame or behind another object (or itself), or another object moving in front of the query point, in the active sense. One particular case of occlusion is shown in <ref type="figure">Figure 1</ref>, where part of the blade moves out-offrame.</p><p>The challenge posed by occlusions can be understood by looking at the underlying assumptions of optical flow algorithms. Traditional optical flow algorithms apply the bright-  <ref type="figure">Figure 2</ref>. Recovering hidden motions. In row 1, the bottom left corner of the ground moves out-of-frame, but reasoning that it belongs to the background allows the motion to be recovered from other parts of the image. In row 2, the girl's staff is mostly occluded in the second frame, but strong cues from the visible parts can resolve its motion. Our approach can estimate many hidden motions despite the presence of occlusions. The flow maps and the error maps have been fetched from the Sintel server <ref type="bibr" target="#b7">[8]</ref>. Best viewed in colour on a screen. ness constancy constraint <ref type="bibr" target="#b13">[14]</ref>, where pixels related by the flow field are assumed to have the same intensities. It is clear that occlusions are a direct violation of such a constraint. In the deep learning era, correlation (cost) volumes <ref type="bibr" target="#b14">[15]</ref> are used to give a matching cost for each potential displacement of a pixel. However, correlations of appearance features are unable to give meaningful guidance for learning the motion of occluded regions. Most existing approaches use smoothness terms in an MRF to interpolate occluded motions <ref type="bibr" target="#b8">[9]</ref> or use CNNs to directly learn the neighbouring relationships, hoping to learn to estimate occluded motions based on the neighbouring pixels <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37]</ref>. However, stateof-the-art methods still fail to estimate occluded motions correctly when occlusions are more significant and local evidence is insufficient to resolve the ambiguity.</p><p>In contrast, humans are able to synthesise information from across the image and apply plausible motion models to accurately estimate occluded motions. This capability is valuable to emulate, because we fundamentally care about recovering the real 3D motion of objects in a scene, for which estimating occluded motion is necessary. Downstream applications, including tracking and activity detection <ref type="bibr" target="#b23">[24]</ref>, can also benefit from short-term predictions of the motion of occluded points, particularly if they reappear later or exhibit some characteristic of interest (e.g., high-velocity out-of-frame motions).</p><p>Let us consider how to estimate these hidden motions for the two-frame case. When direct (local) matching information is absent, the motion information has to be propagated from other pixels. Using convolutions to propagate this information has the drawback of limited range since convolution is a local operation. We propose to aggregate the motion features with a non-local approach. Our design is based on the assumption that the motions of a single object (in the foreground or background) are often homogeneous. One source of information that is overlooked by existing works is self-similarities in the reference frame. For each pixel, understanding which other pixels are related to it, or which object it belongs to, is an important cue for accu-rate optical flow predictions. That is, the motion information of non-occluded self-similar points can be propagated to the occluded points. Inspired by the recent success of transformers <ref type="bibr" target="#b39">[40]</ref>, we introduce a global motion aggregation (GMA) module, where we first compute an attention matrix based on the self-similarities of the reference frame, then use that attention matrix to aggregate motion features. We use these globally aggregated motion features to augment the successful RAFT <ref type="bibr" target="#b38">[39]</ref> framework and demonstrate new state-of-the-art results in optical flow estimation, such as those examples in <ref type="figure">Figure 2</ref>.</p><p>The key contributions of our paper are as follows. We show that long-range connections, implemented using the attention mechanism of transformer networks, are highly beneficial for optical flow estimation, particularly for resolving the motion of occluded pixels where local information is insufficient. We show that self-similarities in the reference frame provide an important cue for selecting the long-range connections to prioritise. We demonstrate that our global motion feature aggregation strategy leads to a significant improvement in optical flow accuracy in occluded regions, without damaging the performance in nonoccluded regions, and analyse this extensively. We improve the average end-point error (EPE) by 13.6% (2.86 ? 2.47) on Sintel Final and 13.7% (1.61 ? 1.39) on Sintel Clean, compared to the strong baseline of RAFT <ref type="bibr" target="#b38">[39]</ref>. Our approach ranks first on both datasets at the time of submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Occlusions in optical flow. Occlusion poses a key challenge in optical flow estimation due to its violation of the brightness constancy constraint <ref type="bibr" target="#b13">[14]</ref>. Most traditional optical flow algorithms treat occlusions as outliers and so develop and optimise robust objective functions. In continuous optimisation for optical flow, Brox et al. <ref type="bibr" target="#b5">[6]</ref> used the L 1 norm due to its robustness to outliers caused by occlusions or large brightness variations. Zach et al. <ref type="bibr" target="#b46">[47]</ref> added total variation regularisation and proposed an efficient numerical scheme to optimise the energy functional. This formulation   <ref type="figure">Figure 3</ref>. Proposed architecture. Our network is based on the successful RAFT <ref type="bibr" target="#b38">[39]</ref> architecture. The proposed global motion aggregation (GMA) module is contained inside the shaded box, a self-contained addition to RAFT with low computational overhead that significantly improves performance. It takes the visual context features and the motion features as input and outputs aggregated motion features that share information across the image. These aggregated global motion features are then concatenated with the local motion features and the visual context features to be decoded by the GRU into residual flow. This gives the network the flexibility to choose between or combine the local and global motion features, depending on the needs of the specific pixel location. For example, a location with poor local image evidence, caused by occlusion for instance, could preference the global motion features. was later improved by Wedel et al. <ref type="bibr" target="#b42">[43]</ref>. Later work introduced additional robust optimisation terms, including the Charbonnier potential <ref type="bibr" target="#b6">[7]</ref> and the Lorentzian potential <ref type="bibr" target="#b3">[4]</ref>.</p><p>More recently, discrete optimisation approaches, especially Markov Random Fields (MRFs) <ref type="bibr" target="#b4">[5]</ref>, have been used to estimate optical flow. These algorithms <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">44]</ref> first estimate the forward and backward flows separately using a robust, truncated data term. They then conduct a forwardbackward consistency check to determine the occluded regions. Lastly, as a post-processing step, they use interpolation methods <ref type="bibr" target="#b31">[32]</ref> to fill in the optical flow of the occluded regions.</p><p>Other work incorporates occlusion estimation as a joint objective together with optical flow estimation. Alvarez et al. <ref type="bibr" target="#b0">[1]</ref> use forward-backward consistency as an optimisation objective, thus estimating time-symmetric optical flow. In addition to forward-backward consistency, MirrorFlow <ref type="bibr" target="#b17">[18]</ref> incorporates occlusion-disocclusion symmetry in the energy function and achieves performance improvements. Since occlusions are caused by 3D motions, other works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b33">34]</ref> explicitly model local depth relationships into layers and reason about occlusions.</p><p>Contrary to the above approaches, we do not overload the loss function with explicit occlusion reasoning. Instead, we adopt a learning approach, similar to other supervised deep optical flow learning approaches <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b22">23]</ref>. Rather than estimating an occlusion map explicitly, our goal is to improve the optical flow accuracy at occluded regions. We take an implicit approach to globally aggregate motion features, which provides extra information to correctly predict flow at occluded regions. Our approach can be thought of as a non-local interpolation approach <ref type="bibr" target="#b34">[35]</ref>, in contrast to local interpolation approaches <ref type="bibr" target="#b31">[32]</ref>. In the deep learning literature, the occlusion problem has been addressed in an unsupervised learning set-ting <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref>, however, existing supervised learning approaches all rely on convolutions to interpolate in occluded regions, which are prone to failure for more significant occlusions.</p><p>Self-attention and transformers. Our design principle is inspired by the recent successes of the transformer literature <ref type="bibr" target="#b39">[40]</ref>. The transformer architecture was first successful in natural language processing (NLP), due to its ability to model long-range dependencies and its scalability for GPU parallel processing. Among various modules in the transformer achitecture, self-attention is the key design feature that make transformers work. Recently, researchers have introduced the transformer and related attention ideas to the vision community, mostly in high-level tasks such as image classification <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b9">10]</ref> and semantic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b15">16]</ref>. To the best of our knowledge, we are the first to use the idea of attention to solve the optical flow problem. Different from many existing works in the transformer literature, we do not use self-attention in our work. Selfattention refers to the query, key and value vectors coming from the same features. In our case, query and key vectors come from the context features modelling the appearance of the image while value vectors come from the motion features, which is an encoding of the correlation volume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background</head><p>We base our network design on the successful RAFT architecture <ref type="bibr" target="#b38">[39]</ref>. Our overall network diagram is shown in <ref type="figure">Figure 3</ref>. For completeness, we briefly describe the main contributions of RAFT from which our model benefits. The first contribution is the introduction of an all-pairs correlation volume, which explicitly models matching correlations  <ref type="figure">Figure 4</ref>. Details of the GMA module. To model the self-similarity of the first frame, we project the context feature map to a query feature map and a key feature map. We then take the dot product of the two feature maps and a softmax to obtain an attention matrix, which encodes self-similarity in appearance feature space. Similar to transformer networks <ref type="bibr" target="#b39">[40]</ref>, we also take the dot product between the query feature map and a set of positional embedding vectors which augments the attention matrix with positional information. Separately, the motion feature map encoded from the correlation volume is projected using the learned value projector. Its weighted sum, using the obtained attention matrix, produces the aggregated global motion features.</p><p>for all possible displacements. The benefit of using all-pairs correlations is its ability to handle large motions. The second major contribution is the use of a gated recurrent unit (GRU) decoder for iterative residual refinement <ref type="bibr" target="#b18">[19]</ref>. The constructed 4D correlation volumes are encoded to 2D motion features, which are iteratively decoded to predict the residual flow. The final flow prediction is a sum of the sequence of residual flows. The benefit of using a GRU to perform iterative refinement lies in the reduction of the search space. In RAFT, convolutions are used in the GRU decoder, which learn to model spatial smoothness. Due to the local nature of convolutions, they can learn to handle small occlusions but tend to fail when these become more significant and local evidence is insufficient to resolve the motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overview</head><p>In his first paper from 1976, Geoffrey Hinton wrote that "local ambiguities have to be resolved by finding the best global interpretation" <ref type="bibr" target="#b12">[13]</ref>. This idea still holds true in the modern deep learning era. To resolve ambiguities caused by occlusions, our core idea is to allow the network to reason at a higher level, that is, to globally aggregate the motion features of similar pixels, having implicitly reasoned about which pixels are similar in appearance feature space. We hypothesise that the network will be able to find points with similar motions by looking for points with similar appearance in the reference frame. This is motivated by the observation that the motions of points on a single object are often homogeneous. For example, the motion vectors of a person running to the right have a bias towards the right, which holds even if we do not see where a large part of the person ends up in the matching frame due to occlusion. We can use this statistical bias to propagate motion information from non-occluded pixels, with high (implicit) confidence, to occluded pixels, with low confidence. Here, confidence can be interpreted as whether there exists a distinct matching, i.e., a high correlation value at the correct displacement.</p><p>With these ideas, we take inspiration from transformer networks <ref type="bibr" target="#b39">[40]</ref>, which are known for their ability to model long-range dependencies. Different from the self-attention mechanism in transformers, where query, key and value come from the same feature vectors, we use a generalized variant of attention. Our query and key features are projections of the context feature map, which are used to model the appearance self-similarities in frame 1. The value features are projections of the motion features, which themselves are an encoding of the 4D correlation volume. The attention matrix computed from the query and key features is used to aggregate the value features which are hidden representations of motions. We name this a Global Motion Aggregation (GMA) module. The aggregated motion features are concatenated with the local motion features as well as the context features, which is to be decoded by the GRU. A detailed diagram of GMA is shown in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mathematical Formulation</head><p>Let x ? R N ?Dc denote the context (appearance) features and y ? R N ?Dm denote the motion features, where N = HW and H and W are the height and width of the feature map, D refers to the channel dimension of the feature map. The i th feature vector is denoted x i ? R Dc . Our GMA module computes the feature vector update as an attentionweighted sum of the projected motion features. The aggregated motion features are given b?</p><formula xml:id="formula_0">y i = y i + ? N j=1 f (?(x i ), ?(x j ))?(y j ),<label>(1)</label></formula><p>where ? is a learned scalar parameter initialised to zero, ?, ? and ? are the projection functions for the query, key, and  <ref type="figure">Figure 5</ref>. Examples of the Sintel Albedo dataset and occlusion maps. The Albedo dataset is rendered without the illumination effects. The occlusion map in this example contains mostly foreground objects occluding the background scene as well as the background on the left moving out of the field-of-view. <ref type="figure">Figure 5(c)</ref> is the occlusion map (Occ) for this example. <ref type="figure">Figure 5</ref>(d) and <ref type="figure">Figure 5</ref>(e) are the in-frame (Occ-in) and out-of-frame (Occ-out) occlusion maps respectively.</p><p>value vectors, and f is a similarity attention function given by</p><formula xml:id="formula_1">f (a i , b j ) = exp a T i b j / ? D N j=1 exp a T i b j / ? D .<label>(2)</label></formula><p>The projection functions for the query, key and value vectors are given by</p><formula xml:id="formula_2">?(x i ) = W qry x i , (3) ?(x i ) = W key x i , (4) ?(y i ) = W val y i ,<label>(5)</label></formula><p>where W qry , W key ? R Din?Dc and W val ? R Dm?Dm . The learnable parameters in our GMA module include W qry , W key , W val and ?.</p><p>The final output is [y |? | x], a concatenation of the three feature maps. The GRU decodes this to obtain the residual flow. Concatenation allows the network to intelligently select from or combine the motion vectors, modulated by the global context feature, without prescribing exactly how it is to do this. It is plausible that the network learns to encode some notion of uncertainty, and decodes the aggregated motion vector only when the model cannot be certain of the flow from the local evidence.</p><p>We also explore the use of a 2D relative positional embedding <ref type="bibr" target="#b2">[3]</ref>, allowing the attention map to depend on both the feature self-similarity and the relative position from the query point. For this, we compute the aggregated motion vector a?</p><formula xml:id="formula_3">y i = y i + ? N j=1 f (?(x i ), ?(x j ) + p j?i )?(y j ),<label>(6)</label></formula><p>where p j?i denotes the relative positional embedding vector indexed by the pixel offset j ? i. Separate embedding vectors are learned for the vertical and horizontal offsets and are summed to obtain p j?i . If it is useful to suppress pixels that are very close or very far from the query point when aggregating the motion vectors, then this positional embedding has the capacity to learn this behaviour. We also investigated computing the attention map from only the query vectors and positional embedding vectors, without any notion of self-similarity. That is,</p><formula xml:id="formula_4">y i = y i + ? N j=1 f (?(x i ), p j?i )?(y j ).<label>(7)</label></formula><p>This can be regarded as learning long-range aggregation without reasoning about the image content. It is plausible that positional biases in the dataset could be exploited by such a scheme. In <ref type="table">Table 2</ref>, the results for <ref type="formula" target="#formula_3">(6)</ref> and <ref type="formula" target="#formula_4">(7)</ref> are denoted as Ours (+p) and Ours (p only).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We follow the standard optical flow training procedure <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> of first pre-training our model on FlyingChairs <ref type="bibr" target="#b10">[11]</ref> for 120k iterations with a batch size of 8 and then on FlyingThings <ref type="bibr" target="#b26">[27]</ref> for another 120k iterations with a batch size of 6. We then fine-tune on a combination of FlyingThings, Sintel <ref type="bibr" target="#b7">[8]</ref>, KITTI 2015 <ref type="bibr" target="#b28">[29]</ref> and HD1K <ref type="bibr" target="#b24">[25]</ref> for 120k iterations for Sintel evaluation and 50k on KITTI 2015 <ref type="bibr" target="#b28">[29]</ref> for KITTI evaluation. A batch size of 6 is set for finetuning. We train our model on two 2080Ti GPUs with the PyTorch library <ref type="bibr" target="#b29">[30]</ref> using the mixed precision strategy. We adopt the same hyperparameters as RAFT <ref type="bibr" target="#b38">[39]</ref> for the base network. We adopt the one-cycle learning rate policy <ref type="bibr" target="#b32">[33]</ref> with the highest learning rate set to 2.5 ? 10 ?4 for FlyingChairs then 1.25 ? 10 ?4 for the rest. For GMA, we choose channel dimensions D in = D c = D m = 128.</p><p>The main evaluation metric we use is average end-point error (AEPE), which refers to the mean pixelwise flow error. KITTI also uses the Fl-All (%) metric which refers to the percentage of optical flow vectors whose end-point error is larger than 3 pixels or over 5% of ground truth.</p><p>The Sintel dataset has been created with different rendering passes that have different levels of complexity. For training and test evaluation on the Sintel server, we used the Clean and Final passes. The Clean pass is rendered with illumination including smooth shading and specular reflections. The Final pass is created with full rendering, which includes motion blur, camera depth-of-field blur, and atmospheric effects.</p><p>In the Sintel training set, they also provided the Albedo pass, which is rendered without illumination effects and has In-frame and out-of-frame occlusions are further split and denoted as 'Occ-in' and 'Occ-out'. The best results and the largest relative improvement in each dataset are styled in bold.</p><p>roughly piecewise-constant colours. An example is shown in <ref type="figure">Figure 5</ref>. We do not use this set for training, but reserve it as an evaluation dataset. The motivation for doing so is that the Albedo set adheres to brightness constancy everywhere apart from occluded regions. By evaluating and analysing on the occluded regions and non-occluded regions separately, we can clearly see how well our method performs when addressing the occlusion problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Occlusion Analysis</head><p>To verify the effectiveness of our proposed GMA module at estimating the motion of occluded points, we make use of the occlusion maps provided in the Sintel training set, which partition the pixels into non-occluded (Noc) and occluded (Occ) pixels. We further divide the occluded pixels into inframe ('Occ-in') and out-of-frame ('Occ-out') occlusions, depending on whether the ground-truth flow vector points inside or outside the image frame. An example is shown in <ref type="figure">Figure 5</ref>.</p><p>We evaluated on all three rendering passes of Sintel, where the results for Clean and Final are training set errors and those for Albedo are test set errors. We evaluated the AEPE for different regions, results of which are shown in <ref type="table">Table 1</ref>. We observe that the relative improvement of our method compared to RAFT is predominantly attributable to better predictions of the flow for occluded points. This is reinforced by the results on the Albedo dataset where the brightness constancy assumption holds exactly for non-occluded points, removing confounding factors. Finally, out-of-frame occlusions are more challenging than in-frame occlusions for both models, but we still observe a significant improvement for these pixels. We hypothesise that the improvement in non-occluded regions is due to GMA's ability to resolve ambiguities caused by other brightness variations, for example specular reflections, blurs, and other sources. This result strongly supports our claim that global aggregation can help resolve ambiguities caused by occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Prior Works</head><p>Having shown that our approach can improve optical flow estimates for occluded regions, we compare against prior works on the overall performance. We evaluate our approach on the Sintel dataset <ref type="bibr" target="#b7">[8]</ref> and the KITTI 2015 optical flow dataset <ref type="bibr" target="#b28">[29]</ref>. At the time of submission, we have obtained the best results on both the Sintel Final and Clean benchmarks among all submitted results published and unpublished. Compared with our baseline approach RAFT <ref type="bibr" target="#b38">[39]</ref>, we have improved the AEPE from 2.86 to 2.47 (13.6% improvement) on Sintel Final and 1.61 to 1.39 (13.7% improvement) on Sintel Clean. This significant improvement over RAFT validates our claim that our approach can improve flow prediction for occluded regions without damaging the performance of non-occluded regions. The Sintel server also reports the metric 'EPE unmatched', which measures the endpoint error over regions that are visible only in one frame, predominantly caused by occlusion. Our approach also ranks first under this metric in both Clean and Final, with a margin of 0.9 EPE on Clean (2. On the KITTI 2015 test set, our results are on par with RAFT. 'Ours (p only)', which uses positional attention only, outperforms RAFT, while 'Ours', which uses content selfsimilarity attention, slightly underperforms. It is likely that the lack of improvement on this dataset is due to having insufficient training data (only 200 pairs of images) for the network to learn high-level appearance feature similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>Qualitative results are shown in <ref type="figure">Figure 2</ref> for two examples in the Sintel Clean dataset. The optical flow error in regions of the image that move out-of-frame or behind another object is significantly reduced compared to RAFT. These scenes are highly challenging with lots of motion and occlusion. For example, it is not unreasonable that RAFT is unable to keep track of the wooden staff that becomes partially occluded in the second image, given that it is wellcamouflaged in a forest, fast-moving, and very thin. However, our model is able to very accurately predict the staff's  <ref type="table">Table 2</ref>. Quantitative results on Sintel and KITTI 2015 datasets. We report the average end-point error (AEPE) where not otherwise stated, as well as the Fl-all measure for the KITTI dataset, which is the percentage of optical flow outliers with an error larger than 3 pixels. "C + T" refers to results that are pre-trained on the Chairs and Things datasets. "S/K (+ H)" refers to methods that are fine-tuned on the Sintel and KITTI datasets, with some also fine-tuned on the HD1K dataset. Parentheses denote training set results and bold font denotes the best result. "Ours (p only)" denotes the position-only attention model defined in <ref type="bibr" target="#b6">(7)</ref>. "Ours (+p)" denotes the joint position and content-wise attention model defined in <ref type="bibr" target="#b5">(6)</ref>. "Ours" denotes our main content-only self-similarity attention model defined in <ref type="bibr" target="#b0">(1)</ref>. Results evaluated with the "warm-start" strategy detailed in the RAFT paper <ref type="bibr" target="#b38">[39]</ref>.</p><p>Reference Frame Attention map 1 Attention map 2 Attention map 3 Flow <ref type="figure">Figure 6</ref>. Attention map visualisations. For each row, we show the first frame and three query points. Then we show the three attention maps corresponding to these query points (brighter colours mean higher attention weights) as well as the predicted optical flow. motion, despite these challenges.</p><p>We also present visualisations of the learned attention maps for two examples in <ref type="figure">Figure 6</ref>. To train effectively, the network should learn to attend to pixels that share similar motion vectors. For foreground points, we expect this to be most easily achieved by attending to points on the same object, while for background points it may be sufficient to attend to any other background point. These examples justify this expectation and provide support for the argument that appearance (and higher-order) self-similarity is being learned by the network, and that this is helpful for estimating the flow of the occluded points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Results</head><p>To verify our design, we conducted the following ablation experiments. We first compare the performance of the tested variants of the model, where positional attention replaces (p only) or adds to (+p) the self-similarity attention, as presented in <ref type="table">Table 2</ref>. We find that self-similarity is sufficient to achieve the performance improvements, with the positional encoding only helping for the KITTI dataset. This coincides with our intuition that long-range connections are helpful and that distance-based suppression is unnecessary. In addition, we ablate over three design choices: (1) learning the scalar parameter ? vs fixing it at 1, (2) concatenating with local motion features vs replacing local mo- tion features, and (3) using a residual connection (adding the output of the aggregator to the local motion features) vs not using residual connection (directly concatenating the output of the aggregator with the motion features and context features). The results are shown in <ref type="table">Table 3</ref>.</p><p>The key experiment here is showing that concatenation is an important part of the network design. The hypothesis was that the network should learn how to select or combine the local and globally-aggregated features, based on some implicit measure of uncertainty. That is, it is not helpful to replace local features in most non-occluded regions, where they may be more reliable and precise than the aggregated features. While the residual connection may also be able to handle this, using both mechanisms leads to the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Timing, Parameter Counts and Memory</head><p>We demonstrate that the computational overhead of GMA is low relative to the performance improvement, as shown in <ref type="table" target="#tab_5">Table 4</ref>. The parameter count for our model is 5.9M compared to RAFT which is 5.3M. We tested the inference time on a single RTX 3090 GPU, with RAFT taking 60ms on average and ours taking 72ms for a single pair of image in the Sintel dataset. The image size is 436?1024. The GRU iteration number is set to 12. We also tested the GPU memory consumption for training. When training on FlyingChairs on a single 3090 card, with a random crop of 368?496 and batch size of 8, RAFT takes 16.0GB memory while our network takes 17.2GB memory. We can see that overall the computational overhead is modest while the improvement in results is significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We have demonstrated empirically that long-range connections, weighted by image self-similarities, are very effective at resolving the optical flow of occluded 3D points. The intuition is that if the network can determine which non-occluded points are moving in the same way, this information can be transmitted to 'in-paint' the motion of the occluded points. Determining which points have similar motion characteristics is a non-trivial task and relies on the exploitation of statistical biases. Similar flow vectors are frequently observed for points belonging to the same class, due to the homogeneous motion in 3D. This suggests that we should enable the network to aggregate over motions of the same scene objects, which motivates our choice to explicitly expose the self-similarity of image features to our GMA module. However, additive aggregation of this kind is only helpful when the flow field of the attended locations is approximately homogeneous. This does not hold exactly for general object and camera motions, where the flow fields may be far from homogeneous, even on the same rigid object. An example is an object that is directly in front of the camera and rotating about the optical axis, where the flow vectors are in opposite directions. To deal with such scenarios, one possible future work is to first transform the motion features based on the relative positions and perform aggregation afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Occlusions have long been considered a significant challenge and a major source of error in optical flow estimation. Inspired by the recent success of transformers, we introduce a global motion aggregation module to globally aggregate motion features based on appearance self-similarity of the first image. This has been validated by experiments that show significantly improved optical flow predictions for occluded regions, particularly the large reduction of EPE on Sintel Clean and Final. Our approach of aggregating information over long-range connections using self-similarity is a simple and effective way to introduce higher-order reasoning into the optical flow problem and is applicable to any supervised flow network. We expect that further development of aggregation mechanisms or alternatives would lead to additional performance improvements.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2 w.r.t. RAFT) and 1.3 EPE on Final (1.7 w.r.t. RAFT). Overall, our model achieves a new state-of-the-art result in optical flow estimation, which demonstrates the usefulness of addressing the occlusion problem in optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Additional visualisations evaluated on the Sintel Albedo training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Additional visualisations evaluated on the Sintel Clean test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Additional visualisations evaluated on the Sintel Final test dataset. Additional visualisations evaluated on the Sintel Final test dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Timing, parameters and memory. The GMA module has a modest computational overhead.</figDesc><table><row><cell cols="2">Chairs</cell><cell cols="2">Things</cell><cell cols="2">Sintel</cell></row><row><cell></cell><cell></cell><cell cols="4">Clean Final Clean Final</cell></row><row><cell>Component</cell><cell>(val)</cell><cell>(test)</cell><cell cols="3">(test) (train) (train)</cell></row><row><cell>1</cell><cell>0.82</cell><cell>3.10</cell><cell>2.78</cell><cell>1.35</cell><cell>2.82</cell></row><row><cell>?</cell><cell>0.79</cell><cell>3.14</cell><cell>2.80</cell><cell>1.30</cell><cell>2.74</cell></row><row><cell>replace</cell><cell>0.88</cell><cell>3.16</cell><cell>2.94</cell><cell>1.41</cell><cell>2.79</cell></row><row><cell>concatenate</cell><cell>0.79</cell><cell>3.14</cell><cell>2.80</cell><cell>1.30</cell><cell>2.74</cell></row><row><cell cols="2">w/o residual 0.88</cell><cell>3.13</cell><cell>2.83</cell><cell>1.40</cell><cell>2.75</cell></row><row><cell>w/ residual</cell><cell>0.79</cell><cell>3.14</cell><cell>2.80</cell><cell>1.30</cell><cell>2.74</cell></row><row><cell cols="6">Table 3. Ablation experiment results. Settings used in our final</cell></row><row><cell cols="2">model are underlined.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell></cell><cell>RAFT [38]</cell><cell></cell><cell>Ours</cell><cell></cell></row><row><cell>Parameters</cell><cell></cell><cell>5.3M</cell><cell></cell><cell>5.9M</cell><cell></cell></row><row><cell>Timing</cell><cell></cell><cell>60ms</cell><cell></cell><cell>72ms</cell><cell></cell></row><row><cell>GPU Memory</cell><cell></cell><cell>16.0GB</cell><cell></cell><cell cols="2">17.7GB</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is funded in part by the ARC Centre of Excellence for Robotic Vision (CE140100016), ARC Discovery Project grant (DP200102274) and (DP190102261), and Continental AG (DFR02541, D.C.). S.J. would like to thank Jing Zhang and Yujiao Shi for helpful discussions. We thank the anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix 7. Screenshots of Sintel Server Results</head><p>The screenshots for Sintel Clean and Final results on the test server are shown in <ref type="figure">Figure 7</ref>. We have obtained the best overall results under the 'EPE all' metric. We have also obtained the best results under the 'EPE unmatched' metric with a large margin over previous approaches, which signifies the effectiveness of our approach in addressing the occlusion problem in optical flow estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Additional Qualitative Results</head><p>Additional visualisations evaluated on the Sintel Albedo training dataset are shown in <ref type="figure">Figure 8</ref>. Note that training has not been conducted on this dataset.</p><p>We also give additional visualisations for Sintel Clean and Sintel Final test dataset in <ref type="figure">Figure 9</ref> and <ref type="figure">Figure 11</ref> respectively. Since no ground-truth is provided for Sintel test set, we cannot give an average EPE for each image.</p><p>Additionally, we provide qualitative results on a realworld dataset Slow Flow <ref type="bibr" target="#b21">[22]</ref> to demonstrate the benefits of our approach on real-world data.  <ref type="figure">Figure 7</ref>. Screenshots for Sintel Clean and Final results on the test server. Our proposed approach GMA ranks first on both datasets under the 'EPE all' metric as of March 17th, 2021. We also rank first under the 'EPE unmatched' metric, with a large margin over previous approaches. This signifies the usefulness of addressing the occlusion problem in optical flow.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Symmetrical dense optical flow estimation with occlusions detection. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachid</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Th?o</forename><surname>Papadopoulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>S?nchez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Scopeflow: Dynamic scene scoping for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviram</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Attention augmented convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Kolmogorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Andr?s Bruhn, Nils Papenberg, and Joachim Weickert. High accuracy optical flow estimation based on a theory for warping. ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lucas/kanade meets horn/schunck: Combining local and global optic flow methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schn?rr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Full flow: Optical flow estimation by global optimization over regular grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flownet</surname></persName>
		</author>
		<title level="m">Learning optical flow with convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using relaxation to find a puppet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Summer Conference on Artificial Intelligence and Simulation of Behaviour</title>
		<meeting>the 2nd Summer Conference on Artificial Intelligence and Simulation of Behaviour</meeting>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Determining optical flow. Techniques and Applications of Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schunck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asmaa</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margrit</forename><surname>Gelautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<title level="m">Liteflownet: A lightweight convolutional neural network for optical flow estimation. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Mirrorflow: Exploiting symmetries in joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Iterative residual refinement for joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised learning of multi-frame optical flow with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Slow flow: Exploiting high-speed cameras for accurate and diverse optical flow reference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning optical flow from a few matches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Robust multiple car tracking with occlusion reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The hci benchmark suite: Stereo and flow ground truth with uncertainties for urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Krispin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Andrulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkhard</forename><surname>Gussefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Rahimimoghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claus</forename><surname>Brenner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>CVPR Workshop</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Selflow: Self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A large dataset to train convolutional networks for disparity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Discrete optimization for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>GCPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint 3d estimation of vehicles and scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Workshop on Image Sequence Analysis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Super-convergence: Very fast training of neural networks using large learning rates. Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Local layering for joint motion estimation and occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Layered image motion with explicit occlusions, temporal consistency, and depth ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Models matter, so does training: An empirical study of cnns for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. NIPS</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks. CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">An improved algorithm for tv-l 1 optical flow. Statistical and Geometrical Approaches to Visual Motion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Asymmetric feature matching with learnable occlusion mask. CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
