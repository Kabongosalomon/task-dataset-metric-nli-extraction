<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton-DML: Deep Metric Learning for Skeleton-Based One-Shot Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Memmesheimer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>H?ring</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Theisen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
						</author>
						<title level="a" type="main">Skeleton-DML: Deep Metric Learning for Skeleton-Based One-Shot Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-shot action recognition allows the recognition of human-performed actions with only a single training example. This can influence human-robot-interaction positively by enabling the robot to react to previously unseen behaviour. We formulate the one-shot action recognition problem as a deep metric learning problem and propose a novel imagebased skeleton representation that performs well in a metric learning setting. Therefore, we train a model that projects the image representations into an embedding space. In embedding space similar actions have a low euclidean distance while dissimilar actions have a higher distance. The one-shot action recognition problem becomes a nearest-neighbor search in a set of activity reference samples. We evaluate the performance of our proposed representation against a variety of other skeletonbased image representations. In addition we present an ablation study that shows the influence of different embedding vector sizes, losses and augmentation. Our approach lifts the state-ofthe-art by +3.3% for the one-shot action recognition protocol on the NTU RGB+D 120 dataset under a comparable training setup. With additional augmentation our result improved over +7.7%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Action recognition is a research topic that is applicable in many fields like surveillance, human robot interaction or in health care scenarios. In the past, a strong research focus was laid on the recognition of known activities, whereas learning to recognize from few samples gained popularity only recently <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Because of RGB-D cameras availability and wide mobile indoor applicability, indoor robot systems are often equipped with them <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>. RGB-D cameras that support the OpenNI SDK not only provide color and depth streams but also provide human pose estimates in form of skeleton sequences. These skeleton estimates allow a wide variety of higher-level applications without investing in the human pose estimation problem. As the pose estimation approach is based on depth streams <ref type="bibr" target="#b32">[33]</ref>, it is robust against background information as well as different lighting conditions and therefore also remains functional in dark environments. Especially in a robotics context one-shot action recognition enables a huge variety of applications to improve the human-robotinteraction. A robot could initiate a dialog, when recognizing an activity that it is unfamiliar with, in order to assign a robot-behavior to the observation. This can be done with a single reference sample, while standard action recognition approaches can only recognize actions that were given during training time. In our proposed one-shot action recognition All authors are with the Active Vision Group, Institute for Computational Visualistics, University of Koblenz-Landau, Germany</p><p>Corresponding email: raphael@uni-koblenz.de <ref type="figure">Fig. 1</ref>: Illustrative example of our method. Prior to training a metric on the initial data, no class association could be formed given a skeleton sequence. After training our one-shot action recognition model, skeleton sequences can be encoded. A euclidean distance on the encoded sequence allows class association by finding the nearest neighbour in embedding space from a set of reference samples. The colors are encoding the following classes: throw, falling, grab other person's stuff. Brighter arrow colors denote higher distance in embedding space. approach observations are projected to an embedding space in which similar actions have a low distance and dissimilar actions have a high distance. A high distance to all known activities can be seen as an indicator for anomalies. The embedding in a metric learning setting allows online association of novel observations which is a high advantage over classification tasks that would require retraining or finetuning.</p><p>Deep metric learning based approaches are popular for image ranking or clustering like face-or person re-identification <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>. They have proven to integrate well as an association metric, e.g. in person tracking settings to reduce the amount of id-switches <ref type="bibr" target="#b29">[30]</ref>. Even though there are skeleton-based image representations for recognizing activities from skeleton sequences, they have only recently been used to learn a metric for one-shot action recognition <ref type="bibr" target="#b17">[18]</ref>. <ref type="figure">Fig. 1</ref> shows an illustrative example of an application of our approach. The contributions of this paper are as follows:</p><p>? We present a representation that reassembles skeleton sequences into images. ? We integrate the representation into a deep metric learning formulation to tackle the one-shot action recognition problem. ? We furthermore provide an evaluation of related skeleton-based image representations for one-shot action recognition. ? The source code to reproduce the results of this paper is made publicly available under https://github. com/raphaelmemmesheimer/skeleton-dml.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Action recognition is a broad research topic that varies not only in different modalities like image sequences, skeleton sequences, data by inertial measurement units but also by their evaluation protocols. Most common protocols are crossview or cross-subject. More recently one-shot protocols have gained attention. As our approach focuses on skeleton-based one-shot action recognition we present related work from the current research state directly related to our method. Skeleton based action recognition gained attention with the release of the Microsoft Kinect RGB-D camera. This RGB-D camera not only streamed depth and color images, but the SDK also supported the extraction of skeleton data. With the NTU RGB+D dataset <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13]</ref> a large scale RGB-D action recognition dataset that also contains skeleton sequences has been released. The progress made on this dataset gives a good indication of the performance of various skeleton-based action recognition approaches.</p><p>Because convolution neural architectures showed great performance in the image-classification domain, a variety of research concentrated on finding image-like representations for different research areas like speech recognition <ref type="bibr" target="#b5">[6]</ref>.</p><p>Representations for encoding spatio-temporal information were explored in-depth for recognizing actions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref>. They focus on a classification context by associating class labels with skeleton sequences in contrast to learning an embedding space. The idea of representing motion in image-like representations lead to serious alternatives to sequence classification approaches based on Recurrent Neural Networks <ref type="bibr" target="#b7">[8]</ref> and Long Short Term Memory (LSTM) <ref type="bibr" target="#b13">[14]</ref>. Wang et al. <ref type="bibr" target="#b27">[28]</ref> presented joint trajectory maps. Viewpoints from each axis were set and encoded 3D trajectories for each of the three main axis views. A simple Convolutional Neural Network (CNN) architecture was used to train a classifier analyzing the joint trajectory maps. Occlusion could not be directly tackled, therefore the representation by Liu, Liu, and Chen <ref type="bibr" target="#b15">[16]</ref> added flexibility by fusing up to nine representation schemes in separate image channels. A similar representation has recently shown to be usable also for action recognition on different modalities and their fusion <ref type="bibr" target="#b16">[17]</ref>. Kim and Reiter <ref type="bibr" target="#b8">[9]</ref> on the other hand presented a compact and humaninterpretable representation. Joint movement contributions over time can be interpreted. Interesting to note is also the skeleton transformer by Li et al. <ref type="bibr" target="#b9">[10]</ref>. They employ a fully connected layer to transform skeleton sequences into a 2 dimensional matrix representation.</p><p>Yang et al. <ref type="bibr" target="#b31">[32]</ref> present a joint order that puts joints closer together if their respective body parts are connected. It is generated by a depth-first tree traversal of the skeleton starting in the lower chest. Skepxels are small 5 ? 5-pixel segments containing the positions of all 25 skeleton joints in a random but fixed order. Liu et al. <ref type="bibr" target="#b10">[11]</ref> use this 2D structure as it is more easily captured by CNNs. Each sample of a sequence is turned into multiple sufficiently different Skepxels which are then stacked on top of each other. These Skepxels differ only in their joint permutation. The full Skepxel-image of a sequence of skeletons is assembled width-wise, without altering the joint permutation within one row of Skepxels. Caetano et al. <ref type="bibr" target="#b0">[1]</ref> generate two images containing motion information in the form of an orientation and a magnitude. The orientation is defined by the angles between the motion vector and the coordinate axes. The angles are stored in the color channels of an image, with time in horizontal and the joints in TSSI order in vertical direction. The gray-scale magnitude image contains the euclidean norm of the motion vectors instead.</p><p>One-shot recognition in general aims at finding a method to classify new instances with a single reference sample. Possible approaches for solving problems of this category are metric learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7]</ref>, or meta-learning <ref type="bibr" target="#b3">[4]</ref>. In action recognition this means a novel action can be learned with a single reference demonstration of the action. In contrast to one-shot image classification, actions consist of sequential data. A single frame might not contain enough context to recognize a novel activity.</p><p>Along with the NTU RGB+D 120 dataset, Liu et al. <ref type="bibr" target="#b12">[13]</ref> presented a one-shot action recognition protocol and corresponding baseline approaches. The Advanced Parts Semantic Relevance (APSR) approach extracts features by using a spatio-temporal LSTM method. They propose a semantic relevance measurement similar to word embeddings. Body parts are associated with an embedding vector and a cosine similarity is used to calculate a semantic relevance score. Sabater et al. <ref type="bibr" target="#b23">[24]</ref> presented a one-shot action recognition approach based on a Temporal Convolutional Network (TCN). After normalization of the skeleton stream, they calculate pose features and use the TCN for the generation of motion descriptors. The descriptors at the last frame, assumed to contain all relevant motion from the skeletonsequence, are used to calculate the distances to the reference samples. Action classes are associated by thresholding the distances. Our previous work on multi-modal one-shot action recognition <ref type="bibr" target="#b17">[18]</ref> proposed to formulate the one-shot action recognition problem as a deep metric learning problem. Signals originating from various sensors are transformed into images and an encoder is trained using triplet-loss. The focus in that work was on showing the multi-modal applicability, whereas in this work we concentrate on skeleton-based oneshot action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>We propose a novel, compact image representation for skeleton sequences. Additionally we present an encoder model that learns to project said representations into a metric embedding space that encodes action similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>A standard approach for action recognition is trained on C classes, where the training and test sets share the same C classes. Thus a test set T share the same classes as the training set D. In an one-shot action recognition setting C classes are known in a training set D, while the test set T contains U novel classes, providing a single reference sample per class in an auxiliary set A, where |A| = U . We consider the one-shot action recognition problem as a metric learning problem. Our goal is to train a feature embedding</p><formula xml:id="formula_0">x = f ? (I ) with parameters ? which projects input images I ? {0, . . . , 255} H?W ?3 , into a feature rep- resentation x ? X d .</formula><p>H denotes the height of the image, W denotes the width of the image in an RGB channel image and d is the given target embedding vector size. The feature representation reflects minimal distances in embedding space for similar classes. For defining the similarity we follow <ref type="bibr" target="#b28">[29]</ref>, where the similarity of two samples</p><formula xml:id="formula_1">(I i , x i ) and (I j , x j ) is defined as D ij :=&lt; x i , x j &gt;, where &lt; ?, ? &gt; denotes the dot product, resulting in an K ? K similarity matrix D.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Skeleton-DML Representation</head><p>We encode skeleton sequences into an image representation. <ref type="figure" target="#fig_0">Fig. 2</ref> shows the skeleton as contained in the NTU RGB+D 120 dataset. On a robotic system, these skeletons can be either directly extracted from the RGB-D camera <ref type="bibr" target="#b32">[33]</ref> or from a camera image stream using a human-pose estimation approach <ref type="bibr" target="#b1">[2]</ref>. The input in our case is a skeleton sequence matrix S ? R N ?M ??3 where each row vector represents a discrete joint sequence (for N joints) and each column vector represents a sample of all joint positions at one specific time step of a sequence length M . The matrix is transformed to an RGB image I ? {0, . . . , 255} H?W ?3 . Note, in contrast to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3]</ref> the joint space is not projected to the color channels but unfolded per axis separately like depicted in <ref type="figure">Fig. 3</ref>, and <ref type="figure">Fig. 4</ref>. This results in a dataset</p><formula xml:id="formula_2">D = {(I i , y i )} K i=1 of K training images I 1,...,K with labels y i ? {1, . . . , C}.</formula><p>In contrast to the representations used for multimodal action recognition <ref type="bibr" target="#b16">[17]</ref> or skeleton based action recognition <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b15">16]</ref> the proposed representation is more compact. In comparison to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> our representation separates the joint values for all axes as blocks over the width, keeping all joint values grouped locally together per axis. In <ref type="bibr" target="#b17">[18]</ref> the color channels are used to unfold the joint values. As the skeleton-sequence is represented as an image, the model needs to be applied only to a single image for inference.</p><formula xml:id="formula_3">... ... ... ... . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton Sequence</head><p>Image Representation <ref type="figure">Fig. 3</ref>: Skeleton-DML skeleton representation. x and z denote the skeleton joint component in joint space, the number of joints is reflected by N , which relates to the height of the image H, the sequence length M relates with the width of the image W . Note, instead of projecting the temporal information throughout the width of the image, we project the joint space locally for each dimension and assemble the joint axis blocks over the width.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Extraction</head><p>For better comparability between the approaches we use the same feature extraction method as previously proposed in SL-DML <ref type="bibr" target="#b17">[18]</ref>. Using a Resnet18 <ref type="bibr" target="#b4">[5]</ref> architecture allows us to train a model that converges fast and serves as a good feature extractor for the embedder. The low amount of parameters allows practical use for inference on autonomous mobile robots. Weights are initialized with a pre-trained model and are optimized throughout the training of the embedder. After the last feature layer we use a two-layer perceptron to transform the features to the given embedding size. The embedder is refined by the metric learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Metric Learning</head><p>Metric learning aims to learn a function to project an image into an embedding space, where the embedding vectors of similar samples are encouraged to be closer, while dissimilar ones are pushed apart from each other <ref type="bibr" target="#b28">[29]</ref>. We use a Multi-Similarity-Loss in combination with a Multi-Similarity-Miner <ref type="bibr" target="#b28">[29]</ref> for mining good pair candidates during training. Positive and negative pairs (by class label) that are assumed to be difficult to push apart in the embedding space are mined. <ref type="figure" target="#fig_2">Fig. 5</ref> gives a constructed example of how positive and negative pairs are mined. Positive pairs are constructed by an anchor and positive image pair {I ? , I ? } and its embedding f (I ? ), preferring pairs with a low similarity in embedding space (high distance in embedding space) with the following condition:</p><formula xml:id="formula_4">D + ?? &lt; max k =? D ?k + .<label>(1)</label></formula><p>Similar, if {I ? , I ? } is a negative pair, the condition is:</p><formula xml:id="formula_5">D ? ?? &gt; min k=? D ?k ? ,<label>(2)</label></formula><p>where k is a class label index and is a given margin. Note, these conditions support the mining of hard pairs, i.e. a positive pair where the sample still has a high distance in embedding space and a negative pair that still has a low distance in embedding space. This forces sampling that concentrates on the hard pairs. A set of positive images to ... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton Sequence</head><p>Image Representation <ref type="figure">Fig. 4</ref>: Exemplary representation for a throwing activity of the NTU-RGB+D 120 dataset. A skeleton-sequence serves an input and can be represented as an image directly <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>. Our Skeleton-DML representation groups x-, y-, z joint values locally in M 3 blocks per axis and assembles them into the final image representation. All axis blocks are laid out aside. an anchor image I ? are denoted P i , analog, a set of negative images to I ? are denoted N i .</p><p>Given mined positive-and negative pairs allows us integration into the Multi-Similarity loss, as derivated by Wang et al. <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_6">L M S = 1 K K i=1 1 ? log 1 + k?Pi e ??(D ik ??) + 1 ? log 1 + k?Ni e ?(D ik ??) ,<label>(3)</label></formula><p>where ?, ? and ? are fixed hyper-parameters. In contrast to SL-DML we do not apply weighting to the classifier-and embedder loss, as no marginal improvement has been achieved in <ref type="bibr" target="#b17">[18]</ref>. After the model optimization, associating an action class to a query sample and set of reference samples is now reduced to a nearest-neighbor search in the embedding space. The classifier and encoder are jointly optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation</head><p>Our implementation is based on PyTorch <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b21">[22]</ref>. We tried to avoid many of the metric learning flaws as pointed out by Musgrave et al. <ref type="bibr" target="#b18">[19]</ref> by using their training setup and hyperparameters where applicable. Key differences are that we use a Resnet18 <ref type="bibr" target="#b4">[5]</ref> architecture and avoid the proposed four-fold cross validation for hyperparameter search in favour of better comparability to the proposed one-shot protocol on the NTU RGB+D 120 dataset <ref type="bibr" target="#b12">[13]</ref>. Note, we did not perform any optimization of the hyperparameters. A batch size of 32 was used on a single Nvidia GeForce RTX 2080 TI with 11GB GDDR-6 memory. We trained for 100 epochs with initialized weights of a pre-trained Resnet18 <ref type="bibr" target="#b4">[5]</ref>. For the multi similarity miner we used an epsilon of 0.05 and a margin of 0.1 for the triplet margin loss. A RMSProp optimizer with a learning rate of 10 ?6 was used in all optimizers. The embedding model outputs a 128 dimensional embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>We used skeleton sequences from the NTU RGB+D 120 <ref type="bibr" target="#b12">[13]</ref> dataset for large scale one-shot action recognition.</p><p>The dataset is split into an auxiliary set, representing action classes that are used for training, and an evaluation set. In the one-shot protocol the evaluation set does only contain novel actions. One sample of each test class serves as reference demonstration. This protocol is based on the one proposed by <ref type="bibr" target="#b12">[13]</ref> for the NTU RGB+D 120 dataset.  Attention Network <ref type="bibr" target="#b11">[12]</ref> 41.0 Fully Connected <ref type="bibr" target="#b11">[12]</ref> 42.1 Average Pooling <ref type="bibr" target="#b14">[15]</ref> 42.9 APSR <ref type="bibr" target="#b12">[13]</ref> 45.3 TCN <ref type="bibr" target="#b23">[24]</ref> 46.5 SL-DML <ref type="bibr" target="#b17">[18]</ref> 50.9 Ours 54.2</p><p>First we trained a model on the auxiliary set. The resulting model transforms skeleton-sequences encoded as an image representation into embeddings for the reference actions and then for the evaluation actions. We then calculate the nearest neighbour from the evaluation embeddings to the reference embeddings. As the embeddings encode action similarity we can estimate to which reference sample the given test sample comes closest. Beside the standard one-shot action protocol and experiments with dataset reduction, we give an ablation study that gives a hint on which combination of embedding size, loss, transformation and representation are yielding best results with our approach. Further, we integrated various related skeleton-based image representations that have been previously proposed for action recognition into our one-shot action recognition approach to compare them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The NTU RGB+D 120 <ref type="bibr" target="#b12">[13]</ref> dataset is a large scale action recognition dataset containing RGB-D image streams and skeleton estimates. The dataset consists of 114,480 sequences containing 120 action classes from 106 subjects in 155 different views. We follow the one-shot protocol as described by the dataset authors. The dataset is split into two parts: an auxiliary set and an evaluation set. The action classes of the two parts are distinct. 100 classes are used for training, 20 classes are used for testing. The unseen classes and reference samples are documented in the accompanied dataset repository 1 . A1, A7, A13, A19, A25, A31, A37, A43, A49, A55, A61, A67, A73, A79, A85, A91, A97, A103, A109, A115 are previously unseen. As reference the demonstration for filenames starting with S001C003P008R001* are used for actions with IDs below 60 and S018C003P008R001* for actions with IDs above 60. As no standard validation set is defined in the dataset paper we use the following classes during development for validation: A2, A8, A14, A20, A26, A32, A38, A44, A50, A56, A62, A68, A74, A80, A86, A92, A98, A104, A110, A116. One-shot action recognition results are given in <ref type="table" target="#tab_0">Table I</ref>. Like Liu et al. <ref type="bibr" target="#b12">[13]</ref> we also experimented with the effect of the auxiliary set reduction. Results are given in <ref type="figure" target="#fig_3">Fig. 6</ref> and <ref type="table" target="#tab_0">Table II</ref>. In addition we analyze different representations in <ref type="table" target="#tab_0">Table IV</ref> and the influence of different embedding vector sizes, metric losses and augmentations on two representations more detailed in <ref type="table" target="#tab_0">Table III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Set Size Reduction</head><p>An interesting question that comes up when evaluating one-shot action recognition approaches is how much training classes are required to get a certain performance. Liu et al. <ref type="bibr" target="#b12">[13]</ref> already proposed to evaluate the one-shot action recognition approach with varying training set sizes. Aligned with Liu et al. <ref type="bibr" target="#b12">[13]</ref> we use training sets containing 20, 40, 60, 80 training classes while remaining a constant evaluation set size of 20. For practical systems, where only a limited amount of training data is available, this evaluation can give an important insight about which performance can be achieved with lower amounts of provided training data. It is also interesting to observe how an approach performs when adding more training data. <ref type="table" target="#tab_0">Table II</ref> and <ref type="figure" target="#fig_3">Fig. 6</ref> give results for different training set sizes for SL-DML <ref type="bibr" target="#b17">[18]</ref>, APSR <ref type="bibr" target="#b12">[13]</ref> and our Skeleton-DML approach, while remaining a static validation set. With just 20 training classes, our approach performs comparably to the APSR approach. With a small amount of training classes the SL-DML approach performs best. In our experiments Skeleton-DML performs better when providing a larger training set size. At a training set size of 60 classes, our approach performs comparably well to SL-DML. With 80 classes in the training set our approach starts outperforming SL-DML. It is interesting to note that, aligned with the results from SL-DML, our approach seems to be confused by the 20 extra classes that are added to the 60 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>To distill the effects of the components we report their individual contributions. We examine influence of the represen-throw falling hugging other person grab other person's stuff <ref type="figure">Fig. 7</ref>: From top to bottom: A RGB Frame, the corresponding skeleton sequences and the image representation of those sequences are shown. The latter is used in our one-shot action recognition approach. The first two sequences contain single person activities, whereas the remaining two contain two person interactions. The grab other person's stuff sequence was shorter than the hugging other person sequence.  Skepxel <ref type="bibr" target="#b10">[11]</ref> 29.6 SkeleMotion Orientation <ref type="bibr" target="#b0">[1]</ref> 34.4 SkeleMotion MagnitudeOrientation <ref type="bibr" target="#b0">[1]</ref> 39.2 TSSI <ref type="bibr" target="#b31">[32]</ref> 41.0 Gimme Signals <ref type="bibr" target="#b16">[17]</ref> 41.5 SkeleMotion Magnitude <ref type="bibr" target="#b0">[1]</ref> 44.4 SL-DML <ref type="bibr" target="#b17">[18]</ref> 50.9 Ours 54.2</p><p>Skeleton-DML representation the augmentation improved the results throughout the experiments for both losses. The best results without augmentation were achieved by the SL-DML representation with an embedding vector of size 128 and a MS loss. The overall best results were achieved with a MS loss and embedding vector size of 512 and augmentation by rotation using the Skeleton-DML representation, which improved the results of +4.4% over our approach under a comparable training setup as SL-DML.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with Related Representations</head><p>To support the effectiveness of our proposed representation in a metric learning setting we compare against other skeleton-based image representations. We use the publicly avail able implementation for the SkeleMotion <ref type="bibr" target="#b0">[1]</ref>, SL-DML <ref type="bibr" target="#b17">[18]</ref>, Gimme Signals <ref type="bibr" target="#b16">[17]</ref> and re-implementations of the TSSI <ref type="bibr" target="#b31">[32]</ref> and Skepxels <ref type="bibr" target="#b10">[11]</ref> representations to integrate them into our metric learning approach. These representations have been described in Section II more detailed.</p><p>The overall training procedure was identical as all models were trained with the parameters described in Section III- E. The experiment only differed in the underlying representation. Results for the representation comparison are given in <ref type="table" target="#tab_0">Table IV</ref>. While most of the representations initially target action recognition and are not optimized for oneshot action recognition, they are still good candidates for integration in our metric learning approach. We did not re-implement the individual architecture proposed by the different representations but decided to use the Resnet18 architecture for better comparability.</p><p>Our Skeleton-DML approach shows best performance followed by SL-DML. The SkeleMotion Magnitude <ref type="bibr" target="#b0">[1]</ref> representation transfers well from an action recognition setting to a one-shot action recognition setting. Interesting to note is that the SkeleMotion Orientation <ref type="bibr" target="#b0">[1]</ref> representation, while achieving comparable results in the standard action recognition protocol, performs 10% worse than the same representation encoding the magnitude of the skeleton joints. An early fusion of Magnitude and Orientation on a representation level did not improve the Skelemotion representation but yields a result in between both representations. Similar observations have been made in <ref type="bibr" target="#b17">[18]</ref> by the fusion of inertial and skeleton sequences. The lower performing modality adds uncertainty to the resulting model in our one-shot setting.</p><p>A UMAP embedding of all evaluation samples is shown in <ref type="figure" target="#fig_4">Fig. 8</ref> for our Skeleton-DML approach. Our approach shows better capabilities in distinguishing the actions throw and arm circles. In our approach those clusters can be separated quite well whereas SL-DML struggles to discriminate those two classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Result Discussion</head><p>We evaluated our approach in an extensive experiment setup. Aside from lower performance on lower amounts of classes for training our approach outperformed other approaches. For fair comparison we report the result of +3.3% over SL-DML for training with 100 epochs and without augmentation, as under these conditions the SL-DML result was reported. With augmentation and training for 200 epochs we could improve the baseline for +7.7%. Our approach learns to learn an embedding model that captures semantic relevance from joint movements well. E.g. Skeleton-DML differentiates well between activities that primarily contain hand-or leg-movements. Interactions between multiple person and single person activities are also separated well. Activities to which similar joint movements contribute to are still challenging. These are the activities that are formed by the main cluster in <ref type="figure" target="#fig_4">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We presented a one-shot action recognition approach based on the transformation of skeleton sequences into an image representation. On the image representations an embedder is learned which projects the images into an embedding vector. Distances between encoded actions reflect semantic similarities. Actions can then be classified, given a single reference sample, by finding the nearest neighbour in embedding space. In an extensive experiment setup we compared different representations, losses, embedding vector sizes and augmentations. Our representation remains flexible and yields improved results over SL-DML. Additional augmentation by random 5 degree rotations have shown to further improve the results. We found the overall approach of transforming skeleton sequences into image representations for one-shot action recognition by metric learning a promising idea that allows future research into various directions like finding additional representations, augmentation methods or mining and loss approaches. Especially in robot applications one-shot action recognition approaches have the potential to improve human robot interaction by allowing robots to adapt to unknown situations. The required computational cost for our approach is low, as only a single image representations of the skeleton-sequence needs be embedded by a comparably slim Resnet18-based embedder.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>NTU RGB+D 120 skeleton joint positions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>A possible intermediate state of the embeddings during the training process of two classes (left). During training, pairs, that are difficult to push apart in embedding space, are mined (middle). Given the blue anchor sample, the most difficult positive pair is the blue sample with the highest distance in embedding space. Similar, the closest red sample in embedding space is the corresponding negative sample. The overall goal is to separate the samples in embedding space (right) by minimizing the inter-class scatter and maximize the intra-class distance to the class centers in embedding space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Result graph for increasing auxiliary set sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>UMAP embedding visualization four our approach. Classes are: drink water ?, throw ?, tear up paper ?, take off glasses ?, reach into pocket ?, pointing to something with finger ?, wipe face ?, falling ?, feeling warm ?, hugging other person ?, put on headphone ?, hush (quite) ?, staple book ?, sniff (smell) ?, apply cream on face ?, open a box ?, arm circles ?, yawn ?, grab other person's stuff ?, take a photo of other person ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>One-shot action recognition results on the NTU RGB+D 120 dataset.</figDesc><table><row><cell>Approach</cell><cell>Accuracy [%]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Results for different auxiliary training set sizes for oneshot recognition on the NTU RGB+D 120 dataset in %.</figDesc><table><row><cell cols="4">#Train Classes APSR [13] SL-DML [18] Ours</cell></row><row><cell>20</cell><cell>29.1</cell><cell>36.7</cell><cell>28.6</cell></row><row><cell>40</cell><cell>34.8</cell><cell>42.4</cell><cell>37.5</cell></row><row><cell>60</cell><cell>39.2</cell><cell>49.0</cell><cell>48.6</cell></row><row><cell>80</cell><cell>42.8</cell><cell>46.4</cell><cell>48.0</cell></row><row><cell>100</cell><cell>45.3</cell><cell>50.9</cell><cell>54.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Ablation study for our proposed one-shot action recognition with different representations, embedding sizes, losses and augmentations. Results are given for a training over 200 epochs.</figDesc><table><row><cell>Units are in %.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Representation</cell><cell>128</cell><cell>256</cell><cell cols="2">512 Transform Loss</cell></row><row><cell>SL-DML [18]</cell><cell cols="3">55.2 50.6 52.7</cell><cell>None</cell><cell>MS</cell></row><row><cell>SL-DML [18]</cell><cell cols="3">51.5 51.7 54.0</cell><cell>None</cell><cell>TM</cell></row><row><cell>SL-DML [18]</cell><cell cols="3">51.8 55.3 55.8</cell><cell>Rot</cell><cell>MS</cell></row><row><cell>SL-DML [18]</cell><cell cols="3">53.6 54.8 55.5</cell><cell>Rot</cell><cell>TM</cell></row><row><cell>Ours</cell><cell cols="3">54.7 51.5 53.1</cell><cell>None</cell><cell>MS</cell></row><row><cell>Ours</cell><cell cols="3">47.5 51.9 54.0</cell><cell>None</cell><cell>TM</cell></row><row><cell>Ours</cell><cell cols="3">55.3 58.0 58.6</cell><cell>Rot</cell><cell>MS</cell></row><row><cell>Ours</cell><cell cols="3">56.0 55.1 56.1</cell><cell>Rot</cell><cell>TM</cell></row><row><cell cols="5">tation, augmentation method and different resulting embed-</cell></row><row><cell cols="5">ding vector sizes. Inspired by Roth et al. [23] we experiment</cell></row><row><cell cols="5">with different embedding vector sizes of 128, 256, 512. In</cell></row><row><cell cols="5">addition we included the SL-DML representation, compare a</cell></row><row><cell cols="5">Triplet Margin loss (TM) and a Multi-Similarity loss (MS)</cell></row><row><cell cols="5">and included an augmentation with random rotations of 5 ? .</cell></row><row><cell cols="5">In total 24 models were trained for this ablation study. We</cell></row><row><cell cols="5">trained these models for 200 epochs as we expected longer</cell></row><row><cell cols="5">convergence due to the additional augmented data. Results</cell></row><row><cell cols="5">are given in Table III. In the table we highlight important</cell></row><row><cell cols="5">results. We highlight interesting results by different colors</cell></row><row><cell cols="5">in the table (best result without augmentation (55.2%), em-</cell></row><row><cell cols="5">bedding size of 128 (56.0%), embedding size of 256 (58.0%),</cell></row><row><cell cols="5">TM loss (56.1%), overall, MS loss, augmentation, embedding</cell></row><row><cell cols="5">size of 512 (58.6%)). For SL-DML the augmentation had a</cell></row><row><cell cols="5">positive influence with higher embedding vector sizes of 512.</cell></row><row><cell cols="5">Whereas the augmentation with embedding sizes of 128 only</cell></row><row><cell cols="5">improved with the TM loss. With the MS loss and a low em-</cell></row><row><cell cols="5">bedding size the augmentation did lower the result. For our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation study for different representations.</figDesc><table><row><cell>Representation</cell><cell>Accuracy [%]</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/shahroudy/NTURGB-D</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SkeleMotion: A New Representation of Skeleton Joint Sequences based on Motion Information for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Caetano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Advanced Video and Signal Based Surveillance, AVSS 2019</title>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="172" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton based action recognition with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="579" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CNN architectures for largescale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ieee international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Similarity-Based Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Early action prediction by soft regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Fang</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2568" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE conference on computer vision and pattern recognition workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia &amp; Expo Workshops</title>
		<imprint>
			<publisher>ICMEW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Skepxels: Spatio-temporal Image Representation of Human Skeleton Joints for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global context-aware attention LSTM networks for 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1647" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NTU RGB+ D 120: A Large-Scale Benchmark for 3D Human Activity Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using spatio-temporal lstm network with trust gates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="3007" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skeleton-based human action recognition with global context-aware attention LSTM networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gimme Signals: Discriminative signal encoding for multimodal activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Memmesheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Theisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="978" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Signal Level Deep Metric Learning for Multimodal One-Shot Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Memmesheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Theisen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><surname>Paulus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11085</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="681" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<ptr target="https://github.com/KevinMusgrave/pytorch-metric-learning.2019" />
	</analytic>
	<monogr>
		<title level="j">PyTorch Metric Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tiago: the modular robot that adapts to different research needs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pages</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Marchionni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International workshop on robot modularity, IROS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting training strategies and generalization performance in deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<idno>PMLR. 2020</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="8242" to="8252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">One-shot action recognition towards novel assistive therapies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Sabater</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08997</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ntu rgb+ d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="43" to="53" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5022" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep cosine metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Wojke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE winter conference on applications of computer vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human support robot (HSR)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2018 emerging technologies</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action recognition with spatio-temporal visual attention on skeleton image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2405" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft Kinect Sensor and Its Effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/MMUL.2012.24</idno>
		<idno>DOI: 10 . 1109 / MMUL . 2012 . 24</idno>
		<ptr target="https://doi.org/10.1109/MMUL.2012.24" />
	</analytic>
	<monogr>
		<title level="j">IEEE Multim</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
