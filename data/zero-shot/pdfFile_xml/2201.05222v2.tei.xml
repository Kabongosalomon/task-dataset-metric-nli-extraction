<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Assemble Foundation Models for Automatic Code Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Gu</surname></persName>
							<email>gu@ifi.uzh.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Salza</surname></persName>
							<email>salza@ifi.uzh.ch</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Zurich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><forename type="middle">C</forename><surname>Gall</surname></persName>
							<email>gall@ifi.uzh.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Zurich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Assemble Foundation Models for Automatic Code Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-transfer learning</term>
					<term>adaptive scheme</term>
					<term>Transformer</term>
					<term>Gaussian noise</term>
					<term>code summarization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic code summarization is beneficial to daily software development since it could help reduce the requirement of manual writing. Currently, artificial intelligence is undergoing a paradigm shift. The foundation models pretrained on massive data and finetuned to downstream tasks surpass specially customized models. This trend inspired us to consider reusing foundation models instead of learning from scratch. Thereby, we propose a flexible and robust approach for automatic code summarization, based on neural models. We assemble available foundation models, such as CodeBERT and GPT-2, into a single neural model named AdaMo. Moreover, we utilize Gaussian noise as the simulation of contextual information to optimize the latent representation. Furthermore, we introduce two adaptive schemes from the perspective of knowledge transfer, namely continuous pretraining and intermediate finetuning, and design intermediate stage tasks for general sequence-to-sequence learning. Finally, we evaluate AdaMo against a benchmark dataset for code summarization, by comparing it with state-of-the-art models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In the process of software development and maintenance, decent comments are crucial to program comprehension and could reduce the burden of directly interpreting the code <ref type="bibr" target="#b0">[1]</ref>. By reading and writing code comments, software developers instantly load and save the working context, such as requirements and implementation details. However, it is laborious to manually write comments in a consistent and decent style and keep them synchronized with the code simultaneously. It seems inevitable to study the automatic way of generating high-quality code comments, namely "code summarization", which could save massive effort and time.</p><p>Code summarization is the task of generating natural language descriptions for the given code snippets. In practice, it serves numerous daily activities in software development and maintenance, such as recording implementation details, templatizing package descriptions, and describing code changes for version updates <ref type="bibr" target="#b1">[2]</ref>. Its automatic solutions could improve productivity in software activities effectively. There are several solutions proposed for automatic code summarization, and among them, neural models are the most valued for their unique generative ability. These machine learning solutions significantly rely on the knowledge learned from a large code corpus. Correspondingly, neural models require massive data and intensive computational resources.</p><p>In this paper, we build a transfer learning model ADAMO composed of relevant foundation models for code summarization and also introduce Gaussian noise to enhance the latent representations. Our experimental results showed that ADAMO defeated state-of-the-art (SOTA) models.</p><p>Recent work tend to introduce structure information extracted from the parsed results of code data, e.g., Abstract Syntax Tree (AST) and Control Flow Graph (CFG), as a modality complement. The representatives are SOTA models, namely BASTS <ref type="bibr" target="#b2">[3]</ref> and SIT <ref type="bibr" target="#b3">[4]</ref>. Instead of lightly processing the code data as a token sequence, they parse it into tree or graph forms, thereby taking heavy computation consumption. Compared with SOTA models, our approach is merely based on the token sequence. As token-based neural models proved to be effective in handling sequential data, especially in Natural Language Processing (NLP), we want to leverage on them and see how far we can go. This brings us an advantage over baseline models that probably requires fully parsable code, whereas our model works as well on corrupted or partial code.</p><p>To make up for the disadvantages that our model only learns from less information, our model emphasizes reusing pretrained checkpoints for better parameter initialization <ref type="bibr" target="#b4">[5]</ref>. With the popularity of transfer learning, foundation models are also emerging <ref type="bibr" target="#b5">[6]</ref>. They are generally trained on intensive data at scale and then adapted to various target tasks. Based on that, we leverage pretrained models instead of training a model from scratch. We assemble the representative encoder model and decoder model, e.g., BERT <ref type="bibr" target="#b6">[7]</ref> and GPT <ref type="bibr" target="#b7">[8]</ref>, and then train their assembly. Furthermore, considering that adaptive schemes are already proposed in transfer learning, as the complement to the standard "pretraining and then finetuning" paradigm, we further design intermediate tasks to adopt continuous pretraining and intermediate finetuning on code summarization.</p><p>To summarize, the main contributions of this paper are:</p><p>? proposing an effective and straightforward approach for code summarization, by assembling foundation models; ? adopting the Gaussian noise emitter as a simulation of contextual information for better latent representations; ? introducing adaptive transfer learning schemes as options to further raise the upper bound of model performance; ? designing intuitive intermediate stage tasks for code summarization and general sequence-to-sequence learning.</p><p>The rest of this paper is structured as follows. In Section II, we introduce the background knowledge. In Section III, we present our transfer learning model and adaptive schemes. Section IV describes the experimental setup, whereas Section V presents and discusses the results. Section VI surveys the related work, and Section VII concludes with a summary of the findings and contributions and an outlook on future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In this section, we describe the background of our work, such as Transformer models, e.g., BERT <ref type="bibr" target="#b6">[7]</ref> and GPT <ref type="bibr" target="#b7">[8]</ref>, transfer learning and adaptive schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transformer Models</head><p>The standard Transformer is an encoder-decoder neural model that merely relies on the attention mechanism <ref type="bibr" target="#b8">[9]</ref> as the main component. We call other models inspired by this simple and effective way of building networks as Transformers.</p><p>The architecture of Transformer is a stack of six encoder layers mingled with a stack of six decoder layers. In the encoder layer, there are a self-attention sublayer and a feed-forward network. Instead, in the decoder layer, there is an extra encoderdecoder attention sublayer. The self-attention sublayer is for the connections within encoder layers or decoder layers, while the encoder-decoder attention sublayer is for the connections between the last encoder layer and each decoder layer. Both the self-attention sublayer and encoder-decoder attention sublayer adopt multiple attention units to quantify the importance of each part of the input data in the same manner.</p><p>Due to the superiority of Transformers in the computation complexity and flexibility over prior models, they are popular, especially in the field of NLP. Besides, its capability of computation parallelization allows more intense use of massive data, which led to the development of large-scale pretrained models, namely foundation models <ref type="bibr" target="#b5">[6]</ref>. In this paper, we consider two examples of Transformers in particular: BERT <ref type="bibr" target="#b6">[7]</ref> for discriminative tasks, and GPT <ref type="bibr" target="#b7">[8]</ref> for generative tasks.</p><p>Bidirectional Encoder Representations from Transformers (BERT). BERT <ref type="bibr" target="#b6">[7]</ref> is the typical encoder model inspired by Transformer and follows the idea of merely using the attention mechanism to build the neural model. In terms of model architecture, BERT stacks 12 encoder layers (24 layers for its large version), but no decoder layers. BERT-like models are used for discriminative tasks. Each encoder layer of BERT consists of a self-attention sublayer and a feed-forward network, the same as Transformer. In the self-attention sublayer, each token can attend context to its left and right, so the attention is referred to as "bidirectional".</p><p>Masked Language Modeling (MLM) is a common objective to train BERT-like models. In MLM, the model is trained to predict 15% randomly masked tokens based on the entire context, namely the tokens that occurred on both two sides. The MLM objective works in pair with the bidirectional selfattention, to guide the model to learn a decent representation of the input. RoBERTa <ref type="bibr" target="#b9">[10]</ref>, as its optimized version, achieves further improvements by dynamically masking tokens and offering a bigger data capacity. CODEBERT <ref type="bibr" target="#b10">[11]</ref> is a specialized RoBERTa for the representation of code data. Generative Pretrained Transformer (GPT). GPT <ref type="bibr" target="#b7">[8]</ref> is the typical decoder model inspired by Transformer that mainly uses the attention mechanism to build the neural model. In terms of model architecture, GPT only stacks 12 decoder layers (24, 36, or 48 layers for its medium, large, and extra large versions), but no encoder layers. GPT-like models are used for generative tasks Each decoder layer in GPT consists of a masked self-attention sublayer and a feed-forward network, which is different from Transformer. Besides, there is no encoder-decoder attention sublayer. In the masked self-attention sublayer, each token only attends context to its left. Thus the attention is referred to as "constrained".</p><p>The only self-supervised training objective of GPT is Casual Language Modeling (CLM). In CLM, the model learns to predict 15% randomly masked tokens when given the partial context, namely the tokens that occurred only on their left sides. The CLM objective works in pair with the constrained self-attention, to guide the model to generate a proper sentence as the output. Its enhanced versions, i.e., GPT-2 <ref type="bibr" target="#b11">[12]</ref>, show stronger capability in text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transfer Learning and Adaptive Schemes</head><p>Different from general learning modes, Transfer Learning is the learning paradigm of transferring the knowledge learned from other data to the new data for given domains or tasks <ref type="bibr" target="#b12">[13]</ref>. It relaxes the requirements on data amount, data distribution, and computation capability <ref type="bibr" target="#b13">[14]</ref>. Besides the conventional "pretraining and then finetuning" practice, adaptive schemes are applicable to transfer learning models.</p><p>The intuition behind transfer learning is that the beforehand self-supervised training for data representation promises general and reliable initialization, i.e., knowledge of data distribution, and then benefits the performance of specific downstream tasks, such as raising the upper bound or accelerating convergence. Compared with starting from scratch, transfer learning could improve the sample efficiency since it reduces the consumption required on data and computational resources <ref type="bibr" target="#b14">[15]</ref>.</p><p>Based on transfer learning, models for various tasks could origin from the same pretrained language model. As an extended definition of pretrained language model, the model that is "trained on broad data at scale and can be adapted to a wide range of downstream tasks" is named "foundation model" <ref type="bibr" target="#b5">[6]</ref>. In NLP, representative examples include BERT and GPT, which are trained on large corpora of text and then adapted to a wide range of downstream tasks, e.g., machine translation, question answering, and sentiment analysis.</p><p>The standard methodology of transfer learning is pretraining a model on a large corpus of unlabeled data and then finetuning it on a small supervised dataset. In the pretraining stage, the model is usually trained in a self-supervised learning manner, where the unlabeled data is sufficient for the objective, therefore the pretraining data is usually extensive and readily available. Instead, during the finetuning stage, the model is trained in a supervised learning manner where the ground truth is required. The data used for the target (downstream) task is supervised data, and its quality matters the most, so the data amount is usually limited and the cost is more expensive. Even though it has been conventional to pretrain and then finetune models, there are still strategies proposed to further improve the data adaptability to target domains or tasks, which mainly work between the usual pretraining and finetuning stages.</p><p>Continuous Pretraining (CP) is defined as tailoring a model to another data domain, or even the data of a target task, via the second phase of pretraining <ref type="bibr" target="#b15">[16]</ref>. It has been shown that domain-adaptive pretraining, namely adapting the model to the data of the same domains, improves the performance. Similarly, adapting the model for designated tasks, called task-adaptive pretraining, leads to performance gain as well. Moreover, multiphase adaptive pretraining, e.g., domain-adaptive training followed by the task-adaptive one, promises a larger gain.</p><p>Intermediate Finetuning (IF) benefits a pretrained model by introducing intermediate tasks during the additional training stage, as the warmup activities before training for the target task <ref type="bibr" target="#b16">[17]</ref>. However, the characteristics of intermediate tasks could affect the effectiveness of adaptive finetuning. Divided by difficulty, intermediate tasks could be either simple or complex. The simple intermediate tasks are close to learning the lowlevel skills such as preserving the raw content and detecting the shallow attributes, such as verb tenses or sentences length. They would only affect the model performance slightly. In contrast, complex intermediate tasks are generally rather beneficial to promote the model, e.g., natural language inference <ref type="bibr" target="#b17">[18]</ref>, and question answering <ref type="bibr" target="#b18">[19]</ref>. Thus, they expect the model to have strong capabilities such as perceiving interrelations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. APPROACH</head><p>This section describes the model architecture and its components and how we applied transfer learning. Then, we introduce adaptive schemes <ref type="bibr" target="#b19">[20]</ref>, e.g., continuous pretraining, and intermediate finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Architecture</head><p>Our transfer learning model follows the encoder-decoder architecture for general sequence-to-sequence learning <ref type="bibr" target="#b20">[21]</ref>, which has stacked encoding layers as the encoder and stacked decoding layers as the decoder.</p><p>Instead of training from scratch, building a model by adopting existing foundation models supports reusing the pretrained weights for model initialization <ref type="bibr" target="#b21">[22]</ref>. Thereby, we propose such a model that adopts CODEBERT <ref type="bibr" target="#b10">[11]</ref>, e.g., a specialized RoBERTa for code representation, as the encoder, and GPT-2 <ref type="bibr" target="#b11">[12]</ref>, e.g., an enhanced GPT for text generation, as the decoder. In addition, we place an additive white Gaussian noise (AWGN) emitter between the encoder and the decoder as an option to further optimize the latent representation. We name such a transfer learning model ADAMO, standing for Adaptive Model with reference to the adaptive schemes it supports.</p><p>Considering the input data is code snippets, and output data is code comments, our model adopts CODEBERT as the encoder and GPT-2 as the decoder. CODEBERT can capture the general representations of code data, and similarly, GPT-2 performs well in English text generation. We make no changes to the metadata of the encoder model and decoder, such as model structure and hyperparameters. Besides, the noise emitter between encoder and decoder is expected to generate Gaussian noise as the rough simulation of contextual information of the input data <ref type="bibr" target="#b22">[23]</ref>. As shown in <ref type="figure">Fig. 1</ref>, the red blocks are input data and output data, and the dashed block is the architecture of our model, whose components are represented as blue blocks. We define the processor manipulating the latent representations between encoder and decoder as Refiner. Furthermore, we define the latent representations of encoder and decoder as Parallel Representations. In our model architecture, encoder and decoder both reuse the network design of existing language models and are initialized with the pretrained weights. Therefore the parallel representations of both sides are not naturally coordinated with each other. The refiner is thereby introduced to optimize the latent representations and reduce their incoordination.</p><p>Prior work in natural language translation found it could be equivalent to introducing multiple encoders to capture contextual information when accumulating Gaussian noise to the latent representations <ref type="bibr" target="#b22">[23]</ref>. Gaussian noise is a basic noise model, following the normal distribution, commonly used to mimic the effect of random processes in nature. Considering both code and text data are sequential, and the context information matters as well in code data, we adopt the AWGN emitter to simulate the context-aware setting of multiple encoders.</p><p>Our ADAMO model is mainly composed of a Transformer encoder and decoder, and its architecture naturally supports practices applicable in transfer learning, such as the adaptive schemes. As shown in <ref type="figure">Fig. 2</ref>, we introduce adaptive schemas, e.g., continuous pretraining and intermediate finetuning, between the common pretraining and finetuning stages, to make the model more adaptive to the data of specific domains or tasks. The adaptive schemes are expected to bring improvements by raising the upper bound of model performance. They do not necessarily appear simultaneously in our approach, but their combination is feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adaptive Schemes</head><p>Based on the standard transfer learning paradigm of "pretraining and then finetuning", we introduce adaptive schemes for better data adaptability to target domains or tasks, e.g., continuous pretraining and intermediate finetuning.</p><p>For continuous pretraining, irrespective whether it is Domain-Adaptive (DA) or Task-Adaptive (TA), there exist no obvious differences with the general pretraining stage. However, DA pretraining specifies that the unlabeled data to be used must come from a related domain, while TA pretraining directly utilizes the unlabeled data of the given task. For instance, assuming that we would use a dataset of Physics papers to finetune a pretrained language model, depending on the concrete data to use, continuous pretraining is identified as domain-adaptive or task-adaptive. If the unlabeled data is not from the given dataset but another related dataset, such as the dataset of general scientific papers, it is DA pretraining. On the contrary, if the unlabeled data is from the given dataset, it is referred to as TA pretraining.</p><p>For both DA and TA, we set Masked Language Modeling (MLM) as the objective of the encoder, and Casual Language Modeling (CLM) as the objective of the decoder. Both MLM and CLM involve randomly masking parts of the sequential data, then training the model to predict the missing tokens correctly. For the prediction, MLM allows the model to consider the entire context, i.e., the data occurring to both left and right sides. Instead, CLM only allows the model to consider the partial context, i.e., the data occurring to the left side only. Specifically, for DA pretraining, we pretrain the model with massive unlabeled data from another related dataset, separately train only either encoder or decoder, or train them both. For TA pretraining, we pretrain the model almost in the same way but directly using the unlabeled data from the given dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain-Adaptive:</head><p>The adaptation to the distribution of the related domain of the target data in the model migration. Task-Adaptive: The adaptation to the distribution of the target data when the model migrates from the source data.</p><p>To the best of our knowledge, there exist no ready-made intermediate tasks designed for code summarization yet, and none for general sequence-to-sequence learning. Inspired by text editing <ref type="bibr" target="#b23">[24]</ref>, we propose that the summarization task could be seen as the fusion of two novel stage tasks: one is to insert tokens that occurred in the target text but not in the source text; the other one is to delete tokens that occurred in the source text but not in the target text. Also, the stage tasks concern the order of tokens as well because reordering tokens is a mandatory implicit operation. Considering this, we regard the changes in both the occurrence and order of tokens in the summarization task as concept shifts. We name the first stage task as Concept Extrapolation (CE) and the second one as Concept Interpolation (CI). In addition, one intuitive but straightforward idea is to directly annotate the tokens in the target text with different marks based on whether they are available in the source text. We propose this idea as the comparison task and name it as Concept Annotation (CA). Not limited to code summarization, CA, CE, and CI are applicable to text summarization, even general sequence-to-sequence learning.</p><p>For a given sentence, both CE and CI cover the operation of reordering tokens, however, CE is more like a typical complex task while CI seems relatively simple. The reason is that the model has to associate and append tokens that never appeared in the sentence in the case of CE. Instead, for CI the model merely determines whether each existing token in the sentence should be kept or dropped. Concept Annotation (CA) is a bit different from others because it only annotates tokens and introduces no information about the order of the target text. We regard it as the most straightforward intermediate task. The number of intermediate tasks is suggested to be 1 because when the amount of involved tasks is less than a critical point, e.g., 15, then the fewer tasks, the better the performance <ref type="bibr" target="#b24">[25]</ref>.</p><p>Concept Annotation: The stage task masks the tokens in the target sequence with different tokens based on whether they have ever appeared in the source sequence or not. Concept Extrapolation: The stage task masks the tokens in the target sequence that have already appeared in the source sequence, to drive the model to connect new tokens. Concept Interpolation: The stage task masks the tokens in the target sequence that have never appeared in the source sequence, to drive the model to discard old tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To check the effectiveness of our approach which takes reusing pretrained language models as a prerequisite, we first experiment with the transfer learning assembly itself and then introduce adaptive schemes, e.g., continuous pretraining and intermediate finetuning. In the context of the study, we thereby formulate the research questions as follows.</p><p>RQ1 How well does our transfer learning model perform in code summarization? RQ2 How well does code summarization benefit from the AWGN refiner? RQ3 How well does code summarization benefit from adaptive continuous pretraining? RQ4 How well does code summarization benefit from adaptive intermediate finetuning? The replication repository is published online 1 , including implementations, configuration, results, and the guidance on reproducing experiments. The implementations are in PYTHON, using PYTORCH <ref type="bibr" target="#b25">[26]</ref> and TRANSFORMERS <ref type="bibr" target="#b26">[27]</ref>. The scoring function invokes the third-party evaluation package <ref type="bibr" target="#b27">[28]</ref>. The experiments are conducted on a machine with an AMD EPYC 7702 CPU with 16 GB RAM, and a single Nvidia Tesla V100 GPU with 32 GB memory.</p><p>In the following, we describe the methodology applied to answer the above-mentioned research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baselines</head><p>We compare our model with modern SOTA models, e.g., BASTS <ref type="bibr" target="#b2">[3]</ref> and SIT <ref type="bibr" target="#b3">[4]</ref>. They both utilize structure information to improve the code summarization and outperformed other prior models but have never been compared to each other.</p><p>BASTS parses the code into an AST and converts it into smaller trees, capturing the local non-linear syntax information from them. It splits the code snippet based on its blocks in the dominator tree of the CFG and generates a split AST for each code split. Each split AST will be encoded in the tree-form separately for concatenation with the usual code embedding.</p><p>SIT introduces a structure-induced self-attention mechanism to capture the information on syntax structure, data flow as well as data dependency from AST. It parses the code snippet into an AST, then transforms the tree into adjacency matrices of three views, and finally combines them evenly to complement the information of sequential code data.</p><p>A crucial difference between our transfer learning model and baseline models is that the baseline models are not compatible with adaptive schemes. It is caused by the extensibility of our model. We successively use model tags BASTS and SIT to indicate the introduced baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Corpus</head><p>In our experiments, we use three datasets in total. All datasets consist of numerous pairs of code snippets and code comments in JAVA or PYTHON. All the experiments are conducted on JAVA and PYTHON separately.</p><p>In code summarization, the commonly used evaluation is the one used in SIT <ref type="bibr" target="#b3">[4]</ref>, in which JAVA and PYTHON are selected as the representative research targets considering their language popularity and data richness. We thereby name this benchmark as SIT dataset, and we conduct experiments on that dataset as well. The other baseline model, i.e., BASTS <ref type="bibr" target="#b2">[3]</ref>, adopted a different benchmark. For the sake of clarity, we name that benchmark as BASTS dataset. Considering the difficulty of reproducing BASTS, due to its weak adaptability of the preprocessing pipeline, especially for the performance loss that happens in switching the benchmark, we evaluate our model on the BASTS dataset for fair comparisons.</p><p>In addition to BASTS and SIT datasets, we introduce the CodeSearchNet dataset <ref type="bibr" target="#b28">[29]</ref>, shortly referred to as CSN, to support the experiments for adaptive schemes. The CSN dataset is collected from publicly available open-source repositories in GITHUB. The full dataset contains over two million snippetcomment pairs spanning multiple programming languages. To keep consistent with other datasets, we only utilize its JAVA and PYTHON data. We follow the given data partition of the CSN dataset but merge the given test set into the given training set, considering we would never use the given test set.</p><p>Actually, the BASTS dataset originates from earlier data sources <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, so does the SIT dataset <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Even though the PYTHON data in the BASTS dataset are taken from the CSN dataset, these two datasets are never used for the same model, so there is no potential data leak. Meanwhile, all the data in the SIT dataset originates from GITHUB, so there might be a data overlap between SIT and CSN datasets. However, in our experimental design, we never directly use data pairs in CSN for sequence-to-sequence learning, so no potential data leak exists as well.</p><p>As shown in <ref type="table" target="#tab_0">Table I</ref>, the first column presents datasets, and the second to ninth columns the statistics on the pairs of code snippets and code comments, per language and dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>Considering BLEU <ref type="bibr" target="#b32">[33]</ref>, METEOR <ref type="bibr" target="#b33">[34]</ref>, and ROUGE <ref type="bibr" target="#b34">[35]</ref> are the most common evaluation metrics in code summarization, we thus use them to evaluate our experiments <ref type="bibr" target="#b35">[36]</ref>.</p><p>The definitions of BLEU, METEOR, and ROUGE are based on the same scenario, i.e., for candidate sentences in a corpus, there are a set of reference sentences that corresponds to each of them. In their implementations, the penalty factor ? is for rational adjustments. Besides, their computations rely on the Precision score and the Recall score, short as P and R:</p><formula xml:id="formula_0">P n = #gram n (c, r) #gram n (c) , R n = #gram n (c, r)</formula><p>#gram n (r) where c and r are the candidate sentence and reference sentence, respectively, and #gram is the number of overlapped n-grams. BLEU. The BLEU score computes the averaged percentage of n-gram matches between the candidate sentence and the reference sentence, typically unigrams through 4-grams:</p><formula xml:id="formula_1">BLEU = ? ? exp 1 N N n=1 log P n<label>(1)</label></formula><p>There are two implementation versions of the BLEU score. In one case, as proposed initially, the computation is on the corpus level, namely computing one score for the whole corpus, the score is "corpus BLEU", referred to as C-BLEU. In the other case, the computation is on the sentence level, i.e., computing one score for each sentence and taking their arithmetic mean as the final score, the score is "sentence BLEU", referred to as S-BLEU. METEOR. The METEOR score builds alignments on unigrams between the candidate sentence and the reference sentence, but during the process, alignments are prioritized on the longer n-grams:</p><formula xml:id="formula_2">METEOR = ? ? 10P n R n R n + 9P n<label>(2)</label></formula><p>Roughly speaking, the penalty factor depends on the actual alignment. When the alignment is mainly on the unigrams, the penalty could be rather heavy because of the low similarity.</p><p>ROUGE. The ROUGE score counts the overlaps of n-grams or Longest Common Sequences (LCS) between the candidate sentence and the corresponding reference sentence. Taking the n-grams as an example, the formula is as follows:</p><formula xml:id="formula_3">ROUGE = 2P n R n R n + P n<label>(3)</label></formula><p>The most common ROUGE scores are ROUGE-1, ROUGE-2, and ROUGE-L. Their distinction lies on the overlapped target to be counted, e.g., unigrams, 2-grams, or LCS.</p><p>In our experiments, we compute C-BLEU, S-BLEU, ME-TEOR, and ROUGE-L to quantify the quality of the summarization results. These metrics are in the range of [0, 1] and will be reported in the percentage form. The larger the value, the better the effect. Based on the conclusion of the empirical study of automatic evaluation metrics for code summarization, we give priority to METEOR <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Methodology for RQ1</head><p>There have been numerous pretrained language models proposed for general code-related tasks <ref type="bibr" target="#b37">[38]</ref>, but there is not yet prior work on leveraging existing checkpoints for code summarization. Meanwhile, what the performance margin is between such transfer learning models and existing welldesigned models is still unknown to the community.</p><p>First, we design experiments to compare ADAMO and SOTA models on the same datasets. Considering both the BASTS and SIT models involve complicated preprocessing steps to extract structure information, it is more fair and intuitive to experiment with ADAMO on corresponding datasets directly. Moreover, the potential performance reduction caused by scripts migration may not be easily avoided. In detail, we experiment with ADAMO on the BASTS dataset to compare with BASTS, and on the SIT dataset to compare with SIT.</p><p>We conducted two experiments to build an intuitive understanding of model performance. In one group of experiments, we directly evaluate the model without the finetuning step. In this way, we could estimate how well the pretrained model performs in itself, i.e., without any knowledge of the downstream task, namely zeroshot learning <ref type="bibr" target="#b38">[39]</ref>. In the other group, we evaluate the encoder-decoder model with the finetuning step. Here we assign a roughly equivalent time budget, e.g., 24 hours, to BASTS, SIT, and ADAMO for a fair comparison.</p><p>The tag used for zeroshot learning model is AdaMo-0shot, and for the normal finetuning is AdaMo-basic. In our experiments, the noise emitter is temporarily turned off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Methodology for RQ2</head><p>To study the effectiveness of the AWGN emitter and how its configuration affects the performance of our transfer learning model, we enable the Gaussian noise emitter but with different configurations. Besides, we run each experiment for the same epoch number with the noise emitter off.</p><p>The experimental design is almost the same as for RQ1 because we merely adjust the variance to specific values, without making any other changes. The experiments are on the SIT dataset, which was commonly used in prior work.</p><p>The tag used for the noise model is AdaMo-noise. We use labels in the form of AdaMo-noise[?] to distinguish different settings, where the standard deviation ? could be 0.1, 0.2, 0.3, 0.4 or 0.5. For example, the label for the AWGN emitter, when the standard deviation ? is 0.1, is AdaMo-noise[0.1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Methodology for RQ3</head><p>Results from NLP indicate that the second phase of pretraining in the domain leads to performance gains <ref type="bibr" target="#b15">[16]</ref>. Therefore, it has a promising potential improvement for code summarization.</p><p>As introduced in Section III-B, we adopt Domain-Adaptive (DA) and Task-Adaptive (TA) pretraining as the additional procedure of tailoring the pretrained model to the data of target domains or tasks. For DA pretraining, we utilize the CSN dataset to train encoder, decoder, or them both separately. For TA pretraining, we separately train encoder, decoder, or both on the SIT dataset. The BASTS dataset is excluded since it has a times amount of JAVA data to CSN and their PYTHON data show an extreme overlap.</p><p>As the experimental settings of continuous pretraining, we train the model in both the DA and TA way. Meanwhile, we separately train the encoder, the decoder, and them both in all experiments. To check the performance of the adaptive model after the adaptive continuous pretraining scheme, we offer 24 hours for the additional training phase. That is, the encoder, the decoder, or them both are pretrained for 24 hours first, and then ADAMO itself is trained for 24 hours. Meanwhile, to validate whether continuous pretraining usually requires a longer time like the normal pretraining, we offer a richer time budget, e.g., 48 hours, as the reference.</p><p>The tag used for continuous pretraining is AdaMo-CP. We use labels in the form of AdaMo-CP[ADAPTIVE][OBJ] to distinguish different settings, where ADAPTIVE can be DA or TA, and OBJ could be mlm, clm or both. For example, the label of continuous pretraining only the encoder in the domain-adaptive manner is referred to as AdaMo-CP[DA][mlm].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Methodology for RQ4</head><p>For NLP tasks that require high-level inference and reasoning capabilities, it proves to be effective to introduce relevant tasks with massive data as the intermediate finetuning <ref type="bibr" target="#b16">[17]</ref>.</p><p>As introduced in Section III-B, we want to check whether the intermediate finetuning might benefit our transfer learning approach. Meanwhile, we expect the ideas of domain and task adaptations could bring potential performance improvements.</p><p>As the experimental settings of intermediate finetuning, we conduct two sets of experiments where the model is trained in the domain-adaptive or task-adaptive ways. In each set, there are three experiments in which CA, CE or CI is adopted as the intermediate stage task. We train the model for the stage task on the CSN dataset as the domain-adaptive setting, and on the SIT dataset as the task-adaptive setting. The BASTS dataset is excluded since it has a times amount of JAVA data to CSN and their PYTHON data show an extreme overlap. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>In this section, we present the results of our experiments to answer the research questions. In the following comparisons, the most competitive results are highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results of RQ1</head><p>The results for RQ1 are the summarization scores of our basic transfer learning approach, in comparison with the baseline models introduced above.</p><p>As shown in <ref type="table" target="#tab_0">Table II</ref>, on both BASTS and SIT datasets, AdaMo-0shot reaches the lowest scores. On the contrary, AdaMobasic always performs better than baseline models on all metrics. On the one hand, it shows the incoordination of parallel representations damages the zeroshot ability of AdaMo. On the other hand, it proves the strategy of directly reusing the wellbehaved model structures and their trained weights is effective, meanwhile, the training work of AdaMo-basic is not merely for tuning the encoder and decoder, but also to overcome the incoordination issue of parallel representations.</p><p>Considering that BASTS and SIT are separately reproduced in their respective dataset, it is infeasible to compare their performance directly. With the reference to AdaMo-basic, it outperforms BASTS significantly but only obtains moderate advantages over SIT, therefore the performance of SIT is closer to ADAMO than BASTS, in terms of the model effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ1 -Takeaway:</head><p>Our transfer learning approach beats the SOTA models smoothly when given the same time budget. Even though ADAMO reuses the existing language models and their trained weights, it is indispensable to train their assembly on the data of target domain or for the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results of RQ2</head><p>The results for RQ2 are the summarization scores of ADAMO accumulating varied intensities of Gaussian noise, compared with our basic transfer learning approach. As shown in <ref type="table" target="#tab_0">Table III</ref>, the effectiveness of the AWGN emitter relies on whether or not the noise emitter is well configured. We observed that with increasing values of ? until it is 0.5, the effects decrease first and then increase. When the standard deviation ? is 0.3, ADAMO obtains the best performance. After that, the results start to fluctuate and enter the downtrend on JAVA or a stable state on PYTHON.</p><p>Comparing the effects of Gaussian noise on specific languages, the results of AdaMo-basic improve on all evaluation metrics only when the ? is set to 0.3 on JAVA. On the contrary, results of almost all versions of AdaMo-noise are always better than AdaMo-basic on PYTHON. The cause is believed to lie in the differences in language expressiveness. The code snippets in PYTHON are more intuitive than those in JAVA, and also the PYTHON programs are more close to the common English expressions. English is a natural language while programming languages are artificial or constructed languages. Considering that AWGN is commonly used to mimic the random processes in nature and that PYTHON programs are more close to the English text than JAVA in terms of coding rules, the Gaussian noise should be more effective when the data itself is more natural, or less artificial.</p><p>Overall, the improvements brought by the Gaussian noise are reliable, but it is not conclusive enough. One reason could be the distinctions in data characteristics. The Gaussian noise could not perfectly simulate the context information of the code data, as it does in the text data. The code data is more artificial while the text data is more natural, therefore the effectiveness of Gaussian noise is reduced. The other reason could be the natural difference between the machine translation task and the code summarization task. Code summarization is a hybrid task of text summarization and machine translation because the code snippets are strictly written in compiler-oriented grammar rules, but the code comments flexibly follow the natural grammar rules, and meanwhile, the information that existed in the source data is largely reduced after summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ2 -Takeaway:</head><p>The Gaussian noise benefits the model when the noise emitter is well configured. The effects are more accessible if the data itself is closer to the natural language. However, the improvements of AWGN are reliable for code summarization but not very obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results of RQ3</head><p>The results for RQ3 show the summarization scores of ADAMO when applying the continuous pretraining scheme, by comparing with our basic transfer learning approach. As  we offer two different time budgets, the falling scores for the same environmental setting are marked with an asterisk. Based on the experimental results shown in <ref type="table" target="#tab_0">Table IV</ref>, the continual pretraining scheme shows poor performance on JAVA but could reliably improve the results on PYTHON.</p><p>If we compare the results of AdaMo-CP <ref type="bibr">[DA]</ref> with AdaMo-CP[TA], then task-adaptive models usually perform more satisfying than domain-adaptive ones. Even though the domain-adaptive scheme owns the advantage in resource intensiveness, the task-adaptive one shows higher efficiency in data utilization. Therefore, we conclude that task-adaptive is more suitable for continual pretraining.</p><p>When other experimental settings are the same, pretraining only the decoder seems the optimal choice since the results of The phenomenon can be explained by two reasons. First, code data follow strict grammar rules than text data; therefore the text data contain more extensive entropy, which indicates the decoder learning the interrelations of text tokens is more efficient. Second, when it is task-adaptive, the representation of PYTHON code could be easier to be optimized by the encoder.</p><p>As shown in <ref type="table" target="#tab_4">Table V</ref>, if we assign a richer time budget for the continuous pretraining scheme, the patterns found before keep constant. Besides, all best results are contributed by CP models. Meanwhile, we found that a long training time seems not necessary, since almost half of the scores are reduced a bit. Overall, that continuously pretraining both encoder and decoder seldom promises better results than training only encoder or decoder. It should be caused by the incoordination of parallel representations and would be solved after sufficient finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ3 -Takeaway:</head><p>The effects of continuous pretraining exist but vary with code data. Its combination with the task-adaptive scheme promises better improvements. It is recommended to only continuously pretrain the decoder since pretraining both the encoder and decoder is not necessarily better.  The results for RQ4 are the summarization scores of ADAMO when applying the intermediate finetuning scheme, in comparison with our basic transfer learning approach.</p><p>According to the experimental results shown in <ref type="table" target="#tab_0">Table VI</ref>, the combination of domain-adaptive finetuning with concept extrapolation achieves the best scores on all metrics. In contrast, the combination of domain-adaptive with concept annotation performs the worst and even worse than AdaMo-basic. All the other combinations have similar performance to AdaMo-basic, with slight improvements or a bit deteriorations.</p><p>When comparing domain-adaptive with task-adaptive, the former performs better than the latter if the intermediate stage task is CE or CI, but the situation becomes opposite if the stage task is CA. We believe that CE and CI are complex tasks while CA is rather a simple task. The domain-adaptive scheme learns from more data compared to the task-adaptive one, therefore it shows better generalization when given the same time budget. However, simple intermediate tasks might mislead the domain-adaptive scheme. The opposite case is that the performance of the task-adaptive scheme almost keeps unchanged no matter which stage task it is. Therefore, the domain-adaptive scheme should pair with complex intermediate tasks, while task-adaptive might not benefit too much from the intermediate finetuning.</p><p>Among all intermediate tasks, concept annotation is weaker than others. Meanwhile, CA is sensitive to the data in some cases, such as pairing with the domain-adaptive scheme on the JAVA data. However, other tasks usually promise better results and are insensitive to languages. On both JAVA and PYTHON, CE and CI obtain at least similar but usually better results than AdaMo-basic, therefore, complex intermediate tasks are more stable and beneficial than simple ones. When pairing with the domain-adaptive scheme, concept extrapolation always performs better than concept interpolation, and while pairing the task-adaptive scheme, their results are close.</p><p>To check the effects of intermediate tasks on the generated comments, especially CE and CI, we could observe selected examples as shown in <ref type="table" target="#tab_0">Table VII</ref>. The longest common substrings occurred in others that include the ground truth, but not in AdaMo-basic, are emphasized in italics, and they contribute the most to the improvements. Besides, the distinctions between CE and CI cause differences in other tokens, which are usually never appeared in the corresponding code data for the former CE case but often already appeared there for the latter CI case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RQ4 -Takeaway:</head><p>The combination of complex intermediate tasks with the domain-adaptive scheme is the best choice. The optimal intermediate task is concept extrapolation. The task-adaptive scheme promises more stable results, but its improvements are not so obvious as for the domain-adaptive scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion</head><p>To build a clear comparison of the effect promotion given by each extension, we compute the growth extents of best scores in each research question relative to those of SIT, as shown in <ref type="table" target="#tab_0">Table VIII</ref>. Summarizing the results, we notice that the intermediate finetuning scheme pairing with domainadaptive always boosts the results. On JAVA, the second largest growth origins from its combination with the task-adaptive way. On PYTHON, the second-best results are contributed by the continual pretraining scheme pairing with the task-adaptive way. Then, the refiner with adequate Gaussian noise performs the optimal while others have close results with the basic ADAMO.</p><p>It seems hard to understand that, when it is domain-adaptive, intermediate finetuning performs better than continuous pretraining. It seems that CP learns the latent representation by optimizing the self-supervised objectives, but IF learns the interrelations of tokens via our stage tasks in a supervised way. Therefore, CP is slower than IF when given the same time budget. When it is task-adaptive, CP and IF show their advantages on separate datasets. We believe it is due to the fact that the knowledge learned from the target dataset is more easily beneficial to the target task. Thereby, the results are likely to be intervened by data characteristics and fortuity.</p><p>As reflected in our results, Gaussian noise and continuous pretraining have a small contribution to the achieved effectiveness. There are no alternatives found yet to the AWGN emitter. We have experimented with other common noises in signal processing but only got negative results. For continuous pretraining, our results show that training the encoder did not help a lot, which indicates the MLM objective could be replaced with other token-level ones <ref type="bibr" target="#b39">[40]</ref> for potential improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Threats to Validity</head><p>Internal Validity. The most crucial limitation to our results comes from the representativeness of our evaluation data, although we already use the most common dataset for evaluation. In our early-stage experiments, we found that results for both baseline models and our transfer learning model, evaluated on the CSN data, are rather low, even though ours are still better. Based on our analysis to the phenomenon, we concluded that it is caused by the quality differences of code comments. Therefore, it would be meaningful to evaluate the performance of models on the data of various quality levels systemically.</p><p>External Validity. The results are limited in the way that they can be generalized to relevant generative tasks. Our approach applies to generative tasks where the input data and output data are sequential data, such as program migration and code generation, respectively from code to code and from text to code. The limitation is whether or not there are already pretrained models available for generating the data. However, the task that involves generating code is harder because the generated programs might be partial, buggy, and even specious. Therefore, there still exists potential challenges on how to effectively generalize our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RELATED WORK</head><p>In the following, we discuss prior works on code summarization and summarize the research status in topics of text summarization and program representation.</p><p>Code Summarization. In recent years, there have been several neural-based approaches proposed for code summarization. CODE-NN <ref type="bibr" target="#b40">[41]</ref> uses the Long Short-Term Memory (LSTM) network <ref type="bibr" target="#b41">[42]</ref> combined with global attention <ref type="bibr" target="#b42">[43]</ref> for both code retrieval and code summarization. HYBRID-DRL <ref type="bibr" target="#b31">[32]</ref> applies reinforcement learning to incorporate the AST structure and sequential content of snippets by using an actor-critic network.</p><p>Besides the token sequence, the parsing results of the code data, e.g., AST and CFG, are used for code summarization as well. HYBRID-DEEPCOM <ref type="bibr" target="#b29">[30]</ref> fuses the lexical and syntactical information of code tokens and serialized ASTs using the Gated Recurrent Unit (GRU) network <ref type="bibr" target="#b43">[44]</ref>. ATTENDGRU <ref type="bibr" target="#b44">[45]</ref> applies the GRU encoders for code sequences and the serialized ASTs, and adopts attention components for their interrelations with the summary tokens.</p><p>Not only do neural models, but also information retrieval methods perform well in code summarization, therefore some work is inspired to combine them. RENCOS <ref type="bibr" target="#b45">[46]</ref> utilizes the search engine to find the most semantically or syntactically similar code snippets to augment samples. HYBRID GNN <ref type="bibr" target="#b46">[47]</ref> constructs code property graph and meanwhile learns attentionbased dynamic graph as the training data. In the process, it retrieves the most similar code and corresponding summary for information augmentation.</p><p>The latest models are focused on adapting Transformer, considering its success in natural language-related tasks. C2NL <ref type="bibr" target="#b47">[48]</ref> is the enhanced Transformer <ref type="bibr" target="#b8">[9]</ref> equipped with copy attention <ref type="bibr" target="#b48">[49]</ref> and relative position encoding <ref type="bibr" target="#b49">[50]</ref>. It merely relies on the knowledge from the sequence of code tokens. Furthermore, SIT <ref type="bibr" target="#b3">[4]</ref> introduces the structure-induced attention mechanism to capture information from syntax structure, data flow, and data dependency. Follow the idea of building representations for AST <ref type="bibr" target="#b50">[51]</ref>, BASTS <ref type="bibr" target="#b2">[3]</ref> decomposes the code data into blocks in CFG to generate a split AST for each of them and eventually generate corresponding representations. Therefore, it utilizes the syntax information from split ASTs, instead of the only original AST. Apart from these models, some work systematically studies current issues in code summarization, including the commonly used datasets and evaluation metrics <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Text Summarization. Inspired by neural machine translation <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b51">[52]</ref>, the sequence-to-sequence model with attention is proposed for abstractive summarization <ref type="bibr" target="#b52">[53]</ref>. By leveraging the pointer network <ref type="bibr" target="#b53">[54]</ref>, PGNET <ref type="bibr" target="#b48">[49]</ref> drives a pointer and a generator in parallel to freely choose from either the copied contents or the generated tokens. Meanwhile, it introduces the coverage mechanism [55] to penalize repetitions. Even further, BOTTOM-UP <ref type="bibr" target="#b55">[56]</ref> first selects potential tokens for the summary and then generates the summary using the PGNET. SEQCOPYNET <ref type="bibr" target="#b57">[57]</ref> extends the copy mechanism, which not merely learns to copy the isolated tokens, but also the subsequences. SAGCOPY <ref type="bibr" target="#b58">[58]</ref> enhances the copy mechanism based on the token importance. It builds a directed graph and adopts the degree centrality to identify the key tokens.</p><p>Gradually, neural models equipped with copy mechanism are replaced by pretrained models, such as PEGASUS <ref type="bibr" target="#b59">[59]</ref> for abstractive summarization, as well as MASS <ref type="bibr" target="#b60">[60]</ref> and BART <ref type="bibr" target="#b56">[61]</ref> for the general sequence-to-sequence tasks. Based on Transformer and transfer learning, universal models represented by T5 <ref type="bibr" target="#b61">[62]</ref> are proposed, which are intended to solve most common NLP tasks at once. As the reflection of text summarization, SUMMEVAL <ref type="bibr" target="#b62">[63]</ref> intends to resolve critical shortcomings in evaluation methods.</p><p>Program Representation. By simply regarding code data as token sequences, self-supervised representation learning could be tailored for code data, such as CODEBERT <ref type="bibr" target="#b10">[11]</ref>, CODEX <ref type="bibr" target="#b63">[64]</ref> and PLBART <ref type="bibr" target="#b64">[65]</ref>. With hypothesis that "programs with the same functionality should have similar underlying representations", CONTRACODE <ref type="bibr" target="#b65">[66]</ref> builds representations of program functionalities by learning from contrastive samples <ref type="bibr" target="#b66">[67]</ref>.</p><p>Also, it is common to first parse the code data into tree or graph structures for richer information. There have been many models proposed to learn the parsed results of code data. ASTNN <ref type="bibr" target="#b50">[51]</ref> splits each AST into a sequence of small trees for better representations. MRNCS <ref type="bibr" target="#b67">[68]</ref> recaps serialization schemes on tree structures and categorized them into samplingbased <ref type="bibr" target="#b68">[69]</ref> and traversal-based ones <ref type="bibr" target="#b30">[31]</ref>. TDLS <ref type="bibr" target="#b69">[70]</ref> utilizes GGNN <ref type="bibr" target="#b70">[71]</ref> to learn both syntactic and semantic information. Opposite to static analysis, DYPRO <ref type="bibr" target="#b71">[72]</ref> and LIGER <ref type="bibr" target="#b72">[73]</ref> learn program representations through dynamic executions as well, from the mixture of symbolic and concrete execution traces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this paper, we demonstrated the transfer learning model performs well in code summarization by assembling available foundation models, CODEBERT and GPT-2. Then we utilized Gaussian noise to optimize the latent representation of the assembly by simulating the context-aware settings. Last, we introduced continuous pretraining and intermediate finetuning as adaptive schemes for optional improvements. In addition, we proposed concept interpolation and concept extrapolation as the intermediate stage tasks for code summarization and validated their effectiveness. These tasks apply to general sequence-tosequence learning as well. Moreover, we experimented with adaptation ideas to tailor foundation models with the data of related domains or designated tasks.</p><p>The goal of our work is to show that the transfer learning model based on existing foundation models is rather competitive and could even outperform SOTA models. Moreover, our results showed that neural models regarding code data merely as sequential data could still be powerful enough. Also, our model is compatible with various adaptive schemes, which promises further improvements in model performance. Compared with SOTA models, ADAMO is more friendly to potential rises in quality from either the model side or the data side.</p><p>Despite our results and findings, there are still questions waiting to be solved. For example, it is challenging to align the latent representations of pretrained encoder and decoder models efficiently, or else, it might be possible to make use of pretrained models in a more flexible way, just like playing building blocks. Besides, it should be valuable to implement the refiner with certain neural models for the manipulation of latent representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>The ADAMO architecture. The Adaptive schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>AdaMo-CP[DA][clm] is better than those of other AdaMo-CP[DA] models, so does AdaMo-CP[TA][mlm]. A special case is AdaMo-CP[TA][mlm], which performs better on PYTHON.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF THE SNIPPET-COMMENT PAIRS IN THE CORPUS</figDesc><table><row><cell></cell><cell cols="2">BASTS</cell><cell></cell><cell>SIT</cell><cell cols="2">CSN</cell></row><row><cell>Dataset</cell><cell>JAVA</cell><cell>PYTHON</cell><cell>JAVA</cell><cell>PYTHON</cell><cell>JAVA</cell><cell>PYTHON</cell></row><row><cell>Train</cell><cell>415,395</cell><cell>216,436</cell><cell>69,708</cell><cell>57,203</cell><cell>170,106</cell><cell>265,734</cell></row><row><cell>Valid</cell><cell>12,885</cell><cell>12,119</cell><cell>8,714</cell><cell>19,067</cell><cell>10,955</cell><cell>14,918</cell></row><row><cell>Test</cell><cell>13,237</cell><cell>12,767</cell><cell>8,714</cell><cell>19,066</cell><cell>-</cell><cell>-</cell></row><row><cell>Total</cell><cell>441,517</cell><cell>241,322</cell><cell>87,136</cell><cell>95,336</cell><cell>181,061</cell><cell>280,652</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARATIVE</head><label>II</label><figDesc>RESULTS ON CODE SUMMARIZATION To investigate the performance of ADAMO after the intermediate finetuning scheme, we offer 24 hours for the additional training phase. For the domain-adaptive and task-adaptive settings, they both have additional 24 hours for intermediate finetuning, apart from the 24 hours for the normal finetuning. The tag used for intermediate finetuning is AdaMo-IF. We use labels in the form of AdaMo-IF[ADAPTIVE][PROXY] to distinguish different settings, where ADAPTIVE could be DA or TA, and PROXY could be CA, CE or CI. For example, the label of the CA stage task for domain-adaptive is referred to as AdaMo-IF[DA][CA].</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>JAVA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PYTHON</cell><cell></cell></row><row><cell>Dataset</cell><cell>Model</cell><cell cols="8">C-BLEU S-BLEU METEOR ROUGE C-BLEU S-BLEU METEOR ROUGE</cell></row><row><cell>BASTS</cell><cell>BASTS</cell><cell>27.82%</cell><cell>34.22%</cell><cell>22.86%</cell><cell>45.78%</cell><cell>2.33%</cell><cell>14.18%</cell><cell>8.65%</cell><cell>20.87%</cell></row><row><cell></cell><cell>AdaMo-0shot</cell><cell>0.00%</cell><cell>1.80%</cell><cell>0.10%</cell><cell>0.24%</cell><cell>0.00%</cell><cell>1.78%</cell><cell>0.15%</cell><cell>0.36%</cell></row><row><cell></cell><cell>AdaMo-basic</cell><cell>31.38%</cell><cell>37.64%</cell><cell>25.59%</cell><cell>49.90%</cell><cell>5.19%</cell><cell>16.46%</cell><cell>12.51%</cell><cell>27.31%</cell></row><row><cell>SIT</cell><cell>SIT</cell><cell>38.01%</cell><cell>44.96%</cell><cell>27.09%</cell><cell>53.54%</cell><cell>26.46%</cell><cell>33.81%</cell><cell>21.37%</cell><cell>41.18%</cell></row><row><cell></cell><cell>AdaMo-0shot</cell><cell>0.00%</cell><cell>1.79%</cell><cell>0.12%</cell><cell>0.22%</cell><cell>0.00%</cell><cell>1.89%</cell><cell>0.07%</cell><cell>0.13%</cell></row><row><cell></cell><cell>AdaMo-basic</cell><cell>40.49%</cell><cell>45.30%</cell><cell>28.19%</cell><cell>53.99%</cell><cell>26.52%</cell><cell>33.85%</cell><cell>21.68%</cell><cell>41.25%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III RESULTS</head><label>III</label><figDesc>OF THE MODEL WITH DIFFERENT AWGN REFINERS</figDesc><table><row><cell></cell><cell></cell><cell>JAVA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PYTHON</cell><cell></cell></row><row><cell>Model</cell><cell>C-BLEU</cell><cell cols="2">S-BLEU METEOR</cell><cell cols="2">ROUGE C-BLEU</cell><cell>S-BLEU</cell><cell>METEOR</cell><cell>ROUGE</cell></row><row><cell>AdaMo-basic</cell><cell>40.49%</cell><cell>45.30%</cell><cell>28.19%</cell><cell>53.99%</cell><cell>26.52%</cell><cell>33.85%</cell><cell>21.68%</cell><cell>41.25%</cell></row><row><cell>AdaMo-noise[0.1]</cell><cell>39.04%</cell><cell>43.41%</cell><cell>26.86%</cell><cell>51.67%</cell><cell>25.21%</cell><cell>32.55%</cell><cell>20.79%</cell><cell>39.45%</cell></row><row><cell>AdaMo-noise[0.2]</cell><cell>40.62%</cell><cell>45.33%</cell><cell>28.18%</cell><cell>53.86%</cell><cell>26.65%</cell><cell>34.03%</cell><cell>21.95%</cell><cell>41.83%</cell></row><row><cell>AdaMo-noise[0.3]</cell><cell>40.52%</cell><cell>45.35%</cell><cell>28.25%</cell><cell>54.06%</cell><cell>26.80%</cell><cell>34.05%</cell><cell>21.92%</cell><cell>41.67%</cell></row><row><cell>AdaMo-noise[0.4]</cell><cell>40.37%</cell><cell>44.92%</cell><cell>27.93%</cell><cell>53.36%</cell><cell>26.78%</cell><cell>34.04%</cell><cell>21.89%</cell><cell>41.71%</cell></row><row><cell>AdaMo-noise[0.5]</cell><cell>40.56%</cell><cell>45.00%</cell><cell>28.04%</cell><cell>53.49%</cell><cell>26.74%</cell><cell>34.05%</cell><cell>21.91%</cell><cell>41.64%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV RESULTS</head><label>IV</label><figDesc>OF THE CONTINUALLY PRETRAINING SCHEME(24 HOURS)    </figDesc><table><row><cell></cell><cell></cell><cell cols="2">JAVA</cell><cell></cell><cell></cell><cell cols="2">PYTHON</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">C-BLEU S-BLEU</cell><cell cols="2">METEOR ROUGE</cell><cell>C-BLEU</cell><cell>S-BLEU</cell><cell cols="2">METEOR ROUGE</cell></row><row><cell>AdaMo-basic</cell><cell>40.49%</cell><cell>45.30%</cell><cell>28.19%</cell><cell>53.99%</cell><cell>26.52%</cell><cell>33.85%</cell><cell>21.68%</cell><cell>41.25%</cell></row><row><cell>AdaMo-CP[DA][mlm]</cell><cell>40.50%</cell><cell>45.05%</cell><cell>27.98%</cell><cell>53.40%</cell><cell>26.47%</cell><cell>33.83%</cell><cell>21.55%</cell><cell>40.99%</cell></row><row><cell>AdaMo-CP[DA][clm]</cell><cell>40.18%</cell><cell>45.22%</cell><cell>28.20%</cell><cell>53.94%</cell><cell>26.35%</cell><cell>33.68%</cell><cell>21.73%</cell><cell>41.41%</cell></row><row><cell>AdaMo-CP[DA][both]</cell><cell>40.62%</cell><cell>45.05%</cell><cell>28.16%</cell><cell>53.82%</cell><cell>26.35%</cell><cell>33.61%</cell><cell>21.46%</cell><cell>40.98%</cell></row><row><cell>AdaMo-CP[TA][mlm]</cell><cell>40.61%</cell><cell>45.27%</cell><cell>28.08%</cell><cell>53.72%</cell><cell>27.10%</cell><cell>34.43%</cell><cell>22.25%</cell><cell>42.53%</cell></row><row><cell>AdaMo-CP[TA][clm]</cell><cell>40.01%</cell><cell>44.97%</cell><cell>28.19%</cell><cell>53.96%</cell><cell>26.53%</cell><cell>33.80%</cell><cell>21.89%</cell><cell>41.66%</cell></row><row><cell>AdaMo-CP[TA][both]</cell><cell>40.45%</cell><cell>44.93%</cell><cell>27.98%</cell><cell>53.64%</cell><cell>26.86%</cell><cell>34.21%</cell><cell>22.16%</cell><cell>42.37%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V RESULTS</head><label>V</label><figDesc>OF THE CONTINUALLY PRETRAINING SCHEME(48 HOURS)    </figDesc><table><row><cell></cell><cell></cell><cell cols="2">JAVA</cell><cell></cell><cell></cell><cell cols="2">PYTHON</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">C-BLEU S-BLEU</cell><cell cols="2">METEOR ROUGE</cell><cell>C-BLEU</cell><cell>S-BLEU</cell><cell cols="2">METEOR ROUGE</cell></row><row><cell>AdaMo-basic</cell><cell>40.49%</cell><cell>45.30%</cell><cell>28.19%</cell><cell>53.99%</cell><cell>26.52%</cell><cell>33.85%</cell><cell>21.68%</cell><cell>41.25%</cell></row><row><cell>AdaMo-CP[DA][mlm]</cell><cell>40.61%</cell><cell>45.18%</cell><cell>28.03%</cell><cell>53.49%</cell><cell cols="2">26.28%* 33.56%*</cell><cell>21.46%*</cell><cell>40.85%*</cell></row><row><cell>AdaMo-CP[DA][clm]</cell><cell cols="2">39.82%* 44.97%*</cell><cell>28.10%*</cell><cell>53.95%</cell><cell cols="2">26.34%* 33.67%*</cell><cell>21.81%</cell><cell>41.69%</cell></row><row><cell>AdaMo-CP[DA][both]</cell><cell cols="2">40.39%* 44.94%*</cell><cell>27.97%*</cell><cell cols="2">53.40%* 26.50%</cell><cell>33.76%</cell><cell>21.78%</cell><cell>41.53%</cell></row><row><cell>AdaMo-CP[TA][mlm]</cell><cell cols="2">40.55%* 45.37%</cell><cell>28.15%</cell><cell>53.97%</cell><cell cols="2">26.96%* 34.39%*</cell><cell>22.08%*</cell><cell>42.18%*</cell></row><row><cell>AdaMo-CP[TA][clm]</cell><cell>40.49%</cell><cell>45.46%</cell><cell>28.36%</cell><cell>54.27%</cell><cell>26.61%</cell><cell>33.86%</cell><cell>21.84%*</cell><cell>41.58%*</cell></row><row><cell>AdaMo-CP[TA][both]</cell><cell>40.51%</cell><cell>45.09%</cell><cell>28.07%</cell><cell>53.68%</cell><cell>27.03%</cell><cell>34.30%</cell><cell>22.25%</cell><cell>42.49%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI RESULTS</head><label>VI</label><figDesc>OF THE INTERMEDIATE FINETUNING SCHEME(24 HOURS)    </figDesc><table><row><cell></cell><cell></cell><cell>JAVA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PYTHON</cell><cell></cell></row><row><cell>Model</cell><cell>C-BLEU</cell><cell cols="2">S-BLEU METEOR</cell><cell cols="2">ROUGE C-BLEU</cell><cell>S-BLEU</cell><cell>METEOR</cell><cell>ROUGE</cell></row><row><cell>AdaMo-basic</cell><cell>40.49%</cell><cell>45.30%</cell><cell>28.19%</cell><cell>53.99%</cell><cell>26.52%</cell><cell>33.85%</cell><cell>21.68%</cell><cell>41.25%</cell></row><row><cell>AdaMo-IF[DA][CA]</cell><cell>29.06%</cell><cell>35.75%</cell><cell>20.53%</cell><cell>44.20%</cell><cell>25.60%</cell><cell>33.05%</cell><cell>21.02%</cell><cell>40.11%</cell></row><row><cell>AdaMo-IF[DA][CE]</cell><cell>41.21%</cell><cell>46.33%</cell><cell>29.11%</cell><cell>55.59%</cell><cell>28.37%</cell><cell>35.44%</cell><cell>23.25%</cell><cell>44.28%</cell></row><row><cell>AdaMo-IF[DA][CI]</cell><cell>40.65%</cell><cell>45.99%</cell><cell>28.83%</cell><cell>55.31%</cell><cell>27.66%</cell><cell>34.88%</cell><cell>22.96%</cell><cell>43.82%</cell></row><row><cell>AdaMo-IF[TA][CA]</cell><cell>39.79%</cell><cell>44.63%</cell><cell>27.73%</cell><cell>53.19%</cell><cell>25.84%</cell><cell>33.42%</cell><cell>21.42%</cell><cell>40.97%</cell></row><row><cell>AdaMo-IF[TA][CE]</cell><cell>40.50%</cell><cell>45.42%</cell><cell>28.02%</cell><cell>53.45%</cell><cell>26.53%</cell><cell>33.91%</cell><cell>21.15%</cell><cell>40.51%</cell></row><row><cell>AdaMo-IF[TA][CI]</cell><cell>40.15%</cell><cell>45.33%</cell><cell>28.30%</cell><cell>54.24%</cell><cell>26.22%</cell><cell>33.65%</cell><cell>21.79%</cell><cell>41.44%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII SHOWCASE</head><label>VII</label><figDesc>OF THE EFFECTS OF THE INTERMEDIATE TASKS</figDesc><table><row><cell>Sample</cell><cell>Model</cell><cell>Comment</cell></row><row><cell>Java#229</cell><cell>AdaMo-basic</cell><cell>returns true if the given word contains a whitespace</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CE]</cell><cell>returns true if the input string contains a word except for word engines</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CI]</cell><cell>check if the input string contains a word</cell></row><row><cell></cell><cell>Ground Truth</cell><cell>returns true if the input string contains a word -breaking character</cell></row><row><cell>Java#517</cell><cell>AdaMo-basic</cell><cell>ensures that the object value is at the given allocation</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CE]</cell><cell>ensures that a value is not null</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CI]</cell><cell>ensures that the given location is not null</cell></row><row><cell></cell><cell>Ground Truth</cell><cell>ensure the given value is not null and return it</cell></row><row><cell>Java#821</cell><cell>AdaMo-basic</cell><cell>draws a face of the specified shape</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CE]</cell><cell>draws a circle for the given parameters</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CI]</cell><cell>draws a dial on the given parameters</cell></row><row><cell></cell><cell>Ground Truth</cell><cell>draws a cylinder for the given parameters</cell></row><row><cell cols="2">Python#201 AdaMo-basic</cell><cell>remove the most recent history from the history</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CE]</cell><cell>remove all completed jobs</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CI]</cell><cell>remove all the completed jobs from the history</cell></row><row><cell></cell><cell>Ground Truth</cell><cell>remove all completed jobs from history</cell></row><row><cell cols="2">Python#439 AdaMo-basic</cell><cell>return a string description of the appropriate description on the given path</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CE]</cell><cell>return a string describing the probable encoding of a file</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CI]</cell><cell>return a string describing the probable encoding of a unicode path</cell></row><row><cell></cell><cell>Ground Truth</cell><cell>return a string describing the probable encoding of a file</cell></row><row><cell cols="2">Python#660 AdaMo-basic</cell><cell>returns a set of all cliques of a chordal graph</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CE]</cell><cell>return the set of maximal cliques of the chordal graph</cell></row><row><cell></cell><cell>AdaMo-IF[DA][CI]</cell><cell>returns set of maximal cliques of the chordal graph g</cell></row><row><cell></cell><cell>Ground Truth</cell><cell>returns the set of maximal cliques of a chordal graph</cell></row><row><cell cols="2">D. Results of RQ4</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII OVERVIEW</head><label>VIII</label><figDesc>OF THE PROMOTION EFFECTS ON CODE SUMMARIZATION</figDesc><table><row><cell></cell><cell></cell><cell>JAVA</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PYTHON</cell><cell></cell></row><row><cell>Model</cell><cell>C-BLEU</cell><cell cols="2">S-BLEU METEOR</cell><cell cols="2">ROUGE C-BLEU</cell><cell>S-BLEU</cell><cell cols="2">METEOR ROUGE</cell></row><row><cell>AdaMo-basic</cell><cell>6.52%</cell><cell>0.76%</cell><cell>4.06%</cell><cell>0.84%</cell><cell>0.23%</cell><cell>0.12%</cell><cell>1.45%</cell><cell>0.17%</cell></row><row><cell>AdaMo-noise</cell><cell>6.87%</cell><cell>0.87%</cell><cell>4.28%</cell><cell>0.97%</cell><cell>1.28%</cell><cell>0.71%</cell><cell>2.71%</cell><cell>1.58%</cell></row><row><cell>AdaMo-CP[DA]</cell><cell>6.87%</cell><cell>0.58%</cell><cell>4.10%</cell><cell>0.75%</cell><cell>0.04%</cell><cell>0.06%</cell><cell>1.68%</cell><cell>0.56%</cell></row><row><cell>AdaMo-CP[TA]</cell><cell>6.84%</cell><cell>0.69%</cell><cell>4.06%</cell><cell>0.78%</cell><cell>2.42%</cell><cell>1.83%</cell><cell>4.12%</cell><cell>3.28%</cell></row><row><cell>AdaMo-IF[DA]</cell><cell>8.42%</cell><cell>3.05%</cell><cell>7.46%</cell><cell>3.83%</cell><cell>7.22%</cell><cell>4.82%</cell><cell>8.80%</cell><cell>7.53%</cell></row><row><cell>AdaMo-IF[TA]</cell><cell>6.55%</cell><cell>1.02%</cell><cell>4.47%</cell><cell>1.33%</cell><cell>0.26%</cell><cell>0.30%</cell><cell>1.97%</cell><cell>0.63%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/jianguda/afm4acs</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Learning for Source Code Modeling and Generation. Models, Applications, and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Babar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Automatic Code Summarization: A Systematic Literature Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04352</idno>
		<ptr target="https://arxiv.org/abs/1909.04352" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.SE</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving Code Summarization with Block-Wise Abstract Syntax Tree Splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07845[cs.SE],2021</idno>
		<ptr target="https://arxiv.org/abs/2103.07845" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Code Summarization with Structure-Induced Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1078" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07139</idno>
		<ptr target="https://arxiv.org/abs/2106.07139" />
		<title level="m">Pre-Trained Models: Past, Present and Future</title>
		<imprint>
			<date type="published" when="2021-08" />
		</imprint>
	</monogr>
	<note>cs.AI</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the Opportunities and Risks of Foundation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<ptr target="https://arxiv.org/abs/2108.07258" />
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/language-unsupervised/" />
	</analytic>
	<monogr>
		<title level="j">OpenAI, Tech. Rep</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08155</idno>
		<ptr target="https://arxiv.org/abs/2002.08155" />
		<title level="m">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Language Models Are Unsupervised Multitask Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/better-language-models/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Survey on Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Decade Survey of Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="166" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-Organizing Maps for Storage and Transfer of Knowledge in Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karimpanal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bouffanais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marasovi{\&amp;apos;c}</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964[cs.CL],2020</idno>
		<ptr target="https://arxiv.org/abs/2004.10964" />
		<title level="m">Don&apos;t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5231" to="5247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Storks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01172</idno>
		<ptr target="https://arxiv.org/abs/1904.01172" />
		<title level="m">Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03868</idno>
		<ptr target="https://arxiv.org/abs/1911.03868" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Recent Advances in Language Model Fine-Tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<ptr target="https://ruder.io/recent-advances-lm-fine-tuning/" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Leveraging Pre-Trained Checkpoints for Sequence Generation Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="264" to="280" />
			<date type="published" when="2020-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3512" to="3518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garrido</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10687</idno>
		<ptr target="https://arxiv.org/abs/2003.10687" />
		<title level="m">Felix: Flexible Text Editing Through Tagging and Insertion</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Muppet: Massive Multi-Task Representations with Pre-Finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page" from="5799" to="5811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. {dAlch{\&apos;e}-Buc}, E. Fox, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020-10" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zumer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09799</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">CodeSearchNet Challenge: Evaluating the State of Semantic Code Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<ptr target="https://arxiv.org/abs/1909.09436" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Code Comment Generation with Hybrid Lexical and Syntactical Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2179" to="2217" />
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Code Comment Generation</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Program Comprehension (ICPC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="200" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving Automatic Source Code Summarization Via Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="397" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Machine Translation (StatMT)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Code to Comment &quot;Translation&quot;: Data, Metrics, Baselining &amp; Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sezhiyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="746" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reassessing Automatic Evaluation Metrics for Code Summarization Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fakhoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arnaoudova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering</title>
		<imprint>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
		<respStmt>
			<orgName>ESEC/FSE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Svyatkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04664</idno>
		<ptr target="https://arxiv.org/abs/2102.04664" />
		<title level="m">CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>cs.SE</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-Shot Learning: The Good, the Bad and the Ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3077" to="3086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On Losses for Modern Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aroca-Ouellette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rudzicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Summarizing Source Code Using a Neural Attention Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-Based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Neural Model for Generating Natural Language Summaries of Program Subroutines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leclair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="795" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Retrieval-Based Neural Source Code Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1385" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Retrieval-Augmented Generation for Code Summarization Via Hybrid GNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Siow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICRL</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Transformer-Based Approach for Source Code Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4998" to="5007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Get to the Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-Attention with Relative Position Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Novel Neural Source Code Representation Based on Abstract Syntax Tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="783" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICRL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pointer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Modeling Coverage for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bottom-up Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4098" to="4109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sequential Copying Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4987" to="4994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Self-Attention Guided Copy Mechanism for Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1355" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">PEGASUS: Pre-Training with Extracted Gap-Sentences for Abstractive Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">MASS: Masked Sequence to Sequence Pre-Training for Language Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
	<note>Linguistics (ACL)</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<ptr target="https://arxiv.org/abs/1910.10683" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">SummEval: Re-Evaluating Summarization Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kry?ci?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="391" to="409" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ponde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating Large Language Models Trained on Code</title>
		<imprint/>
	</monogr>
	<note>cs.LG], 2021. [Online</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unified Pre-Training for Program Understanding and Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2655" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Contrastive Code Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5954" to="5971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction by Learning an Invariant Mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multimodal Representation for Neural Code Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Software Maintenance and Evolution (ICSME)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="483" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A General Path-Based Representation for Predicting Program Properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zilberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="404" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Learning to Represent Programs with Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khademi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00740[cs.LG]</idno>
		<ptr target="https://arxiv.org/abs/1711.00740" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Learning Scalable and Precise Representation of Program Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05251</idno>
		<ptr target="https://arxiv.org/abs/1905.05251" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.PL</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning Blended</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02136</idno>
		<ptr target="https://arxiv.org/abs/1907.02136" />
	</analytic>
	<monogr>
		<title level="m">Precise Semantic Program Embeddings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.SE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
