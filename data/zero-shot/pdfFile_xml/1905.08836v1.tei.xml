<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sample Efficient Text Summarization Using a Single Pre-Trained Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
							<email>urvashik@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
							<email>kevclark@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
							<email>jurafsky@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
							<email>lukaszkaiser@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Brain</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sample Efficient Text Summarization Using a Single Pre-Trained Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="da">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2018. Improving language understanding by generative pre-training. URL https://s3-us-west-2.amazonaws.com/openaiassets/research-covers/languageunsupervised/language understanding paper.pdf.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language model (LM) pre-training has resulted in impressive performance and sample efficiency on a variety of language understanding tasks. However, it remains unclear how to best use pre-trained LMs for generation tasks such as abstractive summarization, particularly to enhance sample efficiency. In these sequence-to-sequence settings, prior work has experimented with loading pre-trained weights into the encoder and/or decoder networks, but used non-pre-trained encoder-decoder attention weights. We instead use a pre-trained decoder-only network, where the same Transformer LM both encodes the source and generates the summary. This ensures that all parameters in the network, including those governing attention over source states, have been pre-trained before the fine-tuning step. Experiments on the CNN/Daily Mail dataset show that our pre-trained Transformer LM substantially improves over pre-trained Transformer encoder-decoder networks in limited-data settings. For instance, it achieves 13.1 ROUGE-2 using only 1% of the training data (?3000 examples), while pre-trained encoder-decoder models score 2.3 ROUGE-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language model (LM) pre-training has led to impressive results on tasks ranging from text classification to sequence tagging to question answering <ref type="bibr" target="#b3">(Dai and Le, 2015;</ref><ref type="bibr">Peters et al., 2018;</ref><ref type="bibr">Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2018)</ref>. Particularly striking is the improved sample efficiency achieved by these models <ref type="bibr" target="#b8">(Howard and Ruder, 2018)</ref>. However, it remains unclear how to best utilize pretrained LMs for generation tasks such as text summarization, and how much sample efficiency gains would still apply given that generation models often require large datasets to perform well.</p><p>While prior work has explored improving sequence-to-sequence models by incorporating pre-trained weights from LSTM LMs <ref type="bibr" target="#b7">(G?lcehre et al., 2015;</ref><ref type="bibr" target="#b12">Ramachandran et al., 2017)</ref>, their models include many non-pre-trained parameters, such as additional LSTM layers or the weights governing the models' attention mechanisms. These parameters have to be trained from scratch, which can require lots of labeled data. On the other hand, <ref type="bibr" target="#b11">Radford et al. (2019)</ref> have recently trained a large Transformer <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> language model and applied it to summarization without any fine-tuning (thus adding no non-pretrained parameters), demonstrating the model's zero-shot abilities. We explore pre-training a large Transformer language model and fine-tuning it for text summarization, demostrating the model's sample efficiency. In order to use pre-trained weights more efficiently, we use a Transformer-based decoderonly network <ref type="bibr">(Liu et al., 2018)</ref> during fine-tuning. This decoder-only network, or Transformer LM, treats summarization as a language modeling task where each example consists of a summary appended to its article. Rather than using separate encoder and decoder components, a single network is used to both encode the source and generate the target. Crucially, it includes pre-trained self-attention parameters which are used to attend to both the source and the previously generated target representations. This approach (1) avoids the redundancy of loading copies of the same pre-trained weights into the encoder and decoder, (2) uses fewer parameters compared to encoderdecoder networks, and most importantly (3) ensures all model weights, including those controlling attention over source states, are pre-trained.</p><p>The pre-trained Transformer LM performs competitively on the CNN/Daily Mail dataset, despite using a simple model without augmentations like a copy mechanism <ref type="bibr" target="#b13">(See et al., 2017)</ref> or reinforcement learning <ref type="bibr" target="#b2">(Chen and Bansal, 2018)</ref>. Crucially, this model is extremely sample efficient. Fine-tuning it on only 1% data (?3000 examples), results in a ROUGE-2 score of 13.1, while pre-trained Transformer encoder-decoder networks perform poorly with a ROUGE-2 of 2.3 (see <ref type="figure">Figure 1</ref>). Such a highly sample efficient model opens the door for training summarization models for narrow domains, low-resource languages, or other settings without abundant labeled training data. In addition, analysis shows that our model is more abstractive than the pointergenerator model <ref type="bibr" target="#b13">(See et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we describe the neural architectures and methods used for training a language model and fine-tuning it for text summarization.</p><p>Language Model Pre-training. In this study, we train a Transformer <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> based language model. Unlike <ref type="bibr">ELMo (Peters et al., 2018)</ref>, which trains LMs in both directions, or BERT <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>, which trains a bidirectional word imputation model, we train a unidirectional LM <ref type="bibr" target="#b11">(Radford et al., 2019)</ref>. This is necessary for initializing an auto-regressive decoder.</p><p>Encoder-Decoder Baselines. We first combine pre-training with standard sequence-to-sequence <ref type="bibr" target="#b15">(Sutskever et al., 2014)</ref> models. These consist of a Transformer encoder network that reads the article, a Transformer decoder network that generates the summary, and an encoder-decoder attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> that allows the decoder to attend to encoder states dur- ing generation, shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Both the encoder and decoder use the same network architecture as the Transformer LM, making it easy to use the pre-trained weights. We compare three ways of incorporating weights from a pre-trained LM, proposed by <ref type="bibr" target="#b12">Ramachandran et al. (2017)</ref>: <ref type="formula">(1)</ref> pretraining the encoder only, (2) pre-training the decoder only, and (3) pre-training both. In <ref type="formula">(3)</ref>, the encoder-decoder attention parameters are the only ones randomly initialized. When fine-tuning these models on summarization data, we follow recent work <ref type="bibr" target="#b8">(Howard and Ruder, 2018;</ref><ref type="bibr">Radford et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2018)</ref> and fine-tune all of the pre-trained as well as non-pre-trained parameters.</p><p>Transformer LM. We can simplify the encoderdecoder model by casting summarization as a language modeling task <ref type="bibr">(Liu et al., 2018)</ref>. This is done by appending each target (the summary) to its source (the article), along with a delimiter, and training a Transformer on this reformulated data. Similar to encoder-decoder models, we only compute loss over the target sequence, as adding loss from the source sequence did not improve performance. In this setting, a Transformer based decoder-only network is used to process both the source and the target, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The self attention of this decoder-only network, or Transformer LM, is now used to attend to source states as well as the states of already generated targets. This approach allows for better utilization of pre-trained LMs by removing all non-pre-trained parameters from the model, compared to encoderdecoder models where encoder-decoder attention parameters are not pre-trained. This is a key benefit of using Transformer LMs over LSTM LMs, which do not include attention mechanisms. Using a Transformer LM also avoids the redundancy of loading copies of the pre-trained weights into both the encoder and the decoder.</p><p>Model Augmentations. While there has been extensive research on model augmentations for text summarization <ref type="bibr" target="#b6">(G?lcehre et al., 2016;</ref><ref type="bibr">Pasunuru and Bansal, 2018;</ref><ref type="bibr" target="#b9">Li et al., 2018)</ref>, they are often complicated or summarization-specific. Therefore, since we want to both clearly isolate the benefits of LM pre-training and experiment with a general model applicable to many domains, we do not augment our simple models. We note, however, that many of these augmentations could be added to our models to further improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we describe our data, models, training details, and results. Finally, we discuss the impressive sample efficiency of our model.</p><p>Pre-Training Data. While recent prior work <ref type="bibr">(Radford et al., 2018</ref>) trains their language model on the Toronto BookCorpus <ref type="bibr" target="#b17">(Zhu et al., 2015)</ref>, this dataset appears to no longer be publicly available. 1 Therefore, we instead collected a new 2-billionword corpus based on Wikipedia called WikiLM. To facilitate future research on generative pretraining, we make the corpus publicly available. 2 Summarization Data. We use the nonanonymized CNN/Daily Mail dataset <ref type="bibr" target="#b13">(See et al., 2017)</ref>, which involves summarizing news articles into 2-3 sentences. Summarization performance is typically evaluated using ROUGE-1 (unigram overlaps), ROUGE-2 (bigram overlaps) and ROUGE-L (subsequence overlaps) <ref type="bibr" target="#b10">(Lin, 2004)</ref>. Although not perfect, these metrics serve as a good first approximation to quantify performance.</p><p>Models. For baselines, we evaluate encoderdecoder models with no pre-training as well as the three pre-training strategies discussed in Section 2. Using weights from a pre-trained LM constrains the model to be unidirectional and very large, so we report results from a smaller model with a bidirectional encoder to quantify how this affects performance. We report results from our single Transformer LM with and without pretraining. Additionally, we provide scores for a small 4-layer Transformer with a copy mechanism and coverage loss <ref type="bibr" target="#b5">(Gehrmann et al., 2018)</ref>, as a reference for the kind of gains achieved by adding these augmentations. We also provide zero-shot summarization scores from <ref type="bibr" target="#b11">Radford et al. (2019)</ref> which uses a ten times larger pre-trained Transformer language model, but without any finetuning, and thus also without introducing non-pretrained parameters. Lastly, we provide scores from the state-of-the-art LSTM based model <ref type="bibr" target="#b1">(Celikyilmaz et al., 2018)</ref>, which includes a reinforcement learning objective.</p><p>Training Details. We use the neural architecture and hyperparameters from <ref type="bibr">Radford et al. (2018)</ref>, which have demonstrated excellent results on a variety of tasks. It is a 12-layer Transformer with 135M parameters. The same architecture is used for both components in our encoder-decoder models. For LM pre-training, we use a byte pair encoding vocabulary of 63,807 subwords <ref type="bibr" target="#b14">(Sennrich et al., 2016</ref>) and train the model on WikiLM for 30 epochs. It converges to a perplexity of 20.5, similar to the 18.4 perplexity reached by <ref type="bibr">Radford et al. (2018)</ref>. <ref type="bibr">3</ref> For supervised training, we found it beneficial to use a lower learning rate (5 ? 10 ?5 instead of 2 ? 10 ?4 ) and train for fewer epochs (6 instead of 12), when incorporating pre-trained weights, compared to when training from scratch. During inference, we use beam search with beam size 2 while generating summaries. 4</p><p>Results using the full training set. <ref type="table">Table 1</ref> shows ROUGE scores for our models with and without pre-training. We find that pre-training improves performance by about 2 ROUGE points, on average. Surprisingly, when only the decoder is pre-trained, ROUGE gets substantially worse. We speculate this is because the model starting out with a well-trained decoder and poor encoder learns to overly rely on its language modeling abilities and not adequately incorporate information from the encoder. The Transformer LM outperforms corresponding models both with and without pre-training, despite having almost half as many parameters. Our best model performs competitively with existing models on the CNN/Daily Mail abstractive summarization task, despite the absence of model augmentations such as a copy mechanism and reinforcement learning objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sample Efficiency Results</head><p>We test the sample efficiency of our summarization models by training them on randomly generated 1%, 2%, 5%, 10%, 20%, and 50% subsets of the CNN/Daily Mail training data (287,227 examples), using the same subsets for all models.</p><p>To strengthen the baseline encoder-decoder network, we make the encoder bidirectional and use a smaller 4-layer model. We make no changes to the other hyperparameters except increasing the number of training epochs. <ref type="figure">Figure 1</ref> illustrates sample efficiency for our encoder-decoder models and Transformer LM, both with and without pretraining. We only report ROUGE-2 scores which are lowest of the three metrics used, though trends are consistent for ROUGE-1 and ROUGE-L. Unsurprisingly pre-training improves sample efficiency, but gains are much larger when also using the Transformer LM. Fine-tuning the pretrained Transformer LM on 1% data, less than 3,000 examples, results in a model that achieves a ROUGE-2 score of 13.1, compared to the pre-trained Transformer encoder-decoder model which only scores 2.3 ROUGE-2 points. To ensure that augmenting the model with a copy mechanism does not close the sample efficiency gap, we trained the 4-layer Transformer + coverage + copy mechanism model from <ref type="bibr" target="#b5">Gehrmann et al. (2018)</ref> on 1% and 5% of the summarization data and Ground Truth: A man in suburban Boston is selling snow online to customers in warmer states. For $89, he will ship 6 pounds of snow in an insulated Styrofoam box.</p><p>Encoder-Decoder + Pre-training: NEW: A snowfall of is forecast for New England. NEW: The Massachusetts-based company hopes to sell more than 30,000 bottles of snow. The company says it will use snow from as far as Canada.</p><p>Transformer LM + Pre-training: Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box -enough for 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.  found that while it slightly outperforms our baseline models (getting 2.5 ROUGE-2 at 1% and 5.1 ROUGE-2 at 5%), it still performs much worse than our pre-trained Transformer LM. Overall, our results indicate that while pre-training does improve sample efficiency, having every parameter being pre-trained instead of only a subset of them provides large gains in limited data settings. <ref type="table" target="#tab_2">Table 2</ref> shows generated outputs for models finetuned on 1% data. The pre-trained Transformer LM succeeds in copying salient information from the source, which indicates it can effectively attend over the source article. On the other hand, the pre-trained Encoder-Decoder model hallucinates facts such as "30,000 bottles of snow", which are topical but never appear in the source, suggesting that the model is unable to utilize information from the source. Instead, it behaves more as a general domain language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>The pre-trained Transformer LM's predictions initially raised the concern: how much of the im-pressive sample efficiency is due to copying from the source? After all, the no-training-required "lead-3" baseline <ref type="bibr" target="#b13">(See et al., 2017)</ref>, which uses the first three sentences of the article as the summary, achieves 17.7 ROUGE-2. Hence, we investigate the extent to which summaries are copied from the articles by computing n-gram overlaps between the articles and summaries. In <ref type="figure" target="#fig_1">Figure 3</ref>, we compare n-gram overlaps for (1) our pretrained Transformer LM fine-tuned on 1% data, (2) <ref type="bibr" target="#b13">See et al. (2017)</ref>'s pointer-generator with coverage model, and (3) gold summaries. While our model copies from the source more often than gold summaries, it is more abstractive than the pointergenerator model which copies 70% of all generated 10-grams, compared to our model which copies 27% of its generated 10-grams, likely due to lack of a copy mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Sample efficiency can be vital for narrow domains and low-resource settings, especially in the case of generation tasks for which models often require large datasets to perform well. In this paper, we have shown that using a single pre-trained Transformer LM for sequence-to-sequence tasks simplifies the model, reduces the number of parameters, and removes the non-pre-trained encoder-decoder attention weights. More importantly, experiments fine-tuning the model on only 1% training data have shown that our approach achieves impressive sample efficiency gains. It would be interesting to further test whether this approach leads to similar sample efficiency gains on tasks beyond summarization, such as dialogue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Our pre-trained Transformer LM uses the same network to process both the source and target. The encoder-decoder attention parameters, which are not pre-trained, are no longer necessary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>n-gram overlaps of predicted and gold summaries with their corresponding source articles. While our model copies more than gold summaries, it copies substantially less than the pointer-generator model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Sample outputs after training on 1% data. See the supplementary materials for more outputs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">See http://yknzhu.wixsite.com/mbweb 2 Available to download at https://github.com/ tensorflow/tensor2tensor</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">However, the perplexities are not directly comparable because they are on different corpora</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">All code available at https://github.com/ tensorflow/tensor2tensor</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was started and, in part, carried out during the first author's internship at Google Brain. We thank Ashwin Paranjape and Yuhao Zhang for their thoughtful comments and suggestions. We gratefully acknowledge support of the DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF15-1-0462 and the NSF via grant IIS-1514268. Kevin is supported by a Google PhD Fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head><p>Our Transformer LM + pre-training system is extremely data-efficient: it generates decent summaries using only 1% of the training data while the baseline pre-trained Encoder-Decoder model essentially generates gibberish. Examples illustrating this behavior are shown in <ref type="table">Table 3</ref>. Furthermore, our Transformer LM appears to be more abstractive than the Pointer-Generator network, which uses a copy mechanism. While the Pointer-Generator often copies over whole sentences from the article, our model mixes and matches sentence fragments, with other generated words sometimes connecting them. The downside of this abstractiveness is the way it occasionally "hallucinates" facts, such as in the amusing summary in row 5.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep communicating agents for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast abstractive summarization with reinforce-selected sentence rewriting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bottom-up abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>G?lcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>G?lcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">On using monolingual corpora in neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving neural abstractive document summarization with explicit information selection modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: ACL Workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised pretraining for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06724</idno>
	</analytic>
	<monogr>
		<title level="m">Encoder-Decoder + Pre-Training Fine-tuned on 1% of training data Transformer LM + Pre-Training Fine-tuned on 1% of training data Pointer-Generator</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>See et al.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">terror attacks that left more than 160 people dead in mumbai , india &apos;s most populous city .] [zaki-ur-rehman lakhvi , a top leader of the terrorist group lashkar-e-taiba , was released early friday from a jail in the pakistani city of rawalpindi</title>
	</analytic>
	<monogr>
		<title level="m">was released early friday from</title>
		<imprint>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
	<note>zaki-ur-rehman lakhvi , a top leader of. lashkar-e-taiba. jail in the pakistani city of rawalpindi] [.] [he] [is. accused of masterminding the. lakhvi was charged in pakistan in 2009 , accused of masterminding the november 2008 terror attacks that left more than 160 people dead in mumbai , india &apos;s most populous city .</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">he saw that buddy holly , ritchie valens and j.p. &quot; the big bopper &quot; richardson had been tragically killed in an airplane crash in clear lake , iowa .] [don mclean &apos;s pop masterpiece &quot; american pie</title>
		<imprint>
			<date type="published" when="1959-02-03" />
		</imprint>
	</monogr>
	<note>] [christie &apos;s sold the 16-page handwritten manuscript of the song &apos;s lyrics for $ 1.2 million to an unnamed buyer . mclean was a paperboy when. he saw that buddy. valens and j.p. &quot; the big bopper &quot; richardson had been tragically killed</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">to end iran &apos;s international isolation under years of crippling sanctions .] [iranians erupted in celebration as young people waved flags from their sunroofs , blasted music from stereos and chatted online with the hashtag # irantalks .] [excitement came after a breakthrough nuclear deal with the united states and other world powers that promises to end iran &apos;s international isolation under years of crippling sanctions</title>
		<imprint/>
	</monogr>
	<note>won the world cup. nuclear deal with the united states] [. the] [deal. promises</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Comparisons of summaries generated by various models</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Colors/brackets correspond to consecutive words that occur in the article (black means the word was not in the article text</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
