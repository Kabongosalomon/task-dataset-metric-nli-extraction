<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Egocentric Deep Multi-Channel Audio-Visual Active Speaker Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><surname>Murdock</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Vamsi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ithapu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Meta Reality Labs</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Egocentric Deep Multi-Channel Audio-Visual Active Speaker Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Augmented reality devices have the potential to enhance human perception and enable other assistive functionalities in complex conversational environments. Effectively capturing the audio-visual context necessary for understanding these social interactions first requires detecting and localizing the voice activities of the device wearer and the surrounding people. These tasks are challenging due to their egocentric nature: the wearer's head motion may cause motion blur, surrounding people may appear in difficult viewing angles, and there may be occlusions, visual clutter, audio noise, and bad lighting. Under these conditions, previous state-of-the-art active speaker detection methods do not give satisfactory results. Instead, we tackle the problem from a new setting using both video and multi-channel microphone array audio. We propose a novel end-to-end deep learning approach that is able to give robust voice activity detection and localization results. In contrast to previous methods, our method localizes active speakers from all possible directions on the sphere, even outside the camera's field of view, while simultaneously detecting the device wearer's own voice activity. Our experiments show that the proposed method gives superior results, can run in real time, and is robust against noise and clutter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding conversational context and dynamics from an egocentric perspective is vital for creating realistic and useful augmented reality (AR) experiences. These attributes characterize the interactions of multiple speakers in a given scene with the AR device wearer (i.e., ego). An example such device may consist of glasses with outward looking cameras and microphones so that audio-visual data is captured from the wearer's point of view. Modeling these attributes involves not only detecting and tracking people within a scene, but also localizing the voice activity within a conversation. In this work, we focus on the task of active speaker localization (ASL) with the goal of detecting the spatio-temporal location of all active speakers both within and outside the camera's field of view <ref type="bibr">(FOV)</ref>. Closely related to the problem of active speaker detection (ASD), ASL involves estimating the relative direction of arrival of speech <ref type="bibr">Figure 1</ref>. Our novel multi-channel audio-visual deep network localizes active speakers from any direction on the sphere. In this illustration, predicted active speaker probability heat maps are shown in the red channel of the images (rows 1,3) alongside the full 360 ? voice map (rows 2,4) where the camera's limited field of view is indicated in the central blue rectangle. This 360?180 voice map (rows 2,4) is a cylindrical 2D projection of the sphere where each pixel corresponds to a direction in the device wearer's local 3D coordinate system. The ground truth is shown as the purple bar under the talking head's lower edge and the blue dots in the 360 ? map. The yellow bar at the upper edge of a head box shows our prediction that the person is talking. Our method also predicts whether the device wearer speaks. from an egocentric perspective. In this paper, active speakers typically correspond to the people who are speaking and 'driving' the conversations. The elements of our proposed egocentric ASL problem are illustrated in <ref type="figure">Fig. 1</ref>. A good ASL system needs to account for the changing orientations of speakers from egocentric point of view and be robust to speakers moving in and out of the visual field of view. In particular, natural conversations entail significant overlap between different speakers' voice activity and involve one or more speakers interrupting each othera classical attribute in conversational ecology called turntaking. Such a system should also ideally be agnostic to the number of microphone channels, thereby allowing for generalization to different AR devices with varying numbers of audio and/or visual channels. Note that the device wearer may also be an active speaker during the conversa-tion whose voice is naturally amplified due to their closeness to the device microphones. An ASL system must account for this false amplification that may nullify competing active speakers in the scene. In this work, we propose a realtime audio-visual ASL system that addresses these aspects to effectively localize active speakers potentially outside of the visual FOV by leveraging audio recorded from a devicemounted microphone array.</p><p>We propose a new end-to-end deep neural network trained to tackle this problem. Our network is partitioned into two branches: an audio network and an audio-visual network. The audio network builds useful representations for constructing a low-resolution sound source localization map with a full 360 ? FOV by utilizing spatio-temporal correlations across different channels. The audio-visual network then combines the extracted audio features with the corresponding video frames, resulting in a higher resolution activity map for the camera's FOV. Visual cues such as the person's mouth movement, facial expressions, and body pose are extracted here and combined with audio features for computing a joint representation. The final 360 ? active speaker map is a combination of the low-resolution audioonly map and the high-resolution audio-visual map. In addition, the device wearer voice detector shares the features from the audio network, and our model estimates the relative 3D orientations of the speakers in the scene from egocentric perspective. The proposed network is also aimed at real-time applications in the immersion-driven domain of AR, enabling systems for the spatialization and localization of audio-visual activity in world-locked frame of reference. Lastly, the lack of reliable multi-channel conversational datasets is another limiting factor for building inthe-wild ASL systems. To that end, we build and evaluate our approach using a very recent egocentric conversations dataset called EasyCom <ref type="bibr" target="#b17">[18]</ref>.</p><p>Our contributions are:</p><p>1. We tackle the new problem of ASL using multichannel audio and video from egocentric perspective. In this new problem, we localize all the active speakers in the scene including the device wearer.</p><p>2. We propose a real-time low-latency egocentric audiovisual active speaker localization system with a 360 ? field of view. Our novel deep multi-channel audiovisual network learns from different audio features and can accommodate different numbers of audio channels without structure changes.</p><p>3. We evaluate our method on the EasyCom dataset and demonstrate significantly improved results in comparison to previous audio-visual ASL and ASD approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Single and multi-channel sound source detection and localization problems have classically been studied by speech and audio signal processing communities <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Most of these works are based on source separation and voice activity detection, and they mainly assume that there is one speaker in the audio stream who dominates the others (i.e., a high signal-to-noise ratio). The primary characteristic of these methods is to build auto-correlation and cross-correlation functions across different channels to account for timing and level differences caused by microphone placement. However, these approaches are sensitive to room acoustics and noisy backgrounds and may be unreliable when multiple sources are present. More recently, machine learning has been used for direction of arrival estimation with some success <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>. Although these methods improve upon the traditional approaches, the lack of visual information limits the efficacy of these systems in real-word settings. Furthermore, most multichannel approaches assume fixed, stationary microphone arrays, which may lead to poor performance with moving arrays in egocentric settings.</p><p>The computer vision community has seen a surge in audio-visual learning research, in particular due to datasets like the AVA Speech and Activity corpus <ref type="bibr" target="#b21">[22]</ref>, Voxconverse <ref type="bibr" target="#b22">[23]</ref>, and Voxceleb <ref type="bibr" target="#b23">[24]</ref>. These approaches are driven by building correspondences between audio and visual modalities, thereby resulting in robust joint representations that improve upon their audio-only or image-only counterparts. For action and activity recognition, several studies have shown evidence that audio disambiguates certain visually ambiguous cues <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Audio-visual models have been explored for speech recognition <ref type="bibr" target="#b24">[25]</ref>, sound source detection <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>, multiple source separation <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b16">17]</ref>, localization of sounds in a 2D image <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>, 3D scene navigation guided by audio <ref type="bibr" target="#b25">[26]</ref>, and others.</p><p>A bulk of the audio-visual learning models follow a simple recipe: audio inputs are often converted to spectrogram images which are then jointly processed with video frames. In addition to traditional network architectures, transformer networks have also been proposed for single-channel active speaker detection <ref type="bibr" target="#b13">[14]</ref>. More recently, turn-taking has also been studied as a means to improve detection performance <ref type="bibr" target="#b15">[16]</ref>. A related problem is that of speech separation, which singles out a speaker's voice by using both audio and cropped facial images <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>. The voice energy of the enhanced speech can then be used to detect active speakers. Although extensively studied, single-channel speaker detection from an egocentric perspective is still a challenging problem. This is mainly because of substantial device motion, occlusions, reduced visibility of speakers' faces, and noise induced by overlapping and interrupting speakers. Most current methods also induce significant latency in detection, which would be ineffective for enabling real-time AR experiences.</p><p>Single-channel audio-visual localization in exocentric settings has received much attention lately <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b14">15]</ref>. These methods either utilize audio-visual joint embeddings similar to those in active speaker detection, or they train audio-visual joint classification modules as the backbone for modality fusion. In addition, due to the lack of multiple channels, localization is restricted to the image frame in a manner similar to traditional visual object localization. The most recent related work is from <ref type="bibr" target="#b1">[2]</ref>, where the authors propose an audio-visual model that can process binaural (twochannel) audio for sound source localization. However, the system cannot be extended to multi-channel settings, and is restricted to localizing targets within the visual field of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Egocentric Active Speaker Localization</head><p>Problem Setup: Given multi-channel audio-visual data captured using AR glasses with a microphone array and RGB camera, we define the egocentric ASL problem as the detection and spatio-temporal localization of all the active speakers in the scene including the voice activity of the device wearer. Let A i with (i = 1..N ) denote the audio signals captured via N -channel microphone array and I denote the video from the RGB camera. The audio signals are normalized to the range [-1,1] based on the maximum bit length of audio samples. At each time instant t, given a segment of audio A t i and the corresponding video frame I t , we estimate two outputs: a heat map V t ?,? of activity in the scene and the device wearer activity W. V t ?,? is a 2D matrix where each element gives the probability of a sound source being present at particular relative angles (?, ?) at the time instant t, where ? ? [?180, 180] and ? ? [?90, 90] correspond to azimuthal (horizontal) elevation (vertical) respectively. Although we focus on human voice in this work, the proposed framework is applicable to any sounds of interest. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates the proposed egocentric ASL framework. Our method is an end-to-end deep learning model which takes the raw audio and video as input and estimates the active speaker activity heat map (V) and wearer's voice activity (W) directly. The framework has two networks: an audio network cascade (A) and an audio-visual network cascade (AV). A converts raw multi-channel audio and compacts a 2D representation aligned to each video frame, which is then used to extract relevant features using a convolutional neural network to estimate a direction of arrival estimate for the sources in the scene. AV then utilizes the outputs from A and incorporates visual information using another network. The resulting outputs from both A and AV are then combined to compute the scene and wearer's activity (V and W).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Audio Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Audio Representation</head><p>In this paper, we consider three audio representations and design our deep network so that it can take these different representations together with video as input in the same fashion. Our experiments show these audio representations are stronger than the raw audio. These different audio representations have different properties that are suitable for different use cases.</p><p>Our first audio representation is adapted from the complex spectrogram representation <ref type="bibr" target="#b1">[2]</ref>. For audio with sampling rate of 48kHz and video frame rate at 20Hz, we compute the short-time Fourier transform (STFT) extract 100 discrete Fourier transforms (DFTs) of length 200 to align with each video frame. The real and imaginary parts of the DFTs from all the channels are stacked together along the depth axis to form the multi-channel 2D tensor.</p><p>Apart from complex spectrogram, we further propose a 2D audio representation that captures the cross correlation between all pairs of the audio channels. Unlike spectrograms, this representation is mostly speaker invariant. In more details, assuming the audio sample n matches the time stamp of video frame at time t, the cross correlation C p,q (n, m) between channel p and q is</p><formula xml:id="formula_0">Cp,q(n, m) = K k=0 [Ap(n ? k)Aq(n ? k + m)] K k=0 Ap(n ? k) 2 ) K k=0 Aq(n ? k + m) 2 )</formula><p>, where m = [?L, L], and K and L are two parameters. In our experiments, audio signals have sampling rate 48kHz, K = 1200 and L = 50. In a discrete format, C p,q (n, m) is a vector of length 2L + 1 at each time n that characterizes not only the time shift of different audio channels due to the different path of the sound transmission, but also other finegrained couplings between different audio channels. Using this C, we construct a 2D audio representation at each time n, which is a stack of all the vectors C p,q (n, m) for each (p, q) pair.</p><p>The short-time energy of audio is a feature that is invariant to sound sources and easy to compute. Therefore, we also include a separate measure of the energies from each audio channel, Ep(n) = ( K k=0 Ap(n ? k) 2 ) 0.5 . Using the E, we stack e p (n) for each p, where e p (n) is a vector that duplicates the E p (n) by 2L + 1 times to form a 2D energy map. These features can also be combined to form richer representations. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates how the combined cross correlation and energy feature correspond to the audio events in videos. The cross-correlation, energy and the combined 2D feature are further resized. In this paper, the width and height are resized to 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Audio Activity Network</head><p>The audio activity network predicts a rough 360 ? audio activity map and the voice activity of the device wearer. Its   structure is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The feature extraction network is adapted from the first several layers of a ResNet18 network whose coefficients are pre-trained on ImageNet. The first convolutional layer is modified to match the channel number of different audio representations. The feature extraction network maps the audio 2D representation to a compact feature, which quantifies the spatial and voice characteristics of audio signals in the scene. The extracted features are flattened and passed to two fully connected layers, which are further reshaped to two 90 ? 45 maps. The two maps are stacked and resized to a 180 ? 90 one-hot representation half the size of the full 360 ? audio activity map. This network thus predicts the voice activity probability from each direction on the sphere with an angular resolution of 2 ? .</p><p>One key design here is to generate the one-hot representation of the heat map and train using cross-entropy loss. This gives more stable results than directly regressing a single heat map of the audio activity using L1 or L2 losses.</p><p>The audio activity map is also used to simultaneously estimate the wearer's voice activity. Due to the spatial position of the wearer's mouth relative to the microphones and the loudness of the wearer's voice, the 2D feature representation learned by the audio localization network also provides useful information for detecting whether the device wearer is speaking. To accomplish this, the audio feature extraction is shared with the 360 ? audio map prediction, and wearer voice activity detection is performed by a separate head that consists of two fully-connected layers trained to predict probability with a cross-entropy loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Audio-Visual Network</head><p>With only multi-channel audio available for speaker localization, the spatial resolution is low. This is due to the inherent physics of sound propagation and the limitations of compact microphone arrays. We therefore also take advantage of video frames to further improve the estimation result. Images not only increase spatial resolution, but also provide extra clues related to voice activity, such as mouth movement, facial expression, and hand gestures.</p><p>In this paper, we propose a different approach to fusing audio and visual information from previous audio-visual methods: we directly stack the video frames with the estimated voice activity map from the audio network. Since the rough 360 ? voice map from the audio network is defined on the unit sphere and the grids are horizontal and vertical angles, we need a procedure to align the audio map to the corresponding video frames. Even though we can map each grid in the voice map to the image, we find a simpler cropping and scaling method is sufficient due to the low resolution of the audio map. More specifically, we crop the region from the audio map within the horizontal and vertical angles corresponding to the four corners of the image. The scaling procedure then upsamples the region so that the audio map in the FOV is aligned with the input video. These operations are integrated in the audio-visual network. As shown in <ref type="figure">Fig. 5</ref>, the fused audio map and the corresponding color video frame form a tensor with depth of 4, which is sent to a fully-convolutional network to estimate the refined voice activity map in the camera's field of view. In this paper, the video resolution is 640 ? 360.</p><p>With such a design, if the faces are visible, the audiovisual network is able to take advantage of image features such as the appearance of the mouth and facial expression to localize audio activity. Due to its wide effective receptive field, the proposed network can also learn to extract other visual features such as body pose. Unlike previous methods, if the faces are not visible, our proposed method can still function because the audio activity map gives the locations of the potential speakers in the scene.</p><p>We combine the rough 360 ? heat map and the more detailed heat map in the FOV. In this paper, we simply pad the refined heat map with zeros outside the camera's FOV and add it to the 360 ? heat map to generate the final estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Model Training</head><p>We train the network in two stages. In the first stage, we train the audio-only and audio-visual network together without the wearer's voice activity classification network. In the second stage, we fix the audio feature layer's weights and train the fully connected network to predict the wearer's voice activity.</p><p>The 360 ? voice map and the voice map in the FOV are represented differently in the ground truth. The 360 ? voice map is a 180?90 2D map. If there is a speaker located at (?, ?), the ground truth voice map has a solid disk with radius 5 centered at the point. Such labeling is uniform for regions inside and outside of the field of view. In contrast, the voice map in the FOV has the same size as the video frames, and the active speaker in the field of view is labeled as a solid rectangle that covers the speaker's head. Therefore inside the FOV, the detection also has an attribute of size which is related to the depth of the target. The training losses are defined as follows.</p><p>The first stage loss function is L a = H(y a ,? 360 ) + H(y av ,? f ov ), and the second stage loss function is:</p><formula xml:id="formula_1">L b = H(y w ,? w ),</formula><p>where H is the mean cross entropy, y a and y av are the one-hot output representations of the audio-only and audiovisual networks,? 360 and? f ov are their corresponding ground truth audio maps, y w is the wearer speech activity prediction, and? w is its ground truth label. The training procedure generally converges quickly within 5 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment Results</head><p>In this section, we evaluate the proposed method on real videos and compare it with different audio-visual approaches for active speaker localization and wearer voice activity detection. Since we consider a novel egocentric problem setting, there are no previous audio-visual methods that are directly applicable. For comparison, we instead adapt our multi-channel audio and video inputs to other approaches to similar problems. We also experiment with variations of the proposed method to justify our design choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation Dataset</head><p>We evaluate our method using the EasyCom <ref type="bibr" target="#b17">[18]</ref> dataset. EasyCom is a multi-channel audio-visual dataset that includes around 6 hours of egocentric videos of conversations within a simulated noisy environment. The dataset is recorded using a microphone array and a RGB camera mounted on a pair of glasses. EasyCom is a challenging dataset with significant background noise, fast head motion, and motion blur. Participants may sit or walk around in the scene, and their faces and mouths are not always visible due to occlusions.</p><p>There are six microphones used for recording: four fixed to the glasses and two placed within the ears of the participants. In this paper, we use the RGB egocentric video together with the multi-channel audio from the four fixed microphones in our experiments. The dataset has 12 video sessions, each of which is about half an hour long. There may be 4, 5, or 6 participants including the camera wearer in each recording session. We use sessions 1-3 for testing and the remaining 9 sessions for training. For fair comparison, we report the best numbers for all competing models trained until convergence after a sufficiently large number of epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Methods in Evaluation</head><p>We compare the proposed method in different variations against other active speaker detection and localization methods. The methods in the evaluation include:</p><p>? Our method and variations ( Ours AV([cor] + [eng] + [spec] + [box])): Variations include different combinations of feature representations (cor: cross correlation, eng: energy, spec: spectrogram, and box: head bounding boxes). In the variation that uses head bounding boxes, we set the background color outside of the detected head regions to black. We also evaluate the audio-only and video-only versions of our method in which the video or audio branches are removed from our full model. ? DOA+headbox: A state-of-the-art signal processing method <ref type="bibr" target="#b19">[20]</ref> for extracting spherical direction-ofarrival (DOA) energy maps from the 4 microphones on the glasses combined with head detection bounding boxes for active speaker detection. This DOA estimation method was designed to achieve more robust results in highly reverberant settings compared to previous signal processing audio localization methods. To detect active speakers in the field of view, we pool regions of the DOA map corresponding to directions within the detected head bounding boxes. If the DOA map accurately estimates sound arrival directions, then the head bounding boxes corresponding to active speakers will include higher energy values.</p><p>? DOA+image: A deep neural network trained to localize active speakers using both traditional signal processing DOA maps <ref type="bibr" target="#b19">[20]</ref> and video frames as inputs. The network is fully convolutional and has the same structure as the audio-visual network in our method. ? AV-rawaudio: A deep neural network trained using multi-channel raw audio and video as the input. Aside from extracting audio features with 1D convolution layers, the overall network architecture is the same as our approach. ? Mouth region classifier (MRC): A visual-only method for classifying active speech from cropped images of mouth regions extracted from a 68-point facial key point detector. Such a scheme has been commonly used in active speaker detection. A ResNet18 network is trained to classify the cropped mouth images. We test two cases: MRC(AVA) trained using the AVA active speaker detection dataset <ref type="bibr" target="#b21">[22]</ref>, and MRC(EasyCom) only trained on EasyCom. ? TalkNet <ref type="bibr" target="#b13">[14]</ref>: A transformer-based single-channel audio-visual active speaker detection method that gave state-of-the-art results in the AVA active speaker detection challenge. We use the method in two modes: TalkNet(AVA) trained on the AVA dataset and TalkNet(EasyCom) trained on EasyCom. ? BinauralAVLocation [2]: A two-channel audiovisual method for sound source localization. Since this method cannot be easily extended to settings with more than two asymmetric microphones, we use only the audio channels from the two frontal microphones in our comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Within-View Active Speaker Localization</head><p>We first evaluate the mean average precision (mAP) of active speaker localization detections within the camera's field of view. We compare against multi-channel as well as one-and two-channel audio-visual methods and visual-only method. The mAP is computed based on the scores within the ground truth head bounding boxes in each video frame. For our methods and the competing methods DOA+headbox, DOA+image, AV-rawaudio, and BinauralAVLocation we extract the voice heat map's maximum value in each ground truth head bounding box and use it as the detection score. The MRC and the TalkNet methods use the classification probability of the corresponding head box as the detection score. Both MRC and TalkNet use the ground truth head bounding boxes for testing.</p><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, our methods give much higher mAP than all of the competing methods. <ref type="figure" target="#fig_5">Fig. 7</ref> shows qualitative comparison results. Due to the difficulty in learning useful features from raw audio, AV-rawaudio gives inferior results in comparison to spectrogram and crosscorrelation audio features. Background noise also causes traditional audio-only signal processing approaches give blurry DOA maps and inaccurate target localization results. The DOA+image deep learning method that combines this DOA map with video frames improves performance, but still gives lower mAP than our proposed method. This emphasizes the benefit of learning spatial audio-visual representations end-to-end. Our method also gives much higher mAP than the previous video-only MRC and single-channel audio-visual active speaker detection method TalkNet trained on both the AVA dataset <ref type="bibr" target="#b21">[22]</ref> and the EasyCom dataset. Our method greatly outperforms the BinauralAVLocation in both the 4-channel and 2channel audio settings.</p><p>For different variations of the proposed method, as shown in <ref type="table" target="#tab_1">Table 1</ref>, the energy feature is significantly worse than the other two features, while spectrogram features give slightly better mAP. The cross correlation and energy features are still attractive due to their speaker-invariant properties and thus have potential to generalize better in real applications and preserve privacy. The cross correlation feature is also invariant to the microphone gain settings; this makes it useful when the gains need to change dynamically for best signal-noise ratio. We also compare our audio-only and video-only variations with the full audio-visual model. In comparison to our full audio-visual method Ours AV(cor+mag+box) with a mAP of 86.32%, the video-only variation gave a much lower mAP of 58.44% and the audio-only version also gave a lower mAP of 78.08%. The results of Ours AV(corr+box) and Ours AV(corr+eng+box) also show that our proposed method can generalize to different environments by removing background visual information outside of head detections, which can potentially improve the result. Even with only two audio channels, our network still gave strong results that outperformed the BinauralAVLoc network architecture designed to leverage the symmetry of binaural audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Spherical Active Speaker Localization</head><p>One unique property of our proposed method is that it gives a full 360 ? spherical speaker localization result. Since there is no head bounding box outside of the field of view, we use the angular error to measure the localization quality.</p><p>The metric is defined as follows: We first extract the detected target locations in the predicted voice heat map using non-maximum suppression. Every peak in the heat map with value greater than a threshold is a potential target. In the experiments, we set the threshold to 0. The positions in the heat map indicate the angles of directions. We compute the minimum distances from the detected points to the ground truth points in the voice heat map, whose mean is denoted as E1. We compute mean E1 and its standard deviation Std1. The corresponding metrics from the ground truth point set to the detected point set are mean E2 and Std2. The reason we use distance metric in two directions is to take both missing detections and false alarms into account.</p><p>Not all the competing methods can give full 360 ? spherical localization results. In this experiment, we compare our method with the methods that use traditional DOA maps and the audio-visual variation with raw audio input. As shown in <ref type="table" target="#tab_2">Table 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Wearer Speech Activity Detection</head><p>Another unique property of the proposed method is that it can simultaneously detect the voice activity of the person wearing the recording glasses. Our method shares the  <ref type="table">Table 3</ref>. Camera wearer voice activity detection. Eng(single channel) is the naive approach of using short-time energy for wearer voice classification.</p><p>learned audio features for both tasks. During the training of the camera wearer voice networks, the shared feature design freezes the network feature extraction parameters while only training the last two fully connected layers. Camera wearer audio activity detection is a new task. We construct different natural solutions in the comparison. <ref type="table">Table 3</ref> summarizes the comparison result. As shown in <ref type="table">Table 3</ref>, our proposed method gives better results than the competing methods. The shared feature design in fact also gives better result than training a separate wearer voice classification model. For instance, our method using cross correlation input features gives 90.2% mAP, but if we retrain a separate wearer classifier the mAP is 88.01%. This is likely because of the additional supervision in training the localization task to explicitly suppress the wearer's speech.</p><p>Comparing to traditional signal processing approaches, our method requires more computationally expensive GPU operations. However, the proposed method is still efficient. It runs in real time at over 180 frames per second using a single GTX2080Ti GPU with about 50% utilization. More optimization could also further improve the efficiency of the network. The proposed method also has a smaller latency compared to traditional signal processing methods, which require estimating signal statistic over longer windows of time. While we only use 4 microphones in our experiments, the proposed method could be easily extended to devices with any number of microphones in any array configuration. With a larger microphone array, the proposed method has the potential to achieve even better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We proposed a novel multi-channel audio-visual method to tackle the 360 ? spherical active speaker detection problem for localizing active speakers both within and beyond an egocentric camera's visual field of view while also simultaneously predicting the wearer's voice activity. Our experiments showed that the proposed method gives superior results to competing methods and can run in real time with short latency. It can be deployed to enable many useful AR functions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Egocentric multi-channel audio-visual localization. Our end-to-end deep network detects a 360 ? voice activity map and the wearer's voice activity at the same time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Odd columns: video frames overlaid with voice activity labels. Even columns: vertical stack of the audio cross correlation and energy feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The audio activity network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Audio-visual network. The blocks B(p) and C(p, q) are defined inFig. 6. For 2D convolution layers, the parameters are input channel number, output channel number, convolution kernel size, stride and padding. For maxpool layer, the parameters are pooling kernel size, stride and padding. Residual blocks in the audio-visual network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparison results. The purple bar indicates when a person is predicted to be talking while the yellow bar is the corresponding ground truth. Rows 2, 4: the predicted 360 ? voice map compared against the the ground truth in blue channel. Rows 1, 2: The result of Ours AV(corr). Rows 3, 4: DOA+headbox, Row 5: DOA+image, Row 6: MRC(EasyCom), Row 7: TalkNet(EasyCom). In Row 7, green boxes indicate active speech while red boxes are inactive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison of mAPs in the visual field of view. Most of these tests use 4-channel audio, except for Ours</figDesc><table><row><cell></cell><cell>ASL mAP</cell></row><row><cell>Ours AV(cor)</cell><cell>84.14</cell></row><row><cell>Ours AV(cor+eng)</cell><cell>83.32</cell></row><row><cell>Ours AV(cor+box)</cell><cell>86.25</cell></row><row><cell cols="2">Ours AV(cor+eng+box) 86.32</cell></row><row><cell>Ours AV(spec)</cell><cell>85.49</cell></row><row><cell>Ours AV (eng)</cell><cell>62.68</cell></row><row><cell>Ours AV(cor)-2ch</cell><cell>80.00</cell></row><row><cell>Ours AV(spec)-2ch</cell><cell>83.30</cell></row><row><cell>AV-rawaudio</cell><cell>72.32</cell></row><row><cell>DOA+headbox</cell><cell>52.62</cell></row><row><cell>DOA+image</cell><cell>54.27</cell></row><row><cell>MRC (AVA)</cell><cell>46.60</cell></row><row><cell>MRC(EasyCom)</cell><cell>64.24</cell></row><row><cell>TalkNet (AVA)</cell><cell>69.13</cell></row><row><cell>TalkNet (EasyCom)</cell><cell>44.24</cell></row><row><cell>BinauralAVLoc</cell><cell>60.75</cell></row></table><note>AV(cor)-2ch, Ours AV(spec)-2ch, BinauralAVLoc, which use 2-channel audio, TalkNet which uses single-channel audio, and video-only MRC.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>, our method gives the lowest angular errors. E1 Std1 Mean E2 Std2 Ours AV (cor) 16.77 12.63 6.56 8.77 Ours AV (spec) 8.81 9.63 6.21 6.89 DOA 129.82 18.26 46.45 21.50 DOA+image 66.81 7.89 36.48 8.97 AV-rawaudio 40.14 10.55 140.75 19.58 Comparison of full 360 ? spherical voice activity localization errors measured in degrees.</figDesc><table><row><cell>Mean</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-Supervised Generation of Spatial Audio for 360-degree Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Langlois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<idno>NIPS 2018. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Binaural Audio-Visual Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">AAAI-21</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efros Audio-Visual Scene Analysis with Self-Supervised Multisensory Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<idno>ECCV 2018. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning to Localize Sound Source in Visual Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senocak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<idno>CVPR 2018. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassidim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to Separate Object Sounds by Watching Unlabeled Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno>CVPR 2018. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04121.2</idno>
		<title level="m">The Conversation: Deep Audio-Visual Speech Enhancement</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Audiovisual speaker localization via weighted clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Alameda-Pineda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Forbes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Machine Learning for Signal Processing (MLSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multiple Sound Sources Localization from Coarse to Fine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dinkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Localizing Visual Sounds the Hard Way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Localization of Sound Sources in Robotics: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Meza</surname></persName>
		</author>
		<idno>184-210. 2</idno>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Direction of Arrival Estimation for Multiple Sound Sources Using Convolutional Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adavanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Politis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Signal Processing Conference</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>EUSIPCO)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust Source Counting and DOA Estimation Using Spatial Pseudo-Spectrum and Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-S</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="page" from="28" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Is Someone Speaking? Exploring Long-term Temporal Features for Audio-visual Active Speaker Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 29th ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">How To Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kopuklu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taseska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<idno>ICCV 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-D</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<title level="m">The Right to Talk: An Audio-Visual Transformer Approach. ICCV 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">VisualVoice: Audio-Visual Speech Separation with Cross-Modal Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno>CVPR 2021. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tourbabin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Broyles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Ithapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04174</idno>
		<title level="m">EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Grumiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kitic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Girin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guerin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03465.2</idno>
	</analytic>
	<monogr>
		<title level="j">A Survey of Sound Source Localization with Deep Learning Methods</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Direction of Arrival Estimation in Highly Reveberant Environments Using Soft Time-Frequency Mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tourbabin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rafaely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Theory and Applications of Spherical Microphone Array Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A P</forename><surname>Habets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Naylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stopczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ava-Activespeaker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01342.2</idno>
		<title level="m">An Audio-Visual Dataset for Active Speaker Detection</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spot The Conversation: Speaker Diarisation in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VoxCeleb2: Deep Speaker Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Audio-visual Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">SoundSpaces: Audio-Visual Navigation in 3D Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schissler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Gari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Al-Halah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Ithapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<idno>ECCV 2020. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<title level="m">EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition. ICCV 2019</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno>arXiv, 2020. 2</idno>
		<title level="m">Audiovisual SlowFast Networks for Video Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
