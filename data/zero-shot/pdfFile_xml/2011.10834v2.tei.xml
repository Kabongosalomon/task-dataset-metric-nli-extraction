<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The complementarity of a diverse range of deep learning features extracted from video content for video recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-01">1 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adolfo</forename><surname>Almeida</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical</orgName>
								<orgName type="department" key="dep2">Electronic and Computer Engineering</orgName>
								<orgName type="institution">University of Pretoria</orgName>
								<address>
									<addrLine>Private Bag X20</addrLine>
									<postCode>0028</postCode>
									<settlement>Hatfield</settlement>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Pieter De Villiers</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical</orgName>
								<orgName type="department" key="dep2">Electronic and Computer Engineering</orgName>
								<orgName type="institution">University of Pretoria</orgName>
								<address>
									<addrLine>Private Bag X20</addrLine>
									<postCode>0028</postCode>
									<settlement>Hatfield</settlement>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>De Freitas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical</orgName>
								<orgName type="department" key="dep2">Electronic and Computer Engineering</orgName>
								<orgName type="institution">University of Pretoria</orgName>
								<address>
									<addrLine>Private Bag X20</addrLine>
									<postCode>0028</postCode>
									<settlement>Hatfield</settlement>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mergandran</forename><surname>Velayudan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MultiChoice Group</orgName>
								<orgName type="institution">AI Centre of Excellence</orgName>
								<address>
									<postBox>PO Box 1502</postBox>
									<postCode>2125</postCode>
									<settlement>Randburg</settlement>
									<country key="ZA">South Africa</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The complementarity of a diverse range of deep learning features extracted from video content for video recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-01">1 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.eswa.2021.116335</idno>
					<note>* Corresponding author. (Adolfo Almeida ), pieter.devilliers@up.ac.za (Johan Pieter de Villiers), allan.defreitas@up.co.za (Allan De Freitas), mergandran.velayudan@multichoice.co.za (Mergandran Velayudan)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video recommendation</term>
					<term>deep learning features</term>
					<term>item cold-start</term>
					<term>item warm-start</term>
					<term>multimodal feature fusion</term>
					<term>beyond-accuracy metrics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following the popularisation of media streaming, a number of video streaming services are continuously buying new video content to mine the potential profit from them. As such, the newly added content has to be handled well to be recommended to suitable users. In this paper, we address the new item</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>cold-start problem by exploring the potential of various deep learning features to provide video recommendations. The deep learning features investigated include features that capture the visual-appearance, audio and motion information from video content. We also explore different fusion methods to evaluate how well these feature modalities can be combined to fully exploit the complementary information captured by them. Experiments on a real-world video dataset for movie recommendations show that deep learning features outperform handcrafted features. In particular, recommendations generated with deep learning audio features and action-centric deep learning features are superior to MFCC and state-of-the-art iDT features. In addition, the combination of various deep learning features with hand-crafted features and textual metadata yields significant improvement in recommendations compared to combining only the former.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As video streaming platforms become more prevalent in our society, large amounts of video data are increasingly being uploaded to video sharing websites <ref type="bibr" target="#b57">(Xu et al., 2017)</ref>. The video sharing websites depend heavily on video recommendation systems to assist users to discover videos they may enjoy. A video recommendation system is a user-level video filtering service which helps users explore the world of videos <ref type="bibr" target="#b3">(Adomavicius &amp; Tuzhilin, 2005)</ref>. It offers a more personalised experience to users by recommending the most relevant and appropriate videos for them. In order to do this, algorithms are used to analyse the information about the videos, users and past interactions between them <ref type="bibr" target="#b18">(Gomez-Uribe &amp; Hunt, 2016;</ref><ref type="bibr" target="#b35">Lu et al., 2015)</ref>.</p><p>Existing recommendation systems mainly use one of three approaches, namely the collaborative filtering (CF) recommendation method, the content based (CB) recommendation method, and the hybrid recommendation method, which is a combination of the two former recommendation approaches <ref type="bibr" target="#b3">(Adomavicius &amp; Tuzhilin, 2005)</ref>. The CF recommendation method uses the user's explicit or implicit feedback, such as previous ratings and watch history in order to predict the preference of a user. This is achieved by recommending a video to a user if like-minded users have given it a positive rating or have watched it <ref type="bibr" target="#b3">(Adomavicius &amp; Tuzhilin, 2005)</ref>. The CB recommendation method uses the target user's profile and video content to predict the target user preferences. A video is recommended to a user if its content is similar to what the user liked or watched before <ref type="bibr" target="#b34">(Lops et al., 2011)</ref>. On the other hand, the hybrid recommendation methods combine both the user's feedback and the consumed video content in order to improve recommendations.</p><p>Most video streaming services that use a video recommendation system to compute the video relevance based on user feedback <ref type="bibr" target="#b31">(Liu et al., 2018)</ref> use CF recommendation methods because of their state-of-the-art accuracy <ref type="bibr" target="#b10">(Deldjoo et al., 2019;</ref><ref type="bibr" target="#b58">Yuan et al., 2016)</ref>. This feedback is used to model the user-video preference and compute video-to-video relevance scores in order to provide personalised recommendations. However, this approach suffers from the new item cold-start problem <ref type="bibr" target="#b56">(Wei et al., 2017;</ref><ref type="bibr" target="#b10">Deldjoo et al., 2019)</ref>. This problem is a core problem in the recommendation field <ref type="bibr" target="#b54">(Wang et al., 2019;</ref><ref type="bibr" target="#b15">Elahi et al., 2018;</ref><ref type="bibr" target="#b52">Volkovs et al., 2017)</ref>. It is a serious problem faced by video streaming services that purchase new movies and TV series from content providers <ref type="bibr" target="#b31">(Liu et al., 2018;</ref><ref type="bibr" target="#b54">Wang et al., 2019)</ref>. Moreover, with the tremendous increase in the number of new videos being continuously uploaded, some video streaming services have to deal with unrated, unaudited and completely new content of which they do not know anything about <ref type="bibr" target="#b29">(Kumar et al., 2018)</ref>. As such the new item cold-start problem has to be handled well in order for the uploaded content to be discovered by most of their users.</p><p>In addition, because of the massive amount of videos being produced, it is unfeasible to rely on manual processing of multimedia data to solve a wide variety of multimedia problems <ref type="bibr" target="#b45">(Shen et al., 2020)</ref>. As a result, recent studies on video content analysis and specially video retrieval tasks use various types of deep learning features extracted using pre-trained models due to their outstanding performance in different domains compared to hand-crafted features <ref type="bibr" target="#b45">(Shen et al., 2020;</ref><ref type="bibr" target="#b49">Tran et al., 2015;</ref><ref type="bibr" target="#b32">Liu et al., 2019;</ref><ref type="bibr" target="#b39">Miech et al., 2019)</ref>. Furthermore, deep learning features also require fewer pre-processing steps compared to traditional methods <ref type="bibr" target="#b45">(Shen et al., 2020)</ref>. Hence, it is a practical solution for a vast number of tasks, particularly when dealing with large-scale video datasets.</p><p>However, in the field of video recommendation, the utilisation of several deep learning features that capture different aspects of the video content is still a rare, explored area compared to hand-crafted features <ref type="bibr" target="#b12">(Deldjoo et al., 2020)</ref>.</p><p>vices <ref type="bibr" target="#b10">(Deldjoo et al., 2019;</ref><ref type="bibr" target="#b14">Du et al., 2020)</ref> have shown that video recommendation based on deep learning object features and hand-crafted features combined with collaborative filtering information have a higher recommendation quality compared to recommendations based on only deep learning object features or metadata such as genre or cast. However, in order to solve the new item coldstart problem, they either combine only two features <ref type="bibr" target="#b10">(Deldjoo et al., 2019)</ref>, i.e., deep learning object features and hand-crafted audio features; or combine deep learning object features along with hand-crafted motion and audio features <ref type="bibr" target="#b14">(Du et al., 2020)</ref>; or combine only deep learning object features and genre features <ref type="bibr" target="#b36">(Ma et al., 2018</ref>). In addition, they limit themselves by not exploring deep learning action features which captures the motion information in the videos and their complementariness among deep learning visual-appearance and audio features. This information is important since it is part of the rich and varied additional multimodal information present in videos. Videos are characterised by actions and scenes that help its narrative and pass on their message to the audience <ref type="bibr" target="#b6">(Carreira &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b24">Huang et al., 2018;</ref><ref type="bibr" target="#b47">Stroud et al., 2020;</ref><ref type="bibr" target="#b1">Adeli et al., 2019;</ref><ref type="bibr" target="#b55">Wehrmann &amp; Barros, 2017)</ref>, which may influence the users' preferences to a considerable extent. For example, temporal sequencing of cars in a video where the cars in the scene might appear stationary, yet the background is continually moving could be an indicative of a car chase; an irregular and complex kind of motion could be an indicative of hand-held shot videos which some people do not like <ref type="bibr">(?lvarez et al., 2019)</ref>.</p><p>It is clearly evident that there is a need to solve the new item cold-start problem by implementing a video recommendation system that uses the users' preference history and considers the complementary information among different deep learning features extracted from the video content. These features should capture the visual-appearance, audio and motion information from the video content in order to best exploit their availability and provide more accurate personalised video recommendation to users in the new item cold-start scenario.</p><p>In this paper, we address the new item cold start problem by enhancing the recommendation task using visual-appearance, audio and action deep learning features to recommend newly added videos to users effectively. In particular, the performance of these features is evaluated in terms of accuracy and beyondaccuracy metrics in the item warm-start and cold-start scenarios. We compare the deep learning features against genre features and hand-crafted features. In addition, we investigate fusion methods to exploit the complementary information captured by the deep learning features and enrich the quality of recommendation. Finally, we also perform an ablation study to empirically assess the importance of using a diverse range of video content features on the overall recommendation quality while taking full advantage of the available data.</p><p>Therefore, this experiment is conducted by combining the video content features evaluated in this research study. The major contributions of this work are as follows:</p><p>1. We evaluate the performance of various state-of-the-art deep learning features that capture object, scene, audio and action information from video content for item warm-start and cold-start video recommendations.</p><p>These features are more versatile compared to hand-crafted features and metadata since they are not computationally expensive, require fewer preprocessing steps and only raw video content, and metadata may not be precise or available.</p><p>2. We compare the deep learning features to genre metadata, motion scaleinvariant feature transform (MoSIFT) features, Mel-frequency cepstral coefficients (MFCC) features and state-of-the-art improved dense trajectories (iDT) features in terms of accuracy and beyond-accuracy metrics.</p><p>3. We evaluate and compare the performance of different early fusion techniques to determine how well each fusion method combines the different deep learning features that capture visual-appearance, audio and motion information contained in the videos in order to enrich the quality of recommendations.</p><p>4. We take full advantage of the available data by combining the video con-tent features explored in this work. The importance of each video content feature on the overall recommendation quality is assessed. To the best of our knowledge, this is the first study to combine this diverse range of video content features, particularly deep learning features, in order to alleviate the new item cold start problem; and where each feature is removed to see how that affects the overall performance of the recommendation system. 5. We propose an improved collaborative embedding regression (CER) model that uses a matrix scaling technique to enhance the performance of the CER model in both item warm start scenario and item cold start scenario.</p><p>The remaining sections of this paper are organised as follows: Section 2 describes the video content features, the video recommendation model and provides details of the feature fusion methods used in this study. In Section 3, the experimental settings and evaluation results of the investigation are discussed.</p><p>Lastly, Section 4 presents the conclusions of the proposed work and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>In this work, it is assumed that all the movie trailers are available when training the recommender model and the visual and audio features from movie trailers are representative of the features extracted from complete films <ref type="bibr" target="#b10">(Deldjoo et al., 2019</ref><ref type="bibr" target="#b11">(Deldjoo et al., , 2016</ref>. For this reason, deep learning features are extracted from movie trailers instead of the complete movie videos in this research work. This allows the exploration of the multimodal information from video content to be computationally efficient. <ref type="figure" target="#fig_0">Figure 1</ref> shows the general overview of the proposed workflow used to generate video recommendations. The time complexity of the feature extraction phase of this workflow is proportional to the total number of videos and their duration, and the time complexity of recommender model is proportional to the total number of features which represent the videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep learning features</head><p>The deep learning features used in this work are convolutional neural network (CNN) embeddings generated by intermediate layers of CNN models.</p><p>These embeddings are chosen because they are more generalised and robust to noise opposed to features extracted from the final output layer <ref type="bibr" target="#b22">(Holzenberger et al., 2019;</ref><ref type="bibr" target="#b27">Kalliatakis et al., 2019)</ref>. The visual-appearance information contained in the videos is represented by object-centric and scene-centric CNN embeddings. The motion information is represented by action-centric CNN embeddings. Lastly, the audio information is represented by audio CNN embeddings.</p><p>The feature extraction process is described below for each visual-appearance, audio and action deep learning features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object features</head><p>The object information from videos is captured using a Obj(IN) model pretrained on the ImageNet dataset <ref type="bibr" target="#b13">(Deng et al., 2009)</ref> for the task of object classification. This model is a ResNet-152 network <ref type="bibr" target="#b20">(He et al., 2016</ref>) that receives as input images of size 224?224 pixels and 3 channels. Therefore, the videos are decoded at 1 frame per second (fps) and each video frame is resized to 224 ? 224 pixels. The object-centric embeddings are extracted from the last convolutional layer with 2048 neurons followed by a global spatial average pooling layer. Thus, each video frame is represented by a 2048-dimensional descriptor which contains video content object features present in the frame. As a result, the output dimension of the Obj(IN) feature extractor for each video decoded at 1 fps is F ? 2048 where F is the total number of frames, i.e. a 120 second video is represented by a descriptor of size 120 ? 2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Scene features</head><p>The scene where an action is taking place may provide relevant information that supports actions with object interactions. In this work, the scene information from video frames is captured using a DenseNet-161 model <ref type="bibr" target="#b26">(Huang et al., 2017)</ref> pre-trained on the Places365 dataset . This model is a 2D-CNN network with 161 layers. It consists of an input layer that receives 224 ? 224 input images. Thus, each frame is first resized to this scale before it is passed to the model. The scene-centric embeddings are extracted from the 7 ? 7 global average pooling layer of the DenseNet-161 model <ref type="bibr" target="#b26">(Huang et al., 2017)</ref> followed by a global spatial average pooling layer resulting in a 2208-dimensional descriptor for each video frame. This descriptor contains video content scene features that represent related contextual information about a scene in a frame. As a result, similar to the object feature extractor, the scene-centric embeddings are extracted from videos decoded at 1 fps. The output dimension of the scene feature extractor is F ? 2208 where F is the total number of frames, i.e. a 120 second video is represented by a descriptor of size 120 ? 2208.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Action features</head><p>The action features are extracted from videos with pre-trained 3D-CNN models. These features capture the motion information in a video <ref type="bibr" target="#b6">(Carreira &amp; Zisserman, 2017)</ref>. In particular, each video is decoded at 24 fps and the visual stream is used as input to Action(IG) and Action(HMDB) models.</p><p>The Action(IG) is a R(2+1)D-34 32-frames model <ref type="bibr" target="#b50">(Tran et al., 2018)</ref> pretrained on the IG-65m dataset <ref type="bibr" target="#b17">(Ghadiyaram et al., 2019</ref>) that includes 359 human action classes that are identical to the action labels of the Kinetics dataset <ref type="bibr" target="#b6">(Carreira &amp; Zisserman, 2017)</ref>. This model consists of 34 layers where 33 layers are convolutional layers and the final layer is a fully-connected layer with softmax (classification layer). The input layer receives clips consisting of 32 consecutive RGB video frames with size 112 ? 112 pixels. Thus, if the clips obtained from the video are composed of video frames with different resolutions, these frames need to be resized.</p><p>The last convolutional layer has 512 neurons. The embeddings generated by this layer are passed to a global spatio-temporal average pooling layer and fed into a fully-connected layer with classification layer that predicts the 359 action classes. The action-centric embeddings generated by the global spatio-temporal average pooling layer are extracted and used as the video-clip content action features. They form a 512-dimensional descriptor for each clip of 32 consecutive 112?112 pixel frames. Having said that, the output dimension of the Action(IG) feature extractor is T v ? 512 where T v = F 32 , i.e. a 120 second video decoded at 24 fps has 2880 frames. As a result, this video is represented by a descriptor of size 90 ? 512 since</p><formula xml:id="formula_0">T v = 2880 32 = 90.</formula><p>The Action(HMDB) model is a ResNeXT-101 64-frames network <ref type="bibr" target="#b19">(Hara et al., 2018)</ref> pre-trained on the Kinetics dataset <ref type="bibr" target="#b6">(Carreira &amp; Zisserman, 2017</ref>) and fine-tuned on the HMDB-51 dataset <ref type="bibr" target="#b28">(Kuehne et al., 2011)</ref>. This model is chosen because it has been trained to recognise sequences of actions from untrimmed digitised movies. It consists of 101 layers where the last convolutional layer is followed by a global average pooling layer and a fully-connected layer with classification layer. The classification layer of the Action(HMDB) model creates a distribution for the 51 labelled classes. The size of the input layer is 3 channels ? 64 frames ? 112 pixels ? 112 pixels. Therefore, a 64-frame clip needs to be resized if it has a different resolution. In this work, video frames are decoded at 24fps</p><p>and processed in clips of 64 consecutive frames. Hence, every single clip spans approximately 2.67 seconds of the video. Each frame is first resized to 112?112 pixels, before passing to the model. Similar to <ref type="bibr" target="#b4">(Almeida et al., 2020)</ref>, we extract the action-centric embeddings generated by the global average pooling layer that comes before the classification layer and yields a 2048-dimensional descriptor for each video-clip. These descriptors are taken as the action features. Therefore, in a similar fashion as the Action(IG) feature extractor, the output dimension of the Action(HMDB)</p><p>feature extractor is T v ? 2048 where T v = F 64 , i.e. a 120 second video decoded at 24 fps is represented by a descriptor of size 45 ? 2048.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Audio features</head><p>Audio features are extracted from audio frames with a VGGish model <ref type="bibr" target="#b21">(Hershey et al., 2017)</ref>. This model is a 2D-CNN network pre-trained on the YouTube-8m dataset for audio classification <ref type="bibr" target="#b21">(Hershey et al., 2017)</ref>.</p><p>The model is a modified VGG architecture.</p><p>In order to extract sound features using this model the audio stream of each video needed to be pre-processed. The raw audio waveform is first downsampled to a 16 kHz mono signal with 16 bit resolution and re- Additionally, to further enhance the discrimination of the video-level feature vectors, we use the signed square root (SSR) normalisation followed by principal component analysis (PCA) on the raw features. SSR is executed in order to weaken the dominant dimensions of each video-level feature vector so they do not overshadow the other dimensions during the similarity computations <ref type="bibr" target="#b14">(Du et al., 2020)</ref>. This normalisation function is defined as</p><formula xml:id="formula_1">SSR(x) = sign(x) ? |x|,<label>(1)</label></formula><p>where x is the video-level feature vectors and sign() is the function that captures the sign of each feature. Moreover, PCA is applied to obtain features that are more discriminative and less redundant. Hence, the number of principal components is equal to the original list of features in order not to lose any information while covering maximum variance among them. Furthermore, each video-level feature vector is scaled into a unit vector by applying L 2 -normalisation (L 2norm) given by Equation <ref type="formula" target="#formula_2">(2)</ref> below</p><formula xml:id="formula_2">L 2 -norm(x) = x ||x|| 2 ,<label>(2)</label></formula><p>where ||x|| 2 is the Euclidean norm of the video-level feature vector defined as</p><formula xml:id="formula_3">||x|| 2 = N i=1 (x i ) 2 .</formula><p>This is performed to ensure that each feature contributes approximately equally to the final similarity measure <ref type="bibr" target="#b43">(Ranjan et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Textual features</head><p>Aside from the video features extracted from the video content, textual metadata features provide a good representation of the videos. Textual feature modality is the most used video representation in traditional CB or hybrid approaches for video recommendation. Although the main of the objective of this research work is to investigate the effect of various visual and audio stimuli on user preferences, it is worth exploiting textual features as complementary information of video description.</p><p>A set of genres of each movie are used as the only type of textual feature in this work. The motivation behind this choice is that genre metadata is highly available in the domain and represent relevant elements in movies <ref type="bibr" target="#b10">(Deldjoo et al., 2019)</ref>. In addition, taking into account that they are high-level semantics attributes of movies, when fused with non-textual content features they will probably remove ambiguity which in turn should lead to an improvement in performance.</p><p>Given the genres provided in the meta information of the videos, the genre feature vector is encoded to a N -dimensional binary vector where N is the total number of unique genres. A bit 1 in the ith column of the vector indicates that the corresponding genre describes the video and a bit 0 indicates that the corresponding genre does not apply to the video.</p><p>The genre feature vector used in this work represent 19 genre labels from the metadata of the movies, namely adventure, animation, children, comedy, fantasy, romance, drama, action, crime, thriller, horror, sci-fi, mystery, IMAX, documentary, war, film-Noir, musical, and western. Thus, the dimensionality of the genre feature vector for each video is 19 where each feature represents one of the 19 annotated genres.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Video recommendation model</head><p>The objective of this work is to explore different features that capture the rich and diverse multimodal information present in videos, which may influence the users' preferences to a considerable extent <ref type="bibr" target="#b12">(Deldjoo et al., 2020)</ref> thereby alleviate the new item cold start problem. For this reason, the state-of-the-art CER model <ref type="bibr" target="#b14">(Du et al., 2020)</ref> is chosen since it is a hybrid recommender model that could lead to the best benefit in terms of recommendation performance in item warm-start and cold-start scenarios using the wide variety of features.</p><p>The CER model is a model based on the weighted matrix factorisation method for implicit feedback datasets where a large matrix is decomposed into smaller matrices to reduce the dimensions and learn latent vectors that describe users and items. The implicit feedback ratings are turned into confidence values as follows <ref type="bibr" target="#b14">(Du et al., 2020)</ref>:</p><formula xml:id="formula_4">c ui = ? ? ? ? ? 1, if r ui = 1, 0.01, if r ui = 0.<label>(3)</label></formula><p>where c ui is the confidence value for the user-video pair (u, i) given its rating r ui obtained from the user rating matrix (URM). The confidence values are used to learn the users and items latent vectors by performing the alternating least squares approach <ref type="bibr" target="#b14">(Du et al., 2020)</ref>. In addition, the CER model leverages the collaborative information from warm items with single type of video content features to effectively recommend warm and cold items. These features should be aggregated into video-level feature vectors that describe the video content of each video.</p><p>Latent vectors are composed of latent factors which represent categories that are present in the data in a much lower dimensional space. These vectors are used by the CER model to predict ratings that are missing in the original URM since every user has videos that they have not watched before. These videos are recommended according to CER's rating predictor <ref type="bibr" target="#b14">(Du et al., 2020)</ref>.</p><p>In this work, the CER model is trained using the optimal hyper-parameter set reported in the original paper <ref type="bibr" target="#b14">(Du et al., 2020)</ref>. However, the original paper does not mention the number of epochs and the stopping criteria used in the training step of the CER model. Therefore, in this work, the number of epochs is selected using the early stopping technique <ref type="bibr" target="#b8">(Dacrema et al., 2021)</ref>. This method decreases the risk of over-fitting and also decreases training time.</p><p>The main limitation of the CER model is that it does not learn from multiple types of video content features at once. Therefore, there is a need to further investigate fusion methods to leverage the complementary information from the diverse range of features explored in this work in order to further enrich the recommendations. In addition, according to existing works <ref type="bibr" target="#b30">(Lee &amp; Abu-El-Haija, 2017;</ref><ref type="bibr" target="#b36">Ma et al., 2018)</ref>, the combination of video-level feature vectors should lead to an even higher recommendation quality in the new item cold-start scenario in contrast to the use of a single feature modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Improving CER model using matrix scaling</head><p>Recently, successful recommender models named EigenRec <ref type="bibr" target="#b41">(Nikolakopoulos et al., 2019)</ref> and hybridSVD <ref type="bibr" target="#b16">(Frolov &amp; Oseledets, 2019)</ref> have shown significant recommendation quality improvement using a simple scaling trick. These models are matrix factorisation recommendation algorithms that apply singular value decomposition (SVD). The scaling trick used by these models is a matrix scaling technique which regulates how the popularity of items affects the predicted ratings. It is defined as <ref type="bibr" target="#b41">(Nikolakopoulos et al., 2019</ref>)</p><formula xml:id="formula_5">R RD d?1 ,<label>(4)</label></formula><p>where R is the U RM , D = diag{||r 1 ||, ||r 2 ||, . . . , ||r m ||} is a diagonal matrix that contains Euclidean norm scaling for a given scaling factor d of the columns r i of R and lastly R is the modified U RM .</p><p>From Equation <ref type="formula" target="#formula_5">(4)</ref> above, it can seen that when d is 1, the standard model is obtained (URM is not modified). However, when the scaling factor is varied, the sensitivity of the SVD based models to the popularity of the items is modified.</p><p>Higher values of the parameter d increase the sensitivity to popular items while smaller values increase the sensitivity to rare items. This adjustment leads to a new model with a latent space with different internal structure. It has been found that values slightly below 1 yield the best recommendation performance for EigenRec and hybridSVD models <ref type="bibr" target="#b41">(Nikolakopoulos et al., 2019;</ref><ref type="bibr" target="#b16">Frolov &amp; Oseledets, 2019</ref> </p><formula xml:id="formula_6">c ui = ? ? ? ? ? 1 ? ||r i || d?1 , if r ui = 1, 0.01, if r ui = 0.<label>(5)</label></formula><p>The optimal scaling factor hyper-parameter d is searched by optimising the quality of the scaled-CER in terms of M AP @5 on the validation set. This measure is defined in section 3.2.1. The hyper-parameter optimisation is conducted on all cross-validation folds individually with Bayesian optimisation <ref type="bibr" target="#b10">(Deldjoo et al., 2019)</ref>. Bayesian optimisation selects the next set of hyper-parameters based on the results of the hyper-parameter sets previously evaluated. Once the optimal scaling factor is found on each CV fold, a single optimal scaling factor is selected corresponding to the best average M AP @5 result across all folds. Recent studies <ref type="bibr" target="#b10">(Deldjoo et al., 2019;</ref><ref type="bibr" target="#b9">Dacrema et al., 2019)</ref> have shown that Bayesian optimisation is an efficient method for hyper-parameter tuning. and textual that are combined into a single feature vector, before being fed to a machine learning algorithm. One recent work in video retrieval tasks shows that early fusion of object, action, face, audio, scene, optical character recognition, and text features allows the system to obtain a better similarity measure and therefore be capable of more robust video retrieval ). An increase on the overall performance of the system is observed when different features are cumulatively fused .</p><p>Inspired by the above-mentioned findings, various early fusion methods are investigated to enrich the recommendations. This is executed to fully exploit the complementary information from the various feature representations extracted from the video content. Furthermore, this is also to determine whether early fusion would achieve a similar outcome observed in recent video retrieval tasks <ref type="bibr" target="#b39">Miech et al., 2019)</ref>. We hypothesise that a video recommendation system that uses videos represented in a shared unified space by diverse deep learning features (visual-appearance, audio and motion) should further improve the quality of recommendations in the new item cold-start scenario.</p><p>The early fusion approaches, investigated to combine information from multiple modalities are the concatenation (concat) method, the summation (sum) method and lastly the maximum (max) method <ref type="bibr" target="#b27">(Kalliatakis et al., 2019)</ref>. The concat method is a technique that merges different feature vectors to obtain one large feature vector that represents the final video representation. As this feature vector contains many features, it increases the training time. Formally, for each video feature vector f , if there are L feature vectors of different modalities that are represented with f i ? R di , the concatenation operation is defined as</p><formula xml:id="formula_7">x = [f 1 , f 2 , . . . , f L ],<label>(6)</label></formula><p>where x is the final multimodal video-level representation by fusing the different features that capture visual-appearance, audio and motion information from videos as well as textual information from their metadata. The final size of this representation is the sum of the dimensions of all feature vectors denoted as</p><formula xml:id="formula_8">d = L i=1 d i .</formula><p>The second early fusion method exploited in this work is the sum method.</p><p>This method adds different feature vectors in order to obtain the final video representation. Given a set of L feature vectors with the same size that represent each video modality separately, their summation is denoted as</p><formula xml:id="formula_9">x = L i=1 f i ,<label>(7)</label></formula><p>where x is the final multimodal representation with size d. As can be seen in Equation <ref type="formula" target="#formula_9">(7)</ref>, the sum fusion technique is only defined if all feature vectors have the same size. For cases where a feature vector i of size d i is greater than</p><formula xml:id="formula_10">min(d 1 , d 2 , . . . , d i , d L )</formula><p>, PCA is applied for feature reduction. The number of features is reduced to the size of the smallest feature vector before performing the fusion operation.</p><p>Finally, the last fusion technique investigated is the max fusion operation.</p><p>This fusion method is similar to the sum fusion method in terms of the final multimodal representation size, however, it differs in the way the feature vectors are combined. The max fusion method selects the highest value of each feature from a set of L feature vectors with the same size as</p><formula xml:id="formula_11">x = max d i=1 (f i 1 , f i 2 , . . . , f i L ),<label>(8)</label></formula><p>where x is the final video representation and d is the feature vector size. Similar to the sum fusion method, PCA is applied as a dimension reduction step for all feature vectors greater than the smallest feature dimension in the set of L feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results and discussion</head><p>In this section, we present the dataset, evaluation metrics, experimental results, and the in-depth analyses of the reported results. The video recommendation system is investigated in the item warm-start and cold-start scenarios.</p><p>The item warm-start scenario represents the case when some preference data for items in that scenario have been used to train the video recommendation model. The new item cold-start scenario represents the case when preference data for the items in that scenario are not known at all during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset Description</head><p>In this work, we use a processed MovieLens-10M dataset <ref type="bibr" target="#b14">(Du et al., 2020)</ref>.</p><p>This dataset is a processed version of the publicly available MovieLens-10M dataset where 10380 movie trailers out of the 10682 movies in the MovieLens-10 dataset are downloaded from YouTube and manually checked if they are correct.</p><p>The movies that have their trailers missing are removed. The data are binarized by converting ratings of 5 to 1, and all other ratings to 0. The processed dataset also provides five cross-validation folds where each fold is divided into a training set, a item warm-start test set and a new item cold-start test set. These sets contain the binarized ratings of 69878 users for each 10380 movie and occupy 60%, 20% and 20%, respectively <ref type="bibr" target="#b14">(Du et al., 2020)</ref>. The item warm-start and cold-start test sets correspond to the item warm-start and cold-start scenarios where the item warm-start test set contains items that have some of their ratings in the training set and the item cold-start test set contains items that do not have any ratings in the training set. In addition, the dataset also provides trailers of movies and pre-computed 4000-dimensional MFCC <ref type="bibr" target="#b14">(Du et al., 2020)</ref>,</p><p>MoSIFT <ref type="bibr" target="#b7">(Chen &amp; Hauptmann, 2009</ref>) and iDT <ref type="bibr" target="#b53">(Wang &amp; Schmid, 2013)</ref> feature vectors for each movie trailer. These hand-crafted features are used as the video content feature baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation metrics</head><p>The performance of the video recommendation system is evaluated using two types of metrics namely: accuracy and beyond-accuracy metrics <ref type="bibr" target="#b10">(Deldjoo et al., 2019;</ref><ref type="bibr" target="#b44">Shani &amp; Gunawardana, 2011)</ref>. These metrics are important when evaluating a video recommendation system since they complement each other.</p><p>Accuracy metrics evaluate the relevance of the recommendations, however better user satisfaction beyond relevance is not necessarily achieved with higher accuracy <ref type="bibr" target="#b46">(Silveira et al., 2019)</ref>. Beyond-accuracy metrics evaluate the value that recommendations can generate to the user where the desire for variety is not ignored <ref type="bibr" target="#b44">(Shani &amp; Gunawardana, 2011)</ref>.</p><p>In this work, the videos in the catalogue are sorted in descending order, based on the ratings estimated by the model being evaluated. Next, the Top-n videos are chosen to be the first n videos in the recommendation list. The length n of the recommendation list returned to each user is also known as the cutoff value. Accuracy and beyond-accuracy metrics are calculated for 3 different cut-off values from {5, 15, 30}. All reported results for the recommendation models evaluated in the item warm-start and cold-start scenarios are obtained using the item warm-start test set and the item cold-start test set, respectively.</p><p>The experiments were conducted using 5-fold cross-validation with the provided five-folds. Therefore, the presented results is the mean of five tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Accuracy metrics</head><p>In order to evaluate if the user enjoyed the videos recommended by the video recommendation system using the various deep learning features, the following rank-aware metrics are used as listed below:</p><p>1. Mean average precision (MAP) is the average of the average precision at top N recommendations (AP @N ) over the whole set of users in the test set. It calculates the overall precision of the recommender system by using precision at all possible recall levels <ref type="bibr" target="#b10">(Deldjoo et al., 2019)</ref>. AP @N is computed by obtaining the arithmetic mean of precision values of the relevant items at their corresponding positions. This metric is chosen because it measures the rate of relevant items in the recommendation list that users may like and therefore consumed, while considering relevant items not in the recommendation list. It is an important metric if it is assumed that many users will not scan the entire recommendation list, but instead they would only look at the top of the recommendation list.</p><p>This metric is defined as <ref type="bibr" target="#b10">(Deldjoo et al., 2019</ref>)</p><formula xml:id="formula_12">AP u @N = 1 min(N, K) N i=1 P @i ? rel(i),<label>(9)</label></formula><formula xml:id="formula_13">M AP = 1 |U | u |U | AP u ,<label>(10)</label></formula><p>where N is the length of the recommendation list, K is the total number of relevant items, P @i is the precision at top i recommendations, rel(i) is a binary indicator which signals if the i th recommended item is relevant or not and |U | is the total number of users in the test set.</p><p>2. Normalised discounted cumulative gain (NDCG) is a utility-based ranking measure which considers the order of recommended items in the list <ref type="bibr" target="#b30">(Lee &amp; Abu-El-Haija, 2017)</ref>. It discounts the positions of the items recommended to a user <ref type="bibr" target="#b10">(Deldjoo et al., 2019)</ref>. This metric is chosen because in a video streaming service, users may be willing to scan all the relevant videos in the recommendation list from the beginning to the end. When the relevant videos appear at a lower ranked position the utility of recommendations is slowly penalised, since videos that are more useful for the user are highly relevant <ref type="bibr" target="#b44">(Shani &amp; Gunawardana, 2011</ref>). This metric also shows high robustness to the changes of the MovieLens-10M dataset after preprocessing <ref type="bibr" target="#b48">(Tousch, 2019)</ref>. Assuming the predicted rating values for the recommendations are sorted in descending order in the recommendation list for user u, DCG u is defined as <ref type="bibr" target="#b10">(Deldjoo et al., 2019;</ref><ref type="bibr" target="#b37">Matveeva et al., 2006</ref>)</p><formula xml:id="formula_14">DCG u @N = N i=1 2 ru,i ? 1 log 2 (i + 1) ,<label>(11)</label></formula><p>where r u,i is the true rating of user u to the item ranked at position i.</p><p>NDCG is the normalised DCG u which is the ratio of DCG u to the ideal discounted cumulative gain (IDCG u ), which is the value that represents the ideal ranking for user u calculated using the ground-truth ranking instead of the predicted one. This is computed as <ref type="bibr" target="#b10">(Deldjoo et al., 2019</ref>)</p><formula xml:id="formula_15">N DCG u = DCG u IDCG u .<label>(12)</label></formula><p>The overall NDCG is obtained by calculating the mean over the whole set of users in the test set <ref type="bibr" target="#b10">(Deldjoo et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Beyond-accuracy metrics</head><p>Evaluating the recommendation generated using the various deep learning features solely according to accuracy is not sufficient since the objective of a recommender system is not only restricted to generate relevant recommendation lists to the users. Instead, the features should also cover the whole set of preferences of the users, given the huge body of video data <ref type="bibr" target="#b46">(Silveira et al., 2019)</ref>. Beyond-accuracy metrics are used to help to assess the quality of the various deep learning features explored in this work by capturing the coverage and diversity of recommendations. These metrics assess if the systems using these features are able leverage the whole catalogue instead of only a few highly popular items <ref type="bibr" target="#b10">(Deldjoo et al., 2019)</ref>. It also assesses if the recommendation lists generated by the system for different users are being diversified. In this work, the video recommendation system using the various video content features is evaluated using the following measures:</p><p>1. Intra-list diversity is a metric which measures the efficiency of the recommender to generate recommendation lists that cover the entire set of preferences of the users <ref type="bibr" target="#b46">(Silveira et al., 2019)</ref>. It is chosen because recommendation lists with similar items may not be of interest of the user <ref type="bibr" target="#b46">(Silveira et al., 2019)</ref>. It is calculated by using the cosine similarity be-tween the items recommended based on genre features as <ref type="bibr" target="#b10">(Deldjoo et al., 2019;</ref><ref type="bibr" target="#b46">Silveira et al., 2019</ref>)</p><formula xml:id="formula_16">IntraL(L) = i L j L\i (1 ? cossim(i, j)) |L| ? (|L| ? 1) (13) cossim(i, j) = f i ? f j || f i || || f j || ,<label>(14)</label></formula><p>where |L| is the length of the recommendation list L, cossim(i, j) is the cosine similarity between items i and j, and f i , f j ? R |F | are the feature vectors of items i and j with |F | number of features, respectively.</p><p>Recommendation lists that contains items very similar to one another in terms of their genres obtain low values for this metric.</p><p>2. Item coverage of a recommender system is the ratio of distinct items for which the system is able to make recommendations <ref type="bibr" target="#b46">(Silveira et al., 2019)</ref>.</p><p>This metric is chosen because it measures the proportion of items in the catalogue that have been recommended at least once over the number of potential items. If a recommender system has low coverage it will limit the recommendations for the user thus having a direct impact on business revenue of the system and the users' satisfaction. This metric is defined as <ref type="bibr" target="#b10">(Deldjoo et al., 2019;</ref><ref type="bibr" target="#b46">Silveira et al., 2019</ref>)</p><formula xml:id="formula_17">coverage = |?| |I| ,<label>(15)</label></formula><p>where |I| is the total number of items in the test set catalogue and |?| is the number of items in I recommended at least once by the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Shannon</head><p>Entropy is a measure that provides an overview of the recommender system as a whole by measuring the distributional inequality of recommendations across all users <ref type="bibr" target="#b10">(Deldjoo et al., 2019)</ref>. This metric is chosen to better understand the capability of each deep learning feature to generate unequally different video recommendations within a certain item coverage value over the whole set of users. Shannon Entropy is defined as <ref type="bibr" target="#b10">(Deldjoo et al., 2019</ref>)</p><formula xml:id="formula_18">SE = ? i I rec(i) rec t ? ln rec(i) rec t ,<label>(16)</label></formula><p>where I is the set of items in the scenario being evaluated, rec(i) is the number of times item i has been recommended across all users, rec t is the total number of recommendations. As can be seen in this equation,</p><p>the Shannon entropy has a value range between 0 and ln(n) that represents when one item is recommended many times and when n items are recommended equally frequently <ref type="bibr" target="#b44">(Shani &amp; Gunawardana, 2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Recommendation performance in warm-start scenario</head><p>In this section, we report the performance of the video recommendation system in the item warm-start scenario when using different features on their content description. <ref type="table" target="#tab_2">Table 1</ref> reports the MAP and NDCG results at different cut-off values for the CER and scaled-CER recommender models using genre, hand-crafted and deep learning features.  <ref type="bibr" target="#b14">(Du et al., 2020)</ref> and suggests that in the item warm-start scenario, the interactions collected for the items are very important in order to obtain outstanding results, and the additional video content features help predictive performance of items with very few interactions. The importance of items prior ratings is more clear when looking at the performance of the scaled-CER model. Furthermore, it is worth pointing out that for each model as the cutoff value increases, MAP and NDCG results also increase. This is understandable because as the number of items being recommended increases, the more likely one of them to be a true label, which means that more correctly predicted videos are obtained.  <ref type="table" target="#tab_4">Table 2</ref> presents the beyond-accuracy performance of the CER and scaled-CER recommender models using genre, hand-crafted and deep learning features.</p><p>The performance is measured in terms of Shannon Entropy, intra-list diversity and item coverage measured at different cut-off values. The CER model achieved the highest results in almost all the metrics with the exception of item coverage at cut-off values 15 and 30. This outcome was expected because it is known that there exists an inherent trade-off between accuracy and beyond-accuracy metrics <ref type="bibr" target="#b2">(Adomavicius &amp; Kwon, 2012)</ref>. The matrix scaling technique used by the scaled-CER model brings great improvements in terms of accuracy metrics at a cost of intra-list diversity and Shannon Entropy. It is interesting to note that for the cut-off values 15 and 30, the highest results for item coverage were obtained by the scaled-CER model. This outcome is in line with the main purpose of the scaling factor which is to increase the sensitivity of the model to rare items. However, the increase in item coverage is accompanied by less diverse recommendation lists where a number of items were recommended more times than the other items.</p><p>By a closer inspections of the video content features, the CER model using genre features obtained the highest results for Shannon Entropy for all cut-off values and the highest item coverage for cut-off value 5. Take into consideration the fact that this variant of the CER, obtained the worst performance in terms of accuracy metrics. In terms of intra-list diversity results, the CER model using genre features obtained the lowest results compared to other CER variants.</p><p>This outcome was expected because genre features are used to calculate this metric. The results suggest that the model is generating recommendation lists with a number of videos of the same genre. Moreover, it is interesting to note that the scaled-CER model using Obj(IN) features did not obtained the lowest results in terms of beyond-accuracy metrics since this model obtained the best performance in terms of accuracy metrics. In addition, similar to accuracy metric results, the beyond-accuracy results for the CER and scaled-CER models are similar for the various video content features along the respective cut-off values, and they increase with an increase in length of the recommendation list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Recommendation performance in cold-start scenario</head><p>The experiments results of the video recommendation system in the new item cold-start scenario are presented in this section. The system is evaluated when using each feature explored in this work on its content description. The results obtained in this scenario represent the ability of the system to alleviate the new item cold-start problem. In contrast to the item warm-start scenario, it can be seen in <ref type="table" target="#tab_5">Table 3</ref> that in the item cold-start scenario, the CER and scaled-CER models exhibit results that are more varied across different types of video content features. This suggests that these models are relying more on the features to generate recommendations and each type of video content feature discriminates the user preferences differently. Similar to the item warm-start scenario, the scaled-CER model achieved the highest results with regards to all the accuracy metrics and a noticeable improvement over the CER model is observed. This outcome shows the effectiveness of the matrix scaling technique in the item cold-start scenario as well, which is able to improve the performance of different types of features.</p><p>It suggests that the collaborative information learnt with the item popularity sensitivity adjustment along with video content features is very important to recommend cold items with high precision. It can be observed that the scaled-CER, using MFCC features, it is the best baseline with regards to the cut-off   In contrast to the item warm-start scenario, in the item cold-start scenario, the scaling factor of the scaled-CER model does not lead to the highest results with regards to item coverage. The scaling factor only increases the item coverage of the genre, Action(IG) and Action(HMDB) features. It is interesting to see that this increase comes with an increase in Shannon Entropy but with a slight decrease in intra-list diversity. This means that the number of items that are recommended equally often increased, however, the number of recommendations of the same genre also increased. In addition, it is important to note that for the highest cut-off value. These results are promising and suggest that the scaled-CER model is able to recommend more than 95% of cold items and these items are highly relevant to users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Evaluation of different fusion methods</head><p>The evaluation of different fusion methods is performed in the item cold-   In terms of beyond-accuracy metrics, it can be observed in <ref type="table" target="#tab_9">Table 6</ref> that the outstanding performance achieved by the concat of Obj(IN), VGGish and Action(IG) features with regards to accuracy metrics comes with a decrease in intra-list diversity. The concat fusion of these three features did not obtain intra-list diversity results higher than the baseline but the difference is between 3.6% and 5.6%.</p><p>Surprisingly, we can see a noticeable performance improvement for the con- The recommendation quality is measured in terms of MAP and item coverage to understand to which extent the video recommendation system is able to explore the catalogue with high precision.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and future work</head><p>In this paper, we investigate multiple video content features, to solve the new item cold-start problem. Various deep learning features extracted from the multi-modal, extremely high dimensional information from the videos are used to enhance the quality of recommendations. The features capture visualappearance, audio and motion information from the media contained in the videos. A comparison between these features is performed using a hybrid recommender model, namely the CER recommender model. In addition, we propose an improvement for this model using a known matrix scaling technique. The proposed improved model is named the scaled-CER. This model is sensitive to rare items by scaling the collaborative information before training.</p><p>It is found that in the item warm-start scenario the video content information captured by the different features does not seem to be important in the recommendation performance achieved. A noticeable boost to recommendation accuracy is achieved by the matrix scaling technique. The scaled-CER obtained the best recommendation accuracy and an improvement in item coverage. However, there is a decrease in Shannon Entropy and intra-list diversity.</p><p>In Finally, it is worth mentioning that the proposed work has a few limitations.</p><p>In the cold-start scenario, the quality of the pre-computed video features have an impact on the recommendations. The current model does not leverage user metadata which is important when dealing with cold-start users. Lastly, in order to make recommendations based on new user-video interaction data, a full model retraining is necessary to refresh the model due to the static nature of the user and video embedding matrices.</p><p>As future work, it will be worth evaluating the features used in this research study in terms of user's quality perception. This can be achieved by deploying the recommender model in a web application that serves the pre-computed recommendations to users. Then, the users will be prompt to answer a list of questions that measure the perceived quality of the recommendations. The outcome of this experiment could assist in better tuning of the video recommendation system in industrial applications and in the creation of high quality recommendation explanations to increase trust in the system. In addition, it would be worth investigating the correlation between multimedia features, electronic programming guide (EPG) information and the user feedback gathered from the use of the remote control. The end goal would be to enhance existing recommender systems in this domain to provide more significant TV program recommendations to users that lead to a decrease in the use of the remote control while improving their experience.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>General overview of the proposed workflow used to generate video recommendations. Given a set of videos watched by a set of users, a diverse range of deep learning features are first extracted from the visual and audio modalities of each video, and textual features are obtained from genre metadata. Next, the deep learning features of each video are temporally aggregated to obtain a video content vector of fixed length, which represents the information about their content. These vectors are then combined to form a video content matrix where each row represents a single video. Finally, the video content matrices are fused and used with the user-video rating matrix as input to the scaled-CER model to train it and then generate recommendations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>scaled to the range [-1.0, 1.0]. Next, the audio signal is divided into a sequence of successive non-overlapping 0.96 sec audio segments of the original video, and subsequently converted from time domain to frequency domain. The conversion is performed with short-time Fourier transform (STFT). This operation is computed using a periodic Hann window that receives as input frames with size of 25 ms and stride of 10 ms. The resulting spectrogram is mapped to 64 log Mel-spectrogram bins which in turn gives patches of 96 audio-frames?64 bins. These log Mel-spectogram patches form the input to the VGGish model that maps them to a 128dimensional descriptor for each audio segment. As a result, each 128dimensional descriptor composed of VGGish features represents 96 audioframes. For this reason, the dimension of the audio descriptor extracted for each video's audio track is T a ? 128 VGGish features, where T a = audioFrames 96 . As the number of frames varies across videos, the deep learning features are aggregated into video-level feature vectors using six statistical feature aggregation methods, namely maximum, mean, median, variance, median absolute deviation and interquartile range (Almeida et al., 2020). These methods are chosen because they are simple and widely used on a number of video understanding tasks which utilise deep learning features (Liu et al., 2019; Miech et al., 2019; Almeida et al., 2020) and obtained better results compared to state-of-theart Fisher vectors (FV) and vectors of locally aggregated descriptors (VLAD) aggregation methods (Abu-El-Haija et al., 2016; Deldjoo et al., 2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>value 5 across the respective metrics, however with regards to the cut-off values 15 and 30, the best baseline is the genre features. The best overall performance is obtained by Obj(IN) features which outperform the MFCC features by 53.4% and 51.5% in terms of MAP@5 and NDCG@5, accordingly. In terms of MAP@15, MAP@30, NDCG@15 and NDCG@30, the Obj(IN) features outperforms genre features by 50.4%, 45.4%, 37.4% and 29.3%, respectively. These results indicate that the scaled-CER model using Obj(IN) features in its content descriptor provides considerable better recommendations that are placed at the top of the recommendation list compared to the genre and hand-crafted features. In addition, as the recommendation list gets longer the more relevant items the model is able to recommend. It is interesting to note that action-centric deep learning features present noticeably better performance compared to the hand-crafted MoSIFT features and state-of-the-art hand-crafted iDT features across all accuracy metrics. The best action features with regards to MAP and NDCG across different cut-off values is Action(IG) followed by Action(HMDB). These results are very promising since it shows that the motion information captured by deep learning features lead to better recommendations compared to the hand-crafted iDT and MoSIFT features in terms of accuracy metrics. Similarly, deep learning audio features outperform the hand-crafted MFCC features. This confirms the success of deep learning features in the video recommendation context in terms of accuracy metrics. In addition, it can also be noted that all the deep learning features explored in this work outperforms genre features which thus emphasises the importance of using non-textual features extracted from videos to improve cold item recommendations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the action-centric deep learning features present a better item coverage with recommendations that are harder to guess in comparison to the hand-crafted iDT and MoSIFT features without compromising recommendation accuracy. A similar outcome is observed for the deep learning audio features compared to MFCC features. Furthermore, it is also worth mentioning that the best visualappearance, action and audio features in terms of MAP and NDCG, namely Obj(IN), Action(IG) and VGGish features, obtained item coverage results in the range of 0.7332 to 0.8656 for the smallest cut-off value and 0.9541 to 0.9834</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>start scenario. The goal of this experiment is to address the problem: Given videos represented by multiple features, namely visual-appearance, audio and action features, how should we further improve the recommendation of newly added videos. The experiment is based on the combination of the most accurate deep learning feature modalities, namely visual-appearance, audio and action features, reported in section 3.4. We use the scaled-CER model to evaluate the fusion methods described in section 2.4. This model is chosen due to the outstanding overall performance presented in section 3.4. The best single video content feature is used as a unimodal baseline to determine whether a fusion method really improve recommendation quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>decrease in recommendation accuracy, for each type of feature with the only exception being the MFCC features. When MFCC features are removed from the combination of all the video content features (All/MFCC ) MAP results slightly above All are obtained for all cut-off values. These MAP results are the highest achieved in this experiment. However, removing MFCC features also provides a noticeable drop in item coverage for all cut-off values which are the lowest item coverage obtained in this experiment. This means that MFCC features provide a good balance between MAP and item coverage when combined with the other features explored in this work. Nevertheless, the combination of all the various video content features provides recommendations that are very precise but about 22.84% of the total number of cold items are never recommended to a user for the lowest cut-off value. However, for cut-off values 15 and 30, more than 90% of cold items are recommended to users meaning that the descriptors obtained from the combination of all the features are highly predictive of the user preferences leading to a wide range of relevant video recommendations. This shows the strong correlation between the deep learning features, hand-crafted features and especially genre features in the overall recommendation quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>the new item cold-start scenario, the various video content features are as important as the matrix scaling technique to achieve outstanding recommendation quality. Overall, the video content information captured by the deep learning features are more discriminative compared to hand-crafted features and genre features. In particular, the motion information captured by action-centric deep learning features extracted with 3D-CNNs are better than hand-crafted action features. They lead to better recommendation accuracy and item coverage where the recommendations are more balanced. For this reason, it can be concluded that the success of 3D-CNN features on tasks like action recognition and video classification also occur in the video recommendation context. A similar outcome is observed for deep learning audio features in comparison to MFCC features. Moreover, we investigate different fusion methods to effectively combine the features before training the model, in order to improve the recommendation quality in terms of accuracy and beyond-accuracy metrics. The best fusion method was found to be the concatenation method. The results suggest that fusion of visual, audio and action features provide more accurate video recommendations to users when compared to the fusion of only visual and audio features. Apart from intra-list diversity measure, an improvement upon the fusion of visual and audio features is also observed for all the beyond-accuracy measures. Furthermore, an ablation study is performed where all the video content features explored in this study are combined. The results of the ablation study demonstrated that apart from one hand-crafted feature (MFCC features), all types of features, namely genre, hand-crafted and deep learning features are necessary to achieve the highest performance observed in this work in terms of accuracy. However, MFCC features provide a worthy balance between accuracy and item coverage. The results also showed that genre features are the most important features in the overall result since the largest drop is observed when they are removed from the combination with all other features. However, the high precision comes with a decrease in item coverage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>). Therefore, enlightened by these new findings, we propose an improved CER model that uses the matrix scaling technique to enhance the performance of the CER model<ref type="bibr" target="#b14">(Du et al., 2020)</ref>. The matrix scaling technique is used to produce a scaled-CER model 1 which has an increased sensitivity to unpopular and new items in contrast to the original non-scaled CER model. This choice is also supported given the fact that a value of 1 for the parameter d leads to the original non-scaled CER model. As a result, this indicates that the original non-scaled CER model implicitly chose this value that could lead to a model biased towards popular items. Therefore, many items that have only few ratings, unpopular or new items, might not be leveraged accordingly during the training process. Consequently, this implicit default choice could inevitably hinders the CER model potential in both item warm-start and cold-start scenarios. In this work, using the matrix scaling technique, the confidence parameter c ui for the user-video pair (u, i) of the scaled-CER model is defined as</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Multimodal fusion can be a very important component in video recommendation systems where improving the overall recommendation quality of the system is considered as one of its most essential aspects. The feature fusion methods commonly used are late fusion and early fusion. Late fusion combines prediction scores of each model in order to obtain a more accurate final set of results. As a result, the main disadvantage of this method is the loss of complementary information represented by different features. This information is important for the final estimation. In addition, late fusion is computationally more expensive given the fact that it requires separate systems and a learning stage for the combination<ref type="bibr" target="#b10">(Deldjoo et al., 2019)</ref>.On the other hand, early fusion obtains a truly multimedia feature representation. It exploits the complementary information about various characteristics of a video at feature level. This in turn improves the discriminativity of the video representations. In contrast to the late fusion approach, the early fusion approach only needs a single model and one learning stage. The video information is represented by features from different modalities, namely visual, aural,</figDesc><table><row><cell>The benefits of this method are a reduction in search time and better param-</cell></row><row><cell>eter values compared to random search or grid search parameter optimisation</cell></row><row><cell>methods.</cell></row><row><cell>2.4. Feature fusion</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results for the CER and scaled-CER recommender models with regards to accuracy metrics in the item warm-start scenario. The performance of different video content features is evaluated using the two recommender models. The highest results across the same metric are marked in bold.As can be seen inTable 1, the CER model obtained the highest results in terms of MAP and NDCG at all cut-off values when using the Action(HMDB) features and the lowest results are obtained using genre features. However, it can be observed that the CER model exhibits similar performance across all types of video content features. This outcome is similar to</figDesc><table><row><cell cols="7">Recommender models MAP@5 NDCG@5 MAP@15 NDCG@15 MAP@30 NDCG@30</cell></row><row><cell>CER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Genres</cell><cell>0.1101</cell><cell>0.1447</cell><cell>0.1224</cell><cell>0.2102</cell><cell>0.1332</cell><cell>0.2515</cell></row><row><cell>Obj(IN)</cell><cell>0.1110</cell><cell>0.1458</cell><cell>0.1233</cell><cell>0.2111</cell><cell>0.1341</cell><cell>0.2523</cell></row><row><cell>Scene</cell><cell>0.1107</cell><cell>0.1452</cell><cell>0.1229</cell><cell>0.2104</cell><cell>0.1336</cell><cell>0.2516</cell></row><row><cell>Action(IG)</cell><cell>0.1107</cell><cell>0.1453</cell><cell>0.1232</cell><cell>0.2108</cell><cell>0.1340</cell><cell>0.2522</cell></row><row><cell>Action(HMDB)</cell><cell>0.1113</cell><cell>0.1459</cell><cell>0.1236</cell><cell>0.2114</cell><cell>0.1344</cell><cell>0.2526</cell></row><row><cell>iDT</cell><cell>0.1105</cell><cell>0.1450</cell><cell>0.1227</cell><cell>0.2102</cell><cell>0.1335</cell><cell>0.2515</cell></row><row><cell>MoSIFT</cell><cell>0.1111</cell><cell>0.1457</cell><cell>0.1233</cell><cell>0.2110</cell><cell>0.1341</cell><cell>0.2520</cell></row><row><cell>MFCC</cell><cell>0.1112</cell><cell>0.1459</cell><cell>0.1232</cell><cell>0.2108</cell><cell>0.1339</cell><cell>0.2519</cell></row><row><cell>VGGish</cell><cell>0.1105</cell><cell>0.1451</cell><cell>0.1227</cell><cell>0.2103</cell><cell>0.1335</cell><cell>0.2517</cell></row><row><cell>scaled-CER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Genres</cell><cell>0.1531</cell><cell>0.1841</cell><cell>0.1562</cell><cell>0.2538</cell><cell>0.1665</cell><cell>0.2962</cell></row><row><cell>Obj(IN)</cell><cell>0.1536</cell><cell>0.1846</cell><cell>0.1568</cell><cell>0.2546</cell><cell>0.1671</cell><cell>0.2971</cell></row><row><cell>Scene</cell><cell>0.1531</cell><cell>0.1841</cell><cell>0.1562</cell><cell>0.2539</cell><cell>0.1665</cell><cell>0.2961</cell></row><row><cell>Action(IG)</cell><cell>0.1530</cell><cell>0.1840</cell><cell>0.1562</cell><cell>0.2538</cell><cell>0.1665</cell><cell>0.2962</cell></row><row><cell>Action(HMDB)</cell><cell>0.1533</cell><cell>0.1843</cell><cell>0.1564</cell><cell>0.2543</cell><cell>0.1668</cell><cell>0.2968</cell></row><row><cell>iDT</cell><cell>0.1536</cell><cell>0.1846</cell><cell>0.1567</cell><cell>0.2546</cell><cell>0.1671</cell><cell>0.2971</cell></row><row><cell>MoSIFT</cell><cell>0.1535</cell><cell>0.1846</cell><cell>0.1566</cell><cell>0.2545</cell><cell>0.1670</cell><cell>0.2970</cell></row><row><cell>MFCC</cell><cell>0.1529</cell><cell>0.1840</cell><cell>0.1561</cell><cell>0.2538</cell><cell>0.1664</cell><cell>0.2962</cell></row><row><cell>VGGish</cell><cell>0.1531</cell><cell>0.1842</cell><cell>0.1563</cell><cell>0.2541</cell><cell>0.1666</cell><cell>0.2966</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The results show that the scaled-CER model improves over the CER model. The scaled-CER model obtained the best overall performance compared to any CER model's variant by using the matrix scaling technique presented in section 2.3.1. The top results were achieved by the scaled-CER model using the Obj(IN) features that outperforms the CER model using Action(HMDB) by 38%, 26.8% and 24.3 % in terms of MAP@5, MAP@15 and MAP@30, accordingly. In terms of NDCG@5, NDCG@15 and NDCG@30, the scaled-CER model, using Obj(IN) features, outperforms the CER model using Action(HMDB) features by 26.5%, 20.4% and 17.6%, respectively. These outcomes clearly show that in the item warm-start scenario, the item content descriptor is not as important as the ratings of the items. Moreover, the results clearly illustrates the effectiveness of the matrix scaling technique, where the scaled-CER recommdender model presents the best capability to generate recommendation lists that contain relevant items at the top positions. In addition, similar to the CER model, the scaled-CER model presents similar results along the different types of video content features, however, the difference between the results is extremely small after proper scaling. Nevertheless, by a closer inspection of the scaled-CER model variants, it can be seen that the state-of-the-art iDT feature vectors is the best baseline video content feature and outperforms almost all the deep learning features with the only exception being the Obj(IN) features. The VGGish features outperformsthe hand-crafted MFCC features. However, as pointed out above, the difference between the results of any type of video content feature is not significant.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results for the CER and scaled-CER recommender models with regards to beyondaccuracy metrics in the item warm-start scenario. The performance of different video content features is evaluated using the two recommender models. The highest results across the same metric are marked in bold.</figDesc><table><row><cell>Recommender models</cell><cell>Div.</cell><cell>@5</cell><cell>Div.</cell><cell>@5</cell><cell>Item</cell><cell>@5</cell><cell>Div.</cell><cell>@15</cell><cell>Div.</cell><cell>@15</cell><cell>Item</cell><cell>@15</cell><cell>Div. @30</cell><cell>Div.</cell><cell>@30</cell><cell>Item</cell><cell>@30</cell></row><row><cell></cell><cell>SE</cell><cell></cell><cell>IntraL</cell><cell></cell><cell>Cov.</cell><cell></cell><cell>SE</cell><cell></cell><cell>IntraL</cell><cell></cell><cell>Cov.</cell><cell></cell><cell>SE</cell><cell>IntraL</cell><cell></cell><cell>Cov.</cell><cell></cell></row><row><cell>CER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Genres</cell><cell cols="2">8.0829</cell><cell cols="2">0.4804</cell><cell cols="4">0.1568 8.6157</cell><cell cols="2">0.5992</cell><cell cols="2">0.2092</cell><cell>9.0273</cell><cell cols="2">0.6383</cell><cell cols="2">0.2561</cell></row><row><cell>Obj(IN)</cell><cell cols="2">8.0806</cell><cell cols="2">0.4899</cell><cell cols="2">0.1540</cell><cell cols="2">8.6059</cell><cell cols="2">0.6076</cell><cell cols="2">0.2007</cell><cell>9.0152</cell><cell cols="2">0.6455</cell><cell cols="2">0.2373</cell></row><row><cell>Scene</cell><cell cols="2">8.0763</cell><cell cols="2">0.4895</cell><cell cols="2">0.1548</cell><cell cols="2">8.6076</cell><cell cols="2">0.6069</cell><cell cols="2">0.2007</cell><cell>9.0184</cell><cell cols="2">0.6452</cell><cell cols="2">0.2383</cell></row><row><cell>Action(IG)</cell><cell cols="2">8.0791</cell><cell cols="2">0.4883</cell><cell cols="2">0.1558</cell><cell cols="2">8.6102</cell><cell cols="2">0.6067</cell><cell cols="2">0.2016</cell><cell>9.0209</cell><cell cols="2">0.6448</cell><cell cols="2">0.2387</cell></row><row><cell>Action(HMDB)</cell><cell cols="2">8.0791</cell><cell cols="2">0.4897</cell><cell cols="2">0.1538</cell><cell cols="2">8.6090</cell><cell cols="2">0.6071</cell><cell cols="2">0.1999</cell><cell>9.0186</cell><cell cols="2">0.6451</cell><cell cols="2">0.2364</cell></row><row><cell>iDT</cell><cell cols="2">8.0725</cell><cell cols="2">0.4901</cell><cell cols="2">0.1542</cell><cell cols="2">8.6051</cell><cell cols="2">0.6073</cell><cell cols="2">0.2001</cell><cell>9.0164</cell><cell cols="2">0.6455</cell><cell cols="2">0.2369</cell></row><row><cell>MoSIFT</cell><cell cols="2">8.0755</cell><cell cols="2">0.4905</cell><cell cols="2">0.1543</cell><cell cols="2">8.6062</cell><cell cols="2">0.6070</cell><cell cols="2">0.1999</cell><cell>9.0195</cell><cell cols="2">0.6451</cell><cell cols="2">0.2359</cell></row><row><cell>MFCC</cell><cell cols="2">8.0725</cell><cell cols="2">0.4913</cell><cell cols="2">0.1539</cell><cell cols="2">8.6080</cell><cell cols="2">0.6076</cell><cell cols="2">0.1995</cell><cell>9.0198</cell><cell cols="2">0.6458</cell><cell cols="2">0.2362</cell></row><row><cell>VGGish</cell><cell cols="2">8.0673</cell><cell cols="2">0.4897</cell><cell cols="2">0.1538</cell><cell cols="2">8.6009</cell><cell cols="2">0.6063</cell><cell cols="2">0.2014</cell><cell>9.0088</cell><cell cols="2">0.6448</cell><cell cols="2">0.2396</cell></row><row><cell>scaled-CER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Genres</cell><cell cols="2">7.5366</cell><cell cols="2">0.4707</cell><cell cols="2">0.1480</cell><cell cols="2">8.3193</cell><cell cols="2">0.5883</cell><cell cols="2">0.2283</cell><cell>8.8602</cell><cell cols="2">0.6277</cell><cell cols="2">0.3096</cell></row><row><cell>Obj(IN)</cell><cell cols="2">7.5225</cell><cell cols="2">0.4777</cell><cell cols="2">0.1372</cell><cell cols="2">8.2966</cell><cell cols="2">0.5953</cell><cell cols="2">0.2055</cell><cell>8.8316</cell><cell cols="2">0.6338</cell><cell cols="2">0.2669</cell></row><row><cell>Scene</cell><cell cols="2">7.5295</cell><cell cols="2">0.4788</cell><cell cols="2">0.1398</cell><cell cols="2">8.3052</cell><cell cols="2">0.5954</cell><cell cols="2">0.2092</cell><cell>8.8406</cell><cell cols="2">0.6337</cell><cell cols="2">0.2742</cell></row><row><cell>Action(IG)</cell><cell cols="2">7.4964</cell><cell cols="2">0.4774</cell><cell cols="2">0.1404</cell><cell cols="2">8.2807</cell><cell cols="2">0.5954</cell><cell cols="2">0.2101</cell><cell>8.8209</cell><cell cols="2">0.6342</cell><cell cols="2">0.2738</cell></row><row><cell>Action(HMDB)</cell><cell cols="2">7.4668</cell><cell cols="2">0.4776</cell><cell cols="2">0.1385</cell><cell cols="2">8.2552</cell><cell cols="2">0.5957</cell><cell cols="2">0.2103</cell><cell>8.7996</cell><cell cols="2">0.6344</cell><cell cols="2">0.2746</cell></row><row><cell>iDT</cell><cell cols="2">7.5018</cell><cell cols="2">0.4765</cell><cell cols="2">0.1361</cell><cell cols="2">8.2871</cell><cell cols="2">0.5954</cell><cell cols="2">0.2040</cell><cell>8.8252</cell><cell cols="2">0.6346</cell><cell cols="2">0.2632</cell></row><row><cell>MoSIFT</cell><cell cols="2">7.5162</cell><cell cols="2">0.4783</cell><cell cols="2">0.1365</cell><cell cols="2">8.2947</cell><cell cols="2">0.5962</cell><cell cols="2">0.2025</cell><cell>8.8288</cell><cell cols="2">0.6351</cell><cell cols="2">0.2609</cell></row><row><cell>MFCC</cell><cell cols="2">7.4987</cell><cell cols="2">0.4787</cell><cell cols="2">0.1375</cell><cell cols="2">8.2777</cell><cell cols="2">0.5969</cell><cell cols="2">0.2051</cell><cell>8.8171</cell><cell cols="2">0.6355</cell><cell cols="2">0.2665</cell></row><row><cell>VGGish</cell><cell cols="2">7.4990</cell><cell cols="2">0.4777</cell><cell cols="2">0.1387</cell><cell cols="2">8.2796</cell><cell cols="2">0.5957</cell><cell cols="2">0.2110</cell><cell>8.8190</cell><cell cols="2">0.6341</cell><cell cols="2">0.2759</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results for the CER and scaled-CER recommender models with regards to accuracy metrics in the item cold-start scenario. The performance of different video content features is evaluated using the two recommender models. The highest results across the same metric are marked in bold.</figDesc><table><row><cell cols="7">Recommender models MAP@5 NDCG@5 MAP@15 NDCG@15 MAP@30 NDCG@30</cell></row><row><cell>CER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Genres</cell><cell>0.0099</cell><cell>0.0141</cell><cell>0.0112</cell><cell>0.0268</cell><cell>0.0130</cell><cell>0.0386</cell></row><row><cell>Obj(IN)</cell><cell>0.0159</cell><cell>0.0223</cell><cell>0.0170</cell><cell>0.0367</cell><cell>0.0189</cell><cell>0.0497</cell></row><row><cell>Scene</cell><cell>0.0152</cell><cell>0.0210</cell><cell>0.0156</cell><cell>0.0334</cell><cell>0.0172</cell><cell>0.0445</cell></row><row><cell>Action(IG)</cell><cell>0.0146</cell><cell>0.0201</cell><cell>0.0150</cell><cell>0.0331</cell><cell>0.0166</cell><cell>0.0447</cell></row><row><cell>Action(HMDB)</cell><cell>0.0136</cell><cell>0.0185</cell><cell>0.0140</cell><cell>0.0309</cell><cell>0.0155</cell><cell>0.0424</cell></row><row><cell>iDT</cell><cell>0.0098</cell><cell>0.0137</cell><cell>0.0102</cell><cell>0.0234</cell><cell>0.0114</cell><cell>0.0326</cell></row><row><cell>MoSIFT</cell><cell>0.0095</cell><cell>0.0131</cell><cell>0.0096</cell><cell>0.0215</cell><cell>0.0106</cell><cell>0.0297</cell></row><row><cell>MFCC</cell><cell>0.0111</cell><cell>0.0155</cell><cell>0.0117</cell><cell>0.0258</cell><cell>0.0129</cell><cell>0.0350</cell></row><row><cell>VGGish</cell><cell>0.0134</cell><cell>0.0187</cell><cell>0.0139</cell><cell>0.0307</cell><cell>0.0154</cell><cell>0.0414</cell></row><row><cell>scaled-CER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Genres</cell><cell>0.0113</cell><cell>0.0159</cell><cell>0.0125</cell><cell>0.0294</cell><cell>0.0143</cell><cell>0.0416</cell></row><row><cell>Obj(IN)</cell><cell>0.0178</cell><cell>0.0247</cell><cell>0.0188</cell><cell>0.0404</cell><cell>0.0208</cell><cell>0.0538</cell></row><row><cell>Scene</cell><cell>0.0162</cell><cell>0.0221</cell><cell>0.0165</cell><cell>0.0350</cell><cell>0.0181</cell><cell>0.0463</cell></row><row><cell>Action(IG)</cell><cell>0.0159</cell><cell>0.0215</cell><cell>0.0160</cell><cell>0.0345</cell><cell>0.0176</cell><cell>0.0459</cell></row><row><cell>Action(HMDB)</cell><cell>0.0137</cell><cell>0.0188</cell><cell>0.0140</cell><cell>0.0311</cell><cell>0.0155</cell><cell>0.0422</cell></row><row><cell>iDT</cell><cell>0.0114</cell><cell>0.0157</cell><cell>0.0117</cell><cell>0.0264</cell><cell>0.0130</cell><cell>0.0362</cell></row><row><cell>MoSIFT</cell><cell>0.0105</cell><cell>0.0143</cell><cell>0.0105</cell><cell>0.0234</cell><cell>0.0116</cell><cell>0.0321</cell></row><row><cell>MFCC</cell><cell>0.0116</cell><cell>0.0163</cell><cell>0.0121</cell><cell>0.0266</cell><cell>0.0134</cell><cell>0.0360</cell></row><row><cell>VGGish</cell><cell>0.0138</cell><cell>0.0193</cell><cell>0.0144</cell><cell>0.0319</cell><cell>0.0160</cell><cell>0.0430</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results for the CER and scaled-CER recommender models with regards to beyondaccuracy metrics in the new item cold-start scenario. The performance of different video content features is evaluated using the two recommender models. The highest results across the same metric are marked in bold.</figDesc><table><row><cell>Recommender models</cell><cell>Div.</cell><cell>@5</cell><cell>Div.</cell><cell>@5</cell><cell>Item</cell><cell>@5</cell><cell>Div.</cell><cell>@15</cell><cell>Div.</cell><cell>@15</cell><cell>Item</cell><cell>@15</cell><cell>Div. @30</cell><cell>Div.</cell><cell>@30</cell><cell>Item</cell><cell>@30</cell></row><row><cell></cell><cell>SE</cell><cell></cell><cell>IntraL</cell><cell></cell><cell>Cov.</cell><cell></cell><cell>SE</cell><cell></cell><cell>IntraL</cell><cell></cell><cell>Cov.</cell><cell></cell><cell>SE</cell><cell>IntraL</cell><cell></cell><cell>Cov.</cell><cell></cell></row><row><cell>CER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Genres</cell><cell cols="2">7.8355</cell><cell cols="2">0.2376</cell><cell cols="2">0.4577</cell><cell cols="2">8.5849</cell><cell cols="2">0.3217</cell><cell cols="2">0.6382</cell><cell>9.0612</cell><cell cols="2">0.3742</cell><cell cols="2">0.7058</cell></row><row><cell>Obj(IN)</cell><cell cols="2">8.7835</cell><cell cols="2">0.4830</cell><cell cols="2">0.7851</cell><cell cols="2">9.3267</cell><cell cols="2">0.5876</cell><cell cols="2">0.9215</cell><cell>9.6660</cell><cell cols="2">0.6283</cell><cell cols="2">0.9683</cell></row><row><cell>Scene</cell><cell cols="2">8.8372</cell><cell cols="2">0.4949</cell><cell cols="2">0.7875</cell><cell cols="2">9.3987</cell><cell cols="2">0.5978</cell><cell cols="2">0.9115</cell><cell>9.7426</cell><cell cols="2">0.6355</cell><cell cols="2">0.9610</cell></row><row><cell>Action(IG)</cell><cell cols="2">8.9225</cell><cell cols="2">0.4740</cell><cell cols="2">0.8029</cell><cell cols="2">9.4278</cell><cell cols="2">0.5757</cell><cell cols="2">0.9179</cell><cell>9.7437</cell><cell cols="2">0.6152</cell><cell cols="2">0.9621</cell></row><row><cell>Action(HMDB)</cell><cell cols="2">9.0500</cell><cell cols="2">0.4825</cell><cell cols="2">0.8306</cell><cell cols="2">9.5716</cell><cell cols="2">0.5876</cell><cell cols="2">0.9322</cell><cell>9.8850</cell><cell cols="2">0.6271</cell><cell cols="2">0.9693</cell></row><row><cell>iDT</cell><cell cols="2">9.0538</cell><cell cols="2">0.4976</cell><cell cols="2">0.8090</cell><cell cols="2">9.5686</cell><cell cols="2">0.6015</cell><cell cols="2">0.9303</cell><cell>9.8801</cell><cell cols="2">0.6389</cell><cell cols="2">0.9722</cell></row><row><cell>MoSIFT</cell><cell cols="2">8.1329</cell><cell cols="2">0.5352</cell><cell cols="2">0.5704</cell><cell cols="2">8.8799</cell><cell cols="2">0.6441</cell><cell cols="2">0.7655</cell><cell>9.3488</cell><cell cols="2">0.6830</cell><cell cols="2">0.8714</cell></row><row><cell>MFCC</cell><cell cols="2">8.6740</cell><cell cols="2">0.5450</cell><cell cols="2">0.7545</cell><cell cols="2">9.3151</cell><cell cols="2">0.6418</cell><cell cols="2">0.9010</cell><cell>9.6971</cell><cell cols="2">0.6709</cell><cell cols="2">0.9589</cell></row><row><cell>VGGish</cell><cell cols="2">9.1818</cell><cell cols="2">0.4962</cell><cell cols="4">0.8777 9.6664</cell><cell cols="2">0.5968</cell><cell cols="2">0.9615</cell><cell>9.9498</cell><cell cols="2">0.6300</cell><cell cols="2">0.9855</cell></row><row><cell>scaled-CER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Genres</cell><cell cols="2">8.0557</cell><cell cols="2">0.2243</cell><cell cols="2">0.5136</cell><cell cols="2">8.7664</cell><cell cols="2">0.3087</cell><cell cols="2">0.6803</cell><cell>9.2270</cell><cell cols="2">0.3611</cell><cell cols="2">0.7442</cell></row><row><cell>Obj(IN)</cell><cell cols="2">8.6569</cell><cell cols="2">0.4791</cell><cell cols="2">0.7332</cell><cell cols="2">9.2425</cell><cell cols="2">0.5819</cell><cell cols="2">0.8899</cell><cell>9.6058</cell><cell cols="2">0.6228</cell><cell cols="2">0.9541</cell></row><row><cell>Scene</cell><cell cols="2">8.8951</cell><cell cols="2">0.4868</cell><cell cols="2">0.7827</cell><cell cols="2">9.4651</cell><cell cols="2">0.5919</cell><cell cols="2">0.9105</cell><cell>9.8077</cell><cell cols="2">0.6315</cell><cell cols="2">0.9611</cell></row><row><cell>Action(IG)</cell><cell cols="2">9.1571</cell><cell cols="2">0.4711</cell><cell cols="2">0.8577</cell><cell cols="2">9.6278</cell><cell cols="2">0.5751</cell><cell cols="2">0.9519</cell><cell>9.9143</cell><cell cols="2">0.6155</cell><cell cols="2">0.9801</cell></row><row><cell>Action(HMDB)</cell><cell cols="2">9.1299</cell><cell cols="2">0.4781</cell><cell cols="2">0.8440</cell><cell cols="2">9.6537</cell><cell cols="2">0.5850</cell><cell cols="2">0.9470</cell><cell>9.9591</cell><cell cols="2">0.6254</cell><cell cols="2">0.9790</cell></row><row><cell>iDT</cell><cell cols="2">8.8032</cell><cell cols="2">0.4874</cell><cell cols="2">0.7342</cell><cell cols="2">9.3868</cell><cell cols="2">0.5919</cell><cell cols="2">0.8888</cell><cell>9.7340</cell><cell cols="2">0.6304</cell><cell cols="2">0.9494</cell></row><row><cell>MoSIFT</cell><cell cols="2">7.9336</cell><cell cols="2">0.5308</cell><cell cols="2">0.4944</cell><cell cols="2">8.7381</cell><cell cols="2">0.6379</cell><cell cols="2">0.7003</cell><cell>9.2389</cell><cell cols="2">0.6772</cell><cell cols="2">0.8167</cell></row><row><cell>MFCC</cell><cell cols="2">8.4761</cell><cell cols="2">0.5417</cell><cell cols="2">0.6790</cell><cell cols="2">9.1700</cell><cell cols="2">0.6390</cell><cell cols="2">0.8496</cell><cell>9.5810</cell><cell cols="2">0.6676</cell><cell cols="2">0.9308</cell></row><row><cell>VGGish</cell><cell cols="2">9.1073</cell><cell cols="2">0.4888</cell><cell cols="2">0.8656</cell><cell cols="2">9.6207</cell><cell cols="2">0.5897</cell><cell cols="2">0.9561</cell><cell>9.9195</cell><cell cols="2">0.6234</cell><cell cols="2">0.9834</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>reports the beyond-accuracy results of the CER model and scaled-CER model in terms of Shannon Entropy, intra-list diversity and item coverage in the item cold-start scenario. Overall, the CER model obtained the highest results for almost all three metrics with the exception being the SE@30. However, these results come at the cost of accuracy. The lowest results are obtained using the genre features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results of different fusion methods with respect to accuracy metrics using the best visual-appearance, audio and action features. The highest results along the respective metric are marked in bold. Obj(IN) features baseline with regards to MAP and NDCG metrics across all cut-off values. An interesting observation is that the sum of Obj(IN) and VGGish features leads to better recommendation accuracy compared to VGGish features alone (Table 3). A noticeable drop in performance is observed when the Action(IG) features are combined with Obj(IN)and VGGish features using the two aforementioned fusion methods. This outcome suggests that the sum and max fusion methods are not able to create a shared latent space which is easy to learn the complementary video information encoded in these features. On the other hand, it is clear that the concat fusion method outperforms the baseline, and the sum and max fusion methods with regards to all accuracy metrics across all cut-off values. The concat of Obj(IN)and VGGish features significantly improves upon the Obj(IN) performance by 20.7% and 19.4% for the @5 cut-off experiments along the MAP and NDCG metrics, respectively. For the @15 and @30 cut-off experiments the increase over the baseline are 18.6% and 17.3% for MAP, and 14.8% and 13.1% for NDCG.In addition, different from the sum and max fusion methods, the concat of Action(IG), Obj(IN)and VGGish features presents a significant positive effect in the overall recommendation performance. More precisely, the recommendation accuracy in terms of MAP@5, MAP@15 and MAP@30 increased by 8.8%, 8.0% and 7.7% over the concat of Obj(IN)and VGGish features, accordingly. In terms of NDCG@5, NDCG@15 and NDCG@30, the recommendation accuracy increased by 7.7%, 6.8% and 6.0%, respectively. These results suggest that the shared latent space created by the concat fusion method is more discriminative thus leading to better recommendation accuracy. VGGish features complement the recommendation accuracy. In addition, Action(IG) features combined with Obj(IN) and VGGish features create video representations that are highly predictive of user preferences. This means that the content present in the videos are better described. As a result, enhanced recommendations are provided.</figDesc><table><row><cell>Features</cell><cell>Feature</cell><cell cols="6">MAP@5 NDCG@5 MAP@15 NDCG@15 MAP@30 NDCG@30</cell></row><row><cell></cell><cell>Fusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Obj(IN)</cell><cell>-</cell><cell>0.0178</cell><cell>0.0247</cell><cell>0.0188</cell><cell>0.0404</cell><cell>0.0208</cell><cell>0.0538</cell></row><row><cell>Obj(IN) + VGGish</cell><cell>concat</cell><cell>0.0215</cell><cell>0.0295</cell><cell>0.0223</cell><cell>0.0464</cell><cell>0.0244</cell><cell>0.0609</cell></row><row><cell>Obj(IN) + VGGish</cell><cell>sum</cell><cell>0.0144</cell><cell>0.0201</cell><cell>0.0148</cell><cell>0.0328</cell><cell>0.0164</cell><cell>0.0439</cell></row><row><cell>Obj(IN) + VGGish</cell><cell>max</cell><cell>0.0122</cell><cell>0.0171</cell><cell>0.0126</cell><cell>0.0268</cell><cell>0.0137</cell><cell>0.0354</cell></row><row><cell>Obj(IN) + VGGish + Action(IG)</cell><cell>concat</cell><cell>0.0234</cell><cell>0.0318</cell><cell>0.0241</cell><cell>0.0496</cell><cell>0.0263</cell><cell>0.0646</cell></row><row><cell>Obj(IN) + VGGish + Action(IG)</cell><cell>sum</cell><cell>0.0111</cell><cell>0.0156</cell><cell>0.0119</cell><cell>0.0277</cell><cell>0.0134</cell><cell>0.0386</cell></row><row><cell>Obj(IN) + VGGish + Action(IG)</cell><cell>max</cell><cell>0.0086</cell><cell>0.0123</cell><cell>0.0091</cell><cell>0.0210</cell><cell>0.0102</cell><cell>0.0292</cell></row><row><cell cols="8">From Table 5, it can be seen that the sum and max fusion methods are</cell></row><row><cell cols="2">not able to outperform the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Results of different fusion methods with respect to beyond-accuracy metrics using the best visual-appearance, audio and action features. The highest results along the respective metric are marked in bold.</figDesc><table><row><cell>Features</cell><cell>Feature</cell><cell>Div. @5</cell><cell>Div.</cell><cell>@5</cell><cell>Item</cell><cell>@5</cell><cell>Div. @15</cell><cell>Div.</cell><cell>@15</cell><cell>Item</cell><cell>@15</cell><cell>Div. @30</cell><cell>Div.</cell><cell>@30</cell><cell>Item</cell><cell>@30</cell></row><row><cell></cell><cell>Fusion</cell><cell>SE</cell><cell>IntraL</cell><cell></cell><cell>Cov.</cell><cell></cell><cell>SE</cell><cell>IntraL</cell><cell></cell><cell>Cov.</cell><cell></cell><cell>SE</cell><cell>IntraL</cell><cell></cell><cell>Cov.</cell><cell></cell></row><row><cell>Obj(IN)</cell><cell>-</cell><cell>8.6569</cell><cell cols="2">0.4791</cell><cell cols="2">0.7332</cell><cell>9.2425</cell><cell cols="2">0.5819</cell><cell cols="2">0.8899</cell><cell>9.6058</cell><cell cols="2">0.6228</cell><cell cols="2">0.9541</cell></row><row><cell>Obj(IN) + VGGish</cell><cell>concat</cell><cell>8.9131</cell><cell cols="2">0.4594</cell><cell cols="2">0.8540</cell><cell>9.4868</cell><cell cols="2">0.5654</cell><cell cols="2">0.9552</cell><cell>9.8176</cell><cell cols="2">0.6068</cell><cell cols="2">0.9866</cell></row><row><cell>Obj(IN) + VGGish</cell><cell>sum</cell><cell>8.7458</cell><cell cols="2">0.5012</cell><cell cols="2">0.7661</cell><cell>9.3273</cell><cell cols="2">0.6004</cell><cell cols="2">0.9056</cell><cell>9.6754</cell><cell cols="2">0.6349</cell><cell cols="2">0.9583</cell></row><row><cell>Obj(IN) + VGGish</cell><cell>max</cell><cell>8.6516</cell><cell cols="2">0.5227</cell><cell cols="2">0.7293</cell><cell>9.3001</cell><cell cols="2">0.6197</cell><cell cols="2">0.8858</cell><cell>9.6776</cell><cell cols="2">0.6497</cell><cell cols="2">0.9513</cell></row><row><cell>Obj(IN) + VGGish + Action(IG)</cell><cell cols="2">concat 8.9552</cell><cell cols="2">0.4534</cell><cell cols="3">0.8655 9.5163</cell><cell cols="2">0.5578</cell><cell cols="2">0.9597</cell><cell>9.8403</cell><cell cols="2">0.6006</cell><cell cols="2">0.9880</cell></row><row><cell>Obj(IN) + VGGish + Action(IG)</cell><cell>sum</cell><cell>8.9218</cell><cell cols="2">0.4869</cell><cell cols="2">0.8101</cell><cell>9.4826</cell><cell cols="2">0.5900</cell><cell cols="2">0.9355</cell><cell>9.8093</cell><cell cols="2">0.6277</cell><cell cols="2">0.9773</cell></row><row><cell>Obj(IN) + VGGish + Action(IG)</cell><cell>max</cell><cell>8.6260</cell><cell cols="2">0.5137</cell><cell cols="2">0.6967</cell><cell>9.2581</cell><cell cols="2">0.6177</cell><cell cols="2">0.8674</cell><cell>9.6366</cell><cell cols="2">0.6547</cell><cell cols="2">0.9414</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>cat of Obj(IN), VGGish and Action(IG) features with regards to Shannon Entropy and item coverage. These improvements do not come at the expense of recommendation accuracy. The results are promising and indicate that the complementariness of Obj(IN), VGGish and Action(IG) features considers more items on the catalogue. These items are being given a better chance of being recommended leading to recommendations that are more equally spread out throughout all cold items.3.6. Ablation studyIn this experiment, we investigate the importance of different features in the overall recommendation quality. The main goal is to empirically assess the importance of using a diverse range of video content features while taking full advantage of the available features in the item cold-start scenario. Thus, the experiment is performed by combining all the video content features explored in this work.The ablation study is based only on the scaled-CER model using the concatenation fusion method, due to the outstanding overall performance shown in the previous experiment. We remove each type of feature from the concatenation of all the features, denoted by All/x where x ? {Obj(IN), VGGish, Action(IG), Genres, Scene, Action(HMDB), MFCC, iDT, MoSIFT} is removed from All.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of the importance of all video content features explored in this work in the overall recommendation quality in terms of MAP and item coverage. All/x denotes removing x from the concatenation of all the video content features. The best results across the respective metric are highlighted in bold. The box around the 5 th row highlights the highest MAP results obtained in this work.Looking atTable 7, when combining the various video content features, the overall results are really interesting. The combination brings a significant increase in recommendation accuracy that comes with a drop in item coverage.However, the item coverage achieved still better than the unimodal Obj(IN)) features(Table 6). It can be observed that the biggest boost to recommendation accuracy and the noticeable decrease in item coverage is provided by the genre features in contrast to any other feature. When the genre features are removed from the combination of all the video content features (All/Genres), we obtain the lowest MAP results, but the highest item coverage. The genre features complement the recommendation accuracy of the deep learning features and the hand-crafted features. This outcome implies that genre features probably remove ambiguity from the non-textual features that in turn lead to an improvement in MAP while sacrificing item coverage. This was expected because each non-textual content feature vector was fused with genres that describe high-level concepts of a movie thus creating a more discriminative semantically meaningful content descriptor. By removing any other feature, we can see a</figDesc><table><row><cell>Features</cell><cell>MAP@5</cell><cell>Item</cell><cell cols="2">@5 MAP@15</cell><cell>Item</cell><cell cols="2">@15 MAP@30</cell><cell>Item</cell><cell>@30</cell></row><row><cell></cell><cell></cell><cell>Coverage</cell><cell></cell><cell></cell><cell>Coverage</cell><cell></cell><cell></cell><cell>Coverage</cell></row><row><cell>All</cell><cell>0.0344</cell><cell cols="2">0.7716</cell><cell>0.0348</cell><cell cols="2">0.9174</cell><cell>0.0377</cell><cell>0.9726</cell></row><row><cell>All/MoSIFT</cell><cell>0.0337</cell><cell cols="2">0.7720</cell><cell>0.0341</cell><cell cols="2">0.9128</cell><cell>0.0370</cell><cell>0.9681</cell></row><row><cell>All/iDT</cell><cell>0.0337</cell><cell cols="2">0.7567</cell><cell>0.0342</cell><cell cols="2">0.9047</cell><cell>0.0372</cell><cell>0.9632</cell></row><row><cell>All/MFCC</cell><cell>0.0347</cell><cell cols="2">0.7468</cell><cell>0.0350</cell><cell cols="2">0.8970</cell><cell>0.0379</cell><cell>0.9595</cell></row><row><cell>All/Action(HMDB)</cell><cell>0.0336</cell><cell cols="2">0.7511</cell><cell>0.0338</cell><cell cols="2">0.9028</cell><cell>0.0368</cell><cell>0.9629</cell></row><row><cell>All/Scene</cell><cell>0.0332</cell><cell cols="2">0.7686</cell><cell>0.0336</cell><cell cols="2">0.9122</cell><cell>0.0366</cell><cell>0.9681</cell></row><row><cell>All/Genres</cell><cell>0.0268</cell><cell cols="2">0.8281</cell><cell>0.0276</cell><cell cols="2">0.9413</cell><cell>0.0302</cell><cell>0.9778</cell></row><row><cell>All/Action(IG)</cell><cell>0.0338</cell><cell cols="2">0.7515</cell><cell>0.0342</cell><cell cols="2">0.9026</cell><cell>0.0372</cell><cell>0.9604</cell></row><row><cell>All/VGGish</cell><cell>0.0325</cell><cell cols="2">0.7747</cell><cell>0.0328</cell><cell cols="2">0.9152</cell><cell>0.0356</cell><cell>0.9678</cell></row><row><cell>All/Obj(IN)</cell><cell>0.0335</cell><cell cols="2">0.7681</cell><cell>0.0339</cell><cell cols="2">0.9139</cell><cell>0.0368</cell><cell>0.9682</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/Adolfo-Almeida/scaled CER</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded by the MultiChoice Research Chair of Machine Learning at the University of Pretoria, South Africa.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">YouTube-8m: A Large-Scale Video Classification Benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A component-based video content representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fazl-Ersi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">103805</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving Aggregate Recommendation Diversity Using Ranking-Based Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="896" to="911" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="734" to="749" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual comparison of statistical feature aggregation methods for video-based similarity applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Villiers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Velayudan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 23rd International Conference on Information Fusion (FUSION)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the influence of low-level visual features in film classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hern?ndez-Pe?aloza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Men?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cisneros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">MoSIFT: Recognizing Human Actions in Surveillance Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Troubling Analysis of Reproducibility and Progress in Recommender Systems Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Dacrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Dacrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Movie genome: alleviating new item cold start in movie recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deldjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Dacrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eghbal-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cereda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="291" to="343" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Content-Based Video Recommendation System Based on Stylistic Visual Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deldjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Garzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Piazzolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Quadrana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Data Semantics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="99" to="113" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recommender Systems Leveraging Multimedia Content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deldjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Personalized Video Recommendation Using Rich Contents from Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="492" to="505" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">User Preference Elicitation, Rating Sparsity and Cold Start</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Braunhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gurbanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Collaborative Recommendations chapter</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="253" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HybridSVD: When Collaborative Information is Not Enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-Scale Weakly-Supervised Pre-Training for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12038" to="12047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Gomez-Uribe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hunt</surname></persName>
		</author>
		<title level="m">The Netflix Recommender System: Algorithms, Business Value, and Innovation. ACM Transactions on Management Information Systems (TMIS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CNN architectures for large-scale audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Holzenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from Multiview Correlations in Open-domain Videos</title>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<biblScope unit="page" from="8628" to="8632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="7366" to="7375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring Object-Centric and Scene-Centric CNN Features and Their Complementarity for Human Rights Violations Recognition in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kalliatakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ehsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fasli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcdonald-Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="10045" to="10056" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Icebreaker: Solving cold start problem for video recommendation engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khaund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumaraguru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="217" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-Scale Content-Only Video Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="987" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Content-based Video Relevance Prediction Challenge: Data, Protocol, and Baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00737</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Use What You Have: Video Retrieval Using Representations From Collaborative Experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="279" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Content-based Recommender Systems: State of the Art and Trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Degemmis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Semeraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook chapter</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="73" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recommender system application developments: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Support Systems</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="12" to="32" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LGA: latent genre aware micro-video recommendation on social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="2991" to="3008" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">High Accuracy Retrieval with Multiple Nested Ranker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Burkard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laucius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th</title>
		<meeting>the 29th</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<title level="m">Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="437" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips</title>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Nikolakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kalantzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gallopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garofalakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">EigenRec: generalizing PureSVD for effective and efficient top-N recommendations</title>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="59" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<title level="m">L2-constrained softmax loss for discriminative face verification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Evaluating Recommendation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gunawardana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems Handbook chapter 8</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="257" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Advance on large scale near-duplicate video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Computer Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="14" to="38" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How good your recommender system is? A survey on evaluations in recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="813" to="831" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">D3D: Distilled 3D Networks for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">How robust is MovieLens? A dataset analysis for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Tousch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12799</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">DropoutNet: Addressing Cold Start in Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poutanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4957" to="4966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Action Recognition with Improved Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Overview of Content-Based Click-Through Rate Prediction Challenge for Video Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2593" to="2596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Movie genre classification: A multi-label approach based on convolutions through time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="973" to="982" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Collaborative filtering and deep learning based recommendation system for cold start items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Caught Red-Handed: Toward Practical Video-Based Subsequences Matching in the Presence of Real-World Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monrose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1397" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shalaby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Korayem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aljadda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Solving cold-start problem in large-scale recommendation engines: A deep learning approach</title>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<biblScope unit="page" from="1901" to="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Places: A 10 Million Image Database for Scene Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
