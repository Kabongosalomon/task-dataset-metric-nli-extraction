<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAR: Class-aware Regularizations for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Kang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Fujian Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefei</forename><surname>Zhe</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linchao Bao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangjian</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Nottingham Ningbo China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CAR: Class-aware Regularizations for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Class-aware regularizations, semantic segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent segmentation methods, such as OCR and CPNet, utilizing "class level" information in addition to pixel features, have achieved notable success for boosting the accuracy of existing network modules. However, the extracted class-level information was simply concatenated to pixel features, without explicitly being exploited for better pixel representation learning. Moreover, these approaches learn soft class centers based on coarse mask prediction, which is prone to error accumulation. In this paper, aiming to use class level information more effectively, we propose a universal Class-Aware Regularization (CAR) approach to optimize the intra-class variance and inter-class distance during feature learning, motivated by the fact that humans can recognize an object by itself no matter which other objects it appears with. Three novel loss functions are proposed. The first loss function encourages more compact class representations within each class, the second directly maximizes the distance between different class centers, and the third further pushes the distance between inter-class centers and pixels. Furthermore, the class center in our approach is directly generated from ground truth instead of from the error-prone coarse prediction. Our method can be easily applied to most existing segmentation models during training, including OCR and CPNet, and can largely improve their accuracy at no additional inference overhead. Extensive experiments and ablation studies conducted on multiple benchmark datasets demonstrate that the proposed CAR can boost the accuracy of all baseline models by up to 2.23% mIOU with superior generalization ability. The complete code is available at https://github.com/edwardyehuang/CAR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation, which assigns a class label for each pixel in an image, is a fundamental task in computer vision. It has been widely used in many realworld scenarios that require the results of scene parsing for further processing, e.g., image editing, autopilot, etc. It also benefits many other computer vision tasks such as object detection and depth estimation.</p><p>After the early work FCN <ref type="bibr" target="#b15">[16]</ref> which used fully convolutional networks to make the dense per-pixel segmentation task more efficient, many works <ref type="bibr">[36,</ref><ref type="bibr" target="#b1">2]</ref> have been proposed which have greatly advanced the segmentation accuracy on various benchmark datasets. Among these methods, many of them have focused on better fusing spatial domain context information to obtain more powerful feature representations (termed pixel features in this work) for the final per-pixel classification. For example, VGG <ref type="bibr">[21]</ref> utilized large square context information by successfully training a very deep network, and DeepLab <ref type="bibr" target="#b1">[2]</ref> and PSPNet <ref type="bibr">[36]</ref> utilized multi-scale features with the ASPP and PPM modules.</p><p>Recently, methods based on dot-product self-attention (SA) have become very popular since they can easily capture the long-range relationship between pixels <ref type="bibr">[26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">32,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">37,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">20,</ref><ref type="bibr">22]</ref>. SA aggregates information dynamically (by different attention maps for different inputs) and selectively (using weighted averaging spatial features according to their similarity scores). Using multi-scale and self-attention techniques during spatial information aggregation has worked very well (e.g., 80% mIOU on Cityscapes <ref type="bibr" target="#b16">[17]</ref> (single-scale w/o flipping)).</p><p>As complements to the above methods, many recent works have proposed various modules to utilize class-level contextual information. The class-level information is often represented by the class center/context prior which are the mean features of each class in the images. OCR <ref type="bibr">[31]</ref> and ACFNet [33] extract "soft" class centers according to the predicted coarse segmentation mask by using the weighted sum. CPNet <ref type="bibr">[30]</ref> proposed a context prior map/affinity map, which indicates if two spatial locations belong to the same class, and used this predicted context prior map for feature aggregation. However, they [31,33,30] simply concatenated these class-level features with the original pixel features for the final classification.</p><p>In this paper, we also focus on utilizing class level information. Instead of focusing on how to better extract class-level features like the existing methods <ref type="bibr">[31,</ref><ref type="bibr">33,</ref><ref type="bibr">30]</ref>, we use the simple, but accurate, average feature according to the GT mask, and focus on maximizing the inter-class distance during feature learning. This is because it mirrors how humans can robustly recognize an object by itself no matter what other objects it appears with.</p><p>Learning more separable features makes the features of a class less dependent upon other classes, resulting in improved generalization ability, especially when the training set contains only limited and biased class combinations (e.g., cows and grass, boats and beach). <ref type="figure" target="#fig_1">Fig. 1</ref> illustrates an example of such a problem, where the classification of dog and sheep depends on the classification of grass class, and has been mis-classified as cow. In comparison, networks trained with our proposed CAR successfully generalize to these unseen class combinations.</p><p>To better achieve this goal, we propose CAR, a class-aware regularizations module, that optimizes the class center (intra-class) and inter-class dependencies during feature learning. Three loss functions are devised: the first encourages more compact class representations within each class, and the other two di-  Our CAR optimizes existing models with three regularization targets: 1) reducing pixels' intra-class distance, 2) reducing inter-class center-to-center dependency, and 3) reducing pixels' interclass dependency. As highlighted in this example (indicated with a red dot in the image), with our CAR, the grass class does not affect the classification of dog/sheep as much as before, and hence successfully avoids previous (w/o CAR) mis-classification. rectly maximize the distance between different classes. Specifically, an intra-class center-to-pixel loss (termed as "intra-c2p", Eq. (3)) is first devised to produce more compact representation within a class by minimizing the distance between all pixels and their class center. In our work, a class center is calculated as the averaged feature of all pixels belonging to the same class according to the GT mask. More compact intra-class representations leave a relatively large margin between classes, thus contributing to more separable representations. Then, an inter-class center-to-center loss ("inter-c2c", Eq. <ref type="formula" target="#formula_5">(6)</ref>) is devised to maximize the distance between any two different class centers. This inter-class center-to-center loss alone does not necessarily produce separable representations for every individual pixels. Therefore, a third inter-class center-to-pixel loss ("inter-c2p", Eq. <ref type="formula" target="#formula_0">(13)</ref>) is proposed to enlarge the distance between every class center and all pixels that do not belong to the class. In summary, our contributions in this work are: 1. We propose a universal class-aware regularization module that can be integrated into various segmentation models to largely improve the accuracy. 2. We devise three novel regularization terms to achieve more separable and less class-dependent feature representations by minimizing the intra-class variance and maximizing the inter-class distance. 3. We calculate the class centers directly from ground truth during training, thus avoiding the error accumulation issue of the existing methods and introducing no computational overhead during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>We provide image-level feature-similarity heatmaps to visualize the learned inter-class features with our CAR are indeed less related to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Self-Attention. Dot-product self-attention proposed in <ref type="bibr">[26,</ref><ref type="bibr">23]</ref> has been widely used in semantic segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">32,</ref><ref type="bibr">35,</ref><ref type="bibr">37]</ref>. Specifically, self-attention determines the similarity between a pixel with every other pixel in the feature map by calculating their dot product, followed by softmax normalization. With this attention map, the feature representation of a given pixel is enhanced by aggregating features from the whole feature map weighted by the aforementioned attention value, thus easily taking long-range relationship into consideration and yielding boosted performance. In self-attention, in order to achieve correct pixel classification, the representation of pixels belonging to the same class should be similar to gain greater weights in the final representation augmentation. Class Center. In 2019 <ref type="bibr">[33,</ref><ref type="bibr">31]</ref>, the concept of class center was introduced to describe the overall representation of each class from the categorical context perspective. In these approaches, the center representation of each class was determined by calculating the dot product of the feature map and the coarse prediction (i.e., weighted average) from an auxiliary task branch, supervised by the ground truth <ref type="bibr">[36]</ref>. After that, those intra-class centers are assigned to the corresponding pixels on feature map. Furthermore, in 2020 [30], a learnable kernel and one-hot ground truth were used to separate the intra-class center from inter-class center, and then concatenated with the original feature representation. All of these works [31,33,30] have focused on extracting the intra (inter) class centers, but they then simply concatenated the resultant class centers with the original pixel representations to perform the final logits. We argue that the categorical context information can be utilized in a more effective way so as to reduce the inter-class dependency.</p><p>To this end, we propose a CAR approach, where the extracted class center is used to directly regularize the feature extraction process so as to boost the differentiability of the learned feature representations (see <ref type="figure" target="#fig_1">Fig. 1</ref>) and reduce their dependency on other classes. <ref type="figure">Fig. 2</ref> contrasts the two different designs. More details of the proposed CAR are provided in Sect. 3. Inter-Class Reasoning. Recently, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> studied the class dependency as a dataset prior and demonstrated that inter-class reasoning could improve the classification performance. For example, a car usually does not appear in the sky, and therefore the classification of sky can help reduce the chance of misclassifying an object in the sky as a car. However, due to the limited training data, such class-dependency prior may also contain bias, especially when the desired class relation rarely appears in the training set. <ref type="figure" target="#fig_1">Fig. 1</ref> shows such an example. In the training set, cow and grass are dependent on each other. However, as shown in this example, when there is a dog or sheep standing on the grass, the class dependency learned from the limited training data may result in errors and predict the target into a class that appears  <ref type="figure">Fig. 2</ref>: The difference between the proposed CAR and previous methods that use class-level information. Previous models focus on extracting class center while using simple concatenation of the original pixel feature and the class/context feature for later classification. In contrast, our CAR uses direct supervision related to class center as regularization during training, resulting in small intra-class variance and low inter-class dependency. See <ref type="figure" target="#fig_1">Fig. 1</ref> and Sec. 3 for details. more often in the training data, i.e., cow in this case. In our CAR, we design inter-class and intra-class loss functions to reduce such inter-class dependency and achieve more robust segmentation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extracting Class Centers from Ground Truth</head><p>Denote a feature map and its corresponding resized one-hot encoded groundtruth mask as X ? R H?W ?C 5 and Y ? R H?W ?N class , respectively. We first get the spatially flattened class mask Y flat ? R HW ?N class and flattened feature map X flat ? R HW ?C . Then, the class center 6 , which is the average features of all pixel features of a class, can be calculated by:</p><formula xml:id="formula_0">? image = Y T flat ? X flat N non-zero ? R N class ?C ,<label>(1)</label></formula><p>where N non-zero denotes the number of non-zero values in the corresponding map of the ground-truth mask Y. In our experiments, to alleviate the negative impact of noisy images, we calculate the class centers using all the training images in a batch, and denote them as ? batch 7 .  similarity in spatial space to encourage similar pixels to have a compact distance implicitly. For example, the self-attention in <ref type="bibr">[26]</ref> implicitly pushed the feature representation of pixels belonging to the same class to be more similar to each other than those of pixels belonging to other classes. In our work, we devise a simple intra-class center-to-pixel loss to guide the training, which can achieve this goal very effectively and produce improved accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reducing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Intra-class</head><p>Center-to-pixel Loss. We define a simple but effective intra-class center-to-pixel loss to suppress the intra-class feature variance by penalizing large distance between a pixel feature and its class center. The Intraclass Center-to-pixel Loss L intra-c2p is defined by:</p><formula xml:id="formula_1">L intra-c2p = fmse(D intra-c2p ),<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">D intra-c2p = (1 ? ?)|Y flat ? ? ? X flat |.<label>(3)</label></formula><p>In Eq. <ref type="formula" target="#formula_2">(3)</ref>, ? is a spatial mask indicating pixels being ignored (i.e., ignore label), Y flat ? ? distributes the class centers ? to the corresponding positions in each image. Thus, our intra-class loss L intra-c2p will push the pixel representations to their corresponding class center, using mean squared error (MSE) in Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Maximizing Inter-class Separation</head><p>3.3.1 Motivation. Humans can robustly recognize an object by itself regardless which other objects it appears with. Conversely, if a classifier heavily relies on the information from other classes to determine the classification result, it will easily produce wrong classification results when a rather rare class combination appears during inference. Maximizing inter-class separation, or in another words, reducing the inter-class dependency, can therefore help the network generalize better, especially when the training set is small or is biased. As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the dog and sheep are mis-classified as the cow because cow and grass appear together more often in the training set. To improve the robustness of the model, we propose to reduce this inter-class dependency. To this end, the following two loss functions are defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>Inter-class Center-to-center Loss. The first loss function is to maximize the distance between any two different class centers. Inspired by the center loss used in face recognition [27], we propose to reduce the similarity between class centers ?, which are the averaged features of each class calculated according to the GT mask. The inter-class relation is defined by the dot-product similarity [23] between any two classes as:</p><formula xml:id="formula_3">A c2c = softmax( ? T ? ? ? C ), A c2c ? R N class ?N class .<label>(4)</label></formula><p>Moreover, since we only need to constrain the inter-class distance, only the non-diagonal elements are retained for the later loss calculation as:</p><formula xml:id="formula_4">D inter-c2c = 1 ? eye(N class ) A c2c .<label>(5)</label></formula><p>We only penalize larger similarity values between any two different classes than a pre-defined threshold ?0 N class ?1 , i.e.,</p><formula xml:id="formula_5">D inter-c2c = f sum max(D inter-c2c ? ? 0 N class ? 1 , 0) .<label>(6)</label></formula><p>Thus, the Inter-class Center-to-center Loss L inter-c2c is defined by:</p><formula xml:id="formula_6">L inter-c2c = f mse (D inter-c2c ).<label>(7)</label></formula><p>Here, a small margin is used in consideration of the feature space size and the mislabeled ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.3</head><p>Inter-class Center-to-pixel Loss. Maximizing only the distances between class centers does not necessarily result in separable representation for every individual pixels. We further maximize the distance between a class center and any pixel that does not belong to this class. More concretely, we first compute the center-to-pixel dot product as:</p><formula xml:id="formula_7">? c2p = ? T ? X flat , ? c2p ? R HW ?N class .<label>(8)</label></formula><p>Ideally, with the previous loss L inter-c2c , the features of all pixels belonging to the same class should be equal to that of the class center. Therefore, we replace the intra-class dot product with its ideal value, namely using the class center ? for calculating the intra-class dot product as:</p><formula xml:id="formula_8">? c = diag(? T ? ?),<label>(9)</label></formula><p>and the replacement effect is achieved by using masks as:</p><formula xml:id="formula_9">? ? = ? c2p (1 ? Y flat ) + ? c .<label>(10)</label></formula><p>This updated dot product ? ? is then used to calculate similarity across class axis with a softmax as:</p><formula xml:id="formula_10">A c2p = softmax(? ? ), A c2p ? R HW ?N class .<label>(11)</label></formula><p>Similar to the calculation of L inter-c2c in the previous subsection, we have</p><formula xml:id="formula_11">D inter-c2p = (1 ? Y flat )A c2p ,<label>(12)</label></formula><formula xml:id="formula_12">D inter-c2p = f sum max(D inter-c2p ? ? 1 N class ? 1 , 0) .<label>(13)</label></formula><p>Thus, the Inter-class Center-to-pixel Loss L inter-c2p is defined by: , which all focus on better utilizing class-level features and differ on how to extract the class centers and context features. However, they all use a simple concatenation to fuse the original pixel feature and the complementary context feature. For example, OCR and ACFNet first produce a coarse segmentation, which is supervised by the GT mask with a categorical cross-entropy loss, and then use this predicted coarse mask to generate the (soft) class centers by weighted summing all the pixel features. OCR then aggregates these class centers according to their similarity to the original pixel feature termed as "pixel-region relation", resulting in a "contextual feature". Slightly differently from OCR, ACFNet directly uses the probability (from the predicted coarse mask) to aggregate class center, obtaining a similar context feature termed as "attentional class feature". CPNet defines an affinity map, which is a binary map indicating if two spatial locations belong to the same class. Then, they use a sub-network to predict their ideal affinity map and use the soft version affinity map termed as "Context Prior Map" for feature aggregation, obtaining a class feature (center) and a context feature. Note that CPNet concatenates class feature, which is the updated pixel feature, and the context feature.</p><formula xml:id="formula_13">L inter-c2p = f mse (D inter-c2p ).<label>(14)</label></formula><p>We also propose to utilize class-level contextual features. Instead of extracting and fusing pixel features with sub-networks, we propose three loss functions to directly regularize training and encourage the learned features to maintain certain desired properties. The approach is simple but more effective thanks to the direct supervision (validated in Tab. 2). Moreover, our class center estimate is more accurate because we use the GT mask. This strategy largely reduces the complexity of the network and introduces no computational overhead during inference. Furthermore, it is compatible with all existing methods, including OCR, ACFNet and CPNet, demonstrating great generalization capability.</p><p>We also notice that Cross-Image Pixel Contrast (CIPC) [25] shares a similar high-level goal as our CAR, i.e., learning more similar representations for pixels belonging to the same class than to a different class. However, the approaches of achieving this goal are very different. CIPC is motivated by contrastive learning while our CAR is motivated by the compositionality of the scene, for better generalization in the cases of rare class combinations. Therefore, CIPC adopts the one-vs-rest style InfoNCE loss, including the typical pixel-to-pixel loss and a special pixel-to-center loss. In contrast, (1) we propose an additional centerto-center loss to regularize the inter-class dependency explicitly and effectively (see <ref type="table" target="#tab_2">Table 1</ref>); (2) we use one-vs-one style inter-class losses while CIPC uses onevs-rest style NCE loss. Compared to our one-vs-one loss, using one-vs-rest loss for training does not necessarily result in small inter-class similarity between the current class and every individual "other" classes and may increase the interclass similarity among those "other" classes. (3) we leave margins to prevent CAR regularizations, which is not the primary task of pixel classification, from dominating the learning process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation</head><p>Training Settings. For both CAR and baselines, we apply the settings common to most works [34,35,11,10,37], including SyncBatchNorm, batch size = 16, weight decay (0.001), 0.01 initial LR, and poly learning decay with SGD during training. In addition, for the CNN backbones (e.g., ResNet), we set output stride = 8 (see <ref type="bibr" target="#b2">[3]</ref>). Training iteration is set to 30k iterations unless otherwise specified. For the thresholds in Eq. 6 and Eq. 13, we set ? 0 = 0.5 and ? 1 = 0.25.</p><p>Determinism and Reproducibility Our implementations are based on the latest NVIDIA deterministic framework (2022), which means exactly the same results can be always reproduced with the same hardware and same training settings (including random seed). To demonstrate the effectiveness of our CAR with equal comparisons, we reproduced all the baselines that we compare, all conducted with exactly the same settings unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Pascal Context</head><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Ablation Studies on Pascal Context</head><p>CAR on ResNet-50 + Self-Attention. We firstly test the CAR with "ResNet-50 + Self-Attention" (w/o image-level block in <ref type="bibr">[35]</ref>) to verify the effectiveness of the proposed loss functions, i.e., L intra-c2p , L inter-c2c , and L inter-c2p . As shown in Tab. 1, using L intra-c2p directly improves 1.30 mIOU (48.32 vs 49.62); Introducing L inter-c2c and L inter-c2p further improves 0.38 mIOU and 0.50 mIOU; Finally, with all three loss functions, the proposed CAR improves 2.18 mIOU from the regular ResNet-50 + Self-attention (48.32 vs 50.50).</p><p>CAR on Swin-Tiny + Uper. "Swin-Tiny + Uper" is a totally different architecture from "ResNet-50 + Self-Attention [26]". Swin <ref type="bibr" target="#b13">[14]</ref> is a recent Transformer-based backbone network. Uper [29] is based on the pyramid pooling modules (PPM) [36] and FPN <ref type="bibr" target="#b11">[12]</ref>, focusing on extracting multi-scale context information. Similarly, as shown in Tab. 1, after adding CAR, the performance of Swin-Tiny + Uper also increases by 1.16, which shows our CAR can generalize to different architectures well.</p><p>The Devil is In the Architecture's Detail. We find it important to replace the leading 3 ? 3 conv (in the original method) with 1 ? 1 conv <ref type="figure" target="#fig_3">(Fig. 3B</ref>). For example, L intra-c2p and L inter-c2p did not improve the performance in Swin-Tiny + Uper (Row S3 vs S1, and S5 vs S4 in Tab. 1). A possible reason is that the network is trained to maximize the separation between different classes. However, if the two pixels lie on different sides of the segmentation boundary, a 3 ? 3 conv will merge the pixel representations from different classes, making the proposed losses harder to optimize.</p><p>To keep simplicity and maximize generalization, we use the same network configurations in our all experiments. However, performance may be further improved with some minor dedicated modifications for each baseline when deploying our CAR. For example, decreasing the filter number to 256 for the last conv layer of ResNet-50 + Self-Attention + CAR results in a further improvement to 51.00 mIOU (from 50.50). Replacing the conv layer after PPM (inside Uper block, A3 in <ref type="figure" target="#fig_3">Fig. 3</ref>) from 3?3 to 1?1 in Swin-Tiny + UperNet boosts Swin (tiny &amp; large) + UperNet + CAR by an extra 0.5-1.0 mIOU. We intentionally did not exhaustively search these variants and not report these results in any table since they did not generalize.</p><p>CAR on Different Baselines. After we have verified the effectiveness of each part of the proposed CAR, we then tested CAR on multiple well-known baselines. All of the baselines were reproduced under similar conditions (see Sect. 4.1). Experimental results shown in Tab. 2 demonstrate the generalizability of our CAR on different backbones and methods.</p><p>Visualization of Class Dependency Maps. In <ref type="figure">Fig. 4</ref>, we present the class dependency maps calculated on the complete Pascal Context test set, where every pixel stores the dot-product similarities between every two class centers. The maps indicate the inter-class dependency obtained with the standard ResNet-50 + Self-Attention and Swin-Tiny + UperNet, and the effect of applying our CAR. <ref type="table">Table 2</ref>: Ablation studies of adding CAR to different baselines on Pascal Context <ref type="bibr">[19]</ref> and COCOStuff-10K <ref type="bibr" target="#b0">[1]</ref>. We deterministically reproduced all the baselines with the same settings. All results are single-scale without flipping. CAR works very well in most existing methods. ? means reducing the class-level threshold to 0.25 from 0.5. We found it is sensitive for some model variants to handle a large number of class. Affinity loss [30] and Auxiliary loss [36] are applied on CPNet and OCR, respectively, since they highly rely on those losses. A hotter color means that the class has higher dependency on the corresponding class, and vice versa. According to <ref type="figure" target="#fig_1">Fig. 4 a1-a2</ref>, we can easily observe that the inter-class dependency has been significantly reduced with CAR on ResNet50 + Self-Attention. <ref type="figure" target="#fig_1">Fig. 4 b1-b2</ref> show a similar trend when tested with different backbones and head blocks. This partially explains the reason why baselines with CAR generalize better on rarely seen class combinations ( <ref type="figure" target="#fig_1">Figs. 1 and 5</ref>). Interestingly, we find that the class-dependency issue is more serious in Swin-Tiny + Uper, but our CAR can still reduce its dependency level significantly.</p><p>Visualization of Pixel-relation Maps. In <ref type="figure">Fig. 5</ref>, we visualize the pixel-topixel relation energy map, based on the dot-product similarity between a red-dot a1. ResNet50 + Self-Attention a2. ResNet50 + Self-Attention + CAR b1. Swin-Tiny + UperNet b2. Swin-Tiny + UperNet + CAR a1. ResNet50 + Self-Attention a2. ResNet50 + Self-Attention + CAR b1. Swin-Tiny + UperNet b2. Swin-Tiny + UperNet + CAR <ref type="figure">Fig. 4</ref>: Class dependency maps generated on Pascal Context test set. One may zoom in to see class names. A hotter color means that the class has higher dependency to the corresponding class, and vice versa. It is obvious that our CAR reduces the inter-class dependency, thus providing better generalizability (see <ref type="figure" target="#fig_1">Figs. 1 and 5)</ref>.</p><p>marked pixel and other pixels, as well as the predicted results for different methods, for comparison. Examples are from Pascal Context test set. As we can see, with CAR supervision, the existing models focus better on objects themselves rather than other objects. Therefore, this reduces the possibility of the classification errors because of the class-dependency bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on COCOStuff-10K</head><p>COCOStuff-10K dataset <ref type="bibr" target="#b0">[1]</ref> is widely used for evaluating the robustness of semantic segmentation models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">31]</ref>. The COCOStuff-10k dataset is a very challenging dataset containing 171 labeled classes and 9000/1000 images for training/test. As shown in Tab. 2, all of the tested baselines gain performance boost ranging from 0.17% to 2.23% with our proposed CAR. This demonstrates the generalization ability of our CAR when handling a large number of classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, we have aimed to make a better use of class level context information. We have proposed a universal class-aware regularizations (CAR) approach to regularize the training process and boost the differentiability of the learned pixel representations. To this end, we have proposed to minimize the intra-class feature variance and maximize the inter-class separation simultaneously. Experiments conducted on benchmark datasets with extensive ablation studies have validated the effectiveness of the proposed CAR approach, which has boosted the existing models' performance by up to 2.18% mIOU on Pascal Context and 2.23% on COCOStuff-10k with no extra inference overhead.  In computer vision, we always use the same backbones and the same datasets when verifying the difference between two methods. Without using "deterministic" technology, all operations in neural networks contain some randomness. Nowadays, with the latest deterministic technology and fixed seeds, experiments can be conducted in a fully-controlled environment. This means that the performance difference between different settings (i.e., w/ and w/o CAR) is not affected by this randomness any more but faithfully reflects the effectiveness of different methods.</p><p>In Tab. 3, we report the performance of our proposed CAR (ResNet-50 + Self-attention and Swin-Tiny + UperNet) with different seeds for readers who are interested in how our CAR performs when trained with different random seeds. As it is shown, our CAR consistently improves the mIOU over its baseline using different random seeds, demonstrating the effectiveness of the CAR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Extra experiments</head><p>A.2.1 Ablation studies on batch class center In our experiments, we calculated the class centers using all the training images in a batch to alleviate the negative impact of noisy images. Here, we investigate the impact of using the class center of each individual image for class-aware regularizations.    . This figure shows that our CAR can further improve the robustness of class center based models by making better use of the class center. Interestingly, as shown in C12 of <ref type="figure">Fig. 6</ref> and <ref type="figure" target="#fig_1">Fig. 1</ref> shown in our main paper what is predicted by ResNet-50 + Self-Attention, we find cow/sheep/dog misclassification is a common issue in many semantic segmentation models, especially when i.e. grass and cow co-exist frequently during training. This issue is better addressed by our CAR due to its reduced inter-class dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Visualization of DeepLab in Pascal Context</head><p>We also visualize the pixel-to-pixel relation energy map of ResNet-50 [8] + DeepLabV3 <ref type="bibr" target="#b2">[3]</ref> in <ref type="figure">Fig. 7</ref>. These visualizations clearly show that the reduced inter-class dependency helps to correct the classification. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>CAR, easily affected by other classes (e..g. grass)Reduce Intra-class centerto-pixel distance Reduce Inter-class centerto-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>The concept of the proposed CAR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>The proposed CAR approach. CAR can be inserted into various segmentation models, right before the logit prediction module (A1-A4). CAR contains three regularization terms, including (C) intra-class center-to-center lossL intra-c2p (Sec. 3.2.2), (D) inter-class center-to-center loss L inter-c2c (Sec. 3.3.2), and (E) inter-class center-to-pixel loss L inter-c2p (Sec. 3.3.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 : 1 Deterministic</head><label>51</label><figDesc>Visualization of the feature similarity between a given pixel (marked with a red dot in the image) and all pixels, as well as the segmentation results on Pascal Context test set. A hotter color denotes larger similarity value. Apparently, our CAR reduces the inter-class dependency and exhibits better generalization ability, where energies are better restrained in the intra-class pixels.18. Mingxing, T., Quoc, L.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: International Conference on Machine Learning (2019) 19. Mottaghi, R., Chen, X., Liu, X., Cho, N.G., Lee, S.W., Fidler, S., Urtasun, R., Yuille, A.: The role of context for object detection and semantic segmentation in the wild. In: IEEE Conference on Computer Vision and Pattern Recognition (2014) 20. Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction.In: ICCV (2021) 21. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: International Conference on Learning Representations (2015) 22. Sixiao, Z., Jiachen, L., Hengshuang, Z., Xiatian, Z., Zekun, L., Yabiao, W., Yanwei, F., Jianfeng, F., Tao, X., H.S., T.P., Li, Z.: Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In: IEEE Conference on Computer Vision and Pattern Recognition(2021)23. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Lukasz Kaiser, Polosukhin, I.: Attention is all you need. In: Conference on Neural Information Processing Systems (2017) 24. Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y., Tan, M., Wang, X., Liu, W., Xiao, B.: Deep high-resolution representation learning for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2020) 25. Wang, W., Zhou, T., Yu, F., Dai, J., Konukoglu, E., Van Gool, L.: Exploring crossimage pixel contrast for semantic segmentation. In: ICCV. pp. 7303-7313 (2021) 26. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition (2018) 27. Wen1, Y., Zhang, K., Li, Z., Qiao, Y.: Discriminative feature learning approach for deep face recognition. In: European Conference on Computer Vision (2016) 28. Wu, H., Zhang, J., Huang, K., Liang, K., Yizhou, Y.: Fastfcn: Rethinking dilated convolution in the backbone for semantic segmentation (2019) 29. Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene understanding. In: European Conference on Computer Vision (2018) 30. Yu, C., Wang, J., Gao, C., Yu, G., Shen, C., Sang, N.: Context prior for scene segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition (2020) 31. Yuan, Y., Chen, X., Wang, J.: Object-contextual representations for semantic segmentation. In: European Conference on Computer Vision (2020) 32. Yuan, Y., Huang, L., Guo, J., Zhang, C., Chen, X., Wang, J.: Ocnet: Object context network for scene parsing. International Journal of Computer Vision (2021) 33. Zhang, F., Chen, Y., Li, Z., Hong, Z., Liu, J., Ma, F., Han, J., Ding, E.: Acfnet: Attentional class feature network for semantic segmentation. In: International Conference on Computer Vision (2019) 34. Zhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A., Agrawal, A.: Context encoding for semantic segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition (2018) 35. Zhang, H., Zhan, H., Wang, C., Xie, J.: Semantic correlation promoted shapevariant context for segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition (2019) 36. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In: IEEE Conference on Computer Vision and Pattern Recognition (2017) 37. Zhu, Z., Xu, M., Bai, S., Huang, T., Bai, X.: Asymmetric non-local neural networks for semantic segmentation. In: International Conference on Computer Vision (2019) Control variables are very important for all scientific research.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>3.4 Differences with OCR, ACFNet, CPNet, and CIPC</figDesc><table><row><cell>Methods that are closely related to ours are OCR [31], ACFNet [33] and CP-</cell></row><row><cell>Net [30]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablation studies of adding CAR to different methods on Pascal Context dataset. All results are obtained with single scale test without flip. "A" means replacing the 3 ? 3 conv with 1 ? 1 conv (detailed in Sec. 4.2.1). CAR improves the performance of different types of backbones (CNN &amp; Transformer) and head blocks (SA &amp; Uper), showing the generalizability of the proposed CAR.</figDesc><table><row><cell>Methods</cell><cell cols="3">Lintra-c2p Linter-c2c Linter-c2p</cell><cell>A</cell><cell>mIOU (%)</cell></row><row><cell>R1 ResNet-50 + Self-Attention [26]</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>48.32</cell></row><row><cell>R2</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>48.56</cell></row><row><cell>R3 + CAR</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>49.17</cell></row><row><cell>R4</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>49.79</cell></row><row><cell>R5</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>50.01</cell></row><row><cell>R6</cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>49.62</cell></row><row><cell>R7</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>50.00</cell></row><row><cell>R8</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>50.50</cell></row><row><cell>S1 Swin-Tiny + UperNet [29]</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell>49.62</cell></row><row><cell>S2</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>49.82</cell></row><row><cell>S3 + CAR</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>49.01</cell></row><row><cell>S4</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>50.63</cell></row><row><cell>S5</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>50.26</cell></row><row><cell>S6</cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>49.62</cell></row><row><cell>S7</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>50.58</cell></row><row><cell>S8</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>50.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Pascal Context [19] dataset is split into 4,998/5,105 for training/test set. We use its 59 semantic classes following the common practice [31,35]. Unless otherwise specified, both baselines and CAR are trained on the training set with 30k iterations. The ablation studies are presented in Sect. 4.2.1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation studies of our proposed CAR using different random seeds on the Pascal Context dataset.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">Seed (mIOU%)</cell></row><row><cell></cell><cell cols="2">0 (default) 1</cell><cell>2</cell></row><row><cell>ResNet-50 + Self-Attention</cell><cell>48.32</cell><cell>47.54</cell><cell>47.69</cell></row><row><cell cols="4">ResNet-50 + Self-Attention + CAR 50.50(+2.18) 50.20(+2.66) 50.59(+2.90)</cell></row><row><cell>Swin-Tiny + UperNet</cell><cell>49.62</cell><cell>49.24</cell><cell>49.54</cell></row><row><cell>Swin-Tiny + UperNet + CAR</cell><cell cols="3">50.78(+1.16) 50.57(+1.33) 50.75(+1.21)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison of mIOUs (%) obtained when using batch class center vs image class center in CAR. CAR using Moving Average. We also implemented a moving average version of CAR which tracks the class center ? with moving average similar</figDesc><table><row><cell>Methods</cell><cell>Baseline</cell><cell>CAR</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Image Class Center Batch Class Center</cell></row><row><cell>ResNet-50 + Self-Attention</cell><cell>48.32</cell><cell>49.78</cell><cell>50.50</cell></row><row><cell>Swin-Tiny + UperNet</cell><cell>49.62</cell><cell>49.45</cell><cell>50.78</cell></row><row><cell>A.2.2</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies of adding moving average to CAR on Pascal Context. Decay rate stands for the effect of old class center.</figDesc><table><row><cell>Methods</cell><cell>CAR</cell><cell></cell><cell>CAR (Moving Average)</cell></row><row><cell></cell><cell></cell><cell>0.8</cell><cell>0.9</cell><cell>0.99</cell></row><row><cell>ResNet-50 + Self-Attention</cell><cell>50.50</cell><cell cols="2">49.80(?0.70) 50.26(?0.24) 49.96(?0.54)</cell></row><row><cell>Swin-Tiny + UperNet</cell><cell>50.78</cell><cell cols="2">49.56(?1.22) 50.03(?0.75) 48.93(?1.85)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Extra ablation studies for CAR without intra-class loss.Table 6below shows that, including only inter-c2c loss and inter-c2p still improves the result.A.2.4 Exceeding state-of-the-art (SOTA) in Pascal ContextThe main motivation of our CAR is to utilize class-level information as regularizations during training to boost the performance of all existing methods. However, following the convention and also for readers who are interested, we compare with stateof-the-art methods in Tab. 7 regardless their architectures are related to ours or not. Since Swin<ref type="bibr" target="#b13">[14]</ref> is not compatible with dilation, we use JPU [28] as the substitution to obtain features with output stride = 8. Uper contains an FPN<ref type="bibr" target="#b11">[12]</ref> module that can obtain features with output stride = 4.Boosted by our CAR, the strong modelConvNext-Large [15] + CAA [9] achieved the performance of 62.70% mIOU under single-scale testing, and 63.91% under multi-scale testing. Also, we found increasing the training iterations from the default 30K to 40K when using Adam optimizer can further increase performance in Pascal Context dataset. Thus, the SOTA single model performance has now been boosted to 62.97% under single-scale testing, and 64.12% under multi-scale testing. This has outperformed the previous SOTA single model, i.e., EfficientNetB7 + CAA, by a large margin. A.2.5 Exceeding SOTA performance in COCOStuff-10K Simliar to Sec. A.2.4, in Tab. 8, boosted by our CAR, the strong model ConvNext-Large [15] + CAA achieved the performance of 49.03% mIOU under single-scale testing, and 50.01% under multi-scale testing. This has also outperformed the previous SOTA single model, i.e., EfficientNetB7 + CAA, by a large margin.</figDesc><table><row><cell></cell><cell cols="4">Lintra-c2p Linter-c2c Linter-c2p A mIOU</cell></row><row><cell>ResNet-50 + Self-Attention</cell><cell>-</cell><cell>-</cell><cell></cell><cell>48.32</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>? 48.56</cell></row><row><cell>+ CAR</cell><cell></cell><cell>?</cell><cell>?</cell><cell>49.31</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>? 50.23</cell></row><row><cell cols="5">to BatchNorm. As shown in Tab. 5, we find this moving average version of CAR</cell></row><row><cell cols="5">negatively impacts both ResNet-50 + Self-Attention and Swin-Tiny + Uper.</cell></row></table><note>A.2.3 CAR without intra-class loss.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Experiments on boosting the SOTA single-model performance on Pascal Context by our CAR. See Sec. A.2.4 for the details. ?: We report previous SOTA score as reference. SS : mIOU on Single scale without flipping. MF : Multi-scale with flipping. JPU is used to get features with output stride = 8. Aux : Apply auxiliary loss during training, see [36]. Iters: training iterations. WP :Linear learning rate warmup Visualization of OCRNet in Pascal Context Similar to the main paper, in Fig. 6, we visualize the pixel-to-pixel relation energy maps obtained with HRNetW48 [31] + OCR [31]</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell cols="5">Aux Optimizer Iters WP SS(%) MF(%)</cell></row><row><cell>CAA ?</cell><cell>EfficientNet-B7-D8</cell><cell>?</cell><cell>SGD</cell><cell>30K</cell><cell>-</cell><cell>60.30</cell></row><row><cell>UperNet</cell><cell>Swin-Large</cell><cell></cell><cell>SGD</cell><cell>30K</cell><cell cols="2">57.48 59.45</cell></row><row><cell cols="2">UperNet + CAR Swin-Large</cell><cell></cell><cell>SGD</cell><cell>30K</cell><cell cols="2">58.97 60.76</cell></row><row><cell>CAA</cell><cell>Swin-Large + JPU</cell><cell></cell><cell>SGD</cell><cell>30K</cell><cell cols="2">58.31 59.75</cell></row><row><cell>CAA + CAR</cell><cell>Swin-Large + JPU</cell><cell></cell><cell>SGD</cell><cell>30K</cell><cell cols="2">59.84 61.46</cell></row><row><cell>CAA + CAR</cell><cell>Swin-Large + JPU</cell><cell></cell><cell cols="2">Adam 30K</cell><cell cols="2">60.68 62.21</cell></row><row><cell>CAA</cell><cell>ConvNeXt-Large + JPU</cell><cell></cell><cell>SGD</cell><cell>30K</cell><cell cols="2">60.48 61.80</cell></row><row><cell>CAA + CAR</cell><cell>ConvNeXt-Large + JPU</cell><cell></cell><cell>SGD</cell><cell>30K</cell><cell cols="2">61.40 62.69</cell></row><row><cell>CAA + CAR</cell><cell>ConvNeXt-Large + JPU</cell><cell></cell><cell cols="2">Adam 30K</cell><cell cols="2">62.65 63.77</cell></row><row><cell>CAA + CAR</cell><cell cols="2">ConvNeXt-Large + JPU ?</cell><cell cols="2">Adam 30K</cell><cell cols="2">62.70 63.91</cell></row><row><cell>CAA + CAR</cell><cell cols="2">ConvNeXt-Large + JPU ?</cell><cell cols="2">Adam 40K</cell><cell cols="2">62.97 64.12</cell></row><row><cell>CAA + CAR</cell><cell cols="2">ConvNeXt-Large + JPU ?</cell><cell cols="4">Adam 40K ? 63.13 64.17</cell></row><row><cell cols="2">A.3 Extra Visualizations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A.3.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Experiments on boosting SOTA on COCOStuff10k, levering the previous single model SOTA and boosted by our CAR. See Sec. A.2.5 for details. ?: We report the original SOTA scores. SS : Single scale without flipping. MF : Multi-scale with flipping. Aux Apply auxiliary loss during training, see [36]. Visualization of the feature similarity between a given pixel (marked with a red dot in the image) and all other pixels, as well as the segmentation results of HRNetW48 [24] + OCR [31] on Pascal Context test set. A hotter color denotes a greater similarity value. Visualization of the feature similarity between a given pixel (marked with a red dot in the image) and all pixels, as well as the segmentation results of ResNet-50 [8] + DeepLab [3] on Pascal Context test set. A hotter color denotes a greater similarity value.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell cols="4">Aux Optimizer SS mIOU(%) MF mIOU(%)</cell></row><row><cell>CAA ?</cell><cell>EfficientNet-B7-D8</cell><cell>?</cell><cell>SGD</cell><cell>-</cell><cell>45.40</cell></row><row><cell>UperNet</cell><cell>Swin-Large</cell><cell></cell><cell>SGD</cell><cell>44.25</cell><cell>46.10</cell></row><row><cell cols="2">UperNet + CAR Swin-Large</cell><cell></cell><cell>SGD</cell><cell>44.88</cell><cell>46.64</cell></row><row><cell>CAA</cell><cell>Swin-Large + JPU</cell><cell></cell><cell>SGD</cell><cell>44.22</cell><cell>45.31</cell></row><row><cell>CAA + CAR</cell><cell>Swin-Large + JPU</cell><cell></cell><cell>SGD</cell><cell>45.48</cell><cell>46.99</cell></row><row><cell>CAA</cell><cell>ConvNeXt-Large + JPU</cell><cell></cell><cell>SGD</cell><cell>46.49</cell><cell>47.23</cell></row><row><cell>CAA + CAR</cell><cell>ConvNeXt-Large + JPU</cell><cell></cell><cell>SGD</cell><cell>46.70</cell><cell>47.77</cell></row><row><cell>CAA + CAR</cell><cell>ConvNeXt-Large + JPU</cell><cell></cell><cell>Adam</cell><cell>48.20</cell><cell>48.83</cell></row><row><cell>CAA + CAR</cell><cell cols="2">ConvNeXt-Large + JPU ?</cell><cell>Adam</cell><cell>49.03</cell><cell>50.01</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">H, W and C denote images' height and width, and number of channels, respectively.<ref type="bibr" target="#b5">6</ref> It is termed as class center in [33] and object region representations in [31].<ref type="bibr" target="#b6">7</ref> We use ? and omit the subscript batch for clarity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This research depends on the NVIDIA determinism framework. We appreciate the support from @duncanriach and @reedwm at NVIDIA and TensorFlow team.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">COCO-Stuff: Thing and Stuff Classes in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cars can&apos;t fly up in the sky: Improving urbanscene segmentation via height-driven attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Channelized axial attention -considering channel relation within spatial attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Expectation-maximization attention networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploit visual dependency relations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schonfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Uwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
