<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Fang</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Kuang</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yi</forename><surname>Lee</surname></persName>
						</author>
						<title level="a" type="main">DFAC Framework: Factorizing the Value Function via Quantile Mixture for Multi-Agent Distributional Q-Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In fully cooperative multi-agent reinforcement learning (MARL) settings, the environments are highly stochastic due to the partial observability of each agent and the continuously changing policies of the other agents. To address the above issues, we integrate distributional RL and value function factorization methods by proposing a Distributional Value Function Factorization (DFAC) framework to generalize expected value function factorization methods to their DFAC variants. DFAC extends the individual utility functions from deterministic variables to random variables, and models the quantile function of the total return as a quantile mixture. To validate DFAC, we demonstrate DFAC's ability to factorize a simple two-step matrix game with stochastic rewards and perform experiments on all Super Hard tasks of StarCraft Multi-Agent Challenge, showing that DFAC is able to outperform expected value function factorization baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In fully cooperative multi-agent reinforcement learning (MARL) settings, the environments are highly stochastic due to the partial observability of each agent and the continuously changing policies of the other agents. To address the above issues, we integrate distributional RL and value function factorization methods by proposing a Distributional Value Function Factorization (DFAC) framework to generalize expected value function factorization methods to their DFAC variants. DFAC extends the individual utility functions from deterministic variables to random variables, and models the quantile function of the total return as a quantile mixture. To validate DFAC, we demonstrate DFAC's ability to factorize a simple two-step matrix game with stochastic rewards and perform experiments on all Super Hard tasks of StarCraft Multi-Agent Challenge, showing that DFAC is able to outperform expected value function factorization baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In multi-agent reinforcement learning (MARL), one of the popular research directions is to enhance the training procedure of fully cooperative and decentralized agents. Examples of such agents include a fleet of unmanned aerial vehicles (UAVs), a group of autonomous cars, etc. This research direction aims to develop a decentralized and cooperative behavior policy for each agent, and is especially difficult for MARL settings without an explicit communication channel. The most straightforward approach is independent Q-learning (IQL) <ref type="bibr" target="#b24">(Tan, 1993)</ref>, where each agent is trained independently, with their behavior policies aimed to optimize the overall rewards in each episode. Nevertheless, each agent's policy may not converge owing to two main difficulties: <ref type="bibr" target="#b30">(1)</ref> non-stationary environments caused by the changing behaviors of the agents, and (2) spurious reward signals originated from the actions of the other agents. The agent's partial observability of the environment further exacerbates the above issues. Therefore, in the past few years, a number of MARL researchers turned their attention to centralized training with decentralized execution (CTDE) approaches, with an objective to stabilize the training procedure while maintaining the agents' abilities for decentralized execution . Among these CTDE approaches, value function factorization methods <ref type="bibr" target="#b23">(Sunehag et al., 2018;</ref><ref type="bibr" target="#b18">Rashid et al., 2018;</ref><ref type="bibr" target="#b22">Son et al., 2019)</ref> are especially promising in terms of their superior performances and data efficiency <ref type="bibr" target="#b20">(Samvelyan et al., 2019)</ref>.</p><p>Value function factorization methods introduce the assumption of individual-global-max (IGM) <ref type="bibr" target="#b22">(Son et al., 2019)</ref>, which assumes that each agent's optimal actions result in the optimal joint actions of the entire group. Based on IGM, the total return of a group of agents can be factorized into separate utility functions <ref type="bibr" target="#b7">(Guestrin et al., 2001</ref>) (or simply 'utility' hereafter) for each agent. The utilities allow the agents to independently derive their own optimal actions during execution, and deliver promising performance in Star-Craft Multi-Agent Challenge (SMAC) <ref type="bibr" target="#b20">(Samvelyan et al., 2019)</ref>. Unfortunately, current value function factorization methods only concentrate on estimating the expectations of the utilities, overlooking the additional information contained in the full return distributions. Such information, nevertheless, has been demonstrated beneficial for policy learning in the recent literature <ref type="bibr" target="#b10">(Lyle et al., 2019)</ref>.</p><p>In the past few years, distributional RL has been empirically shown to enhance value function estimation in various single-agent RL (SARL) domains <ref type="bibr">(Bellemare et al., 2017;</ref><ref type="bibr" target="#b4">Dabney et al., 2018b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b19">Rowland et al., 2019;</ref><ref type="bibr" target="#b27">Yang et al., 2019)</ref>. Instead of estimating a single scalar Q-value, it approximates the probability distribution of the return by either a categorical distribution <ref type="bibr">(Bellemare et al., 2017)</ref> or a quantile function <ref type="bibr" target="#b4">(Dabney et al., 2018b;</ref><ref type="bibr">a)</ref>. Even though the above methods may be beneficial to the MARL domain due to the ability to capture uncertainty, it is inherently incompat-arXiv:2102.07936v2 [cs.MA] 22 Dec 2021 ible to expected value function factorization methods (e.g., value decomposition network (VDN) <ref type="bibr" target="#b23">(Sunehag et al., 2018)</ref> and QMIX <ref type="bibr" target="#b18">(Rashid et al., 2018)</ref>). The incompatibility arises from two aspects: <ref type="bibr" target="#b30">(1)</ref> maintaining IGM in a distributional form, and (2) factorizing the probability distribution of the total return into individual utilities. As a result, an effective and efficient approach that is able to solve the incompatibility is crucial and necessary for bridging the gap between value function factorization methods and distributional RL.</p><p>In this paper, we propose a Distributional Value Function Factorization (DFAC) framework, to efficiently integrate value function factorization methods with distributional RL. DFAC solves the incompatibility by two techniques: (1) Mean-Shape Decomposition and (2) Quantile Mixture. The former allows the generalization of expected value function factorization methods (e.g., VDN and QMIX) to their DFAC variants without violating IGM. The latter allows the total return distribution to be factorized into individual utility distributions in a computationally efficient manner. To validate the effectiveness of DFAC, we first demonstrate the ability of distribution factorization on a two-step matrix game with stochastic rewards. Then, we perform experiments on all Super Hard maps in SMAC. The experimental results show that DFAC offers beneficial impacts on the baseline methods in all Super Hard maps. In summary, the primary contribution is the introduction of DFAC for bridging the gap between distributional RL and value function factorization methods efficiently by mean-shape decomposition and quantile mixture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Works</head><p>In this section, we introduce the essential background material for understanding the contents of this paper. We first define the problem formulation of cooperative MARL and CTDE. Next, we describe the conventional formulation of IGM and the value function factorization methods. Then, we walk through the concepts of distributional RL, quantile function, as well as quantile regression, which are the fundamental concepts frequently mentioned in this paper. After that, we explain the implicit quantile network, a key approach adopted in this paper for approximating quantiles.</p><p>Finally, we bring out the concept of quantile mixture, which is leveraged by DFAC for factorizing the return distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Cooperative MARL and CTDE</head><p>In this work, we consider a fully cooperative MARL environment modeled as a decentralized and partially observable Markov Decision Process (Dec-POMDP)  with stocastic rewards, which is described as a tuple S, K, O jt , U jt , P , O, R, ? and is defined as follows:</p><p>? S is the finite set of global states in the environment, where s ? S denotes the next state of the current state s ? S. The state information is optionally available during training, but not available to the agents during execution. ? K = {1, ..., K} is the set of K agents. We use k ? K to denote the index of the agent. </p><formula xml:id="formula_0">? O jt = ? k?K O k is</formula><formula xml:id="formula_1">, where u = u 1 , ..., u K ? U jt . The individual action u k ? U k of each agent k is determined based on its stochastic policy ? k (u k |h k ) : H k ? U k ? [0, 1], expressed as u k ? ? k (?|h k ).</formula><p>Similarly, in single agent scenarios, we use u and u to denote the actions of the agent at state s and s under policy ?, respectively. ? T = {1, ..., T} represents the set of timesteps with horizon T, where the index of the current timestep is denoted as t ? T. s t , o t , h t , and u t correspond to the environment information at timestep t. Under such an MARL formulation, this work concentrates on CTDE value function factorization methods, where the agents are trained in a centralized fashion and executed in a decentralized manner. In other words, the joint observation history h is available during the learning processes of individual policies [? k ] k?K . During execution, each agent's policy ? k only conditions on its observation history h k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">IGM and Factorizable Tasks</head><p>IGM is necessary for value function factorization <ref type="bibr" target="#b22">(Son et al., 2019)</ref>. For a joint action-value function Q jt (h, u) :</p><formula xml:id="formula_2">H jt ? U jt ? R, if there exist K individual utility functions [Q k (h k , u k ) : H k ? U k ? R] k?K such that the following condition holds: arg max u Q jt (h, u) = ? ? ? arg max u1 Q 1 (h 1 , u 1 ) . . . arg max uK Q K (h K , u K ) ? ? ? ,<label>(1)</label></formula><p>then [Q k ] k?K are said to satisfy IGM for Q jt under h.</p><p>In this case, we also say that Q jt (h, u) is factorized by <ref type="bibr" target="#b22">(Son et al., 2019)</ref>. If Q jt in a given task is factorizable under all h ? H jt , we say that the task is factorizable. Intuitively, factorizable tasks indicate that there exists a factorization such that each agent can select the greedy action according to their individual utilities [Q k ] k?K independently in a decentralized fashion. This enables the optimal individual actions to implicitly achieve the optimal joint action across the K agents. Since there is no individual reward, the factorized utilities do not estimate expected returns on their own <ref type="bibr" target="#b7">(Guestrin et al., 2001)</ref> and are different from the value function definition commonly used in SARL.</p><formula xml:id="formula_3">[Q k (h k , u k )] k?K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Value Function Factorization Methods</head><p>Based on IGM, value function factorization methods enable centralized training for factorizable tasks, while maintaining the ability for decentralized execution. In this work, we consider two such methods, VDN and QMIX, which can solve a subset of factorizable tasks that satisfies Additivity (Eq. (2)) and Monotonicity (Eq. (3)), respectively, given by:</p><formula xml:id="formula_4">Q jt (h, u) = K k=1 Q k (h k , u k ),<label>(2)</label></formula><p>Q jt (h, u) = M (Q 1 (h 1 , u 1 ), ..., Q K (h K , u K )|s), (3) where M is a monotonic function that satisfies ?M ?Q k ? 0, ?k ? K, and conditions on the state s if the information is available during training. Either of these two equation is a sufficient condition for IGM <ref type="bibr" target="#b22">(Son et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Distributional RL</head><p>For notational simplicity, we consider a degenerated case with only a single agent, and the environment is fully observable until the end of Section 2.6. Distributional RL generalizes classic expected RL methods by capturing the full return distribution Z(s, u) instead of the expected return Q(s, u), and outperforms expected RL methods in various single-agent RL domains <ref type="bibr">(Bellemare et al., 2017;</ref><ref type="bibr" target="#b4">Dabney et al., 2018b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b19">Rowland et al., 2019;</ref><ref type="bibr" target="#b27">Yang et al., 2019)</ref>. Moreover, distributional RL enables improvements <ref type="bibr" target="#b15">(Nikolov et al., 2019;</ref><ref type="bibr" target="#b28">Zhang &amp; Yao, 2019;</ref><ref type="bibr">Mavrin et al., 2019)</ref> that require the information of the full return distribution. We define the distributional Bellman operator T ? as follows:</p><formula xml:id="formula_5">T ? Z(s, u) D := R(s, u) + ?Z(s , u ),<label>(4)</label></formula><p>and the distributional Bellman optimality operator T * as:</p><formula xml:id="formula_6">T * Z(s, u) D := R(s, u) + ?Z(s , u * ),<label>(5)</label></formula><p>where u * = arg max u E[Z(s , u )] is the optimal action at state s , and the expression X D = Y denotes that random variable X and Y follow the same distribution. Given some initial distribution Z 0 , Z converges to the return distribution Z ? under ?, contracting in terms of p-Wasserstein distance for all p ? [1, ?) by applying T ? repeatedly; while Z alternates between the optimal return distributions in the set Z * := {Z ? * : ? * ? ? * }, under the set of optimal policies ? * by repeatedly applying T * <ref type="bibr">(Bellemare et al., 2017)</ref>. The p-Wasserstein distance W p between the probability distributions of random variables X, Y is given by:</p><formula xml:id="formula_7">W p (X, Y ) = 1 0 |F ?1 X (?) ? F ?1 Y (?)| p d? 1/p ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">(F ?1 X , F ?1 Y ) are quantile functions of (X, Y ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Quantile Function and Quantile Regression</head><p>The relationship between the cumulative distribution function (CDF) F X and the quantile function F ?1 X (the generalized inverse CDF) of random variable X is formulated as:</p><formula xml:id="formula_9">F ?1 X (?) = inf{x ? R : ? ? F X (x)}, ?? ? [0, 1]. (7)</formula><p>The expectation of X expressed in terms of F ?1 X (?) is:</p><formula xml:id="formula_10">E[X] = 1 0 F ?1 X (?) d?.<label>(8)</label></formula><p>In <ref type="bibr" target="#b4">(Dabney et al., 2018b)</ref>, the authors model the value function as a quantile function F ?1 (s, u|?). During optimization, a pair-wise sampled temporal difference (TD) error ? for two quantile samples ?, ? ? U ([0, 1]) is defined as:</p><formula xml:id="formula_11">? ?,? t = r + ?F ?1 (s , u |? ) ? F ?1 (s, u|?). (9)</formula><p>The pair-wise loss ? ? ? is then defined based on the Huber quantile regression loss L ? <ref type="bibr" target="#b4">(Dabney et al., 2018b)</ref> with threshold ? = 1, and is formulated as follows:</p><formula xml:id="formula_12">? ? ? (? ?,? ) = |? ? I{? ?,? &lt; 0}| L ? (? ?,? ) ? , with (10) L ? (? ?,? ) = 1 2 (? ?,? ) 2 , if |? ?,? | ? ? ?(|? ?,? | ? 1 2 ?), otherwise . (11) Given N quantile samples [? i ] N i=1</formula><p>to be optimized with regard to N target quantile samples [? j ] N j=1 , the total loss L(s, u, r, s ) is defined as the sum of the pair-wise losses, and is expressed as the following:</p><formula xml:id="formula_13">L(s, u, r, s ) = 1 N N i=1 N j=1 ? ? ?i (? ?i,? j ).<label>(12)</label></formula><p>2.6. Implicit Quantile Network Implicit quantile network (IQN) <ref type="bibr" target="#b3">(Dabney et al., 2018a)</ref> is relatively light-weight when it is compared to other distributional RL methods. It approximates the return distribution Z(s, u) by an implicit quantile function F ?1 (s, u|?) = g(?(s), ?(?)) u for some differentiable functions g, ?, and ?. Such an architecture is a type of universal value function approximator (UVFA) <ref type="bibr" target="#b21">(Schaul et al., 2015)</ref>, which generalizes its predictions across states s ? S and goals ? ? [0, 1], with the goals defined as different quantiles of the return distribution. In practice, ? first expands the scalar ? to an n-dimensional vector by [cos(?i?)] n?1 i=0 , followed by a single hidden layer with weights [w ij ] and biases</p><formula xml:id="formula_14">[b j ] to pro- duce a quantile embedding ?(?) = [?(?) j ] dim(?(?))?1 j=0</formula><p>. The expression of ?(?) j can be represented as the following:</p><formula xml:id="formula_15">?(?) j := ReLU( n?1 i=0 cos(?i?)w ij + b j ),<label>(13)</label></formula><p>where n = 64 and dim(?(?)) = dim(?(s)). Then, ?(?) is combined with the state embedding ?(s) by the elementwise (Hadamard) product ( ), expressed as g := ? ?. The loss of IQN is defined as Eq. <ref type="formula" target="#formula_2">(12)</ref>  </p><formula xml:id="formula_16">Q(s, u) = 1 0 F ?1 (s, u|?) d? ? 1 NN i=1 F ?1 (s, u|? i ).<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Quantile Mixture</head><p>Multiple quantile functions (e.g., IQNs) sharing the same quantile ? may be combined into a single quantile function F ?1 (?), in a form of quantile mixture expressed as follows:</p><formula xml:id="formula_17">F ?1 (?) = K k=1 ? k F ?1 k (?),<label>(15)</label></formula><p>where [F ?1 k (?)] k?K are quantile functions, and [? k ] k?K are model parameters <ref type="bibr" target="#b8">(Karvanen, 2006)</ref>. The condition for [? k ] k?K is that F ?1 (?) must satisfy the properties of a quantile function. The concept of quantile mixture is analogous to the mixture of multiple probability density functions (PDFs), expressed as follows:</p><formula xml:id="formula_18">f (x) = K k=1 ? k f k (x),<label>(16)</label></formula><p>where [f k (x)] k?K are PDFs, K k=1 ? k = 1, and ? k ? 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we walk through the proposed DFAC framework and its derivation procedure. We first discuss a naive distributional factorization and its limitation in Section 3.1. Then, we introduce the DFAC framework to address the limitation, and show that DFAC is able to generalize distributional RL to all factorizable tasks in Section 3.2. After that, DDN and DMIX are introduced as the DFAC variants of VDN and QMIX, respectively, in Section 3.4. Finally, a practical implementation of DFAC based on quantile mixture is presented in Section 3.3. All proofs of the theorems in this section are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Distributional IGM</head><p>Since IGM is necessary for value function factorization, a distributional factorization that satisfies IGM is required for factorizing stochastic value functions. We first discuss a naive distributional factorization that simply replaces deterministic utilities Q with stochastic utilities Z. Then, we provide a theorem to show that the naive distributional factorization is insufficient to guarantee the IGM condition.</p><formula xml:id="formula_19">Definition 1 (Distributional IGM). A finite number of indi- vidual stochastic utilities [Z k (h k , u k )] k?K , are said to sat- isfy Distributional IGM (DIGM) for a stochastic joint action- value function Z jt (h, u) under h, if [E[Z k (h k , u k )]] k?K sat- isfy IGM for E[Z jt (h, u)] under h, represented as: arg max u E[Z jt (h, u)] = ? ? ? arg max u1 E[Z 1 (h 1 , u 1 )] . . . arg max uK E[Z K (h K , u K )] ? ? ? .</formula><p>Theorem 1. Given a deterministic joint action-value function Q jt , a stochastic joint action-value function Z jt , and a factorization function ? for deterministic utilities:</p><formula xml:id="formula_20">Q jt (h, u) = ?(Q 1 (h 1 , u 1 ), ..., Q K (h K , u K )|s), such that [Q k ] k?K satisfy IGM for Q jt under h, the follow- ing distributional factorization: Z jt (h, u) = ?(Z 1 (h 1 , u 1 ), ..., Z K (h K , u K )|s). is insufficient to guarantee that [Z k ] k?K satisfy DIGM for Z jt under h.</formula><p>In order to satisfy DIGM for stochastic utilities, an alternative factorization strategy is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Proposed DFAC Framework</head><p>We propose Mean-Shape Decomposition and the DFAC framework to ensure that DIGM is satisfied for stochastic utilities.</p><p>Definition 2 (Mean-Shape Decomposition). A given ran-dom variable Z can be decomposed as follows:</p><formula xml:id="formula_21">Z = E[Z] + (Z ? E[Z]) = Z mean + Z shape ,</formula><p>where Var(Z mean ) = 0 and E[Z shape ] = 0.</p><p>We propose DFAC to decompose a joint return distribution Z jt into its deterministic part Z mean (i.e., expected value) and stochastic part Z shape (i.e., higher moments), which are approximated by two different functions ? and ?, respectively. The factorization function ? is required to precisely factorize the expectation of Z jt in order to satisfy DIGM. On the other hand, the shape function ? is allowed to roughly factorize the shape of Z jt , since the main objective of modeling the return distribution is to assist non-linear approximation of the expectation of Z jt <ref type="bibr" target="#b10">(Lyle et al., 2019)</ref>, rather than accurately model the shape of Z jt .</p><p>Theorem 2 (DFAC Theorem). Given a deterministic joint action-value function Q jt , a stochastic joint action-value function Z jt , and a factorization function ? for deterministic utilities:</p><formula xml:id="formula_22">Q jt (h, u) = ?(Q 1 (h 1 , u 1 ), ..., Q K (h K , u K )|s)</formula><p>, such that [Q k ] k?K satisfy IGM for Q jt under h, by Mean-Shape Decomposition, the following distributional factorization:</p><formula xml:id="formula_23">Z jt (h, u) = E[Z jt (h, u)] + (Z jt (h, u) ? E[Z jt (h, u)]) = Z mean (h, u) + Z shape (h, u) = ?(Q 1 (h 1 , u 1 ), ..., Q K (h K , u K )|s) + ?(Z 1 (h 1 , u 1 ), ..., Z K (h K , u K )|s).</formula><p>is sufficient to guarantee that [Z k ] k?K satisfy DIGM for Z jt under h, where Var(?) = 0 and E[?] = 0.</p><p>Theorem. 2 reveals that the choice of ? determines whether IGM holds, regardless of the choice of ?, as long as E[?] = 0. Under this setting, any differentiable factorization function of deterministic variables can be extended to a factorization function of random variables. Such a decomposition enables approximation of joint distributions for all factorizable tasks under appropriate choices of ? and ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">A Practical Implementation of DFAC</head><p>In this section, we provide a practical implementation of the shape function ? in DFAC, effectively extending any differentiable factorization function ? (e.g., the additive function of VDN, the monotonic mixing network of QMIX, etc.) that satisfies the IGM condition into its DFAC variant.</p><p>Theoretically, the sum of random variables appeared in DDN and DMIX can be described precisely by a joint CDF. However, the exact derivation of this joint CDF is usually computationally expensive and impractical . As a result, DFAC utilizes the property of quantile mixture to approximate the shape function ? in O(KN) time.</p><p>Theorem 3. Given a quantile mixture:</p><formula xml:id="formula_24">F ?1 (?) = K k=1 ? k F ?1 k (?) with K components [F ?1 k ] k?K and non-negative model pa- rameters [? k ] k?K .</formula><p>There exist a set of random variables Z and [Z k ] k?K corresponding to the quantile functions F ?1 and [F ?1 k ] k?K , respectively, with the following relationship:</p><formula xml:id="formula_25">Z = k?K ? k Z k .</formula><p>Based on Theorem 3, the quantile function F ?1 shape of Z shape in DFAC can be approximated by the following:</p><formula xml:id="formula_26">F ?1 shape (h, u|?) = F ?1 state (s|?) + k?K ? k (s)(F ?1 k (h k , u k |?) ? Q k (h k , u k )),<label>(17)</label></formula><p>where F ?1 state (s|?) and [? k (s)] k?K are respectively generated by function approximators ? state (s|?) and [? k (s)] k?K , satisfying constraints ? k (s) ? 0, ?k? K and 1 0 F ?1 state (s|?) d? = 0. The term F ?1 state models the shape of an additional state-dependent utility (introduced by QMIX at the last layer of the mixing network), which extends the state-dependent bias in QMIX to a full distribution. The full network architecture of DFAC is illustrated in <ref type="figure">Fig. 1</ref>.</p><p>This transformation enables DFAC to decompose the quantile representation of a joint distribution into the quantile representations of individual utilities. In this work, ? is implemented by a large IQN composed of multiple IQNs, optimized through the loss function defined in Eq. (12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">DFAC Variant of VDN and QMIX</head><p>In order to validate the proposed DFAC framework, we next discuss the DFAC variants of two representative factorization methods: VDN and QMIX. DDN extends VDN to its DFAC variant, expressed as:</p><formula xml:id="formula_27">Z jt = k?K Q k + k?K (Z k ? Q k ), given<label>(18)</label></formula><formula xml:id="formula_28">Z mean = k?K Q k , Z shape = k?K (Z k ? Q k )</formula><p>; while DMIX extends QMIX to its DFAC variant, expressed as:  <ref type="figure">Figure 1</ref>: The DFAC framework consists of a factorization network ? and a shape network ? for decomposing the deterministic part Z mean (i.e., Q jt ) and the stochastic part Z shape of the total return distribution Z jt , as described in Theorem 2. The shape network contains parameter networks ? state (s; ?) and [? k (s)] k?K for generating Z state (s) and ? k (s).</p><formula xml:id="formula_29">Z jt = M (Q 1 , ..., Q K |s) + k?K (Z k ? Q k ), given (19) Z mean = M (Q 1 , ..., Q K |s), Z shape = k?K (Z k ? Q k ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A Stochastic Two-Step Game</head><p>In the previous expected value function factorization methods (e.g., VDN, QMIX, etc.), the factorization is achieved by modeling Q jt and [Q k ] k?K as deterministic variables, overlooking the information of higher moments in the full return distributions Z jt and [Z k ] k?K . In order to demonstrate DFAC's ability of factorization, we begin with a toy example modified from <ref type="bibr" target="#b18">(Rashid et al., 2018)</ref> to show that DFAC is able to approximate the true return distributions, and factorize the mean and variance of the approximated total return Z jt into utilities [Z k ] k?K . <ref type="table" target="#tab_2">Table 1</ref> illustrates the flow of a two-step game consisting of two agents and three states 1, 2A, and 2B, where State 1 serves as the initial state, and each agent is able to perform an action from {A, B} in each step. In the first step (i.e., State 1), the action of agent 1 (i.e., actions A 1 or B 1 ) determines which of the two matrix games (State 2A or State 2B) to play in the next step, regardless of the action performed by agent 2 (i.e., actions A 2 or B 2 ). For all joint actions performed in the first step, no reward is provided to the agents. In the second step, both agents choose an action and receive a global reward according to the payoff matrices depicted in <ref type="table" target="#tab_2">Table 1</ref>, where the global rewards are sampled from a normal distribution N (?, ? 2 ) with mean ? and standard deviation ?. The hyperparameters of the two-step game are offered in the supplementary material in detail. <ref type="table" target="#tab_3">Table 2</ref> presents the learned factorization of DMIX for each state after convergence, where the first rows and the first columns of the tables correspond to the factorized distributions of the individual utilities (i.e., Z 1 and Z 2 ), and the main content cells of them correspond to the joint return distributions (i.e., Z jt ). From Tables 2(b) and 2(c), it is observed that no matter the true returns are deterministic (i.e., State 2A) or stochastic (i.e., State 2B), DMIX is able to approximate the true returns in <ref type="table" target="#tab_2">Table 1</ref> properly, which are not achievable by expected value function factorization methods. The results demonstrate DFAC's ability to factorize the joint return distribution rather than expected returns. DMIX's ability to reconstruct the optimal joint policy in the two-step game further shows that DMIX can represent the same set of tasks as QMIX.</p><p>To further illustrate DFAC's capability of factorization, Figs. 2(a)-2(b) visualize the factorization of the joint action B 1 , B 2 in <ref type="figure" target="#fig_2">State 2A and B 1 , B 2</ref> in State 2B, respectively. As IQN approximates the utilities Z 1 and Z 2 implicitly, Z 1 , Z 2 , and Z jt can only be plotted in terms of samples. Z jt in <ref type="figure" target="#fig_2">Fig. 2(a)</ref> shows that DMIX degenerates to QMIX when approximating deterministic returns (i.e., N (7, 0)), while Z jt in <ref type="figure" target="#fig_2">Fig. 2(b)</ref> exhibits DMIX's ability to capture the uncertainty in stochastic returns (i.e. <ref type="figure" target="#fig_2">, N (8, 29)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment Results on SMAC</head><p>In this section, we present the experimental results and discuss their implications. We start with a brief introduction to our experimental setup in Section 5.1. Then, we demon-   Bessel's correction. The main content cells correspond to the joint return distributions for different combinations of states and actions. The first columns and first rows of these tables correspond to the distributions of the utilities for agents 1 and 2, respectively. The top-left cells of these tables are the state-dependent utility V . DFAC enables the approximation of the true joint return distributions in <ref type="table" target="#tab_2">Table 1</ref>, and allows them to be factorized into the distributions of the utilities for the agents. strate that modeling a full distribution is beneficial to the performance of independent learners in Section 5.2. Finally, we compare the performances of the CTDE baseline methods and their DFAC variants in Section 5.3.</p><formula xml:id="formula_30">A 1 B 1 Agent 2 A2 B2 Agent 1 A1 N (7, 0) N (7, 0) B1 N (7, 0) N (7, 0) State 2A Agent 2 A2 B2 A1 N (0, 2) N (1, 13) B1 N (1, 13) N (8, 29) State 2B (a) B1, B2 at State 2A (b) B1, B2 at State 2B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Environment. We verify the DFAC framework in the SMAC benchmark environments <ref type="bibr" target="#b20">(Samvelyan et al., 2019)</ref> built on the popular real-time strategy game StarCraft II. Instead of playing the full game, SMAC is developed for evaluating the effectiveness of MARL micro-management algorithms. Each environment in SMAC contains two teams. One team is controlled by a decentralized MARL algorithm, with the policies of the agents conditioned on their local observation histories. The other team consists of enemy units controlled by the built-in game artificial intelligence based on carefully handcrafted heuristics, which is set to its highest difficulty equal to seven. The overall objective is to maximize the win rate for each battle scenario, where the rewards employed in our experiments follow the default settings of SMAC. The default settings use shaped rewards based on the damage dealt, enemy units killed, and whether the RL agents win the battle. If there is no healing unit in the enemy team, the maximum return of an episode (i.e., the score) is 20; otherwise, it may exceed 20, since enemies may receive more damages after healing or being healed.</p><p>The environments in SMAC are categorized into three different levels of difficulties: Easy, Hard, and Super Hard scenarios <ref type="bibr" target="#b20">(Samvelyan et al., 2019)</ref>. In this paper, we focus on all Super Hard scenarios including (a) 6h vs 8z, (b) 3s5z vs 3s6z, (c) MMM2, (d) 27m vs 30m, and (e) corridor, since these scenarios have not been properly addressed in the previous literature without the use of additional assumptions such as intrinsic reward signals <ref type="bibr" target="#b6">(Du et al., 2019)</ref>, explicit communication channels <ref type="bibr" target="#b25">Wang et al., 2019)</ref>, common knowledge shared among the   Hyperparameters. For all of our experimental results, the training length is set to 8M timesteps, where the agents are evaluated every 40k timesteps with 32 independent runs. The curves presented in this section are generated based on five different random seeds. The solid lines represent the median win rate, while the shaded areas correspond to the 25 th to 75 th percentiles. For a better visualization, the presented curves are smoothed by a moving average filter with its window size set to 11. The detailed hyperparameter setups are provided in the supplementary material.</p><p>Baselines. We select IQL, VDN, and QMIX as our baseline methods, and compare them with their distributional variants in our experiments. The configurations are optimized so as to provide the best performance for each of the methods considered. Since we tuned the hyperparameters of the baselines, their performances are better than those reported in <ref type="bibr" target="#b20">(Samvelyan et al., 2019)</ref>. The hyperparameter searching process is detailed in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Independent Learners</head><p>In order to validate our assumption that distributional RL is beneficial to the MARL domain, we first employ the simplest training algorithm, IQL, and extend it to its distributional variant, called DIQL. DIQL is simply a modified IQL that uses IQN as its underlying RL algorithm without any additional modification or enhancements <ref type="bibr">(Matignon et al., 2007;</ref><ref type="bibr" target="#b11">Lyu &amp; Amato, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From Figs. 3(a)-3(e) and</head><p>Tables 3-4, it is observed that DIQL is superior to IQL even without utilizing any value function factorization methods. This validates that distributional RL has beneficial influences on MARL, when it is compared to RL approaches based only on expected values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Value Function Factorization Methods</head><p>In order to inspect the effectiveness and impacts of DFAC on learning curves, win rates, and scores, we next summarize the results of the baselines as well as their DFAC variants on the Super Hard scenarios in <ref type="figure" target="#fig_4">Fig. 3</ref>(a)-(e) and <ref type="table" target="#tab_4">Table 3</ref>-4. <ref type="figure" target="#fig_4">Fig. 3</ref>(a)-(e) plot the learning curves of the baselines and their DFAC variants, with the final win rates presented in <ref type="table" target="#tab_4">Table 3</ref>, and their final scores reported in <ref type="table" target="#tab_5">Table 4</ref>. The win rates indicate how often do the player's team wins, while the scores represent how well do the player's team performs. Despite the fact that SMAC's objective is to maximize the win rate, the true optimization goal of MARL algorithms is the averaged score. In fact, these two metrics are not always positively correlated (e.g., VDN and QMIX in 6h vs 8z and 3s5z vs 3s6z, and QMIX and DMIX in 3s5z vs 3s6z).</p><p>It can be observed that the learning curves of DDN and DMIX grow faster and achieve higher final win rates than their corresponding baselines. In the most difficult map: 6h vs 8z, most of the methods fail to learn an effective policy except for DDN and DMIX. The evaluation results also show that DDN and DMIX are capable of performing consistently well across all Super Hard maps with high win rates. In addition to the win rates, <ref type="table" target="#tab_5">Table 4</ref> further presents the final averaged scores achieved by each method, and provides deeper insights into the advantages of the DFAC framework by quantifying the performances of the learned policies of different methods.</p><p>The improvements in win rates and scores are due to the benefits offered by distributional RL <ref type="bibr" target="#b10">(Lyle et al., 2019)</ref>, which enables the distributional variants to work more effectively in MARL environments. Moreover, the evaluation results reveal that DDN performs especially well in most environments despite its simplicity. Further validations of DDN and DMIX on our self-designed Ultra Hard scenarios that are more difficult than Super Hard scenarios can be found in our GitHub repository (https://github.com/j3soon/dfac), along with the gameplay recording videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we provided a distributional perspective on value function factorization methods, and introduced a framework, called DFAC, for integrating distributional RL with MARL domains. We first proposed DFAC based on a mean-shape decomposition procedure to ensure the Distributional IGM condition holds for all factorizable tasks. Then, we proposed the use of quantile mixture to implement the mean-shape decomposition in a computationally friendly manner. DFAC's ability to factorize the joint return distribution into individual utility distributions was demonstrated by a toy example. In order to validate the effectiveness of DFAC, we presented experimental results performed on all Super Hard scenarios in SMAC for a number of MARL baseline methods as well as their DFAC variants. The results show that DDN and DMIX outperform VDN and QMIX. DFAC can be extended to more value function factorization methods and offers an interesting research direction for future endeavors.</p><p>Assume, to the contrary, that Monotonicity for utility distributions is a sufficient condition for DIGM. By the definition of DIGM:</p><formula xml:id="formula_31">arg max u E[Z jt (h, u)] = arg max u1 E[Z 1 (h 1 , u 1 )] ? arg max u1 E[M (Z 1 (h 1 , u 1 )|s)] = arg max u1 E[Z 1 (h 1 , u 1 )] ? u 1 = u * 1 (?? contradiction).</formula><p>A contradiction occurs since u 1 = u * 1 , showing that Monotonicity is not a sufficient condition for DIGM. Since there exist a case where DIGM does not hold for K = 1, it certainly does not hold for all K ? Z. Theorem 1. Given a deterministic joint action-value function Q jt , a stochastic joint action-value function Z jt , and a factorization function ? for deterministic utilities:</p><formula xml:id="formula_32">Q jt (h, u) = ?(Q 1 (h 1 , u 1 ), ..., Q K (h K , u K )|s),</formula><p>such that [Q k ] k?K satisfy IGM for Q jt under h, the following distributional factorization:</p><formula xml:id="formula_33">Z jt (h, u) = ?(Z 1 (h 1 , u 1 ), ..., Z K (h K , u K )|s).</formula><p>is insufficient to guarantee that [Z k ] k?K satisfy DIGM for Z jt under h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. A contradiction is provided by Proposition 1.</head><p>Theorem 2 (DFAC Theorem). Given a deterministic joint action-value function Q jt , a stochastic joint action-value function Z jt , and a factorization function ? for deterministic utilities:</p><formula xml:id="formula_34">Q jt (h, u) = ?(Q 1 (h 1 , u 1 ), ..., Q K (h K , u K )|s),</formula><p>such that [Q k ] k?K satisfy IGM for Q jt under h, by Mean-Shape Decomposition, the following distributional factorization:</p><formula xml:id="formula_35">Z jt (h, u) = E[Z jt (h, u)] + (Z jt (h, u) ? E[Z jt (h, u)]) = Z mean (h, u) + Z shape (h, u) = ?(Q 1 (h 1 , u 1 ), ..., Q K (h K , u K )|s) + ?(Z 1 (h 1 , u 1 ), ..., Z K (h K , u K )|s).</formula><p>is sufficient to guarantee that [Z k ] k?K satisfy DIGM for Z jt under h, where Var(?) = 0 and E[?] = 0.</p><p>Proof. By mean-shape decomposition:</p><formula xml:id="formula_36">arg max u {E[Z jt (h, u)]} = arg max u {E[Z mean (h, u) + Z shape (h, u)]} = arg max u {E[Z mean (h, u)] + E[Z shape (h, u)]} = arg max u {E[?(Q 1 (h 1 , u 1 ), ..., Q K (h K , u K )|s)] + E[?(Z 1 (h 1 , u 1 ), ..., Z K (h K , u K )|s)]} = arg max u {?(Q 1 (h 1 , u 1 ), ..., Q K (h K , u K )|s) + 0} = arg max u {?(Q 1 (h 1 , u 1 ), ..., Q K (h K , u K )|s)} = ? ? ? arg max u1 Q 1 (h 1 , u 1 ) . . . arg max uK Q K (h K , u K ) ? ? ? ? arg max u E[Z jt (h, u)] = ? ? ? arg max u1 E[Z 1 (h 1 , u 1 )] . . . arg max uK E[Z K (h K , u K )] ? ? ? .</formula><p>The equations above show that [Z k ] k?K satisfy DIGM for Z jt under h.</p><p>Theorem 3. Given a quantile mixture: </p><formula xml:id="formula_37">F ?1 (?) = K k=1 ? k F ?1 k (?) with K components [F ?</formula><formula xml:id="formula_38">Z D = k?K ? k Z k .</formula><p>Proof. We first prove the case for a quantile mixture with K = 2 components, and then generalize it to all K ? Z. For K = 2, the quantile mixture is simplified as follows:</p><formula xml:id="formula_39">F ?1 (? ) = ? 1 F ?1 1 (? ) + ? 2 F ?1 2 (? )</formula><p>For notational simplicity, let X = ? 1 Z 1 , Y = ? 2 Z 2 , and ? is a latent variable shared among the random variables X, Y , and Z. The corresponding CDFs of the random variables X, Y , and Z are F X , F Y , and F Z , respectively, with X(? ) = F ?1 X (? ), Y (? ) = F ?1 Y (? ), and Z(? ) = F ?1 Z (? ). Under this notation, the above equation can be re-written as:</p><formula xml:id="formula_40">F ?1 Z (? ) = F ?1 X (? ) + F ?1 Y (? ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>The goal is to prove that there exist random variables (X, Y, Z) such that the following holds:</p><formula xml:id="formula_41">Z D = X + Y</formula><p>By the definition of the CDF of X + Y , the following holds: </p><formula xml:id="formula_42">(? ) + F ?1 Y (? ) ? z}), ?z ? R = sup{? ? [0, 1] : F ?1 X (? ) + F ?1 Y (? ) ? z}, ?z ? R = inf{? ? [0, 1] : z ? F ?1 X (? ) + F ?1 Y (? )}, ?z ? R = inf{? ? [0, 1] : z ? F ?1 Z (? )}, ?z ? R = F Z (z), ?z ? R. ? Z D = X + Y.</formula><p>The proof for quantile mixtures with two components can be iteratively applied to quantile mixtures with K ? Z components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2 Hyperparameters and Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.1 Stochastic Two Step Game</head><p>In the stochastic two step game described in Section 4, each agent is implemented as an IQN with two hidden layers comprised of 64 units and 512 units, respectively, with a ReLU nonlinearity at the end of each layer. We optimize the IQNs with N = N = 32 quantile samples, where each of them is encoded into a 64-dimensional intermediate embedding and projected to a 512-dimensional quantile embedding by a single hidden layer. Each agent performs independent -greedy action selection, with full exploration (i.e., = 1). We set the discount factor ? to 0.99. The replay buffer contains the state-action pairs of the latest 2k episodes, from which we uniformly sample a batch of size 512 for training. The target network is updated every 100 episodes. The optimizer is set to Adam, in which its learning rate is set to 1 ? 10 ?4 . We train for 20k timesteps (10k episodes). All agent networks share parameters, and the one-hot encoded agent id ([1 0] T for agent 1 and [0 1] T for agent 2) is concatenated to each agent's observation. We do not pass the previous actions taken by the agents as their inputs. Each agent receives the full state as its input. For DMIX, we use a mixing network with 8 units.</p><p>Each state is one-hot encoded. The starting state for the first timestep is State 1 (one-hot: [1 0 0] T ). At State 1, if Agent 1 selects Action A, the agents transit to State 2A (one-hot: [0 1 0] T ). On the other hand, if agent 1 selects Action B, the agents transit to State 2B (one-hot: [0 0 1] T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.2 SMAC</head><p>We tuned the hyperparameters of both the baselines and their distributional variants by selecting their hidden layer sizes from {32, 64, 128, 256, 512} and choose the best ones. The quantile samples of DIQL and DDN <ref type="table" target="#tab_2">Table S1</ref>: A summary of the optimal hidden state sizes of the baseline methods and their distributional variants. are simply set to N = N = 1, since they do not require the calculation of the expected value during the optimization process. As for DMIX, the numbers of quantile samples are set to N = N = 8 as in <ref type="bibr" target="#b30">[1]</ref>. The optimizers follow those used in DQN and IQN. All of the other hyperparameters follow those used in SMAC. <ref type="table" target="#tab_2">Table S1</ref> lists the hyperparameters adopted for the baselines and their distributional variants. The StarCraft version we used is 4.10.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>The transition function P (s |s, u) : S ? U jt ? S ? [0, 1] specifies the state transition probabilities. Given s and u, the next state is denoted as s ? P (?|s, u). ? The observation function O(o|s) : O jt ? S ? [0, 1] specifies the joint observation probabilities. Given s, the joint observation is represented as o ? O(?|s). ? R(r|s, u) : S ? U jt ? R ? [0, 1] is the joint reward function shared among all agents. Given s, the team reward is expressed as r ? R(?|s, u). ? ? ? R is the discount factor with value within (0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Both</head><label></label><figDesc>DDN and DMIX choose F ?1 state = 0 and [? k = 1] k?K for simplicity. Automatically learning the values of F ?1 state and [? k ] k?K is proposed as future work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) and (b) plot the value function factorization of the joint action B 1 , B 2 in State 2A and State 2B.The black line/curve shows the true return CDFs. The blue circles and the orange cross marks depict agent 1's and agent 2's learned utility, respectively, while the green squares indicate the estimated joint return.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>The win rate curves evaluated on the five Super Hard maps in SMAC for different CTDE methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F</head><label></label><figDesc>X+Y (z), ?z ? R = Pr(X + Y ? z), ?z ? R = Pr({? ? [0, 1] : X(? ) + Y (? ) ? z}), ?z ? R = Pr({? ? [0, 1] : F ?1 X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>the set of joint observations. At each timestep, a joint observation o = o 1 , ...o K ? O jt is received. Each agent k is only able to observe its individual observation o k ? O k . ? H jt = ? k?K H k is the set of joint action-observation histories. The joint history h = h 1 , ...h K ? H jt</figDesc><table><row><cell>concatenates all received observations and performed actions before a certain timestep, where h k ? H k rep-resents the action-observation history from agent k. ? U jt = ? k?K U k is the set of joint actions. At each timestep, the entire group of the agents take a joint ac-tion u</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>by sampling a batch of N and N quantiles from the policy network and the target network respectively. During execution, the action with the largest expected return Q(s, u) is chosen. Since IQN does not model the expected return explicitly, Q(s, u) is approximated by calculating the mean of the sampled return throughN quantile samples? i ? U ([0, 1]), ?i ? [1,N] based on Eq. (8), expressed as follows:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>An illustration of the flow of the stochastic twostep game. Each agent is able to perform an action from {A, B} in each step, with a subscript denoting the agent index. In the first step, action A 1 takes the agents from the initial State 1 to State 2A, while action B 1 takes them to State 2B instead. The transitions from State 1 to State 2A or State 2B yield zero reward. In the second step, the global rewards are sampled from the normal distributions defined in the payoff matrices.</figDesc><table><row><cell>State 1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The learned factorization of DMIX. All of the cells show the sampled mean ? and the sampled variance ? 2 with</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The median win rate % of five independent test runs.</figDesc><table><row><cell>Map</cell><cell cols="4">IQL VDN QMIX DIQL DDN DMIX</cell></row><row><cell cols="2">(a) (b) 29.83 89.20 0.00 0.00 (c) 68.92 89.20 (d) 2.27 63.12 (e) 84.87 85.34</cell><cell>12.78 67.22 92.44 84.77 37.61</cell><cell>0.00 83.92 62.22 94.03 85.23 97.22 6.02 91.48 91.62 95.40</cell><cell>49.43 91.08 95.11 85.45 90.45</cell></row><row><cell cols="4">* Maps (a)-(e) correspond to the maps in Fig. 3.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The averaged scores of five independent test runs.</figDesc><table><row><cell>Map</cell><cell cols="4">IQL VDN QMIX DIQL DDN DMIX</cell></row><row><cell cols="2">(a) 13.78 15.41 (b) 16.54 19.75 (c) 17.50 19.36 (d) 14.01 18.45 (e) 19.42 19.47</cell><cell>14.37 20.16 19.42 19.41 15.07</cell><cell>14.94 19.40 17.52 20.94 19.21 20.90 14.45 19.71 19.68 20.00</cell><cell>17.14 19.70 19.87 19.43 19.66</cell></row><row><cell cols="4">* Maps (a)-(e) correspond to the maps in Fig. 3.</cell></row></table><note>agents (de Witt et al., 2019; Wang et al., 2020), and so on. Three of these scenarios have their maximum scores higher than 20. In 3s5z vs 3s6z, the enemy Stalkers have the ability to regenerate shields; in MMM2, the enemy Medivacs can heal other units; in corridor, the enemy Zerglings slowly regenerate their own health.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1 k ] k?K and non-negative model parameters [? k ] k?K . There exist a set of random variables Z = F ?1 (? ) and [Z k = F ?1 k (? )] k?K corresponding to the quantile functions F ?1 and [F ?1 k ] k?K , respectively, where ? is a random variable uniformly distributed on [0, 1], with the following relationship:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S2 :</head><label>S2</label><figDesc>The detailed settings of the Super Hard maps.</figDesc><table><row><cell>Difficulty</cell><cell>Map</cell><cell>Player's Team</cell><cell>Enemy's Team</cell></row><row><cell>Super Hard</cell><cell>6h vs 8z 3s5z vs 3s6z MMM2 27m vs 30m corridor</cell><cell>6 Hydralisks 3 Stalkers &amp; 5 Zealots 7 Marines, 2 Marauders &amp; 1 Medivac 27 Marines 6 Zealots</cell><cell>8 Zealots 3 Stalkers &amp; 6 Zealots 8 Marines, 3 Marauders &amp; 1 Medivac 30 Marines 24 Zerglings</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>The authors acknowledge the support from NVIDIA Corporation and NVIDIA AI Technology Center (NVAITC). The authors thank Kuan-Yu Chang for his helpful critiques of this research work. The last author would like to thank the funding support from Ministry of Science and Technology (MOST) in Taiwan under grant nos. MOST 110-2636-E-007-010 and MOST 110-2634-F-007-019.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head><p>Wei-Fang Sun <ref type="bibr" target="#b30">1,</ref><ref type="bibr">2</ref> Cheng-Kuang Lee 2 Chun-Yi Lee 1 1 Department of Computer Science, National Tsing Hua University, Taiwan 2 NVIDIA AI Technology Center, NVIDIA Corporation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1 Theorems and Proofs</head><p>In this section, we elaborate on the definitions, and provide the proofs of the theorems discussed in the main manuscript.</p><p>Proposition 1. Monotonicity for utility distributions:</p><p>where M is a monotonic transformation that satisfies ?M ?Q k ? 0, ?k ? K, is not a sufficient condition for DIGM, although the equality may hold for special cases of M and</p><p>Proof. We consider a degenerated case and prove the theorem by contradiction. Consider a case where there is only a single agent (K = 1), with a single fully observable state and an exponential transformation M (Z 1 (h 1 , u 1 )|s) = exp(Z 1 (h 1 , u 1 )). The (joint) action space of this case consists of two (joint) actions: U jt = U 1 = {u * 1 , u 1 }, where u * 1 is the optimal action (with expected return 2) and u 1 is the suboptimal action (with expected return 1.5). We define the probability mass function (PMF) of Z 1 (h 1 , u * 1 ) to be:</p><p>and the PMF of Z 1 (h 1 , u 1 ) to be:</p><p>By the definition above, we can calculate the followings: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int</title>
		<meeting>Int</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Conf. on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Distributional reinforcement learning with linear function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moitra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03149</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Implicit quantile networks for distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1096" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributional reinforcement learning with quantile regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018-02" />
			<biblScope unit="page" from="2892" to="2901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-agent common knowledge reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>De Witt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>B?hmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9924" to="9935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning individual intrinsic reward in multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4405" to="4416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiagent planning with factored mdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimation of quantile mixtures via l-moments and trimmed l-moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karvanen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csda.2005.09.014</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="947" to="959" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributional reward decomposition for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6212" to="6221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comparative analysis of expected and distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019-02" />
			<biblScope unit="page" from="4504" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Likelihood quantile networks for coordinating multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems</title>
		<meeting>the 19th International Conference on Autonomous Agents and MultiAgent Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="798" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Hysteretic q-learning : an algorithm for decentralized reinforcement learning in cooperative multi-agent teams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fort-Piat</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS.2007.4399095</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2007</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Distributional reinforcement learning for efficient exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mavrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="4424" to="4434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Information-directed exploration for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirschner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Berkenkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Byx83s09Km" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learning Representations (ICLR)</title>
		<meeting>Int. Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A Concise Introduction to Decentralized POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">3319289276</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A concise introduction to decentralized POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rashid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="4295" to="4304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistics and samples in distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="5528" to="5536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The starcraft multi-agent challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Samvelyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Autonomous Agents and MultiAgent Systems (AAMAS)</title>
		<meeting>Int. Conf. on Autonomous Agents and MultiAgent Systems (AAMAS)</meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="page" from="2186" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Universal value function approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1312" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to factorize with transformation for cooperative multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Hostallero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qtran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="5887" to="5896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Value-decomposition networks for cooperative multi-agent learning based on team reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sunehag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Autonomous Agents and MultiAgent Systems (AAMAS)</title>
		<meeting>Int. Conf. on Autonomous Agents and MultiAgent Systems (AAMAS)</meeting>
		<imprint>
			<date type="published" when="2018-05" />
			<biblScope unit="page" from="2085" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: Independent versus cooperative agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="1993-06" />
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning nearly decomposable value functions via communication minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05366</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multiagent reinforcement learning with emergent roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lesser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08039</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully parameterized quantile function for distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Conf. Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="6190" to="6199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The quantile option architecture for reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019-02" />
			<biblScope unit="page" from="5797" to="5804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient communication in multi-agent reinforcement learning via variance based control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3230" to="3239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Implicit quantile networks for distributional reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Machine Learning (ICML)</title>
		<meeting>Int. Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1096" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
