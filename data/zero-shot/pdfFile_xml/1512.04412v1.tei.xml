<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance-aware Semantic Segmentation via Multi-task Network Cascades</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<email>jifdai@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<email>kahe@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>jiansun@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance-aware Semantic Segmentation via Multi-task Network Cascades</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multitask Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems.</p><p>The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the development of fully convolutional networks (FCNs) <ref type="bibr" target="#b22">[23]</ref>, the accuracy of semantic segmentation has been improved rapidly <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31]</ref> thanks to deeply learned features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>, large-scale annotations <ref type="bibr" target="#b21">[22]</ref>, and advanced reasoning over graphical models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31]</ref>. Nevertheless, FCNs <ref type="bibr" target="#b22">[23]</ref> and improvements <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31]</ref> are designed to predict a category label for each pixel, but are unaware of individual object instances. Accurate and fast instance-aware semantic segmentation is still a challenging problem. To encourage the research on this problem, the recently established COCO <ref type="bibr" target="#b21">[22]</ref> dataset and competition only accept instance-aware semantic segmentation results.</p><p>There have been a few methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref> addressing instance-aware semantic segmentation using convolu-shared features task 1 task 2 task 3 shared features task 1 task 2 task 3 <ref type="figure">Figure 1</ref>. Illustrations of common multi-task learning (left) and our multi-task cascade (right). tional neural networks (CNNs) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>. These methods all require mask proposal methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref> that are slow at inference time. In addition, these mask proposal methods take no advantage of deeply learned features or large-scale training data, and may become a bottleneck for segmentation accuracy.</p><p>In this work, we address instance-aware semantic segmentation solely based on CNNs, without using external modules (e.g., <ref type="bibr" target="#b0">[1]</ref>). We observe that the instance-aware semantic segmentation task can be decomposed into three different and related sub-tasks. 1) Differentiating instances. In this sub-task, the instances can be represented by bounding boxes that are class-agnostic. 2) Estimating masks. In this sub-task, a pixel-level mask is predicted for each instance. 3) Categorizing objects. In this sub-task, the category-wise label is predicted for each mask-level instance. We expect that each sub-task is simpler than the original instance segmentation task, and is more easily addressed by convolutional networks.</p><p>Driven by this decomposition, we propose Multi-task Network Cascades (MNCs) for accurate and fast instanceaware semantic segmentation. Our network cascades have three stages, each of which addresses one sub-task. The three stages share their features, as in traditional multi-task learning <ref type="bibr" target="#b3">[4]</ref>. Feature sharing greatly reduces the test-time computation, and may also improve feature learning thanks to the underlying commonality among the tasks. But unlike many multi-task learning applications, in our method a later stage depends on the outputs of an earlier stage, forming a causal cascade (see <ref type="figure">Fig. 1</ref>). So we call our structures "multi-task cascades". Training a multi-task cascade is nontrivial because of the causal relations among the multiple outputs. For example, our mask estimating layer takes convolutional features and predicted box instances as inputs, both of which are outputs of other layers. According to the chain rule of backpropagation <ref type="bibr" target="#b20">[21]</ref>, the gradients involve those with respect to the convolution responses and also those with respect to the spatial coordinates of predicted boxes. To achieve theoretically valid backpropagation, we develop a layer that is differentiable with respect to the spatial coordinates, so the gradient terms can be computed.</p><p>Our cascade model can thus be trained end-to-end via a clean, single-step framework. This single-step training algorithm naturally produces convolutional features that are shared among the three sub-tasks, which are beneficial to both accuracy and speed. Meanwhile, under this training framework, our cascade model can be extended to more stages, leading to improvements on accuracy.</p><p>We comprehensively evaluate our method on the PAS-CAL VOC dataset. Our method results in 63.5% mean Average Precision (mAP r ), about 3.0% higher than the previous best results <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref> using the same VGG network <ref type="bibr" target="#b26">[27]</ref>. Remarkably, this result is obtained at a test-time speed of 360ms per image, which is two orders of magnitudes faster than previous systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Thanks to the end-to-end training and the independence of external modules, the three sub-tasks and the entire system easily benefit from stronger features learned by deeper models. We demonstrate excellent accuracy on the challenging MS COCO segmentation dataset using an extremely deep 101-layer residual net (ResNet-101) <ref type="bibr" target="#b15">[16]</ref>, and also report our 1st-place result in the COCO segmentation track in ILSVRC &amp; COCO 2015 competitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object detection methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref> involve predicting object bounding boxes and categories. The work of R-CNN <ref type="bibr" target="#b9">[10]</ref> adopts region proposal methods (e.g., <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref>) for producing multiple instance proposals, which are used for CNN-based classification. In SPPnet <ref type="bibr" target="#b14">[15]</ref> and Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>, the convolutional layers of CNNs are shared on the entire image for fast computation. Faster R-CNN <ref type="bibr" target="#b25">[26]</ref> exploits the shared convolutional features to extract region proposals used by the detector. Sharing convolutional features leads to substantially faster speed for object detection systems <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Using mask-level region proposals, instance-aware semantic segmentation can be addressed based on the R-CNN philosophy, as in R-CNN <ref type="bibr" target="#b9">[10]</ref>, SDS <ref type="bibr" target="#b12">[13]</ref>, and Hypercolumn <ref type="bibr" target="#b13">[14]</ref>. Sharing convolutional features among masklevel proposals is enabled by using masking layers <ref type="bibr" target="#b6">[7]</ref>. All these methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7]</ref> rely on computationally expensive mask proposal methods. For example, the widely used MCG [1] takes 30 seconds processing an image, which becomes a bottleneck at inference time. DeepMask <ref type="bibr" target="#b24">[25]</ref> is recently developed for learning segmentation candidates using convolutional networks, taking over 1 second per image. Its accuracy for instance-aware semantic segmentation is yet to be evaluated.</p><p>Category-wise semantic segmentation is elegantly tackled by end-to-end training FCNs <ref type="bibr" target="#b22">[23]</ref>. The output of an FCN consists of multiple score maps, each of which is for one category. This formulation enables per-pixel regression in a fully-convolutional form, but is not able to distinguish instances of the same category. The FCN framework has been further improved in many papers (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31]</ref>), but these methods also have the limitations of not being able to predict instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-task Network Cascades</head><p>In our MNC model, the network takes an image of arbitrary size as the input, and outputs instance-aware semantic segmentation results. The cascade has three stages: propos-ing box-level instances, regressing mask-level instances, and categorizing each instance. These three stages are designed to share convolutional features (e.g., the 13 convolutional layers in VGG-16 <ref type="bibr" target="#b26">[27]</ref>). Each stage involves a loss term, but a later stage's loss relies on the output of an earlier stage, so the three loss terms are not independent. We train the entire network cascade end-to-end with a unified loss function. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates our cascade model.</p><p>In this section we describe the definition for each stage. In the next section we introduce an end-to-end training algorithm to address the causal dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Regressing Box-level Instances</head><p>In the first stage, the network proposes object instances in the form of bounding boxes. These bounding boxes are class-agnostic, and are predicted with an objectness score.</p><p>The network structure and loss function of this stage follow the work of Region Proposal Networks (RPNs) <ref type="bibr" target="#b25">[26]</ref>, which we briefly describe as follows for completeness. An RPN predicts bounding box locations and objectness scores in a fully-convolutional form. On top of the shared features, a 3?3 convolutional layer is used for reducing dimensions, followed by two sibling 1?1 convolutional layers for regressing box locations and classifying object/nonobject. The box regression is with reference to a series of pre-defined boxes (called "anchors" <ref type="bibr" target="#b25">[26]</ref>) at each location.</p><p>We use the RPN loss function given in <ref type="bibr" target="#b25">[26]</ref>. This loss function serves as the loss term L 1 of our stage 1. It has a form of:</p><formula xml:id="formula_0">L 1 = L 1 (B(?)).<label>(1)</label></formula><p>Here ? represents all network parameters to be optimized. B is the network output of this stage, representing a list of boxes:</p><formula xml:id="formula_1">B = {B i } and B i = {x i , y i , w i , h i , p i }, where B i is a box indexed by i. The box B i is centered at (x i , y i ) with</formula><p>width w i and height h i , and p i is the objectness probability. The notations in Eqn. <ref type="bibr" target="#b0">(1)</ref> indicate that the box predictions are functions of the network parameters ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Regressing Mask-level Instances</head><p>The second stage takes the shared convolutional features and stage-1 boxes as input. It outputs a pixel-level segmentation mask for each box proposal. In this stage, a masklevel instance is still class-agnostic.</p><p>Given a box predicted by stage 1, we extract a feature of this box by Region-of-Interest (RoI) pooling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9]</ref>. The purpose of RoI pooling is for producing a fixed-size feature from an arbitrary box, which is set as 14?14 at this stage. We append two extra fully-connected (fc) layers to this feature for each box. The first fc layer (with ReLU) reduces the dimension to 256, followed by the second fc layer that regresses a pixel-wise mask. This mask, of a pre-defined spatial resolution of m ? m (we use m = 28), is parameterized by an m 2 -dimensional vector. The second fc layer has m 2 outputs, each performing binary logistic regression to the ground truth mask.</p><p>With these definitions, the loss term L 2 of stage 2 for regressing masks exhibits the following form:</p><formula xml:id="formula_2">L 2 = L 2 (M (?) | B(?)).<label>(2)</label></formula><p>Here M is the network outputs of this stage, representing a list of masks: M = {M i } and M i is an m 2 -dimensional logistic regression output (via sigmoid) taking continuous values in [0, 1]. Eqn. <ref type="bibr" target="#b1">(2)</ref> indicates that the mask regression loss L 2 is dependent on M but also on B.</p><p>As a related method, DeepMask <ref type="bibr" target="#b24">[25]</ref> also regresses discretized masks. DeepMask applies the regression layers to dense sliding windows (fully-convolutionally), but our method only regresses masks from a few proposed boxes and so reduces computational cost. Moreover, mask regression is only one stage in our network cascade that shares features among multiple stages, so the marginal cost of the mask regression layers is very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Categorizing Instances</head><p>The third stage takes the shared convolutional features, stage-1 boxes, and stage-2 masks as input. It outputs category scores for each instance.</p><p>Given a box predicted by stage 1, we also extract a feature by RoI pooling. This feature map is then "masked" by the stage-2 mask prediction, inspired by the feature masking strategy in <ref type="bibr" target="#b6">[7]</ref>. This leads to a feature focused on the foreground of the prediction mask. The masked feature is given by element-wise product:</p><formula xml:id="formula_3">F M ask i (?) = F RoI i (?) ? M i (?).<label>(3)</label></formula><p>Here F RoI i is the feature after RoI pooling, M i (?) is a mask prediction from stage 2 (resized to the RoI resolution), and ? represents element-wise product. The masked feature F M ask i is dependent on M i (?). Two 4096-d fc layers are applied on the masked feature F M ask i . This is a mask-based pathway. Following <ref type="bibr" target="#b12">[13]</ref>, we also use another box-based pathway, where the RoI pooled features directly fed into two 4096-d fc layers (this pathway is not illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>). The mask-based and box-based pathways are concatenated. On top of the concatenation, a softmax classifier of N +1 ways is used for predicting N categories plus one background category. The box-level pathway may address the cases when the feature is mostly masked out by the mask-level pathway (e.g., on background).</p><p>The loss term L 3 of stage 3 exhibits the following form:</p><formula xml:id="formula_4">L 3 = L 3 (C(?) | B(?), M (?)).<label>(4)</label></formula><p>Here C is the network outputs of this stage, representing a list of category predictions for all instances:</p><formula xml:id="formula_5">C = {C i }.</formula><p>This loss term is dependent on B(?) and M (?) (where B(?) is used for generating the RoI feature).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">End-to-End Training</head><p>We define the loss function of the entire cascade as:</p><formula xml:id="formula_6">L(?) =L 1 (B(?)) + L 2 (M (?) | B(?)) + L 3 (C(?) | B(?), M (?)),<label>(5)</label></formula><p>where balance weights of 1 are implicitly used among the three terms. L(?) is minimized w.r.t. the network parameters ?. This loss function is unlike traditional multi-task learning, because the loss term of a later stage depends on the output of the earlier ones. For example, based on the chain rule of backpropagation, the gradient of L 2 involves the gradients w.r.t. B.</p><p>The main technical challenge of applying the chain rule to Eqn.(5) lies on the spatial transform of a predicted box B i (?) that determines RoI pooling. For the RoI pooling layer, its inputs are a predicted box B i (?) and the convolutional feature map F(?), both being functions of ?. In Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>, the box proposals are pre-computed and fixed, and the backpropagation of RoI pooling layer in <ref type="bibr" target="#b8">[9]</ref> only involves F(?). However, this is not the case in the presence of B(?). Gradients of both terms need to be considered in a theoretically sound end-to-end training solution.</p><p>In this section, we develop a differentiable RoI warping layer to account for the gradient w.r.t. predicted box positions and address the dependency on B(?). The dependency on M (?) is also tackled accordingly.</p><p>Differentiable RoI Warping Layers. The RoI pooling layer <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref> performs max pooling on a discrete grid based on a box. To derive a form that is differentiable w.r.t. the box position, we perform RoI pooling by a differentiable RoI warping layer followed by standard max pooling.</p><p>The RoI warping layer crops a feature map region and warps it into a target size by interpolation. We use F(?) to denote the full-image convolutional feature map. Given a predicted box B i (?) centered at (x i (?), y i (?)) with width w i (?) and height h i (?), an RoI warping layer interpolates the features inside the box and outputs a feature of a fixed spatial resolution. This operation can be written as linear transform on the feature map F(?):</p><formula xml:id="formula_7">F RoI i (?) = G(B i (?))F(?).<label>(6)</label></formula><p>Here F(?) is reshaped as an n-dimensional vector, with n = W H for a full-image feature map of a spatial size W ? H. G represents the cropping and warping operations, and is an n -by-n matrix where n = W H corresponds to the pre-defined RoI warping output resolution W ? H . F RoI i (?) is an n -dimensional vector representing the RoI warping output. We note that these operations are performed for each channel independently.</p><p>The computation in Eqn. <ref type="bibr" target="#b5">(6)</ref> has this form:</p><formula xml:id="formula_8">F RoI i (u ,v ) = W ?H (u,v) G(u, v; u , v |B i )F (u,v) ,<label>(7)</label></formula><p>where the notations ? in Eqn. </p><formula xml:id="formula_9">x i ? w i /2, x i + w i /2) ? [y i ?h i /2, y i +h i /2) into another size of [?W /2, W /2)? [?H /2, H /2). Using bilinear interpolation, G is sep- arable: G(u, v; u , v |B i ) = g(u, u |x i , w i )g(v, v |y i , h i ) where: g(u, u |x i , w i ) = ?(x i + u W w i ? u),<label>(8)</label></formula><p>where ?(?) = max(0, 1 ? | ? |) is the bilinear interpolation function, and x i + u W w i maps the position of u ? [?W /2, W /2) to the full-image feature map domain. g(v, v |y i , h i ) is defined similarly. We note that because ? is non-zero in a small interval, the actual computation of Eqn. <ref type="bibr" target="#b6">(7)</ref> involves a very few terms.</p><p>According to the chain rule, for backpropagation involving Eqn.(6) we need to compute:</p><formula xml:id="formula_10">?L 2 ?B i = ?L 2 ?F RoI i ?G ?B i F<label>(9)</label></formula><p>where we use ?B i to denote ?x i , ?y i , ?w i , and ?h i for simplicity. The term ?G ?Bi in Eqn.(9) can be derived from Eqn. <ref type="bibr" target="#b7">(8)</ref>. As such, the RoI warping layer can be trained with any preceding/succeding layers. If the boxes are constant (e.g., given by Selective Search <ref type="bibr" target="#b28">[29]</ref>), Eqn. <ref type="formula" target="#formula_10">(9)</ref> is not needed, which becomes the case of the existing RoI pooling in <ref type="bibr" target="#b8">[9]</ref>.</p><p>After the differentiable RoI warping layer, we append a max pooling layer to perform the RoI max pooling behavior. We expect the RoI warping layer to produce a sufficiently fine resolution, which is set as W ? H = 28 ? 28 in this paper. A max pooling layer is then applied to produce a lower-resolution output, e.g., 7?7 for VGG-16.</p><p>The RoI warping layer shares similar motivations with the recent work of Spatial Transformer Networks <ref type="bibr" target="#b17">[18]</ref>. In <ref type="bibr" target="#b17">[18]</ref>, a spatial transformation of the entire image is learned, which is done by feature interpolation that is differentiable w.r.t. the transformation parameters. The networks in <ref type="bibr" target="#b17">[18]</ref> are used for image classification. Our RoI warping layer is also driven by the differentiable property of interpolating features. But the RoI warping layer is applied to multiple proposed boxes that are of interest, instead of the entire image. The RoI warping layer has a pre-defined output size and arbitrary input sizes, in contrast to <ref type="bibr" target="#b17">[18]</ref>.  Masking Layers. We also compute the gradients involved in In summary, given the differentiable RoI warping module, we have all the necessary components for backpropagation (other components are either standard, or trivial to implement). We train the model by stochastic gradient descent (SGD), implemented in the Caffe library <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Cascades with More Stages</head><p>Next we extend the cascade model to more stages within the above MNC framework.</p><p>In Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>, the (N +1)-way classifier is trained jointly with class-wise bounding box regression. Inspired by this practice, on stage 3, we add a 4(N +1)-d fc layer for regression class-wise bounding boxes <ref type="bibr" target="#b8">[9]</ref>, which is a sibling layer with the classifier layer. The entire 3-stage network cascade is trained as in Sec. 4.</p><p>The inference step with box regression, however, is not as straightforward as in object detection, because our ultimate outputs are masks instead of boxes. So during inference, we first run the entire 3-stage network and obtain the regressed boxes on stage 3. These boxes are then considered as new proposals 1 . Stages 2 and 3 are performed for the second time on these proposals. This is in fact 5stage inference. Its inference-time structure is illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. The new stages 4 and 5 share the same structures as stages 2 and 3, except that they use the regressed boxes from stage 3 as the new proposals. This inference process can be iterated, but we have observed negligible gains.</p><p>Given the above 5-stage cascade structure ( <ref type="figure" target="#fig_3">Fig. 3)</ref>, it is easy to adopt our algorithm in Sec. 4 to train this cascade end-to-end by backpropagation. Training the model in this way makes the training-time structure consistent with the <ref type="bibr" target="#b0">1</ref> To avoid multiplying the number of proposals by the number of categories, for each box we only use the highest scored category's bounding box regressor. inference-time structure, which improves accuracy as will be shown by experiments. It is possible to train a cascade with even more stages in this way. But due to concerns on fast inference, we only present MNCs with up to 5 stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Implementation Details</head><p>Non-maximum suppression. On stage 1, the network produces ?10 4 regressed boxes. For generating the proposals for stage 2, we use non-maximum suppression (NMS) to reduce redundant candidates. The threshold of the Intersection-over-Union (IoU) ratio for this NMS is 0.7 as in <ref type="bibr" target="#b25">[26]</ref>. After that, the top-ranked 300 boxes <ref type="bibr" target="#b25">[26]</ref> will be used for stage 2. During training, the forward/backward propagated signals of stages 2 and 3 only go through the "pathways" determined by these 300 boxes. NMS is similar to max pooling, maxout <ref type="bibr" target="#b10">[11]</ref>, or other local competing layers <ref type="bibr" target="#b27">[28]</ref>, which are implemented as routers of forward/backward pathways. During inference, we use the same NMS strategy to produce 300 proposals for stage 2.</p><p>Positive/negative samples. (i) On stage 1, their definitions follow <ref type="bibr" target="#b25">[26]</ref>. (ii) On stage 2, for each proposed box we find its highest overlapping ground truth mask. If the overlapping ratio (IoU) is greater than 0.5, this proposed box is considered as positive and contributes to the mask regression loss; otherwise is ignored in the regression loss. The mask regression target is the intersection between the proposed box and the ground truth mask, resized to m ? m pixels. (iii) On stage 3, we consider two sets of positive/negative samples. In the first set, the positive samples are the instances that overlap with ground truth boxes by box-level IoU ? 0.5 (the negative samples are the rest). In the second set, the positive samples are the instances that overlap with ground truth instances by box-level IoU ? 0.5 and mask-level IoU ? 0.5. The loss function of stage 3 involves two (N +1)-way classifiers, one for classifying mask-level instances and the other for classifying box-level instances (whose scores are not used for inference). The reason for considering both box-level and mask-level IoU is that when the proposed box is not a real instance (e.g., on the background or poorly overlapping with ground truth), the regressed mask might be less reliable and thus the box-level IoU is more confident.</p><p>Hyper-parameters for training. We use the ImageNet pretrained models (e.g., VGG-16 <ref type="bibr" target="#b26">[27]</ref>) to initialize the shared convolutional layers and the corresponding 4096-d fc layers. The extra layers are initialized randomly as in <ref type="bibr" target="#b16">[17]</ref>. We adopt an image-centric training framework <ref type="bibr" target="#b8">[9]</ref>: the shared convolutional layers are computed on the entire image, while the RoIs are randomly sampled for computing loss functions. In our system, each mini-batch involves 1 image, 256 sampled anchors for stage 1 as in <ref type="bibr" target="#b25">[26]</ref>  sampled RoIs for stages 2 and 3. We train the model using a learning rate of 0.001 for 32k iterations, and 0.0001 for the next 8k. We train the model in 8 GPUs, each GPU holding 1 mini-batch (so the effective mini-batch size is ?8). The images are resized such that the shorter side has 600 pixels <ref type="bibr" target="#b8">[9]</ref>. We do not adopt multi-scale training/testing <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b8">9]</ref>, as it provides no good trade-off on speed vs. accuracy <ref type="bibr" target="#b8">[9]</ref>.</p><p>Inference. We use 5-stage inference for both 3-stage and 5-stage trained structures. The inference process gives us a list of 600 instances with masks and category scores (300 from the stage 3 outputs, and 300 from the stage 5 outputs). We post-process this list to reduce similar predictions. We first apply NMS (using box-level IoU 0.3 <ref type="bibr" target="#b9">[10]</ref>) on the list of 600 instances based on their category scores. After that, for each not-suppressed instance, we find its "similar" instances which are defined as the suppressed instances that overlap with it by IoU ? 0.5. The prediction masks of the not-suppressed instance and its similar instances are merged together by weighted averaging, pixel-by-pixel, using the classification scores as their averaging weights. This "mask voting" scheme is inspired by the box voting in <ref type="bibr" target="#b7">[8]</ref>. The averaged masks, taking continuous values in [0, 1], are binarized to form the final output masks. The averaging step improves accuracy by ?1% over the NMS outcome. This postprocessing is performed for each category independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Experiments on PASCAL VOC 2012</head><p>We follow the protocols used in recent papers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref> for evaluating instance-aware semantic segmentation. The models are trained on the PASCAL VOC 2012 training set, and evaluated on the validation set. We use the segmentation annotations in <ref type="bibr" target="#b11">[12]</ref> for training and evaluation, following <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref>. We evaluate the mean Average Precision, which is referred to as mean AP r <ref type="bibr" target="#b12">[13]</ref> or simply mAP r . We evaluate mAP r using IoU thresholds at 0.5 and 0.7.</p><p>Ablation Experiments on Training Strategies. <ref type="table" target="#tab_0">Table  1</ref> compares the results of different training strategies for MNCs. We remark that in this table all results are obtained function, the network of stage 1 is still computed fully-convolutionally on the entire image and produces all proposals that are used by later stages. via 5-stage inference, so the differences are contributed by the training strategies. We show results using ZF net <ref type="bibr" target="#b29">[30]</ref> that has 5 convolutional layers and 3 fc layers, and VGG-16 net <ref type="bibr" target="#b26">[27]</ref> that has 13 convolutional layers and 3 fc layers.</p><p>As a simple baseline <ref type="figure">(Table 1, a)</ref>, we train the three stages step-by-step without sharing their features. Three separate networks are trained, and a network of a later stage takes the outputs from the trained networks of the earlier stages. The three separate networks are all initialized by the ImageNet-pre-trained model. This baseline has an mAP r of 60.2% using VGG-16. We note that this baseline result is competitive (see also <ref type="table">Table 2</ref>), suggesting that decomposing the task into three sub-tasks is an effective solution.</p><p>To achieve feature sharing, one may follow the step-bystep training in <ref type="bibr" target="#b25">[26]</ref>. Given the above model (a), the shared convolutional layers are kept unchanged by using the last stage's weights, and the three separate networks are trained step-by-step again with the shared layers not tuned, following <ref type="bibr" target="#b25">[26]</ref>. Doing so leads to an mAP r of 60.5%, just on par with the baseline that does not share features. This suggests that sharing features does not directly improve accuracy.</p><p>Next we experiment with the single-step, end-to-end training algorithm developed in Sec. 4. <ref type="table" target="#tab_0">Table 1</ref> (c) shows the result of end-to-end training a 3-stage cascade. The mAP r is increased to 62.6%. We note that in <ref type="table" target="#tab_0">Table 1</ref> (a), (b), and (c), the models have the same structure for training. So the improvement of (c) is contributed by end-to-end training this cascade structure. This improvement is similar to other gains observed in many practices of multi-task learning <ref type="bibr" target="#b3">[4]</ref>. By developing training algorithm as in Sec. 4, we are able to train the network by backpropagation in a theoretically sound way. The features are naturally shared by optimizing a unified loss function, and the benefits of multi-task learning are witnessed.</p><p>Table 1 (d) shows the result of end-to-end training a 5stage cascade. The mAP r is further improved to 63.5%. We note that all results in <ref type="table" target="#tab_0">Table 1</ref> are based on the same 5-stage inference strategy. So the accuracy gap between (d) and (c) is contributed by training a 5-stage structure that is consistent with its inference-time usage.</p><p>The series of comparisons are also observed when using the ZF net as the pre-trained model ( </p><formula xml:id="formula_11">O 2 P [2]</formula><p>25.2 --SDS (AlexNet) <ref type="bibr" target="#b12">[13]</ref> 49.7 25.3 48 Hypercolumn <ref type="bibr" target="#b13">[14]</ref> 60.0 40.4 &gt;80 CFM <ref type="bibr" target="#b6">[7]</ref> 60.7 39. <ref type="bibr">6 32 MNC [ours]</ref> 63.5 41.5 0.36 <ref type="table">Table 2</ref>. Comparisons of instance-aware semantic segmentation on the PASCAL VOC 2012 validation set. The testing time per image (including all steps) is evaluated in a single Nvidia K40 GPU, except that the MCG <ref type="bibr" target="#b0">[1]</ref> proposal time is evaluated on a CPU. MCG is used by <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b6">7]</ref> and its running time is about 30s. The running time of <ref type="bibr" target="#b13">[14]</ref> is our estimation based on the description from the paper. The pre-trained model is VGG-16 for <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref> and ours. O 2 P is not based on deep CNNs, and its result is reported by <ref type="bibr" target="#b12">[13]</ref>.</p><p>conv stage 2 stage 3 stage 4 stage 5 others total 0.15 0.01 0.08 0.01 0.08 0.03 0.36 <ref type="table">Table 3</ref>. Detailed testing time (seconds) per image of our method using 5-stage inference. The model is VGG-16. "Others" include post-processing and communications among stages.</p><p>Comparisons with State-of-the-art Methods. In <ref type="table">Table 2</ref> we compare with SDS <ref type="bibr" target="#b12">[13]</ref>, Hypercolumn <ref type="bibr" target="#b13">[14]</ref>, and CFM <ref type="bibr" target="#b6">[7]</ref>, which are existing CNN-based semantic segmentation methods that are able to identify instances. These papers reported their mAP r under the same protocol used by our experiments. Our MNC has ?3% higher mAP r @0.5 than previous best results. Our method also has higher mAP r @0.7 than previous methods. <ref type="figure" target="#fig_6">Fig 4</ref> shows some examples of our results on the validation set. Our method can handle challenging cases where multiple instances of the same category are spatially connected to each other (e.g., <ref type="figure" target="#fig_6">Fig 4, first row)</ref>.</p><p>Running Time. Our method has an inference-time speed of 360ms per image <ref type="table">(Table 2)</ref>, evaluated on an Nvidia K40 GPU. <ref type="table">Table 3</ref> shows the details. Our method does not require any external region proposal method, whereas the region proposal step in SDS, Hypercolumn, and CFM costs 30s using MCG. Furthermore, our method uses the shared convolutional features for the three sub-tasks and avoids redundant computation. Our system is about two orders of magnitude faster than previous systems.</p><p>Object Detection Evaluations. We are also interested in the box-level object detection performance (mAP b ), so that we can compare with more systems that are designed for object detection. We train our model on the PASCAL VOC 2012 trainval set, and evaluate on the PASCAL VOC 2012 test set for object detection. Given mask-level instances generated by our model, we simply assign a tight bounding box to each instance. <ref type="table">Table 4</ref> shows that our result (70.9%) compares favorably to the recent Fast/Faster R-CNN systems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>. We note that our result is obtained with fewer training images (without the 2007 set), but with mask-level annotations. This experiment shows the effectiveness of our algorithm for detecting both box-and mask-level instances.</p><p>The above detection result is solely based on the masklevel outputs. But our method also has box-level outputs from the box regression layers in stage 3/5. Using these box layers' outputs (box coordinates and scores) in place of the mask-level outputs, we obtain an mAP b of 73.5% ( <ref type="table">Table 4</ref>). Finally, we train the MNC model on the union set of 2007 trainval+test and 2012 trainval. As the 2007 set has no mask-level annotation, when a sample image from the 2007 set is used, its mask regression loss is ignored (but the mask is generated for the later stages) and its mask-level IoU measure for determining positive/negative samples is ignored. These samples can still impact the box proposal stage and the categorizing stage. Under this setting, we obtain an mAP b of 75.9% <ref type="table">(Table 4</ref>), substantially better than Fast/Faster R-CNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Experiments on MS COCO Segmentation</head><p>We further evaluate on the MS COCO dataset <ref type="bibr" target="#b21">[22]</ref>. This dataset consists of 80 object categories for instance-aware semantic segmentation. Following the COCO guidelines, we use the 80k+40k trainval images to train, and report the results on the test-dev set. We evaluate the standard COCO metric (mAP r @IoU=[0.5:0.95]) and also the PASCAL metrics (mAP r @IoU=0.5). <ref type="table" target="#tab_2">Table 5</ref> shows our method using VGG-16 has a result of 19.5%/39.7%.</p><p>The end-to-end training behavior and the independence of external models make our method easily enjoy gains from deeper representations. By replacing VGG-16 with an extremely deep 101-layer network (ResNet-101) <ref type="bibr" target="#b15">[16]</ref>, we achieve 24.6%/44.3% on the MS COCO test-dev set (Ta-  ble 5). It is noteworthy that ResNet-101 leads to a relative improvement of 26% (on mAP r @[.5:.95]) over VGG-16, which is consistent to the relative improvement of COCO object detection in <ref type="bibr" target="#b15">[16]</ref>. This baseline result is close to the 2nd-place winner's ensemble result (25.1%/45.8% by FAIRCNN). On our baseline result, we further adopt global context modeling and multi-scale testing as in <ref type="bibr" target="#b15">[16]</ref>, and ensembling. Our final result on the test-challenge set is 28.2%/51.5%, which won the 1st place in the COCO segmentation track 3 of ILSVRC &amp; COCO 2015 competitions. <ref type="figure">Fig. 5</ref> shows some examples.</p><p>3 http://mscoco.org/dataset/#detections-challenge2015</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have presented Multi-task Network Cascades for fast and accurate instance segmentation. We believe that the idea of exploiting network cascades in a multi-task learning framework is general. This idea, if further developed, may be useful for other recognition tasks.</p><p>Our method is designed with fast inference in mind, and is orthogonal to some other successful strategies developed previously for semantic segmentation. For example, one may consider exploiting a CRF <ref type="bibr" target="#b4">[5]</ref> to refine the boundaries of the instance masks. This is beyond the scope of this paper and will be investigated in the future. <ref type="figure">Figure 5</ref>. Our instance-aware semantic segmentation results on the MS COCO test-dev set using ResNet-101 <ref type="bibr" target="#b15">[16]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Multi-task Network Cascades for instance-aware semantic segmentation. At the top right corner is a simplified illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 6 )</head><label>6</label><figDesc>are omitted for simplifying presentation. Here (u , v ) represent a spatial position in the target W ? H feature map, and (u, v) run over the fullimage feature map F. The function G(u, v; u , v |B i ) represents transforming a proposed box B i from a size of [</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>A 5-stage cascade. On stage 3, bounding boxes updated by the box regression layer are used as the input to stage 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>L 3 (C(?) | B(?), M (?)), where the dependency on B(?) and M (?) is determined by Eqn.(3). With the differentiable RoI warping module (F RoI i ), the operations in Eqn.(3) can be simply implemented by an element-wise product module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Our instance-aware semantic segmentation results on the PASCAL VOC 2012 validation set. One color denotes one instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation experiments on PASCAL VOC 2012 validation. For (a), (b), and (c), the cascade structures for training have 3 stages. The inference process (5-stage, see 5) is the same for all cases; the models are only different in the training methods. The pre-trained models are ZF net<ref type="bibr" target="#b29">[30]</ref> (left) and VGG-16 net<ref type="bibr" target="#b26">[27]</ref> (right).</figDesc><table><row><cell>2 , and 64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 ,</head><label>1</label><figDesc>left), showing the generality of our findings.method mAP r @0.5 (%) mAP r @0.7 (%) time/img (s)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Our baseline segmentation result (%) on the MS COCO test-dev set. The training set is the trainval set.</figDesc><table><row><cell>system</cell><cell></cell><cell>training data</cell><cell>mAP b (%)</cell></row><row><cell>R-CNN [10]</cell><cell></cell><cell>VOC 12</cell><cell>62.4</cell></row><row><cell>Fast R-CNN [9]</cell><cell></cell><cell>VOC 12</cell><cell>65.7</cell></row><row><cell>Fast R-CNN [9]</cell><cell></cell><cell>VOC 07++12</cell><cell>68.4</cell></row><row><cell cols="2">Faster R-CNN [26]</cell><cell>VOC 12</cell><cell>67.0</cell></row><row><cell cols="2">Faster R-CNN [26]</cell><cell>VOC 07++12</cell><cell>70.4</cell></row><row><cell>MNC [ours]</cell><cell></cell><cell>VOC 12</cell><cell>70.9</cell></row><row><cell>MNC box [ours]</cell><cell></cell><cell>VOC 12</cell><cell>73.5</cell></row><row><cell>MNC box [ours]  ?</cell><cell></cell><cell>VOC 07++12</cell><cell>75.9</cell></row><row><cell cols="4">Table 4. Evaluation of (box-level) object detection mAP on the</cell></row><row><cell cols="4">PASCAL VOC 2012 test set. "12" denotes VOC 2012 trainval, and</cell></row><row><cell cols="4">"07++12" denotes VOC 2007 trainval+test and 2012 trainval. The</cell></row><row><cell cols="4">pre-trained model is VGG-16 for all methods.  ? : http://host.</cell></row><row><cell cols="4">robots.ox.ac.uk:8080/anonymous/NUWDYX.html</cell></row><row><cell>network</cell><cell cols="2">mAP@[.5:.95] (%)</cell><cell>mAP@.5 (%)</cell></row><row><cell>VGG-16 [27]</cell><cell></cell><cell>19.5</cell><cell>39.7</cell></row><row><cell>ResNet-101 [16]</cell><cell></cell><cell>24.6</cell><cell>44.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cpmc: Automatic object segmentation using constrained parametric min-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multitask learning. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion &amp; semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4389</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Maxout networks</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compete to compute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazerounian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
