<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhang</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Colburn</surname></persName>
							<email>alexco@zillow.com</email>
							<affiliation key="aff1">
								<orgName type="department">Zillow Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Shan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Zillow Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
							<email>dhoiem@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an algorithm to predict room layout from a single image that generalizes across panoramas and perspective images, cuboid layouts and more general layouts (e.g. "L"-shape room). Our method operates directly on the panoramic image, rather than decomposing into perspective images as do recent works. Our network architecture is similar to that of RoomNet [16], but we show improvements due to aligning the image based on vanishing points, predicting multiple layout elements (corners, boundaries, size and translation), and fitting a constrained Manhattan layout to the resulting predictions. Our method compares well in speed and accuracy to other existing work on panoramas, achieves among the best accuracy for perspective images, and can handle both cuboid-shaped and more general Manhattan layouts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the 3D layout of a room from one image is an important goal, with applications such as robotics and virtual/augmented reality. The room layout specifies the positions, orientations, and heights of the walls, relative to the camera center. The layout can be represented as a set of projected corner positions or boundaries, or as a 3D mesh. Existing works apply to special cases of the problem, such as predicting cuboid-shaped layouts from perspective images or from panoramic images.</p><p>We present LayoutNet, a deep convolution neural network (CNN) that estimates the 3D layout of an indoor scene from a single perspective or panoramic image <ref type="figure">(Figure. 1</ref>). Our method compares well in speed and accuracy on panoramas and is among the best on perspective images. Our method also generalizes to non-cuboid Manhattan layouts, such as "L"-shaped rooms. Code is available at: https://github.com/zouchuhang/ LayoutNet.</p><p>Our LayoutNet approach operates in three steps <ref type="figure">( Figure.</ref> 2). First, our system analyzes the vanishing points LayoutNet <ref type="figure">Figure 1</ref>. Illustration. Our LayoutNet predicts a non-cuboid room layout from a single panorama under equirectangular projection. and aligns the image to be level with the floor (Sec. 3.1). This alignment ensures that wall-wall boundaries are vertical lines and substantially reduces error according to our experiments. In the second step, corner (layout junctions) and boundary probability maps are predicted directly on the image using a CNN with an encoder-decoder structure and skip connections (Sec. 3.2). Corners and boundaries each provide a complete representation of room layout. We find that jointly predicting them in a single network leads to better estimation. Finally, the 3D layout parameters are optimized to fit the predicted corners and boundaries (Sec. <ref type="bibr">3.4)</ref>. The final 3D layout loss from our optimization process is difficult to back-propagate through the network, but direct regression of the 3D parameters during training serves as an effective substitute, encouraging predictions that maximize accuracy of the end result.</p><p>Our contributions are:</p><p>? We propose a more general RGB image to layout algorithm that is suitable for perspective and panoramic images with Manhattan layouts. Our system compares well in speed and accuracy for panoramic images and achieves the second best for perspective images, while also being the fastest. ? We demonstrate gains from using precomputed vanishing point cues, geometric constraints, and postprocess optimization, indicating that deep network approaches still benefit from explicit geometric cues and constraints. We also show that adding an objective to directly regress 3D layout parameters leads to better predictions of the boundaries and corners that are used to solve for the final predicted layout. ? We extend the annotations for the Stanford 2D-3D dataset <ref type="bibr" target="#b0">[1]</ref>, providing room layout annotations that can be used in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Single-view room layout estimation has been an active topic of research for the past ten years. Delage et al. <ref type="bibr" target="#b6">[7]</ref> fit floor/wall boundaries in a perspective image taken by a level camera to create a 3D model under "Manhattan world" assumptions <ref type="bibr" target="#b2">[3]</ref>. The Manhattan world assumptions are that all walls are at right angles to each other and perpendicular to the floor. A special case is the cuboid model, in which four walls, ceiling, and floor enclose the room. Lee et al. <ref type="bibr" target="#b17">[18]</ref> produce Orientation Maps, generate layout hypotheses based on detected line segments, and select a best-fitting layout from among them. Hedau et al. <ref type="bibr" target="#b10">[11]</ref> recover cuboid layouts by solving for three vanishing points, sampling layouts consistent with those vanishing points, and selecting the best layout based on edge and Geometric Context <ref type="bibr" target="#b12">[13]</ref> consistencies. Subsequent works follow a similar approach, with improvements to layout generation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b22">23]</ref>, features for scoring layouts <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b22">23]</ref>, and incorporation of object hypotheses <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34]</ref> or other context. The most recent methods train deep network features to classify pix-els into layout surfaces (walls, floor, ceiling) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>, boundaries <ref type="bibr" target="#b21">[22]</ref>, corners <ref type="bibr" target="#b15">[16]</ref>, or a combination <ref type="bibr" target="#b24">[25]</ref>.</p><p>Nearly all of these works aim to produce cuboid-shaped layouts from perspective RGB images. A few works also operate on panoramic images. Zhang et al. <ref type="bibr" target="#b32">[33]</ref> propose the PanoContext dataset and method to estimate room layout from 360 ? panoramic images (more on this later). Yang et al. <ref type="bibr" target="#b30">[31]</ref> recover layouts from panoramas based on edge cues, Geometric Context, and other priors. Xu et al. <ref type="bibr" target="#b29">[30]</ref> estimate layout based on surface orientation estimates and object hypotheses. Other works recover indoor layout from multiple images (e.g., <ref type="bibr" target="#b1">[2]</ref>) or RGBD images (e.g., <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>), where estimates rely heavily on 3D points obtained from sensors or multiview constraints. Rent3D <ref type="bibr" target="#b19">[20]</ref> takes advantage of a known floor plan. Our approach simplifies reconstruction by estimating layout directly on a single RGB equirectangular panorama. Our final output is a sparse and compact planar Manhattan layout parameterized by each wall's distance to camera, height, and the layout rotation.</p><p>Our work is most similar in goal to PanoContext <ref type="bibr" target="#b32">[33]</ref> and in approach to RoomNet <ref type="bibr" target="#b15">[16]</ref>. PanoContext extends the frameworks designed for perspective images to panoramas, estimating vanishing points, generating hypotheses, and scoring hypotheses according to Orientation Maps, Geometric Context, and object hypotheses. To compute these features, PanoContext first projects the panoramic image into multiple overlapping perspective images, and then combines the feature maps back into a panoramic image. Our approach is more direct: after aligning the panoramic image based on vanishing points, our system uses a deep network to predict boundaries and corners directly on the panoramic image. In this regard, we are similar to Room-Net, which uses a deep network to directly predict layout corners in perspective images, as well as a label that indicates which corners are visible. Our method differs from RoomNet in several ways. Our method applies to panoramic images. Our method also differs in the alignment step (RoomNet performs none) and in our multitask prediction of boundaries, corners, and 3D cuboid parameters. Our final inference is constrained to produce a Manhattan 3D layout. RoomNet uses an RNN to refine 2D corner position predictions, but those predictions might not be consistent with any 3D cuboid layout. Our experiments show that all of these differences improve results.</p><p>More generally, we propose the first method, to our knowledge, that applies to both perspective and panoramic images. We also show that our method extends easily to non-cuboid Manhattan layouts. Thus, our method is arguably the most general and effective approach to date for indoor layout estimation from a single RGB image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We first describe our method for predicting cuboidshaped layouts from panoramas: alignment (Sec. 3.1), corner and boundary prediction with a CNN (Sec. 3.2 and 3.3), and optimization of 3D cuboid parameters (Sec. 3.4). Then, we describe modifications to predict on more general (non-cuboid) Manhattan layouts and perspective images (Sec. 3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Panoramic image alignment</head><p>Given the input as a panorama that covers a 360 ? horizontal field of view, we first align the image by estimating the floor plane direction under spherical projection, rotate the scene, and reproject it to the 2D equirectangular projection. Similar to Zhang et al.'s approach <ref type="bibr" target="#b32">[33]</ref>, we select long line segments using the Line Segment Detector (LSD) <ref type="bibr" target="#b23">[24]</ref> in each overlapped perspective view, then vote for three mutually orthogonal vanishing directions using the Hough Transform. This pre-processing step eases our network training. The detected candidate Manhattan line segments also provide additional input features that improve the performance, as shown in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network structure</head><p>An overview of the LayoutNet network is illustrated in <ref type="figure">Fig. 2</ref>. The network follows an encoder-decoder strategy. Deep panorama encoder: The input is a 6-channel feature map: the concatenation of single RGB panorama with resolution of 512 ? 1024 (or 512 ? 512 for perspective images) and the Manhattan line feature map lying on three orthogonal vanishing directions using the alignment method in Sec. 3.1. The encoder contains 7 convolution layers with kernel size of 3?3. Each convolution is followed by a ReLU operation and a max pooling layer with the down-sampling factor of 2. The first convolution contains 32 features, and we double size after each convolution. This deep structure ensures a better feature learning from high resolution images and help ease the decoding step. We tried Batch Normalization after each convolution layer but observe lower accuracy. We also explored an alternative structure that applies a separate encoder for the input image and the Manhattan lines, but observe no increase in performance compared to our current simpler design.</p><p>2D layout decoder: The decoder consists of two branches as shown in <ref type="figure">Fig. 2</ref>. The top branch, the layout boundary map (m E ) predictor, decodes the bottleneck feature into the 2D feature map with the same resolution as the input. m E is a 3-channel probability prediction of wall-wall, ceilingwall and wall-floor boundary on the panorama, for both visible and occluded boundaries. The boundary predictor contains 7 layers of nearest neighbor up-sampling operation, each followed by a convolution layer with kernel size of 3 ? 3, and the feature size is halved through layers from 2048. The final layer is a Sigmoid operation. We add skip connections to each convolution layer following the spirit of the U-Net structure <ref type="bibr" target="#b25">[26]</ref>, in order to prevent shifting of predictions results from the up-sampling step. The lower branch, the 2D layout corner map (m C ) predictor, follows the same structure as the boundary map predictor and additionally receives skip connections from the top branch for each convolution layer. This stems from the intuition that layout boundaries imply corner positions, especially for the case when a corner is occluded. We show in our experiments (Sec. 4) that the joint prediction helps improve the accuracy of the both maps, leading to a better 3D reconstruction result. We experimented with fully convolutional layers <ref type="bibr" target="#b20">[21]</ref> instead of the up-sampling plus convolutions structure, but observed worse performance with checkerboard artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D layout regressor:</head><p>The function to map from 2D corners and boundaries to 3D layout parameters is simple mathematically, but difficult to learn. So we train a regressor for 3D layout parameters with the purpose of producing better corners and boundaries, rather than for its own sake. As shown in <ref type="figure">Fig. 2</ref>, the 3D regressor gets as input the concatenation of the two predicted 2D maps and predicts the parameters of the 3D layout. We parameterize the layout with 6 parameters, assuming the ground plane is aligned on the x ? z axis: width s w , length s l , height s h , translation T = (t x , t z ) and rotation r ? on the x ? z plane. The regressor follows an encoder structure with 7 layers of convolution with kernel size 3 ? 3, each followed by a ReLU operation and a max pooling layer with the down sampling factor of 2. The convolution feature size doubles through layers from the input 4 feature channel. The next four fully-connected layers have sizes of 1024, 256, 64, and 6, with ReLU in between. The output 1 ? 6 feature vector d = {s w , s l , s h , t x , t z , r ? } is our predicted 3D cuboid parameter. Note that the regressor outputs the parameters of the 3D layout that can be projected back to the 2D image, presenting an end-to-end prediction approach. We observed that the 3D regressor is not accurate (with corner error of 3.36% in the PanoContext dataset compared with other results in <ref type="table">Table 1</ref>), but including it in the loss objective tends to slightly improve the predictions of the network. The direct 3D regressor fails due to the fact that small position shifts in 2D can have a large difference in the 3D shape, making the network hard to train.</p><p>Loss function. The overall loss function of the network is in Eq. 1:</p><formula xml:id="formula_0">L(m E , m C , d) = ?? 1 n p?m E p log p + (1 ?p) log(1 ? p) ? ? 1 n q?m C q log q + (1 ?q) log(1 ? q) + ? d ?d 2<label>(1)</label></formula><p>The loss is the summation over the binary cross entropy error of the predicted pixel probability in m E and m C compared to ground truth, plus the Euclidean distance of regressed 3D cuboid parameters d to the ground truthd. p is the probability of one pixel in m E , andp is the ground truth of p in m E . q is the pixel probability in m C , andq is the ground truth. n is the number of pixels in m E and m C which is the image resolution. Note that the RoomNet approach <ref type="bibr" target="#b15">[16]</ref> uses L2 loss for corner prediction. We discuss the performance using two different losses in Sec. 4. ?, ? and ? are the weights for each loss term. In our experiment, we set ? = ? = 1 and ? = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training details</head><p>Our LayoutNet predicts pixel probabilities for corners and boundaries and regresses the 3D layout parameters. We find that joint training from a randomly initialized network sometimes fails to converge. Instead, we train each subnetwork separately and then jointly train them together. For the 2D layout prediction network, we first train on the layout boundary prediction task to initialize the parameters of the network. For the 3D layout regressor, we first train the network with ground truth layout boundaries and corners as input, and then connect it with the 2D layout decoder and train the whole network end-to-end.</p><p>The input Manhattan line map is a 3 channel 0-1 tensor. We normalize each of the 3D cuboid parameter into zero mean and standard deviation across training samples. We use ADAM <ref type="bibr" target="#b14">[15]</ref> to update network parameters with a learning rate of e ?4 , ? = 0.95 and = e ?6 . The batch size for training the 2D layout prediction network is 5 and changes to 20 for training the 3D regressor. The whole end-to-end training uses a batch size of 20. Ground truth smoothing: Our target 2D boundary and corner map is a binary map with a thin curve or point on the image. This makes training more difficult. For example, if the network predicts the corner position slightly off the ground truth, a huge penalty will be incurred. Instead, we dilate the ground truth boundary and corner map with a factor of 4 and then smooth the image with a Gaussian kernel of 20 ? 20. Note that even after smoothing, the target image still contains95% zero values, so we re-weight the back propagated gradients of the background pixels by multiplying with 0.2. for j = 1 : |L i | do 7:</p><p>Sample candidate Layouts L ij by varying floor and ceiling position in 3D; <ref type="bibr">8:</ref> Rank the best scored Layout L B ? {L ij } based on Eq. 3; <ref type="bibr">9:</ref> if E best &lt; Score(L B ) then 10:</p><formula xml:id="formula_1">E best = Score(L B ), L best = L B ; 11:</formula><p>Update w i from L best , fix it for following sampling return L best</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">3D layout optimization</head><p>The initial 2D corner predictions are obtained from the corner probability maps that our network outputs. First, the responses are summed across rows, to get a summed response for each column. Then, local maxima are found in the column responses, with distance between local maxima of at least 20 pixels. Finally, the two largest peaks are found along the selected columns. These 2D corners might not satisfy Manhattan constraints, so we perform optimization to refine the estimates.</p><p>Given the predicted corner positions, we can directly recover the camera position and 3D layout, up to a scale and translation, by assuming that bottom corners are on the same ground plane and that the top corners are directly above the bottom ones. We can further constrain the layout shape to be Manhattan, so that intersecting walls are perpendicular, e.g. like a cuboid or "L"-shape in a topdown view. For panoramic images, the Manhattan constraints can be easily incorporated, by utilizing the characteristic that the columns of the panorama correspond to rotation angles of the camera. We parameterize the layout coordinates in the top-down view as a vector of 2D points L v = {v 1 = (0, 0), v 2 = (x 1 , y 1 ), . . . , v N = (x N , y N )}. v 1 resolves the translation ambiguity, and |v 1 ? v 2 | = 1 sets the scale. Because the layout is assumed to be Manhattan, neighboring vertices will share one coordinate value, which further reduces the number of free parameters. We recover the camera position v c = {x c , y c } and L v based on the following generalized energy minimization inspired by Farin et al. <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_2">E(L v , v c ) = min vc,Lv (i,j)?Lv |?(v i , v j ) ? ?(v i , v j )| (2)</formula><p>where v i , v j are pairs of neighboring vertices, and ? ij = arccos v i ?vc?v j ?vc v i ?vc v j ?vc is the rotation angle of the camera v c between v i and v j . We denote ? ij as the pixel-wise horizontal distance on the image between v i and v j divided by the length of the panorama. Note that this L2 minimization also applies to general Manhattan layouts. We use L-BFGS <ref type="bibr" target="#b34">[35]</ref> to solve for Eq. 2 efficiently.</p><p>We initialize the ceiling level as the average (mean) of 3D upper-corner heights, and then optimize for a better fitting room layout, relying on both corner and boundary information using the following score to evaluate 3D layout candidate L:</p><formula xml:id="formula_3">Score(L) = w junc lc?C log P corner (l c ) + w ceil le?Le max log P ceil (l e ) + w f loor l f ?L f max log P floor (l f )<label>(3)</label></formula><p>where C denotes the 2D projected corner positions of L. Cardinality of L is #walls? 2. We connect the nearby corners on the image to obtain L e which is the set of projected wall-ceiling boundaries, and L f which is the set of projected wall-floor boundaries (each with cardinality of #walls). P corner (?) denotes the pixel-wise probability value on the predicted m C . P ceil (?) and P floor (?) denote the probability on m E . The 2nd and 3rd term take the maximum value of log likelihood response in each boundary l e ? L e and l f ? L f . w junc , w ceil and w f loor are the term weights, we set to 1.0, 0.5 and 1.0 respectively using grid search. This weighting conforms with the observation that wallfloor corners are often occluded, and the predicted boundaries could help improve the layout reconstruction. We find that adding wall-wall boundaries in the scoring function helps less, since the vertical pairs of predicted corners already reveals the wall-wall boundaries information. Directly optimizing Eq. 3 is computationally expensive, since we penalize on 2D projections but not direct 3D properties. In this case, we instead sample candidate layout shapes and select the best scoring result based on Eq. 3. We use line search to prune the candidate numbers to speed up the optimization. Algorithm 1 demonstrates the procedure.</p><p>In each step, we sample candidate layouts by shifting one of the wall position within ?%10 of its distance to the camera center. Each candidate's ceiling and floor level is then optimized based on the same sampling strategy and scored based on Eq. 3. Once we find the best scored layout by moving one of the walls, we fix this wall position, move to the next wall and perform the sampling again. We start from the least confident wall based on our boundary predictions. In total, ? 1000 layout candidates are sampled. The optimization step spends less then 30 sec for each image and produces better 3D layouts as demonstrated in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Extensions</head><p>With small modifications, our network, originally designed to predict cuboid layouts from panoramas, can also predict more general Manhattan layouts from panoramas and cuboid-layouts from perspective images. General Manhattan layouts: To enable more general layouts, we include training examples that have more than four walls visible (e.g. "L"-shaped rooms), which applies to about 10% of examples. We then determine whether to generate four or six walls by thresholding the score of the sixth strongest wall-wall boundary. Specifically, the average probability along the sixth strongest column of the corner map is at least 0.05. In other words, if there is evidence for more than four walls, our system generates additional walls; otherwise it generates four. Since the available test sets do not have many examples with more than four walls, we show qualitative results with our additional captured samples in Sec. 4.2 and in the supplemental material.</p><p>Note that there will be multiple solutions given noncuboid layout when solving Eq. 2. We experimented with predicting a concave/convex label as part of the corner map prediction to obtain single solution, but observed degraded 2D prediction. We thus enumerate all possible shapes (e.g. for room with six walls, there will be six variations) and choose the one with the best score. We found this heuristic search to be efficient as it searches in a small discrete set. We do not train with the 3D parameter regressor for the non-cuboid layout. Perspective images: When predicting on perspective images, we skip the alignment and optimization steps, instead directly predicting corners and boundaries on the image. We also do not use the 3D regressor branch. The network predicts a 3-channel boundary layout map with ceiling-wall, wall-wall and wall-floor boundaries, and the corner map has eight channels for each possible corner. Since perspective images have smaller fields of view and the number of visible corners varies, we add a small decoding branch that predicts the room layout type, similar to RoomNet <ref type="bibr" target="#b15">[16]</ref>. The predictor has 4 fully-connected (fc) layers with 1024, 256, 64 and 11 nodes, with ReLU operations in between. The predicted layout type then determines which corners are detected, and the corners are localized as the most probable positions in the corner maps. We use cross entropy loss to jointly train the layout boundary and corner predictors. To ease training, similar to the procedure in Sec. 3.3, we first train the boundary/corner predictors, and then add the type predictor branch and train all components together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We implement our LayoutNet with Torch and test on a single NVIDIA Titan X GPU. The layout optimization is implemented with Matlab R2015a and is performed on Linux machine with Intel Xeon 3.5G Hz in CPU mode.</p><p>We demonstrate the effectiveness of our approach on the following tasks: 1) predict 3D cuboid layout from a single panorama, 2) estimate 3D non-cuboid Manhattan layout from a single panorama, and 3) estimate layout from a single perspective image. We train only on the training split of each public dataset and tune the hyper-parameters on the validation set. We report results on the test set. Our final corner/boundary prediction from the LayoutNet is averaged over results with input of the original panoramas/images and the left-right flipped ones. Please find more results in the supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cuboid layout for panorama</head><p>We evaluate our approach on three standard metrics: 2. Corner error, the L2 distance between predicted room corner and the ground truth, normalized by the image diagonal and averaged across all images; 3. Pixel error, the pixel-wise accuracy between the layout and the ground truth, averaged across all images. We perform our method using the same hyper-parameter on the following two datasets. PanoContext dataset: The PanoContext dataset <ref type="bibr" target="#b32">[33]</ref> contains 500 annotated cuboid layouts of indoor environments such as bedrooms and living rooms. Since there is no existing validation set, we carefully split 10% validation images from the training samples so that similar rooms do not appear in the training split. <ref type="table">Table 1</ref> shows the quantitative comparison of our method, denoted as "ours full (corner+boundary+3D)", compared with the state-ofthe-art cuboid layout estimation by Zhang et al. <ref type="bibr" target="#b32">[33]</ref>, denoted as "PanoContext". Note that PanoContext incorporates object detection as a factor for layout estimation. Our LayoutNet directly recovers layouts and outperforms the state-of-the-art on all the three metrics. <ref type="figure">Figure 3</ref> shows the qualitative comparison. Our approach presents better localization of layout boundaries, especially for a better estimate on occluded boundaries, and is much faster in time as shown in <ref type="table" target="#tab_0">Table 2</ref>. Our labeled Stanford 2D-3D annotation dataset: The dataset contains 1413 equirectangular RGB panorama collected in 6 large-scale indoor environment including office and classrooms and open space like corridors. Since the dataset does not contain applicable layout annotations, we extend the annotations with carefully labeled 3D cuboid shape layout, providing 571 RGB panoramas with room layout annotations. We evaluate our LayoutNet quantitatively in <ref type="table">Table 3</ref> and qualitatively in <ref type="figure">Figure 4</ref>. Although the Stanford 2D-3D annotation dataset is more challenging with smaller vertical field of view (FOV) and more occlusions on the wall-floor boundaries, our LayoutNet recovers the 3D layouts well. Ablation study: We show, in <ref type="table">Table 1</ref> and <ref type="table">Table 3</ref>, the performance given the different configurations of our approach: 1) with only room corner prediction, denoted as  <ref type="figure">Figure 3</ref>. Qualitative results (randomly sampled) for cuboid layout prediction on PanoContext dataset <ref type="bibr" target="#b32">[33]</ref>. We show both our method's performance (even columns) and the state-of-the-art <ref type="bibr" target="#b32">[33]</ref> (odd columns). Each image consists predicted layout from given method (orange lines) and ground truth layout (green lines). Our method is very accurate on the pixel level, but as the IoU measure shows in our quantitative results, the 3D layout can be sensitive to even small 2D prediction errors. Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PanoContext Ours PanoContext Ours</head><p>PanoContext Ours PanoContext Ours PanoContext Ours PanoContext Ours <ref type="figure">Figure 4</ref>. Qualitative results (randomly sampled) for cuboid layout prediction on the Stanford 2D-3D annotation dataset. This dataset is more challenging than the PanoContext dataset, due to a smaller vertical field of view and more occlusion. We show our method's predicted layout (orange lines) compared with the ground truth layout (green lines). Best viewed in color.</p><p>"ours (corner)"; 2) joint prediction of corner and boundary, denoted as "ours (corner+boundary)"; 3) our full approach with 3D layout loss, denoted as "ours full (cor-ner+boundary+3D)"; 4) our full approach trained on a combined dataset; 5) our full approach without alignment step; 6) our full approach without cuboid constraint; 7) our full approach without layout optimization step; and 8) our full approach using L2 loss for boundary/corner prediction instead of cross entropy loss. Our experiments show that the full approach that incorporates all configurations performs better across all the metrics. Using cross entropy loss appears to have a better performance than using L2. Training with 3D regressor has a small impact, which is the part of the reason we do not use it for perspective images.   consumption, Yang et al. report to be less than 1 minute, Pano2CAD takes 30s to process one room. One forward pass of LayoutNet takes 39ms. In CPU mode (w/o parallel for loop) using Matlab R2015a, our cuboid constraint takes 0.52s, alignment 13.73s, and layout optimization 30.5s. <ref type="figure" target="#fig_2">Figure 6</ref> shows qualitative results of our approach to reconstruct non-cuboid Manhattan layouts from single panorama. Due to the limited number of non-cuboid room layouts in the existing datasets, we captured several images using a Ricoh Theta-S 360 ? camera. Our approach is able to predict 3D room layouts with complex shape that are difficult for existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Non-cuboid layout for panorama</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Perspective images</head><p>We use the same experimental setting as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. We train our modified approach to jointly predict room type on the training split of the LSUN layout estimation challenge. We do not train on the validation split. <ref type="table">Table 5</ref> shows our performance compared with the stateof-the-art on Hedau's dataset <ref type="bibr" target="#b10">[11]</ref>. Our method ranks second among the methods. Our method takes 39ms <ref type="bibr">(25 FPS)</ref> to process a perspective image, faster than the 52ms <ref type="bibr">(19 Method</ref> Pixel Error (%) Schwing et al. <ref type="bibr" target="#b26">[27]</ref> 12.8 Del Pero et al. <ref type="bibr" target="#b5">[6]</ref> 12.7 Dasgupta et al. <ref type="bibr" target="#b3">[4]</ref> 9.73 LayoutNet (ours) 9.69 RoomNet recurrent 3-iter <ref type="bibr" target="#b15">[16]</ref> 8.36 <ref type="table">Table 5</ref>. Performance on Hedau dataset <ref type="bibr" target="#b10">[11]</ref>. We show the top 5 results, LayoutNet ranks second to RoomNet recurrent 3-iter in Pixel Error (%).</p><p>FPS) of RoomNet basic <ref type="bibr" target="#b15">[16]</ref> or 168ms (6 FPS) of Room-Net recurrent, under the same hardware configuration. We report the result on LSUN dataset in the supplemental material. <ref type="figure" target="#fig_1">Figure 5</ref> shows qualitative results on the LSUN validation split. Failure cases include room type prediction error (last row, right column) and heavy occlusion from limited field of view (last row, left column).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose LayoutNet, an algorithm that predicts room layout from a single panorama or perspective image. Our approach relaxes the commonly assumed cuboid layout limitation and works well with non-cuboid layouts (e.g. "L"shape room). We demonstrate how pre-aligning based on vanishing points and Manhattan constraints substantially improve the quantitative results. Our method operates directly on panoramic images (rather than decomposing into perspective images) and is among the state-of-the-art for the perspective image task. Future work includes extending to handle arbitrary room layouts, incorporating object detection for better estimating room shapes, and recovering a complete 3D indoor model recovered from single images. <ref type="table">Table 6</ref> shows our performance compared with the stateof-the-art on the LSUN dataset <ref type="bibr" target="#b8">[9]</ref>. Our method ranks second in Keypoint Error (%) and ranks third in Pixel Error (%) among the methods. We also report results of the Room-Net basic approach <ref type="bibr" target="#b15">[16]</ref> that does not apply recurrent refinement, which is closer in design to our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative Results on LSUN layout Challenge [9]</head><p>The lower accuracy in pixel error mainly results from our simplified room keypoint representation. Different from RoomNet <ref type="bibr" target="#b15">[16]</ref> that assumes all keypoints are distinguished across different room types, our LayoutNet directly predicts the 8 keypoints, and selects among them based on the room type to produce the final prediction. Applying the layout optimization step as explained in the paper could possibly further enhance our performance on the perspective image task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Keypoint Error (%) Pixel Error (%) Hedau et al. <ref type="bibr" target="#b10">[11]</ref> 15.48 24.23 Mallya et al. <ref type="bibr" target="#b21">[22]</ref> 11.02 16.71 Dasgupta et al. <ref type="bibr" target="#b3">[4]</ref> 8.20 10.63 LayoutNet (ours) 7.63 11.96 RoomNet recurrent 3-iter <ref type="bibr" target="#b15">[16]</ref> 6.30 9.86 RoomNet basic <ref type="bibr" target="#b15">[16]</ref> 6.95 10.46 <ref type="table">Table 6</ref>. Performance on LSUN dataset <ref type="bibr" target="#b8">[9]</ref>. LayoutNet ranks second to RoomNet recurrent 3-iter in Keypoint Error (%) and ranks third in Pixel Error (%). We also report the RoomNet basic approach that does not apply recurrent refinement step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Non-cuboid layout from panorama</head><p>We show more qualitative results of non-cuboid room layout reconstruction from single panorama as in <ref type="figure" target="#fig_3">Figure 7</ref>. We use samples from the dataset collected by Yang et al. <ref type="bibr" target="#b30">[31]</ref>. We exclude samples that overlap with the PanoContext dataset <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Cuboid layout from panorama</head><p>We show more qualitative results in PanoContext dataset <ref type="bibr" target="#b32">[33]</ref> in <ref type="figure">Figure 8</ref> and <ref type="figure">Figure 9</ref>. We compare our method with the state-of-the-art.</p><p>We show more qualitative results in our labeled Stanford 2D-3D annotation dataset compared with our ground truth annotation, as shown in <ref type="figure">Figure 10</ref> and <ref type="figure">Figure 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Perspective images</head><p>We show more qualitative results on the LSUN layout Challenge <ref type="bibr" target="#b8">[9]</ref> compared with the ground truth annotation, as shown in <ref type="figure">Figure 12</ref>. Qualitative results for non-cuboid layout prediction. We show our method's predicted layout (orange lines) for non-cuboid layouts such as "L"-shaped rooms. Best viewed in color.  <ref type="figure">Figure 8</ref>. Qualitative results for cuboid layout prediction on PanoContext dataset <ref type="bibr" target="#b32">[33]</ref>. We show both our method's performance (even columns) and the state-of-the-art <ref type="bibr" target="#b32">[33]</ref> (odd columns). Each image consists predicted layout from given method (orange lines) and ground truth layout (green lines). Best viewed in color.  <ref type="figure">Figure 9</ref>. Qualitative results for cuboid layout prediction on PanoContext dataset <ref type="bibr" target="#b32">[33]</ref>. We show both our method's performance (even columns) and the state-of-the-art <ref type="bibr" target="#b32">[33]</ref> (odd columns). Each image consists predicted layout from given method (orange lines) and ground truth layout (green lines). Best viewed in color.  <ref type="figure">Figure 10</ref>. Qualitative results (randomly sampled) for cuboid layout prediction on the Stanford 2D-3D annotation dataset. This dataset is more challenging than the PanoContext dataset, due to a smaller vertical field of view and more occlusion. We show our method's predicted layout (orange lines) compared with the ground truth layout (green lines). Best viewed in color.  <ref type="figure">Figure 11</ref>. Qualitative results (randomly sampled) for cuboid layout prediction on the Stanford 2D-3D annotation dataset. This dataset is more challenging than the PanoContext dataset, due to a smaller vertical field of view and more occlusion. We show our method's predicted layout (orange lines) compared with the ground truth layout (green lines). Best viewed in color.  <ref type="figure">Figure 12</ref>. Qualitative results for perspective images. We show the input RGB image, our predicted boundary/corner map and the final estimated layout (orange lines) compared with ground truth (green lines). Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Data augmentation: We use horizontal rotation, left-right flipping and luminance change to augment the training samples. The horizontal rotation varies from 0 o ? 360 o . The luminance varies with ? values between 0.5-2. For perspective images, we apply ?10 ? rotation on the image plane. Algorithm 1 3D layout optimization 1: Given panorama I, layout corner prediction m C , and boundary prediction m E ; 2: Initialize 3D layout L 0 based on Eq. 2; 3: E best = Score(L 0 ) by Eq. 3, L best = L 0 ; 4: for i = 1 :wallNum do 5: Sample candidate layouts L i by varying wall position w i in 3D, fix other wall positions; 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results for perspective images. We show the input RGB image, our predicted boundary/corner map and the final estimated layout (orange lines) compared with ground truth (green lines). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results for non-cuboid layout prediction. We show our method's predicted layout (orange lines) for noncuboid layouts such as "L"-shaped rooms. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Qualitative results for non-cuboid layout prediction. We show our method's predicted layout (orange lines) for non-cuboid layouts such as "L"-shaped rooms. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>3D IoU (%)</cell><cell>Corner error (%)</cell><cell>Pixel error (%)</cell></row><row><cell>PanoContext [33]</cell><cell>67.23</cell><cell>1.60</cell><cell>4.55</cell></row><row><cell>ours (corner)</cell><cell>73.16</cell><cell>1.08</cell><cell>4.10</cell></row><row><cell>ours (corner+boundary)</cell><cell>73.26</cell><cell>1.07</cell><cell>3.31</cell></row><row><cell>ours full (corner+boundary+3D)</cell><cell>74.48</cell><cell>1.06</cell><cell>3.34</cell></row><row><cell>ours w/o alignment</cell><cell>69.91</cell><cell>1.44</cell><cell>4.39</cell></row><row><cell>ours w/o cuboid constraint</cell><cell>72.56</cell><cell>1.12</cell><cell>3.39</cell></row><row><cell>ours w/o layout optimization</cell><cell>73.25</cell><cell>1.08</cell><cell>3.37</cell></row><row><cell>ours w/ L2 loss</cell><cell>73.55</cell><cell>1.12</cell><cell>3.43</cell></row><row><cell>ours full w/ Stnfd. 2D-3D data</cell><cell>75.12</cell><cell>1.02</cell><cell>3.18</cell></row><row><cell cols="4">Table 1. Quantitative results on cuboid layout estimation from</cell></row><row><cell cols="4">panorama using PanoContext dataset [33]. We compare the</cell></row><row><cell cols="4">PanoContext method, and include an ablation analysis on a va-</cell></row><row><cell cols="4">riety of configurations of our method. Bold numbers indicate the</cell></row><row><cell cols="3">best performance when training on PanoContext data.</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Average CPU time (s)</cell></row><row><cell>PanoContext [33]</cell><cell></cell><cell>&gt; 300</cell><cell></cell></row><row><cell cols="2">ours full (corner+boundary+3D)</cell><cell>44.73</cell><cell></cell></row><row><cell>ours w/o alignment</cell><cell></cell><cell>31.00</cell><cell></cell></row><row><cell cols="2">ours w/o cuboid constraint</cell><cell>13.75</cell><cell></cell></row><row><cell cols="2">ours w/o layout optimization</cell><cell>14.23</cell><cell></cell></row></table><note>. Average CPU time for each method. We evaluate the methods on the PanoContext dataset [33] using Matlab on Linux machine with an Intel Xeon 3.5G Hz (6 cores).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 Table 4 .</head><label>24</label><figDesc>Depth distribution error compared with Yang et al.<ref type="bibr" target="#b30">[31]</ref>.shows the average runtimes for different configurations.Comparison to other approaches: We compare with Yang et al. based on their depth distribution metric. We directly run our full cuboid layout prediction (deep net trained on PanoContext + optimization) on 88 indoor panoramas collected by Yang et al. As shown inTable 4, our approach outperforms Yang et al. in L2 distance and is slightly worse in cosine distance. Another approach, Pano2CAD<ref type="bibr" target="#b29">[30]</ref>, has not made their source code available and has no evaluation on layout, making direct comparison difficult. For time</figDesc><table><row><cell>Method</cell><cell cols="2">L2 dist cosine dist</cell></row><row><cell cols="2">Yang et al. [31] 27.02</cell><cell>4.27</cell></row><row><cell>Ours</cell><cell>18.51</cell><cell>5.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Input RGB LayoutNet boundary LayoutNet corner LayoutNet result Input RGB LayoutNet boundary LayoutNet corner LayoutNet result</head><label></label><figDesc></figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported in part by NSF award 14-21521, ONR MURI grant N00014-16-1-2007, and Zillow Group. We thank Zongyi Wang for his invaluable help with panorama annotation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Piecewise planar and compact floorplan reconstruction from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Manhattan world: Compass direction from a single image by bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="941" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Delay: Robust spatial layout estimation for cluttered indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian geometric modeling of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bowdish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kermgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2719" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding bayesian rooms using composite 3d object models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bowdish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kermgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A dynamic bayesian network model for autonomous 3d reconstruction from a single indoor image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2418" to="2428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Floor-plan reconstruction from panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Farin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Effelsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<biblScope unit="page" from="823" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">LSUN challenge on room</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Group</surname></persName>
		</author>
		<ptr target="http://lsun.cs.princeton.edu/leaderboard/index_2016.html#roomlayout" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Predicting complete 3d models of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.02437</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Thinking inside the box: Using appearance models and context based on room geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="224" to="237" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06241</idno>
		<title level="m">Roomnet: End-to-end room layout estimation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1288" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric reasoning for single image structure recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2136" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Layered scene decomposition via the occlusion-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="165" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rent3d: Floor-plan priors for monocular layout estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3413" to="3421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning informative edge maps for indoor scene layout prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Manhattan junction catalogue for spatial reasoning of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lsd: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="722" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A coarse-to-fine indoor layout estimation (CFILE) method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00598</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>MIC-CAI</editor>
		<imprint>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient structured prediction for 3d indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2815" to="2822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient exact inference for 3d indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="299" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kerola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
		<title level="m">Pano2CAD: Room layout from a single panorama image. WACV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="354" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient 3d room shape recovery from a single panorama</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating the 3d layout of indoor scenes and its clutter from depth sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1273" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Panocontext: A whole-room 3d context model for panoramic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="668" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene parsing by integrating function, geometry and appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3119" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale boundconstrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="550" to="560" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
