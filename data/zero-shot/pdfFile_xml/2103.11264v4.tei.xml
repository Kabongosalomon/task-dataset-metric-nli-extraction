<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporally-Weighted Hierarchical Clustering for Unsupervised Action Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saquib Sarfraz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naila</forename><surname>Murray</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Luc</roleName><forename type="first">Van</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporally-Weighted Hierarchical Clustering for Unsupervised Action Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">, 2 Facebook AI Research, 3 MIT, 4 Harvard Medical School, 5 KU Leuven, 6 ETH Zurich 7 Daimler TSS</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Action segmentation refers to inferring boundaries of semantically consistent visual concepts in videos and is an important requirement for many video understanding tasks. For this and other video understanding tasks, supervised approaches have achieved encouraging performance but require a high volume of detailed frame-level annotations. We present a fully automatic and unsupervised approach for segmenting actions in a video that does not require any training. Our proposal is an effective temporally-weighted hierarchical clustering algorithm that can group semantically consistent frames of the video. Our main finding is that representing a video with a 1-nearest neighbor graph by taking into account the time progression is sufficient to form semantically and temporally consistent clusters of frames where each cluster may represent some action in the video. Additionally, we establish strong unsupervised baselines for action segmentation and show significant performance improvements over published unsupervised methods on five challenging action segmentation datasets. Our code is available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human behaviour understanding in videos has traditionally been addressed by inferring high-level semantics such as activity recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b2">3]</ref>. Such works are often limited to tightly clipped video sequences to reduce the level of labelling ambiguity and thus make the problem more tractable. However, a more fine-grained understanding of video content, including for un-curated content that may be untrimmed and therefore contain a lot of material unrelated to human activities, would be beneficial for many downstream video understanding applications. Consequently, the less-constrained problem of action segmentation in untrimmed videos has received increasing attention. Action segmentation refers to labelling each frame of a video with an action, where the sequence of actions is usually performed by a human engaged in a high-level activity such as making coffee (illustrated in <ref type="figure">Figure 1</ref>). Action segmentation is more challenging than activity recognition of trimmed videos for several reasons, including the presence of background frames that don't depict actions of relevance to the high-level activity. A major challenge is the need for significantly more detailed annotations for supervising learning-based approaches. For this reason, weaklyand unsupervised approaches to action segmentation have gained popularity <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>. Some approaches have relied on natural language text extracted from accompanying audio to provide frame-based action labels for training action segmentation models <ref type="bibr" target="#b1">[2]</ref>. This of course makes the strong assumption that audio and video frames are wellaligned. Other approaches assume some a priori knowledge of the actions, such as the high-level activity label or the list of actions depicted, in each video <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34]</ref>. Even this level of annotation however, requires significant annotation effort for each training video as not all activities are performed using the same constituent actions.</p><p>Most weakly-and unsupervised methods, whatever their degree of a priori knowledge, focus on acquiring pseudolabels that can be used to supervise training of task-specific feature embeddings <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. As pseudolabels are often quite noisy, their use may hamper the efficacy of the learned embeddings. In this work, we adopt the view that action segmentation is fundamentally a grouping problem, and instead focus on developing clustering methods that effectively delineate the temporal boundaries between actions. This approach leads to an illuminating finding: for action segmentation, a simple clustering (e.g., with Kmeans) of appearance-based frame features achieves performance on par with, and in some cases superior to, SoTA weakly-supervised and unsupervised methods that require training on the target video data (please refer to section 3 for details). This finding indicates that a sufficiently discriminative visual representation of video frames can be used to group frames into visually coherent clusters. However, for action segmentation, temporal coherence is also critically Ours GT ? Take cup Spoon Powder Pour Milk Stir Milk ? <ref type="figure">Figure 1</ref>. Segmentation output example from Breakfast Dataset <ref type="bibr" target="#b13">[14]</ref>: P46 webcam02 P46 milk. Colors indicate different actions in chronological order: ?, take cup, spoon powder, pour milk, stir milk, ?, where ? is background shown in while color.</p><p>important. Building on these insights, we adapt a hierarchical graph-based clustering algorithm to the task of temporal video segmentation by modulating appearance-based graph edges between frames by their temporal distances. The resulting spatio-temporal graph captures both visually-and temporally-consistent neighbourhoods of frames that can be effectively extracted. Our work makes the following main contributions:</p><p>? We establish strong appearance-based clustering baselines for unsupervised action segmentation that outperform SoTA models;</p><p>? We propose to use temporally-modulated appearancebased graphs to represent untrimmed videos;</p><p>? We combine this representation with a hierarchical graph-based clustering algorithm in order to perform temporal action segmentation.</p><p>Our proposed method outperforms our strong baselines and existing SOTA unsupervised methods by a significant margin on 5 varied and challenging benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There exists a large body of work on spatial and spatiotemporal action recognition in videos (see <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b2">3]</ref> for recent surveys). In this section we review works related to our problem of interest, temporal action segmentation, focusing on weakly-and unsupervised methods.</p><p>Most existing temporal action segmentation methods, be they fully supervised <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17]</ref>, weakly supervised <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11]</ref> or unsupervised <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2]</ref>, use frame-level annotations to train their models. They differ in whether the annotations are collected by human annotators or extracted in a semi-or unsupervised manner. These models largely follow a paradigm in which an embedding is trained on top of pre-extracted frame-level video features, such as I3D <ref type="bibr" target="#b4">[5]</ref>, as in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11]</ref>, or hand-crafted video features such as improved dense trajectories IDT <ref type="bibr" target="#b37">[38]</ref>, as in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref>. To train the embedding, a discriminative objective function is used in conjunction with the collected annotations <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. Weaklysupervised and unsupervised methods, discussed next, vary largely in the manner in which they extract and exploit pseudo-labels.</p><p>Weakly-supervised methods generally assume that both the video-level activity label and the ordering of actions, termed transcripts, are known during training. Some weakly-supervised works have a two-stage training process where pseudo-labels are first generated using transcripts and then used to train a frame classification network <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26]</ref>. In contrast, the method NN-Vit <ref type="bibr" target="#b27">[28]</ref> directly leverages transcripts while learning a frame classification model. For this they introduce a loss based on Viterbi decoding that enforces consistency between frame-level label predictions. In a similar spirit, a recent proposal called MuCoN <ref type="bibr" target="#b33">[34]</ref> aims to leverage transcripts while learning a frame classification model. They learn two network branches, only one of which has access to transcripts, while ensuring that both branches are mutually consistent. Another recent method called CDFL <ref type="bibr" target="#b21">[22]</ref> also aims to use transcripts when training their frame labelling model. They first build a fullyconnected, directed segmentation graph whose paths represent actions. They then train their model by maximizing the energy difference between valid paths (i.e paths that are consistent with the ground-truth transcript) and invalid ones. In SCT <ref type="bibr" target="#b10">[11]</ref>, the authors assume that the set of action labels for a given video, but not their order, is known. They determine the ordering and temporal boundaries of the actions by alternatively optimizing set and frame classification objectives to ensure that frame-level action predictions are consistent with the set-level predictions.</p><p>Unsupervised methods generally assume knowledge only of the video-level activity label <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">36]</ref>. In Mallow <ref type="bibr" target="#b31">[32]</ref>, the authors use video-level annotations in an iterative approach to action segmentation, alternating optimization of a discriminative appearance model and a generative temporal model of action sequences. In Frank-Wolfe <ref type="bibr" target="#b1">[2]</ref>, video narrations are extracted using ASR and used to extract an action sequence for a set of videos of an activity. This is accomplished by separately clustering the videos and the ASR-recovered speech to identify action verbs in the spe-cific video. Temporal localization is then obtained by training a linear classifier. CTE <ref type="bibr" target="#b17">[18]</ref> proposes to learn frame embeddings that incorporate relative temporal information. They train a video activity model using pseudo-labels generated from Kmeans clustering of the videos' IDT features. The trained embeddings are then re-clustered at the groundtruth number of actions and ordered using statistics of the relative time-stamps with a GMM+Viterbi decoding. VTE-UNET <ref type="bibr" target="#b35">[36]</ref> uses similarly learned embeddings in combination with temporal embeddings to improve upon <ref type="bibr" target="#b17">[18]</ref>. Another interesting approach is LSTM+AL <ref type="bibr" target="#b0">[1]</ref>, which finetunes a pre-trained VGG16 model with an LSTM, using future frame prediction as a self-supervision objective, to learn frame embeddings. These embeddings are then used to train an action boundary detection model.</p><p>All of these methods require training on the target video dataset, which from a practical standpoint is a very restrictive requirement. In contrast, our method does not require any training, and relies only on frame clustering to segment a given video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>As mentioned in the introduction, unsupervised temporal video segmentation is inherently a grouping and/or clustering problem. We observe that, given a relatively good video frame representation, the boundaries of actions in a video are discernible without the need for further training on objectives that use noisy pseudo-labels, something that almost all current methods pursue. To substantiate this observation and to have a basis for our later discussion we provide results of directly clustering a commonly used untrimmed video benchmark (Breakfast dataset <ref type="bibr" target="#b13">[14]</ref> with 1712 videos) in <ref type="table" target="#tab_0">Table 1</ref>. The goal of clustering is to group the frames of each video into its ground-truth actions. We consider two representative clustering methods: (1) Kmeans <ref type="bibr" target="#b22">[23]</ref>, representing centroid-based methods; and (2) a recent proposal called FINCH <ref type="bibr" target="#b30">[31]</ref>, representing state-of-the-art hierarchical agglomerative clustering methods. We cluster the extracted 64-dim IDT features of each video to its required number of actions (clusters). The performance is computed by mapping the estimated cluster labels of each video to the ground-truth labels using the Hungarian method, and the accuracy is reported as mean over frames (MoF). Section 4 contains more details about the experimental setup. As can be seen, simple clustering baselines Kmeans/FINCH performs at par with the best reported weakly/un-supervised methods in this video level evaluation. These results establish new, strong baselines for temporal video segmentation, and suggest that focusing on more specialized clustering techniques may be promising.</p><p>Among existing clustering methods, hierarchical clustering methods such as <ref type="bibr" target="#b30">[31]</ref> are an attractive choice for the task at hand, as such methods provide a hierarchy of par-Weakly Sup.</p><p>Unsupervised CDFL <ref type="bibr" target="#b21">[22]</ref> LSTM+AL <ref type="bibr">[</ref> titions of the data as opposed to a single partition. In this paper we adopt a hierarchical clustering approach to action segmentation that does not require video-level activity labels. In contrast, the existing body of work requires not only such prior knowledge but also requires training on the target video data. The ability to generate a plausible video segmentation without relying on training is highly desirable from a practical standpoint. To the best of our knowledge there is no existing prior work that addresses this challenging and practical scenario. Our proposal is similar in spirit to the FINCH <ref type="bibr" target="#b30">[31]</ref> algorithm. The authors in <ref type="bibr" target="#b30">[31]</ref> make use of the observation that the nearest and the shared neighbor of each sample can form large linking chains in the data. They define an adjacency matrix that links all samples to their nearest first neighbour, thus building a 1-nearest neighbor (1-NN) graph. They showed that the connected components of this adjacency graph partitions the data into fixed clusters. A recursive application of this on the obtained partition(s) yields a hierarchy of partitions. The algorithm typically provides hierarchical partitions of the data in only a few recursion steps.</p><p>Based on this observation of finding linking chains in the data with nearest or shared neighbours, we propose a similar hierarchical clustering procedure for the problem of temporal video segmentation. We propose to use a spatiotemporal graphical video representation by linking frames based on their feature space proximity and their respective positions in time. In particular, we would like this representation to encode both feature-space and temporal proximity. We achieve this by using time progression as a modulating factor when constructing the graph.</p><p>For a video with N frames X = {x 1 , x 2 , ? ? ? , x N }, we define a directed graph G = (V, E) with edges describing the proximity of frames in feature space and time. We construct G by computing the frames' feature space distances and then modulating them by their respective temporal positions, using the following:</p><formula xml:id="formula_0">G f (i, j) = 1 ? x i , x j if i = j 1 otherwise (1)</formula><p>where G f represents a graph with edge weights computed in the feature-space. The inner product is computed on L2-normalized feature vectors to ensure that the distance is in the [0, 1] range. A similar graph G t is defined in the time-space, and edge weights are computed from the time-stamps. For N frames the time-stamps are defined as T = {1, 2, ? ? ? , N }, and the edge weights are computed as:</p><formula xml:id="formula_1">G t (i, j) = |t i ? t j |/ N if i = j 1 otherwise (2)</formula><p>The edges in Equation 2 represent the temporal difference between the nodes, weighted by the total length of the sequence. Because we want to use the temporal graph as a modulating factor for the feature-space graph, The term |t i ? t j |/ N provides a weighing mechanism relative to the sequence length. We then compute temporally-modulated appearance-based distances as follows:</p><formula xml:id="formula_2">W (i, j) = G f (i, j) ? G t (i, j).<label>(3)</label></formula><p>W (i, j) therefore specifies the temporally weighted distance between graph nodes (i.e. frames) i and j. Finally, from this we construct a 1-NN graph by keeping only the closest node to each node (according to W (i, j)) and setting all other edges to zero.</p><formula xml:id="formula_3">G(i, j) = 0 if W (i, j) &gt; min ?j W (i, j) 1 otherwise (4)</formula><p>The 1-NN temporal graph G defines an adjacency matrix where each node is linked to its closest neighbor according to the temporally weighted distances W . For all non-zero edges G(i, j), we make the links symmetric by setting G(j, i) = 1. This results in a symmetric sparse matrix that encodes both feature space and temporal distances, and whose connected components form clusters. Note that Equation 4 only creates absolute links in the graph and we do not perform any additional graph segmentation steps. In contrast, popular methods that build similar nearestneighbour graphs, such as spectral clustering <ref type="bibr" target="#b36">[37]</ref>, need to solve a graph-cut problem that involves solving an eigenvalue decomposition and thus have cubic complexities.</p><p>The connected components of the graph in Equation 4 automatically partition the data into discovered clusters. We use a recursive procedure to obtain further successive groupings of this partition. Each step forms groups of previously-obtained clusters, and the recursion terminates when only one cluster remains. Because in each recursion the graph's connected components form larger linking chains <ref type="bibr" target="#b30">[31]</ref>, in only a few recursions a small set of hierarchical partitions can be obtained, where each successive partition contains clusters of the previous partition's clusters.</p><p>The main steps of the proposed algorithm are shown in Algorithm 1. After computing the temporal 1-NN graph through Equations 1-4, its connected components provide the first partition. We then merge these clusters recursively based on the cluster averages of features and time-stamps. <ref type="figure">Figure 2</ref>. Segmentation output on a video from Breakfast Dataset <ref type="bibr" target="#b13">[14]</ref>: Our method provides more accurate segment lengths of actions occurring in this video.</p><p>Algo. 1 produces a hierarchy of partitions where each successive partition has fewer clusters. To provide the required number of clusters K we choose a partition in this hierarchy with the minimal number of clusters that is ? K. If the selected partition has more than K clusters, we refine it, one merge at a time as outlined in Algo. 2, until K clusters (i.e. actions) remain.</p><p>Note that since in each successive merge time-stamps represent the average or central time of the cluster, this automatically ensures that merged clusters are highly temporally consistent. This aspect of our proposal is important as it may provides better temporal ordering of actions. In temporal video segmentation, obtaining correct ordering of actions is crucial and quite challenging. Existing SoTA unsupervised methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28]</ref> employ expensive postprocessing mechanisms such as Generalized Mallow models <ref type="bibr" target="#b23">[24]</ref>, Gaussian mixture models and Viterbi decoding to improve the ordering of their predicted action segments. In contrast, because of our temporal weighing, our clustering algorithm inherently produces time-consistent clusters, thus largely preserving the correct lengths of the actions occurring in a video. In <ref type="figure">Figure 2</ref> we visualize the obtained action segments and their order (by mapping the obtained segments under Hungarian matching) on a sample video. This video depicts 4 ground-truth clusters and has ? 800 frames. The first step of Algo. 1 (lines 4-5) provides a partition of these 800 frames with 254 clusters. The successive merges of this partitioning produce 3 hierarchical partitions with 67, 20, 3 and 1 cluster(s) and the algorithm stops in only 4 steps. We then use Algo. 2 to obtain the required number of ground-truth action segments, 4, for this video. The partition with the minimal number of clusters ? 4 (in this case partition 3 with 20 clusters) is refined one merge at a time to produce the 4 clusters or action segments. Note that, in direct contrast to Kmeans and FINCH, our temporally-weighted clustering provides better action segments and also preserves their order in the video.</p><p>While we use a similar procedure for hierarchical merges as in FINCH <ref type="bibr" target="#b30">[31]</ref>, our work differs in scope and technical approach. In our proposal we build a temporally modulated 1-NN graph which, unlike FINCH, requires us to use all pairwise distances of samples both in space and in time for building the adjacency matrix. Our method, thus, can be Algorithm 1 Temporally Weighted Clustering Hierarchy 1: Input: Video X = {x 1 , x 2 , ? ? ? , x N }, X ? R N ?d 2: Output: Set of Partitions S = {P 1 , P 2 , ? ? ? , P S } such that P i+1 ? P i ?i ? S. Each partition P i = {C 1 , C 2 , ? ? ? , C Pi } is a valid clustering of X. 3: Initialization: 4: Initialize time-stamps T = {1, 2, ? ? ? , N }. Compute 1-NN temporally weighted graph G via Equation 1-4 5: Get first partition P 1 with C P1 clusters from connectedcomponents of G . <ref type="bibr">6:</ref> while there are at least two clusters in P i do <ref type="bibr">7:</ref> Given input data X and its partition P i prepare averaged data matrix M = {x 1 ,x 2 , ? ? ? ,x C? i } and averaged time-stamps T M = {t 1 ,t 2 , ? ? ? ,t C P i } , where M C P i ?d and T M C P i ?1 . Get partition P M of P i from connected-components of G M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>if P M has one cluster then end if 15: end while considered a special case of FINCH which is well suited for videos. Because of these differences, and for clarity in comparing both, we term our method Temporally Weighted FIrst NN Clustering Hierarchy (TW-FINCH). For action segmentation, TW-FINCH shows clear performances advantages over both Kmeans and FINCH, as we will show next in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the datasets, features, and metrics used to evaluate our TW-FINCH method, before comparing it both to baseline and SoTA approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We conduct experiments on five challenging and popular temporal action segmentation datasets, namely Breakfast (BF) <ref type="bibr" target="#b13">[14]</ref>, Inria Instructional Videos (YTI) <ref type="bibr" target="#b1">[2]</ref>, 50Salads (FS) <ref type="bibr" target="#b34">[35]</ref>, MPII Cooking 2 (MPII) <ref type="bibr" target="#b29">[30]</ref>, and Hollywood Extended (HE) <ref type="bibr" target="#b3">[4]</ref>. As shown in <ref type="table" target="#tab_3">Table 2</ref>, these 5 datasets cover a wide variety of activities (from cooking different types of meals to car maintenance), contain videos of varying lengths (from 520 frames on average to up to 11788 frames), and have different levels of average action granularity (from 3 up to <ref type="bibr" target="#b18">19)</ref>. Features: To ensure a fair comparison to related work, Algorithm 2 Final Action Segmentation 1: Input: # of actions K, Video X = {x 1 , x 2 , ? ? ? , x N } and a partition P i from the output of Algorithm 1. 2: Output: Partition P K with required number of action labels. <ref type="bibr">3:</ref> Merge two clusters at a time: <ref type="bibr">4:</ref> for steps = # of clusters in P i -K do 5:</p><p>Initialize time-stamps T = {1, 2, ? ? ? , N }. Given input data X and its partition P i prepare averaged data matrix M = {x 1 ,x 2 , ? ? ? ,x C P i } and averaged time-stamps T M = {t 1 ,t 2 , ? ? ? ,t C P i }  we use the same input features that were used by recent methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27]</ref>. Specifically, for the BF, FS, MPII, and HE datasets we use the improved dense trajectory (IDT) <ref type="bibr" target="#b37">[38]</ref> features computed and provided by the authors of CTE <ref type="bibr" target="#b17">[18]</ref> (for BF and FS) and SCT <ref type="bibr" target="#b10">[11]</ref> (for MPII and HE). For YTI <ref type="bibr" target="#b1">[2]</ref>, we use the features provided by the authors, which are 3000-dimensional feature vectors formed by a concatenation of HOF <ref type="bibr" target="#b18">[19]</ref> descriptors and features extracted from VGG16-conv5 <ref type="bibr" target="#b32">[33]</ref>. For all datasets, we report performance for the full dataset, consistent with literature. Metrics: To evaluate the temporal segmentation, we require a one-to-one mapping between the predicted segments and ground-truth labels. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27]</ref>, we generate such a mapping using the Hungarian algorithm then evaluate with four metrics: (i) accuracy, calculated as the mean over frames (MoF); (ii) the F1-score; (iii) the Jaccard index, calculated as the intersection over union (IoU); and (iv) the midpoint hit criterion <ref type="bibr" target="#b28">[29]</ref>, where the midpoint of the predicted segment must be within the ground-truth. We report MoF and IoU for all datasets, and in addition F1score for YTI and midpoint hit for MPII, as used in previous works. For all metrics, a higher result indicates better performance.  <ref type="table">Table 3</ref>. Comparison on the Breakfast dataset <ref type="bibr" target="#b13">[14]</ref> (* denotes results with Hungarian computed over all videos of an activity together). T denotes whether the method has a training stage on target activity/videos. evaluate at ground-truth number of actions for an activity. We adopt a similar approach and set K, for a video of a given activity, as the average number of actions for that activity. To provide an upper limit on the performance of our method we also evaluate with K set as the groundtruth of each video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with baseline methods</head><p>As established in section 3, Kmeans <ref type="bibr" target="#b22">[23]</ref> and FINCH <ref type="bibr" target="#b30">[31]</ref> are strong baselines for temporal action segmentation. In this section we establish an additional baseline, which we call Equal Split, that involves simply splitting the frames in a video into K equal parts. It can be viewed as a temporal clustering baseline based only on the relative time-stamps of each frame. This seemingly trivial baseline is competitive for all datasets and actually outperforms many recent weakly-supervised and unsupervised methods for the BF <ref type="table">(Table 3</ref>) and FS <ref type="table">(Table 5)</ref> datasets. TW-FINCH, however, consistently outperforms all baselines by significant margins on all five datasets, as shown in Table 3 (BF), <ref type="table">Table 4</ref> (YTI), <ref type="table">Table 5</ref> (FS), <ref type="table">Table 6</ref> (MPII) and <ref type="table" target="#tab_7">Table 7</ref> (HE). We attribute these strong results to better temporal consistency and ordering of actions, which TW-FINCH is able to achieve due to temporal weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State-of-the-art</head><p>We now compare TW-FINCH to current state-of-thearts, discussing results for each of the 5 datasets in turn. However, as noted in <ref type="bibr" target="#b17">[18]</ref> even though evaluation metrics are comparable to weakly and fully supervised approaches, one needs to consider that the results of the unsupervised learning are reported with respect to an optimal assignment of clusters to ground-truth classes and therefore report the best possible scenario for the task. For each dataset, we report IoU and MoF results for TW-FINCH. We report additional metrics when they are commonly used for a given dataset.</p><p>The column T in the tables denotes whether the method requires training on the target videos of an activity before being able to segment them. A dash indicates no known reported results.</p><p>Breakfast dataset (BF): BF contains an average of 6 actions per video, and 7% of frames in the dataset are background frames.</p><p>In <ref type="table">Table 3</ref> we report results on BF and compare TW-FINCH with recent state-of-the-art unsupervised, weaklysupervised and fully-supervised approaches. TW-FINCH outperforms all unsupervised methods, with absolute improvements of 10.5% over the best reported unsupervised method VTE-UNET and 19.8% over LSTM+AL <ref type="bibr" target="#b0">[1]</ref>. Similarly TW-FINCH outperforms the best reported weaklysupervised method CDFL <ref type="bibr" target="#b21">[22]</ref> with a 8.6/12.5% gain on the IoU/MoF metrics.</p><p>Methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref> train a separate segmentation model for each activity, and set K to the maximum number of groundtruth actions for that activity. They then report results by computing Hungarian over all videos of one activity. Since we are clustering each video separately, using K as maximum would over segment most of the videos. This however still enable us to show the impact on performance in such a case. When we set K to the maximum # actions per activity on the BF dataset, our performance is 57.8%, as many of the videos are over segmented. To see the purity of these over-segmented clusters we computed the weighted cluster purity in this setting, which comes out to be 83.8%. This high purity indicates that, even with an inexact K, our clusters can still be used for downstream tasks such as training self-supervised video recognition models. Inria Instructional Videos (YTI): YTI contains an average of 9 actions per video, and 63.5% of all frames in the dataset are background frames.</p><p>In <ref type="table">Table 4</ref> we summarize the performance of TW-FINCH on YTI and compare to recent state-of-the-art unsupervised and weakly-supervised approaches. To enable direct comparison, we follow previous works and remove a ST-CNN <ref type="bibr" target="#b20">[21]</ref> 68.0 58.1 ED-TCN <ref type="bibr" target="#b19">[20]</ref> 72.0 64.7 TricorNet <ref type="bibr" target="#b7">[8]</ref> 73.4 67.5 MS-TCN <ref type="bibr" target="#b9">[10]</ref> 80.7 -SSTDA <ref type="bibr" target="#b6">[7]</ref> 83.  <ref type="table">Table 5</ref>. Comparison to SoTA approaches at eval and mid granularity levels on the 50Salads dataset <ref type="bibr" target="#b34">[35]</ref>. We report MoF.</p><p>ratio (? = 75%) of the background frames from the video sequence and report the performance. TW-FINCH outperforms other methods and achieves F1-Score of 48.2% and MoF of 56.7%, which constitute absolute improvements of 8.5% on F1-Score over the best published unsupervised method. Impact of Background on YTI. As 63.5% of all frames in the YTI dataset are background, methods that train on this dataset tend to over-fit on the background. In contrast, a clustering based method is not strongly impacted by this: when we evaluate TW-FINCH while including all of the background frames our MoF accuracy drops from 56.7 ? 43.4% as is expected due to having more frames and thus more errors. Given such a significant data bias on background frames this relatively small drop indicates that TW-FINCH works reasonably with widely-varying degrees of background content. 50Salads (FS): FS contains an average of <ref type="bibr" target="#b18">19</ref>  frames.We evaluate with respect to two action granularity levels, as described in <ref type="bibr" target="#b34">[35]</ref>. The mid granularity level evaluates performance on the full set of 19 actions while the eval granularity level merges some of these action classes, resulting in 10 action classes. In <ref type="table">Table 5</ref> we show that TW-FINCH obtains a MoF of 66.5% in the mid granularity, 11.8% higher (in absolute terms) than the best weaklysupervised method CDFL <ref type="bibr" target="#b21">[22]</ref>. We see similar performance gains in the eval granularity level evaluation as well. The IoU score of TW-FINCH for mid and eval granularity is 48.4% and 51.5% respectively. MPII Cooking 2 (MPII): MPII contains 17 actions per video on average, and 29% of all frames in the dataset are background frames. For MPII we report the midpoint hit criterion <ref type="bibr" target="#b28">[29]</ref> (multi-class precision and recall), the standard metric for this dataset, in addition to IoU and MoF.</p><p>The dataset provides a fixed train/test split. We report performance on the test set to enable direct comparisons with previously reported results. As <ref type="table">Table 6</ref> shows, TW-FINCH outperforms our strong unsupervised baselines for all 4 reported metrics. Our method also outperforms SoTA fullysupervised methods that report the mid-point hit criterion. Hollywood Extended (HE): HE contains an average of 3 (including background) actions per video, and 61% of all frames in the dataset are background frames. We report (a) P39 cam02 P39 scrambledegg (b) changing tire 0023 <ref type="figure">Figure 3</ref>. Segmentation examples from (a) the Breakfast dataset <ref type="bibr" target="#b13">[14]</ref>, and (b) the Inria Instructional Videos dataset <ref type="bibr" target="#b1">[2]</ref>. Colors indicate different actions and are arranged in chronological order. We compare the segmentation quality of our method to Kmeans, FINCH, and two state-of-the-art unsupervised methods, CTE and Mallow. Our method has predicted better lengths of actions occurring in these videos. results for HE in <ref type="table" target="#tab_7">Table 7</ref>, which shows that TW-FINCH outperforms CDFL <ref type="bibr" target="#b21">[22]</ref> by 15.5% (19.5?35.0) and 10.0% (45?55.0) in IoU and MoF, respectively. Further, note that the performance of our appearance-based clustering baselines is quite similar to the performance of our method. We attribute this to the small number of clusters per video (3 clusters on average). As a result, Kmeans and FINCH are roughly as effective as TW-FINCH, as temporally ordering 3 clusters is less difficult.</p><p>Qualitative Results. <ref type="figure">Fig. 3</ref> shows representative results for two videos taken from the BF dataset (a) and the YTI dataset (b). Note that in the visualization (b) we set the background frames to white for all methods, according to the ground-truth. This allows the misclassification and mis-ordering of background frames to be more clearly seen. In (a), one can observe that TW-FINCH accurately predicts the length of segments, yielding better segmentation boundaries. Both clustering baselines, neither of which leverage temporal information, have noisy segments. Other SoTA unsupervised methods either have inaccurate temporal boundaries, incorrect ordering of actions, or are missing actions altogether. In addition, background frames are more often misclassified and mis-ordered for competing methods. Similar observations can be made in (b), where we show qualitative segmentation results on a more challenging YTI video with 9 actions of varying lengths, and several interspersed background scenes.</p><p>Limitations. Two main limitations for our work exist, which are inherent to our unsupervised clustering-based approach. The first, illustrated in <ref type="figure">Figure 3</ref>(b), occurs when we over-segment a temporally-contiguous sequence due to low visual coherence. The second may occur when we assign frames that depict the same action to different clusters because they are temporally distant.</p><p>Computational complexity. As we need to compute the N ?N temporal distances, the computational complexity of  <ref type="table">Table 8</ref>. Run-time comparison of method with other state-of-theart methods on Breakfast dataset. Testing duration is measured as the average inference for split 1 test set (252 videos). The runtime of all the Weakly Sup. methods were taken from <ref type="bibr" target="#b33">[34]</ref> TW-FINCH is O(N 2 ). In contrast FINCH is O(N log(N )) while other similar graph-based clustering methods such as spectral methods are O(N 3 ) and hierarchical agglomerative linkage-based schemes are O(N 2 log(N )). <ref type="table">Table 8</ref> provides the total run-time of TW-FINCH and other state-ofthe-art methods on Breakfast dataset split 1 (252 videos). Unlike previous methods that require hours of model training on GPUs, our method runs on a computer with an AMD 16-core processor, taking approximately 0.16 seconds on average to segment one video (? 2000 frames).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We addressed the problem of temporal action segmentation and found that simple clustering baselines produce results that are competitive with, and often outperform, recent SoTA unsupervised methods. We then proposed a new unsupervised method, TW-FINCH which encodes spatiotemporal similarities between frames on a 1-nearest-neighbor graph and produces a hierarchical clustering of frames. Our proposal is practical as unlike existing approaches it does not require training on the target activity videos to produce its action segments. Our extensive quantitative experiments demonstrate that TW-FINCH is effective and consistently outperforms SoTA methods on 5 benchmark datasets by wide margins on multiple metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>13 :</head><label>13</label><figDesc>Update cluster labels in P i : P M ? P i 14:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Simple clustering with Kmeans or FINCH is competitive with the best reported weakly or unsupervised methods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">1] VTE-UNET [36] Kmeans FINCH</cell></row><row><cell>MoF</cell><cell>50.2</cell><cell>42.9</cell><cell>52.2</cell><cell>42.7</cell><cell>51.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Statistics of datasets used in the experiments: Background refers to the % of background frames in a dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>actions per video, and 14.1% of all frames in the dataset are background Comparison on the Hollywood Extended dataset<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MPI Cooking 2</cell><cell></cell><cell></cell></row><row><cell>Supervision</cell><cell cols="2">Method</cell><cell>IoU</cell><cell cols="2">Midpoint-hit</cell><cell>MoF T</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Precision Recall</cell></row><row><cell></cell><cell cols="2">Pose + Holistic [29]</cell><cell>-</cell><cell>19.8</cell><cell>40.2</cell><cell>-</cell></row><row><cell>Fully Sup.</cell><cell cols="2">Fine-grained [25]</cell><cell>-</cell><cell>28.6</cell><cell>54.3</cell><cell>-</cell></row><row><cell></cell><cell cols="2">GMM+CNN [16]</cell><cell>45.5</cell><cell>-</cell><cell>-</cell><cell>72.0</cell></row><row><cell>Weakly Sup.</cell><cell cols="2">GMM+CNN [16]</cell><cell>29.7</cell><cell>-</cell><cell>-</cell><cell>59.7</cell></row><row><cell></cell><cell cols="2">Equal Split</cell><cell>6.9</cell><cell>25.6</cell><cell>44.6</cell><cell>14.6</cell></row><row><cell>Unsup. Baselines</cell><cell cols="2">Kmeans</cell><cell>14.5</cell><cell>21.9</cell><cell>34.8</cell><cell>30.4</cell></row><row><cell></cell><cell cols="2">FINCH</cell><cell>18.3</cell><cell>26.3</cell><cell>41.9</cell><cell>40.5</cell></row><row><cell>Unsup.</cell><cell cols="2">TW-FINCH</cell><cell>23.1</cell><cell>34.1</cell><cell>54.9</cell><cell>42.0</cell></row><row><cell>Unsup.</cell><cell cols="3">TW-FINCH (K=gt/video) 24.6</cell><cell>37.5</cell><cell>59.2</cell><cell>43.4</cell></row><row><cell cols="7">Table 6. Comparison on the MPII Cooking 2 dataset [30].</cell></row><row><cell></cell><cell></cell><cell cols="3">Hollywood Extended</cell><cell></cell></row><row><cell>Supervision</cell><cell></cell><cell>Method</cell><cell></cell><cell cols="3">IoU MoF T</cell></row><row><cell>Fully Sup.</cell><cell></cell><cell>GMM+CNN [16]</cell><cell></cell><cell cols="2">8.4</cell><cell>39.5</cell></row><row><cell></cell><cell></cell><cell>GMM+CNN [16]</cell><cell></cell><cell cols="2">8.6</cell><cell>33.0</cell></row><row><cell></cell><cell></cell><cell>ActionSet [27]</cell><cell></cell><cell cols="2">9.3</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>RNN-FC [26]</cell><cell></cell><cell cols="2">11.9</cell><cell>-</cell></row><row><cell>Weakly Sup.</cell><cell></cell><cell>TCFPN [9]</cell><cell></cell><cell cols="3">12.6 28.7</cell></row><row><cell></cell><cell></cell><cell>SCT [11]</cell><cell></cell><cell cols="2">17.7</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>D3TW [6]</cell><cell></cell><cell>-</cell><cell></cell><cell>33.6</cell></row><row><cell></cell><cell></cell><cell>CDFL [22]</cell><cell></cell><cell cols="3">19.5 45.0</cell></row><row><cell></cell><cell></cell><cell>Equal Split</cell><cell></cell><cell cols="3">24.6 39.6</cell></row><row><cell cols="2">Unsup. Baselines</cell><cell>Kmeans</cell><cell></cell><cell cols="3">33.2 55.3</cell></row><row><cell></cell><cell></cell><cell>FINCH</cell><cell></cell><cell cols="3">37.1 56.8</cell></row><row><cell>Unsup.</cell><cell></cell><cell>TW-FINCH</cell><cell></cell><cell cols="3">35.0 55.0</cell></row><row><cell>Unsup.</cell><cell></cell><cell cols="5">TW-FINCH (K=gt/video) 38.5 57.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ssarfraz/FINCH-Clustering/tree/master/TW-FINCH</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A perceptual prediction framework for self supervised event segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sathyanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Aakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4575" to="4583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for action and gesture recognition in image sequences: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Asadi-Aghbolaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Clap?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bellantonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><forename type="middle">Jair</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?ctor</forename><surname>Ponce-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohreh</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gesture Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="539" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">D3tw: Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3546" to="3555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Action segmentation with joint selfsupervised temporal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Al-Regib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9454" to="9463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Tricornet: A hybrid temporal convolutional and recurrent network for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07818</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazan</forename><surname>Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sct: Set constrained temporal transformer for set supervised action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="501" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Going deeper into action recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samitha</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goaldirected human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An end-toend generative framework for video segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of actions from transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A hybrid rnn-hmm approach for weakly supervised temporal action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of action classes with continuous temporal embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Kukleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12066" to="12074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Segmental spatiotemporal cnns for fine-grained action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly supervised energy-based learning for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6243" to="6251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-null ranking models. i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Colin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mallows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="114" to="130" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiple granularity analysis for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vignesh R Paramathayalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="756" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action sets: Weakly supervised action segmentation without ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5987" to="5996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neuralnetwork-viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognizing fine-grained and composite activities using hand-centric features and script data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="373" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient parameter-free clustering using first neighbor relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Saquib</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning and segmentation of complex activities from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8368" to="8376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fast weakly supervised action segmentation using mutual consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Souri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianpiero</forename><surname>Francesca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03116</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</title>
		<meeting>the 2013 ACM international joint conference on Pervasive and ubiquitous computing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Joint visual-temporal embedding for unsupervised learning of actions in untrimmed sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rosaura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vidalmata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuehne</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
		<respStmt>
			<orgName>Applications of Computer Vision (WACV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A tutorial on spectral clustering. Statistics and computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxburg</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
