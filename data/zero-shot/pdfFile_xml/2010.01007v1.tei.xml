<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DecAug: Augmenting HOI Detection via Decomposition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Xie</surname></persName>
							<email>xieyichen@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
							<email>yongluli@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DecAug: Augmenting HOI Detection via Decomposition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human-object interaction (HOI) detection requires a large amount of annotated data. Current algorithms suffer from insufficient training samples and category imbalance within datasets. To increase data efficiency, in this paper, we propose an efficient and effective data augmentation method called DecAug for HOI detection. Based on our proposed object state similarity metric, object patterns across different HOIs are shared to augment local object appearance features without changing their state. Further, we shift spatial correlation between humans and objects to other feasible configurations with the aid of a pose-guided Gaussian Mixture Model while preserving their interactions. Experiments show that our method brings up to 3.3 mAP and 1.6 mAP improvements on V-COCO and HICO-DET dataset for two advanced models. Specifically, interactions with fewer samples enjoy more notable improvement. Our method can be easily integrated into various HOI detection models with negligible extra computational consumption. Our code will be made publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-object interaction (HOI) detection aims to localize humans and objects as well as infer their interaction categories in a still image. For each interaction, a triplet of subject, predicate, object should be retrieved. As a subtask of visual relationship detection, HOI detection pays attention to human-centric interactions with objects. It plays an essential role in the understanding of scenes, which facilitates many other fields like activity understanding <ref type="bibr" target="#b5">[6]</ref>, image captioning <ref type="bibr" target="#b28">[29]</ref> and robot learning <ref type="bibr" target="#b1">[2]</ref>.</p><p>Along with the recent achievements computer vision has reached, many exciting deep neural network (DNN) models for HOI detection have been developed. They took * Equal contribution. Names in alphabetical order.</p><p>? Cewu Lu is the corresponding author.</p><p>(a) Instance-Level Augmentation Example: heatmap-guided instaboost <ref type="bibr" target="#b12">[13]</ref> (left: original, right: augmented) (b) Our Approach: local object appearance augmentation (left) and global spatial correlation augmentation (right) <ref type="figure">Figure 1</ref>. (a) shows the result of heatmap-guided instaboost. The left is the original image while the right has been augmented. The board is moved far away, which has no negative effect on object detection or instance segmentation. However, it devastates the relationship between the human and object. In contrast, (b) shows the two steps of our DecAug. Local object appearance is changed in the left image. Then, global spatial correlation augmentation is applied in the right one. The human-object interaction remains distinguishable in both.</p><p>various types of features into account such as visual features <ref type="bibr" target="#b18">[19]</ref>, spatial location <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47]</ref>, human poses <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b17">18]</ref> or text corpus <ref type="bibr" target="#b32">[33]</ref>. However, the progress of HOI detection is still slower compared with the achievement in other tasks like object detection and instance segmentation. There are currently two main hindrances to further performance gains. For one thing, HOI detection depends on a better understanding of contextual information. It calls for a large amount of high quality data. However, large datasets are not easily accessible due to the labor intensity of annotation. For another thing, an apparent imbalance inevitably exists between different interaction categories in current large datasets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b52">53]</ref>. Some interactions naturally have much more positive samples than others, such as look at, sit on and stand on, which causes a serious long-tail issue.</p><p>To tackle such problems, a natural idea is to resort to data augmentation, whose power has been witnessed in many other tasks of computer vision <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b12">13]</ref>. Unfortunately, previous research in cognition <ref type="bibr" target="#b2">[3]</ref> demonstrated the difficulty of data augmentation for the task of HOI detection. Specifically, image-level random cropping cannot improve the diversity of interactions while instance movement damages the spatial correlation between humans and objects. As shown in <ref type="figure">Fig. 1(a)</ref>, it is hard to identify the interaction in the images using such simple augmentation.</p><p>In this paper, we propose a novel data augmentation method named DecAug. Aiming to improve the diversity of interactions without semantic loss, DecAug mainly includes two components: local object appearance augmentation and global spatial correlation augmentation.</p><p>To elaborate, for local object appearance, we propose a simple but effective cross-image instance substitution technique to increase the generalization ability of models towards entity concepts instead of object patterns. An object state similarity metric is also introduced to justify the replacement of an object with another based on their state coherency.</p><p>Furthermore, we try to augment the global spatial correlations between humans and objects without contextual loss. According to <ref type="bibr" target="#b25">[26]</ref>, the perceptual inference of human derives from information available to observers and some empirical knowledge of the world. Intuitively, reasonable placement of objects could also be obtained with prior knowledge from the whole dataset. Inspired by the strong correlation between human pose and HOI <ref type="bibr" target="#b49">[50]</ref>, we build a probability distribution of object location for each training sample, which comes from the spatial relationship of other samples with similar human poses. With this distribution aware augmentation, we are able to improve the diversity within each interaction without damaging their semantic meanings.</p><p>We conduct extensive experiments on two mainstream datasets: V-COCO <ref type="bibr" target="#b18">[19]</ref> and HICO-DET <ref type="bibr" target="#b6">[7]</ref>. After augmentation, the performance of two advanced open-source models (iCAN <ref type="bibr" target="#b15">[16]</ref> and Transferable Interactiveness Network <ref type="bibr" target="#b31">[32]</ref>) can be improved by a large margin (3.3 and 2.6 mAP on V-COCO; 1.6 and 1.3 mAP on HICO-DET). Same object detection proposals are used to ensure the improvements come from interaction recognition instead of object detection. Specifically, for those interactions with fewer positive samples, the improvement is more notable, suggesting our method helps tackle the long-tail issue. Our code will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual Relationship Detection</head><p>Visual relationship detection <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> needs to not only find objects location in an image but also detect the relationships between them. These relationships includes actions <ref type="bibr" target="#b41">[42]</ref>, interactions <ref type="bibr" target="#b16">[17]</ref> or other more general relationships <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b51">52]</ref>. Different from object detection or instance segmentation, visual relationship detection requires to exploit more semantic information <ref type="bibr" target="#b2">[3]</ref> like the spatial positions of humans and objects <ref type="bibr" target="#b6">[7]</ref>. Since such semantic information is difficult to extract, enough training samples are necessary for these models. Requirement for maintaining the semantic information also poses an extra challenge to data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Human-Object Interaction Detection</head><p>Human-object interaction (HOI) detection task is significant for understanding human behavior with objects. Some early work <ref type="bibr" target="#b18">[19]</ref> tried to detect humans and objects separately, which led to limited performance. Christopher et. al. <ref type="bibr" target="#b2">[3]</ref> proposed that rather than the sum of parts, more information should be taken into consideration. Gao et. al. <ref type="bibr" target="#b15">[16]</ref> proposed an instance-centric attention module to enhance regions of interest. Chao et. al. <ref type="bibr" target="#b6">[7]</ref> added the relative spatial relationship between humans and objects into the input of CNN. The significance of pair spatial configuration was further emphasized by Ulutan et.al. and Wang et.al. <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref>, which helped associate the interacted humans and objects. Some recent works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31]</ref> also thought of human poses as a crucial indicator of interaction.</p><p>More information means a higher requirement for data amount. There exist some popular datasets for this task such as V-COCO <ref type="bibr" target="#b18">[19]</ref>, HICO-DET <ref type="bibr" target="#b6">[7]</ref>, HAKE <ref type="bibr" target="#b29">[30]</ref> and HCVRD <ref type="bibr" target="#b52">[53]</ref>. However, these datasets suffer from internal imbalance between different interaction categories, which is the so-called long-tail issue. Some interaction categories lack positive samples, which encumbers the overall performance. By increasing the diversity of data, data augmentation may help to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Data Augmentation</head><p>Data augmentation has been widely used in many tasks in the field of computer vision, such as image classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b26">27]</ref>, object detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36]</ref>, and pose estimation <ref type="bibr" target="#b37">[38]</ref>. By generating additional training data, these methods helped to improve performance of various datahungry models. Specifically, one branch of data augmentation focused on the instance-level, which fully exploited the fine-annotated segmentation of instances. Transformation applied on instances included scaling, rotation <ref type="bibr" target="#b22">[23]</ref>, jitter <ref type="bibr" target="#b12">[13]</ref>, pasting <ref type="bibr" target="#b24">[25]</ref> and affine transform <ref type="bibr" target="#b23">[24]</ref>. However, all these above just utilized the information in a single image instead of the whole dataset. Some other work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b34">35]</ref> generated new images with Generative Adversarial Networks (GAN). Despite the impressive results, GAN needs plentiful extra training data, which is not applicable for current HOI datasets.</p><p>Another challenge rises about the placement of segmented instances on augmented images. Dvornik et. al. <ref type="bibr" target="#b10">[11]</ref> placed objects on the background according to the context. However, extra model needed to be trained beforehand. Fang et. al. <ref type="bibr" target="#b12">[13]</ref> replaced the offline trained model with online context comparison. Yet, such a method does not preserve the visual relation information between instances inside an image.</p><p>Due to the difficulty in context preservation, there exists no effective data augmentation approach to generate extra training samples for visual relation detection tasks. Some prior effort <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref> generated new interaction patterns based on word embedding but these could hardly improve visual diversity in training samples. In contrast, we develop a novel data augmentation method to visually boost data diversity for HOI detection. It makes use of information across the whole dataset as well as reserves visual relationships between humans and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>For the task of human-object interaction detection, we need to identify the interacting human-object pair, localize their positions and recognize their interaction category. In this paper, we focus on the interaction identification and recognition parts. Given detected humans and objects, a classifier f needs to capture the very subtle details in the image to recognize the relationship R. A human-object interaction can be decomposed into the background I, the human state h including human appearance, pose, parsing, shape, gaze, etc., the object state o including category, 6D pose, occlusion, functionality, etc., and the spatial relationship s between the human and object. Mathematically, we have</p><formula xml:id="formula_0">R = f (I, h, o, s).<label>(1)</label></formula><p>In this paper, we mainly augment the object state and spatial correlations, coherent with the human perception process. This is nontrivial, since R is very sensitive to the object state and spatial relations. We must find a manifold space in pixel level that could augment the object appearance while preserving the object state. In Sec. 3.2, we introduce our local object appearance augmentation where an object state similarity metric is proposed. Meanwhile, to find feasible spatial configurations for global spatial correlation augmentation, we propose the pose-guided probability distribution map in Sec. 3.3. An overview of our method is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Object Appearance Augmentation</head><p>When recognizing the HOI, the state of an object is far more important than its texture pattern. For example, when identifying the interaction of holding a mug, the standing pose and the occlusion with hands are more important than the mug's color and texture. Thus, we propose to augment the local object appearance features to improve the generalization ability of the network, helping it pay more attention to the crucial object state instead of appearance. The key of such augmentation is to preserve the object state as much as possible. Meanwhile, patterns of augmented objects should be photo-realistic to avoid too many artifacts. Naturally, we can utilize the same category objects from the dataset during training i.e. we replace the object with suitable same category instances in other images. We then explain our principle for objects appearance replacement as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Whether to Replace an Object</head><p>We first judge whether an object can be substituted or not. Some objects are not suitable to be replaced if they interlock with its neighbours too tightly. In this case, adjacent humans or objects are likely to overlap with each other. As a consequence, it is difficult to find a proper replacement to maintain this interaction.</p><p>Intuitively, tightly interlocked instances share a long common borderline. Therefore, we develop a metric called instance interlocking ratio measuring the interlocking extent between two adjacent instances in the same image.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, we define C i as the contour of instance O i and M i as the mask of this instance. The contour C i serves as the outline of the mask with width w. For two adjacent instances O i , O j in the same image, we define their interlocking area as U i,j and their union contour area as V i,j :</p><formula xml:id="formula_1">U i,j = S(M i ? C j ) + S(C i ? M j ) (2) V i,j = S(C i ? C j ),<label>(3)</label></formula><p>where S(A ? B) denotes the intersection area of A while B and S(A ? B) denotes the union area of A and B Further, the instance interlocking ratio between instance O i , O j is defined as r i,j :</p><formula xml:id="formula_2">r i,j = U i,j V i,j ? [0, 1].<label>(4)</label></formula><p>If two adjacent instances have a high interlocking ratio, chances are that they seriously overlap with each other. As a result, neither of them will be replaced. Thus, objects in image I that can be replaced are selected from the following   </p><formula xml:id="formula_3">O = {O i |O i ? I, ?O j ? I, j = i : r i,j &lt; t} ,<label>(5)</label></formula><p>where t is a hyper-parameter as a threshold. We empirically set it to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Find Objects with Similar States</head><p>Despite the same category, objects show various states including pose variance, shape variance, occlusion variance, etc. Objects to be substituted should be matched with others with similar states. Otherwise, the interaction may be affected. Fortunately, we find that the mask of an object can serve as an indicator of the object state. As the projection of an object on the camera plane given a specific pose, instance mask implicitly encodes the shape and 6D pose of the object. Same category objects may share similar shapes and 6D poses if they have similar masks. What's more, an object's occlusion state can also be reflected from the combination of its own and its neighbours' masks. Thus, we build our object state descriptor based on the object mask.</p><p>For object O i with a W ? H bounding box X i , we divide X i into three parts: object mask M i , background B i and adjacent mask A i . Based on that, we construct the corresponding object state matrix E i ? R W ?H for each instance i. Each element in this matrix corresponds with a pixel in the bounding box of instance i. The mapping is shown as follows:</p><formula xml:id="formula_4">E x,y i = ? ? ? ? ? 1 I x,y ? M i 0 I x,y ? B i ?1 I x,y ? A i x ? {1, ? ? ? , W }, y ? {1, ? ? ? , H}<label>(6)</label></formula><p>where I x,y denotes the pixel with coordinate (x, y) in the bounding box. This matrix E i serves as a descriptor of the shape, 6D pose and overlapping condition of instance O i .</p><p>With such descriptor, for objects O i and O j with state matrix E i ? R W ?H and E j ? R W ?H , we define their object state distance D(i, j) as</p><formula xml:id="formula_5">D(i, j) = x,y |E i ? E j | W ? H , x ? {1, 2, ? ? ? , W }, y ? {1, 2, ? ? ? , H}<label>(7)</label></formula><p>where E j is the resized matrix of E j with same size with E i . In the training period, when we process a replaceable object instance O i in a given image, we randomly select 20 same category objects from other images and calculate their object state distance to O i . Object with the smallest state distance is selected to replace O i . <ref type="figure" target="#fig_3">Fig. 5</ref> shows some positive or negative examples for replacement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Object Replacement</head><p>After finding substitution candidate O s for object O i , we extract both instances from background using instance masks. For datasets without ground-truth segmentation annotations (like HICO-DET), we generate instance masks with Deep Mask <ref type="bibr" target="#b38">[39]</ref>. Matting <ref type="bibr" target="#b19">[20]</ref> with alpha channel is adopted to extracted instances so that smoother outlines are acquired. At the same time, we conduct inpainting with Fast Marching <ref type="bibr" target="#b4">[5]</ref> to fill the hole of O i in the background, which ensures the continuous distribution of the raw image. Finally, we resize object O s to have the same bounding box size as O i and paste the segmented instance O s to the original location of object O i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Global Spatial Correlation Augmentation</head><p>In Sec.3.2, the substituted object is pasted at the original position. Although it augments the object appearance, the variance in the image is too slight to cover other unobserved situations. As a supplement, movement with longer distance can effectively improve the performance. In the meantime, such movement should not pose damage to the contextual information.</p><p>Therefore, We develop a pose-guided probability map to obtain feasible positions of an object. To get the pose data, we follow <ref type="bibr" target="#b31">[32]</ref> to employ AlphaPose <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref> on each (a) hold vs sit (b) look vs kick <ref type="figure">Figure 6</ref>. For same atomic pose, object placement of different interactions has distinct probability distribution. As shown above, objects of holding are close to hands, sitting close to buttock or legs, kicking close to feet, while objects of looking extends from eyes and dispersed extensively.</p><p>human. The generated pose data K is in COCO <ref type="bibr" target="#b33">[34]</ref> format with 17 keypoints of each person.</p><p>For each human-object interaction category, the relative spatial correlation between the human and object can be described with a 2-dimension vector v sp . We perform normalization to deal with different scales of instances and images. Specifically, torso centers of human poses are set as the origins and torso lengths are normalized to one. Also, the relative spatial position vector v sp is normalized by dividing the torso length. We denote the normalized pose asK and the normalized offset asv sp .</p><p>To get feasible configurations to augment spatial correlations between human-object pairs, we model the object location L as a conditional probability distribution w.r.t normalized human poseK. Considering the proper object location distribution differs across different HOI categories, we learn the conditional distribution for each HOI category separately. Given category h, we model p(L|K, h) as a mixture of Gaussian distribution. Mathematically, we have <ref type="bibr" target="#b8">(9)</ref> where N G denotes the number of Gaussian distributions, ? j is the combination weight for the j-th component, N(v sp ; ? j , ? j ) denotes the j-th multivariate Gaussian distribution with mean ? j and covariance ? j . Following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>, we set N G as the number of atomic poses in the dataset, which is 42 in practice. By enforcing the probability distributions independent among each HOI category, we can ensure the object location coherence within each distribution.</p><formula xml:id="formula_6">p(L|K, h) = p(v sp |h) = N G j=1 ? j N(v sp ; ? j , ? j ),</formula><p>We learn the Gaussian mixture distribution p(L|K, h) efficiently using an EM algorithm, where the E-step estimates the combination weights ? and M-step updates the Gaussian parameters ? and ?. To simplify the learning process, we utilize K-means clustering to group the pose data in different HOI categories and initialize the parameters as a warm start. Our learned Gaussian Mixture Model (GMM) constitutes the prior knowledge of relative spatial position distribution of the object. The learned mean ? j of each Gaussian represents the average of a group of similar 2D poses, which is referred to as atomic pose. Some atomic poses and their corresponding object placement distribution are visualized in <ref type="figure">Figure 6</ref>.</p><p>When augmenting an HOI sample in category h given a human poseK, we determine the new relative spatial position vector v sp by sampling the distribution p(L|K, h). The augmentation process was illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. Objects are more likely to be placed in a relative spatial position with more prior samples of current interaction type, where they share human poses of the same cluster. With our poseguided probability map, we are able to augment the spatial correlations between humans and objects in an effective manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first describe the datasets and metrics. We then introduce the base models on which DecAug is performed, including other implementation details. Next, improvements brought by our method is revealed. Analysis shows that our methods alleviate the long-tail issue. Detailed ablation studies are also conducted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Metric</head><p>Dataset We evaluate our methods on two mainstream benchmarks: V-COCO <ref type="bibr" target="#b18">[19]</ref> and HICO-DET <ref type="bibr" target="#b6">[7]</ref>. V-COCO is a subset of COCO dataset <ref type="bibr" target="#b33">[34]</ref> annotated with HOI labels. It includes 10,346 images (2,533 for training, 2,867 for validating and 4,946 for testing) and 16,199 human instances. Each person is annotated with 29 action types, 5 of which have no object. The objects are split into two types: object and instrument. HICO-DET is a subsect of HICO <ref type="bibr" target="#b7">[8]</ref> dataset annotated with bounding boxes. It contains 47,776 images <ref type="bibr" target="#b37">(38,</ref><ref type="bibr">118</ref> for training and 9,658 for testing), 600 HOI categories over 80 object types and 117 verbs.</p><p>Metric We apply the mainstream metric for HOI detection: role mean average precision (role mAP). A prediction is true positive only when 1) HOI classification is correct, and 2) both the IoUs between the predicted bounding boxes of human and object v.s. the ground truth &gt; 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Models We apply DecAug to the following two representative HOI detection models: iCAN <ref type="bibr" target="#b15">[16]</ref> and Transferable Interactiveness Network (TIN) <ref type="bibr" target="#b31">[32]</ref>. Same object proposals are applied so that we can ensure the performance gain comes from interaction recognition instead of object detection. Baseline results are those reported in their original papers.</p><p>Hyper-parameters We adopt stochastic gradient descent in training. All hyper-parameters strictly follow the original setting of our baseline models including iteration number, learning rate, weight decay, backbones and so on.</p><p>Augmentation Pipeline During training, the proposed local and global augmentation strategies are incorporated simultaneously since they are complimentary. Each input image will be augmented with a probability of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and Comparison</head><p>The HOI detection results are evaluated by following the detailed metrics defined by each specific dataset. Results of all the experiments verify the effectiveness and generality of the proposed DecAug.</p><p>For V-COCO, we evaluate mAP role in Tab. 1. We can see that substantial improvements (3.3 mAP) are achieved by applying DecAug.</p><p>For HICO-DET, we evaluate mAP role of Full (600 HOIs), Rare (138 HOIs), Non-Rare (462 HOIs) interactions of two different settings: Default and Known Object. Results are shown in Tab. 2. Unsurprisingly, notable performance gain is also achieved (1.6 mAP), indicating the effectiveness of our methods on large datasets without groundtruth segmentation or keypoints.</p><p>In <ref type="figure" target="#fig_5">Fig 7,</ref> we show some visualized results trained with and w/o DecAug. We can see examples that our DecAug  compensates for some ignorance and corrects some detection mistakes, as it makes full use of the information within the whole dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analysis</head><p>Long-tail Issue is a pervasive problem in HOI datasets. In <ref type="figure" target="#fig_6">Fig. 8(a)</ref>, we plot the number of samples from each interaction categories in V-COCO dataset. Severe data imbalance could be observed. <ref type="figure" target="#fig_6">Fig. 8(b)</ref> then shows the effectiveness of DecAug, from which we can clearly see that more remarkable improvement could be made for interaction categories with fewer training samples. This is because DecAug could make full use of favourable information (e.g. object appearance, spatial locations) across the whole dataset.</p><p>Transferability and generality It's noteworthy that besides making the best of the prior knowledge in the current dataset, our DecAug is also capable of capturing information from other datasets. This could be achieved since both our object state descriptor and pose-guided probability map can be transferred across datasets. For local object appearance augmentation, annotated instances from other large datasets (e.g. COCO <ref type="bibr" target="#b33">[34]</ref>) may also serve as candidates for replacement. For global spatial correlation augmentation, the Gaussian mixture model can be constructed Training Efficiency As a data augmentation method, De-cAug can be embedded into various existing models conveniently with negligible offline data preprocessing. During training, it could generate augmented samples online with- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>In this part, the impact of 1) local object appearance augmentation (LOA), and 2) global spatial correlation augmentation (GSC) in DecAug is separately analyzed. The results are shown in Tab. 4. We can see that both strategies contribute notably to the final performance. Next, we evaluate the effectiveness of some key techniques in each strategy. Local Object Appearance Augmentation Here we evaluate the two key components in LOA, instance interlocking ratio (IIR) and object state matrix (OSM), by replacing them with other possible metrics. For IIR, we try other two possible choices: simply replacing all objects (replace all) and applying bbox IoU between neighbours as the metric (bbox IoU). For OSM, we also select other four alternatives: random selection, chamfer distance, instance mask distance and l 2 distance of the image inside a bounding box. In Tab. 5(a), results show apparent degradation using other metrics, verifying the significance of our proposed metric.</p><p>Global Spatial Correlation Augmentation Global spatial correlation augmentation can greatly increase the data diversity without harming the context. We exhibit its value by comparing our results with the other two possible choices: random placement and appearance consistent metric heatmap in <ref type="bibr" target="#b12">[13]</ref>. Tab. 5(b) reveals that performance drops notably with the other alternatives, further proving the power of our pose-guided method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel data augmentation method, DecAug, for HOI detection, which mainly includes two components: local object appearance augmentation and global spatial correlation augmentation. With negligible cost, our method can be easily combined with various existing models to further improve their performance. De-cAug has good generalizability, which could utilize information transferred from other datasets, and it helps address the long-tail problem. We hope our DecAug gives a new insight into the data augmentation of visual relationship detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) skateboard with the board (b) kick/look the ball Overview of our method: the first image is the original input (red box: human, blue box: object). The second image is the result of local object appearance augmentation (Sec. 3.2). The third and forth images show the pose-guided probability distribution map and the result of global spatial correlation augmentation (Sec. 3.3). Multiple translucent pastes are visualized in the rightest images to reflect the randomness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Left images are original ones. We replace the objects (blue boxes) with instances from the middle images (blue boxes). The rightest images are the augmentation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>In the middle image, light blue region shows the object mask while dark blue denotes the contour. In the right image, for the two instances Oi, Oj, Ui,j is colored in green and Vi,j is composed of the green, dark yellow and dark blue regions. set:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>In (a) and (b), the left is the original image with blue box showing the object. The right above (green box) two images show instances which have high similarity with original object while the right below two (yellow box) in each sub-figure are with low similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>v</head><label></label><figDesc>sp = c o ? c h (8) where c h = (x c,h , y c,h ), c o = (x c,o , y c,o ) are the torso center of human and bounding box center of object respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>HOI detection results of iCAN trained with (top) and w/o (bottom) DecAug. DecAug brings more accurate detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>(a) shows the training sample number of each interaction category in V-COCO dataset. Interaction names are ignored for clarity. Grey points in (b) show the AP role improvement of each interaction category (corresponding with (a)). The blue dotted line in (b) reveals the fitted trend line of AP role improvement. We can see that the elevation increases as the sample number decreases. based on human-object pairs with similar interactions from the mixed dataset.In Tab. 3, we show the results of applying DecAug on HICO-DET dataset with information transferred from the mixture data of COCO and V-COCO dataset. This well demonstrates the generality of DecAug (such results are not listed in the main table to avoid unfair comparison).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on V-COCO: Original models' results come from their papers.</figDesc><table><row><cell>Model</cell><cell>DecAug mAP role</cell></row><row><cell>iCAN [16]</cell><cell>44.7</cell></row><row><cell>iCAN</cell><cell>48.0</cell></row><row><cell>Improvement</cell><cell>3.3?</cell></row><row><cell>TIN (RP D C D ) [32]</cell><cell>47.8</cell></row><row><cell>TIN (RP D C D )</cell><cell>50.4</cell></row><row><cell>Improvement</cell><cell>2.6?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on HICO-DET: Original models' results come from their papers.</figDesc><table><row><cell>Model</cell><cell>DecAug</cell><cell>Full</cell><cell cols="3">Default Rare Non-Rare Full</cell><cell>Known Object Rare Non-Rare</cell></row><row><cell>iCAN [16]</cell><cell></cell><cell cols="2">14.84 10.45</cell><cell>16.15</cell><cell>16.26 11.33</cell><cell>17.73</cell></row><row><cell>iCAN</cell><cell></cell><cell cols="2">16.39 12.23</cell><cell>17.63</cell><cell>17.85 13.68</cell><cell>19.10</cell></row><row><cell>Improvement</cell><cell></cell><cell cols="2">1.55? 1.78?</cell><cell>1.48?</cell><cell>1.59? 2.35?</cell><cell>1.37?</cell></row><row><cell>TIN (RP D C D ) [32]</cell><cell></cell><cell cols="2">17.03 13.42</cell><cell>18.11</cell><cell>19.17 15.51</cell><cell>20.26</cell></row><row><cell>TIN (RP D C D )</cell><cell></cell><cell cols="2">18.38 14.99</cell><cell>19.39</cell><cell>20.50 16.93</cell><cell>21.57</cell></row><row><cell>Improvement</cell><cell></cell><cell cols="2">1.35? 1.57?</cell><cell>1.28?</cell><cell>1.33? 1.42?</cell><cell>1.31?</cell></row><row><cell></cell><cell></cell><cell cols="3">(a) iCAN with DecAug</cell></row><row><cell></cell><cell></cell><cell cols="3">(b) iCAN w/o DecAug</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Dataset Transferability: Last two lines show the results on HICO-DET with extra information transferred from the COCO or V-COCO dataset.</figDesc><table><row><cell cols="2">Model DecAug Transfer</cell><cell>Full</cell><cell cols="3">Default Rare Non-Rare Full</cell><cell>Known Object Rare Non-Rare</cell></row><row><cell></cell><cell></cell><cell cols="2">14.84 10.45</cell><cell>16.15</cell><cell cols="2">16.26 11.33</cell><cell>17.73</cell></row><row><cell>iCAN</cell><cell>objects</cell><cell cols="2">16.39 12.23 16.65 12.28</cell><cell>17.63 17.96</cell><cell cols="2">17.85 13.68 18.09 13.49</cell><cell>19.10 19.47</cell></row><row><cell></cell><cell>spatial</cell><cell cols="2">16.56 12.32</cell><cell>17.83</cell><cell cols="2">18.15 13.70</cell><cell>19.48</cell></row><row><cell cols="4">out burdening GPUs. As shown in Tab. 4, when applying</cell><cell></cell><cell></cell></row><row><cell cols="4">multi-threads data loader, the training efficiency almost re-</cell><cell></cell><cell></cell></row><row><cell>mains unaffected.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation Study by Removing Either Component: LOA denotes local object appearance augmentation and GSC denotes global spatial correlation augmentation.</figDesc><table><row><cell cols="3">Model LOA GSC Train Rate (s/it) mAP role</cell></row><row><cell></cell><cell>0.183</cell><cell>44.7</cell></row><row><cell>iCAN</cell><cell>0.191 0.190</cell><cell>46.8 47.2</cell></row><row><cell></cell><cell>0.193</cell><cell>48.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation Study of Object Appearance and Spatial Correlation Augmentation (a) Local Object Appearance Augmentation Ablation Study: Apply other alternative interchangeability metrics or object similarity metrics. IIR and OSM denote instance interlocking ratio and object state matrix respectively Global Spatial Correlation Augmentation: we compare three placement metrics: random, heatmap<ref type="bibr" target="#b12">[13]</ref> and our poseguided GMM.</figDesc><table><row><cell cols="2">Interchangeability Similarity</cell><cell>mAP role</cell></row><row><cell>IIR</cell><cell>random</cell><cell>46.6</cell></row><row><cell>IIR</cell><cell>chamfer distance</cell><cell>47.2</cell></row><row><cell>IIR</cell><cell>mask distance</cell><cell>47.5</cell></row><row><cell>IIR</cell><cell>bbox distance</cell><cell>47.1</cell></row><row><cell>replace all</cell><cell>OSM</cell><cell>47.1</cell></row><row><cell>bbox IoU</cell><cell>OSM</cell><cell>47.5</cell></row><row><cell>IIR</cell><cell>OSM</cell><cell>48.0</cell></row><row><cell cols="2">(b) Object Placement Approach</cell><cell>mAP role</cell></row><row><cell>random</cell><cell></cell><cell>43.6</cell></row><row><cell>heatmap</cell><cell></cell><cell>45.3</cell></row><row><cell>pose-guided GMM</cell><cell></cell><cell>48.0</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 9</ref><p>. We show some augmented images as below. In each row, the first image is the original input (red box: human, blue box: object). The second image is the result of local object appearance augmentation. Some objects are not changed because they do not meet our requirement (instance interlocking ratio) in the paper. The third and forth images show the pose-guide probability map and the result of global spatial correlation augmentation. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of robot learning from demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Argall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chernova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Browning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and autonomous systems</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="469" to="483" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human-object interactions are more than the sum of their parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baldassano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2276" to="2288" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Detecting human-object interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Navier-stokes, fluid dynamics, and image and video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001</title>
		<editor>I-I. IEEE</editor>
		<meeting>the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ieee winter conference on applications of computer vision (wacv)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="381" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1017" to="1025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-ensembling with ganbased data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Modeling visual context is key to augmenting object detection datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="364" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pairwise body-part attention for recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="51" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instaboost: Boosting instance segmentation via probability map guided copy-pasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="682" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">ican: Instance-centric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10437</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A global sampling method for alpha matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2049" to="2056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An annotation saved is an annotation earned: Using fully synthetic training for object instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bokeloh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09967</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1175" to="1197" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Augmentation for small object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kisantal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Murawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naruniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07296</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introduction: A bayesian formulation of visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception as Bayesian inference</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10863" to="10872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1261" to="1270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06539</idno>
		<title level="m">Hake: Human activity knowledge engine</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Pastanet: Toward human activity knowledge engine</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Visual-semantic graph attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rojas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02302</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno>abs/1506.06204</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semi-parametric image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8808" to="8816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Intra-and inter-action understanding via temporal action parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icdar</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S M</forename><surname>Iftekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9469" to="9478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Interact as you intend: Intention-driven human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5410" to="5419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recognizing human-object interactions in still images by modeling the mutual context of objects and human poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5831" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5532" to="5540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09892</idno>
		<title level="m">Care about you: towards large-scale human-centric visual relationship detection</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
