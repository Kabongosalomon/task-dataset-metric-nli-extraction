<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhanced Correlation Matching based Video Frame Interpolation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narae</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woong</forename><forename type="middle">Il</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung Research</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhanced Correlation Matching based Video Frame Interpolation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel DNN based framework called the Enhanced Correlation Matching based Video Frame Interpolation Network to support high resolution like 4K, which has a large scale of motion and occlusion. Considering the extensibility of the network model according to resolution, the proposed scheme employs the recurrent pyramid architecture that shares the parameters among each pyramid layer for the optical flow estimation. In the proposed flow estimation, the optical flows are recursively refined by tracing the location with maximum correlation. The forward warping based correlation matching enables to improve the accuracy of flow update by excluding incorrectly warped features around the occlusion area. Based on the final bidirectional flows, the intermediate frame at arbitrary temporal position is synthesized using the warping and blending network and it is further improved by refinement network. Experiment results demonstrate that the proposed scheme outperforms the previous works at 4K video data and low-resolution benchmark datasets as well in terms of objective and subjective quality with the smallest number of model parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video frame interpolation (VFI) is a task to generate intermediate frames between consecutive frames and its main application is to increase frame-rate <ref type="bibr" target="#b13">[14]</ref> or to represent slow-motion video <ref type="bibr" target="#b8">[9]</ref>. It is also applicable to synthesize novel views of 3D contents <ref type="bibr" target="#b5">[6]</ref>. Advanced deep learning model based approaches have been introduced to achieve a better quality of the interpolated frames by discovering knowledge from large-scale diverse video scenes.</p><p>However, those approaches have a limitation on the range of motion coverage because the network only explores the motion of pixels within the receptive field of the CNN model that is pre-defined by their kernel size. As the most of capturing and display devices already support 4K resolution, the support of higher resolution is an inevitable feature of the practical application of VFI. The limitation of * indicates equal contribution.</p><p>handling large motion will become more pronounced as the image resolution increases and will be a major problem of practical use.</p><p>We can simply consider increasing the receptive field of a model by increasing kernel size or depth of network which attend utilizing more parameters to overcome the limitation of motion range. However, it is an inefficient solution because the memory size and computational complexity will be increased accordingly. Instead of expanding network kernel, the pyramid based architecture <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> has been proposed which progressively updates optical flow from coarse-scale to fine. Even though the network of each layer has a limited receptive field, the pyramid structure can extend the motion coverage by adding the number of pyramid layers. Recently, a recurrent model based pyramid network has been proposed to avoid the model parameters increasing according to the number of layers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref>. It basically re-uses the fixed model parameters in each layer, and the optical flows are iteratively refined from residual flows using the same network.</p><p>We propose an enhanced correlation matching based video frame interpolation network (ECMNet) to support high-resolution video with the lightweight model. The proposed network consists of three sub-networks; The first one is the bi-directional optical flow estimation between two consecutive input frames. Inspired by recurrent pyramid structure, this module adopts the shared pyramid structure to extend the receptive field efficiently. Also, we present an enhanced correlation matching algorithm to improve the accuracy of flow estimation. The next part is a frame synthesis module to generate intermediate flows and frame at an arbitrary temporal position. The last is a refinement module to improve the interpolated frame. Unlike the first module, the last two modules operate once with the original scale of input frames. This leads to computational efficiency and reduction of network parameters. In summary, the major contributions of our paper are as follows:</p><p>? We propose an enhanced correlation computation method based on the forward warped feature for flow estimation.</p><p>? At each pyramid layer, flows are gradually updated us-ing the highest feature correlation based on a novel feature matching algorithm.</p><p>? The proposed network achieves the state-of-the-art performance with the fewest number of parameters at 4K dataset compared to the existing VFI methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works 2.1. Video frame interpolation</head><p>In this section, we focus on recently proposed learning based algorithms. It can be categorized as kernel based and flow based approaches. Long et al. <ref type="bibr" target="#b12">[13]</ref> firstly introduced a generic CNN model to directly generate an intermediate frame. Since those generic models are not enough to exploit various motion characteristics of natural video, the result images are damaged by blur artifacts. It is enhanced by AdaConv and SepConv methods proposed by Niklaus et al. <ref type="bibr" target="#b15">[16]</ref>, which generate interpolated pixels by locally convolving the input frames. They formulated pixel interpolation as convolution over pixel patches instead of relying on optical flow, and it is able to synthesize pixels from relatively large neighborhoods than previous work. AdaCoF <ref type="bibr" target="#b10">[11]</ref> proposed to apply a deformable convolution kernel and CAIN <ref type="bibr" target="#b4">[5]</ref> proposed to utilize a feature reshaping operation, PixelShuffle with channel attention to synthesis image. These kernel based approaches tend to have a relatively simpler design of network model, while those have a limitation on the range of motion coverage because the pixels that are located out of range of the receptive field could not participate in interpolation. If it is tried to increase the coverage of large motion in high resolution, it requires a high memory footprint and heavy computational complexity.</p><p>The second approach is to estimate the motion flow of each pixel between consecutive frames and synthesize an image from the corresponding pixels guided by the estimated flow. Liu et al. <ref type="bibr" target="#b11">[12]</ref> proposed the Deep Voxel Flow (DVF) to perform trilinear interpolation using the spatiotemporal CNN model. Bao et al. <ref type="bibr" target="#b8">[9]</ref> proposed Super-Slomo which combines U-Net architecture of DVF and kernel based image synthesis between warped images. The main architecture in Super-Slomo <ref type="bibr" target="#b8">[9]</ref> is the cascade model of bidirectional flow estimation between input frames and image synthesis to blend two warped images guided by estimated bi-directional flows and it becomes a standard framework of flow based frame interpolation methods. However, those flow estimation based on U-Net structure still has a limitation on handling large motion. Instead of U-Net <ref type="bibr" target="#b16">[17]</ref>, it has been proposed to employ an advanced flow estimation model like PWC-Net <ref type="bibr" target="#b20">[21]</ref> applied iterative refinement using coarse-to-fine pyramid structure.</p><p>For the image synthesis part, a key issue is how to properly handle the occlusion area during blending warped pix-els. MEMC-Net <ref type="bibr" target="#b2">[3]</ref> implicitly handles occlusion by estimating occlusion masks from features of input frames, while DAIN <ref type="bibr" target="#b1">[2]</ref> explicitly detects occlusion from estimated depth information. From depth, it is determined whether the object is foreground or background and it is possible to select pixels from the foreground object in the area where both objects are overlapped. The performance should depend on the accuracy of depth and it is still challenging to obtain accurate depth from a single image.</p><p>While most flow based approaches perform backward warping for image synthesis, it is proposed in Softmax splatting <ref type="bibr" target="#b14">[15]</ref> to perform forward warping by solving the ambiguity on overlapping the pixels from different flows to the same pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Flow Estimation Network for Large Motion</head><p>From traditional approaches of an optical flow estimation, multi-resolution based approaches were already introduced which refine the resolution of flows from coarse-scale to fine in order to avoid drastically increased computational complexity of full search <ref type="bibr" target="#b3">[4]</ref>. In contrast to traditional coarse-to-fine approaches, PWC-Net <ref type="bibr" target="#b20">[21]</ref> utilizes a learnable feature pyramid instead of image pyramid to overcome the variation of lighting changes or shadow. Since it has been the state-of-the-art method until RAFT <ref type="bibr" target="#b21">[22]</ref> is introduced in 2020, PWC-Net has been mainly adopted in flow based VFI scheme <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref>. RAFT <ref type="bibr" target="#b21">[22]</ref> pointed out the drawbacks of the coarse-to-fine approach; the difficulty of recovering estimation error at a coarse resolution and the tendency to miss the small fast-moving object. It proposes a multi-scale correlation based estimation approach to overcome those issues. However, those approaches still have limitations in two aspects; one is that network size increases in proportion to the number of pyramid layers and the other is that it is required to re-train the network if the target resolution is different to the resolution of trained images. To overcome those issues, RRPN <ref type="bibr" target="#b26">[27]</ref> proposed the recurrent pyramid model to share the same weights and base network among different pyramid layers. Compared to RRPN <ref type="bibr" target="#b26">[27]</ref>, XVFI-Net <ref type="bibr" target="#b17">[18]</ref> proposes more generalized model to synthesis frames at the arbitrary temporal position because RRPN was only applicable to synthesize the center frame.</p><p>The network architecture of the proposed ECMNet is designed based on the recurrent pyramid approach as well. Compared to RRPN and XVFI-Net <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref>, our ECMNet applied correlation matching based flow estimation instead of directly deriving flows from U-Net. Compared to PWC-Net and RAFT <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, the correlation cost is constructed by comparing forward warped feature instead of backward warped one. Besides, while the existing methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> derive the flows by the network with cost volume as an input, the proposed method determines the optical flows directly by checking the maximum correlation value.  Note that w n (n = 0 or 1) is the learnable importance weight in F n coordinate to resolve the forward warping issues, where multiple pixels from F n map to the same target pixels in F 1?n <ref type="bibr" target="#b14">[15]</ref>, and learnable blending mask M is the confidence ratio of the warped image I 0?t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed algorithm</head><p>The overview of our framework is shown in figure 1. Our network takes consecutive images I 0 and I 1 , and the desired temporal position t ? (0, 1) as inputs to synthesize the intermediate frame I t .</p><p>In the flow estimation module, optical flows between input frames are estimated by comparing two encoded features. The intermediate flow estimation network then computes optical flows from the temporal position t to each input. Given the input images and intermediate flows, warping and blending are performed to synthesis the output frame at time t. Finally, the output image of the frame synthesis network is further improved at the following refinement network. The flow estimation module adopts a pyramid structure to compute the optical flow for input images from low to original resolution. After flow estimation, intermediate frame synthesis and refinement are performed only with the original scale of input images. This structure enables frame synthesis at any temporal position with estimated bi-directional optical flows. The details of the network architecture are described in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bi-directional optical flow estimation</head><p>Optical flow estimation for frame interpolation can be categorized as U-Net based and correlation based methods. U-Net based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref> have a relatively simple structure but are more difficult to explicitly compare input  features than correlation based methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. The correlation based one utilizes the feature correlation as an input of the flow estimation network to guide the warped (a) PWC-Net <ref type="bibr" target="#b20">[21]</ref> performs backward warping to compute F 0?1 and correlation matching is followed. (b) RAFT <ref type="bibr" target="#b21">[22]</ref> extracts the candidate from the corresponding location of (x, y) in the F 0 , candidates are warped using F 1?0 (x, y) to align the spatial coordinate. (c) Our model performs forward warping to compute F 0?1 , and the correlation is derived with the candidates extracted from F 1 .</p><p>feature matches to the original feature. However, these algorithms are difficult to ensure that the output flow is corrected to a position with the highest correlation. Therefore, we propose the flow correction with the highest feature correlation based on novel feature matching algorithms in order to estimate bi-directional flows. To estimate more accurate flows, forward warping is performed to compute feature correlations. Inspired by softmax splatting approach <ref type="bibr" target="#b14">[15]</ref>, the learnable importance weights w l+1 0 and w l+1 1 for blending forward warped pixels are derived at each pyramid layer.</p><p>The bi-directional flow estimation network consists of an encoder, feature downsampling network, a flow update module, and a flow upscaling network.</p><p>Encoder and feature downsampling network Encoder network extracts the features F l 0 and F l 1 from each input images I 0 and I 1 . We denote the pyramid level as l. To generate the input feature for each pyramid layer, the strided convolution network is used to downsample the features by a factor of 2.</p><p>Flow update module The bi-directional flows are updated through each pyramid layer from the initial flows flow 1 0?1 and flow 1 1?0 which are set to zero. The module computes similarities between the input features as the correlation. The flows from the previous pyramid level are updated by the comparison of correlation values. To align the coordinates of two features for deriving correlation, forward warping is applied in contrast to the existing methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> which use backward warping. <ref type="figure" target="#fig_3">Figure 2</ref> illustrates backward warped image has duplicated region when it fails to map the corresponding pixels. These miss-aligned features make ambiguity in determining the optimal values of the correlation. Since the forward-warped feature does not include these miss-aligned features, it is possible to estimate more accurate flows. <ref type="figure" target="#fig_4">Figure 3</ref> and 4 describe the correlation calculation and flow update process at a certain spatial location (x, y). The proposed correlation matching method shown in figure 3(c) calculates feature correlation between forward warped feature F 0?1 and the input feature F 1 . Since the location of warped feature F 0?1 is aligned to the feature F 1 , the correlation is computed between the feature F 0?1 at (x, y) and the feature The module calculates the similarity inside the search range by dot product according to the feature channel. This procedure is similar to derive the relevance between 'query' and 'key' in the attention mechanism <ref type="bibr" target="#b22">[23]</ref>. Applying softargmax operation enables the flow to be determined the highest correlation in a differential way. At each pyramid layer, the flow is gradually updated using the highest feature correlation within the limited search range. Since the updated flow is projected to the spatial coordination of F l 1 , it should be re-warped to the original coordination. We omit the re-warping process of the updated flow in <ref type="figure">figure 4</ref> for compact expression. The optical flow of the opposite direction is also estimated by the same process to obtain bidirectional optical flows.</p><formula xml:id="formula_0">F 1 at (x ? , y ? ), where (x ? , y ? ) ? {x ? r ? x ? ? x + r, y ? r ? y ? ? y + r} with search range r.</formula><p>Flow upscaling network Flow upsampling is required to use the flows of the previous pyramid level estimated with lower resolution. Since simple bi-linear interpolation leads to degradation of flow details, we design the network to minimize the error that occurred in the upsampling process. The network takes inputs of the flow update module outputs, the importance weight outputs of the previous level, and the features of pyramid levels. The network consists of decoder network and two sub-networks to estimate the residuals and importance weights for the current pyramid level. The residuals contribute to improving the quality of the upsampled flows.   <ref type="figure">Figure 4</ref>: Flow update process. Correlations of candidates are calculated with dot product according to the feature channel. Based on the soft-argmax operation on the correlation matrix, it is possible to determine the candidate having the highest similarity. After that, flow is updated with the selected candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Frame synthesis network</head><p>Intermediate frame is derived using two input frames and estimated bi-directional flows (flow 0?1 and flow 1?0 ). Green boxes of figure 1 describe frame synthesis network which consists of intermediate flow estimation network and frame synthesis layer. Frame synthesis network performs once at the original scale of input images, unlike flow estimation module in section 3.1 adopts pyramidal structure. We consider if the bi-directional flows are already estimated with enough receptive field which can be managed with a shared pyramidal structure, frame synthesis does not need to repeat with multi-scaled input images. We firstly calculate the intermediate flows flow t?0 and flow t?1 and then warping and blending are performed to interpolate the target frame I t .</p><p>Feature extraction network Frame synthesis network uses contextual information to improve the details and to deal with occlusions. The contextual feature is generated using two kinds of features, one is extracted from the encoder of the flow estimation module, and the other is extracted from the conv1 layer of the pre-trained VGG16 <ref type="bibr" target="#b18">[19]</ref>. The final contextual feature presented in figure 1 is obtained through a convolution layer that has inputs of the aforementioned extracted features, two input images, and flows.</p><p>Intermediate flow estimation network We use backward warping to synthesis an intermediate frame to avoid hole generation. Since backward warping needs a flow that is starting from the temporal target position t to input images, we adopt the flow reversal approach <ref type="bibr" target="#b24">[25]</ref> to estimate a flow aligned to coordinate at t. Since it is possible for flow reversal to generate holes in the reversed flow mostly around the occluded area due to forward warping, flow residuals are computed to compensate for each reversed flow. Additional refinement for the derived flows, such as the weighted average of corresponding flows, is not performed to maintain a characteristic of each flow.</p><formula xml:id="formula_1">flow t?0 = rev(t ? flow 0?1 ) + ?flow t?0 , flow t?1 = rev((1 ? t) ? flow 1?0 ) + ?flow t?1 ,<label>(1)</label></formula><p>where the function 'rev' denotes the flow reversal, and ?flow t?0 and ?flow t?1 are the flow residuals.</p><p>To estimate the flow residuals, we use the U-Net architecture composed of three layers of down-convolution and three layers of up-convolution. The network takes the bidirectional flows and the contextual feature as inputs, and generates residuals of intermediate flows and blending mask M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intermediate Frame Synthesis</head><p>We simply perform frame synthesis with warping and blending. In the warping process, two input images are backward warped to time t using estimated intermediate flows. The warped images I 0?t and I 1?t usually contain artifacts especially the region which is unseen at the input images and appeared at the time t. To overcome the issue, we blend two warped images with the learnable blending mask M as follows.</p><formula xml:id="formula_2">I blend t = (M ? I 0?t ) + ((1 ? M ) ? I 1?t ),<label>(2)</label></formula><p>where ? is the pixel-wise multiplication operation, and blending mask M denotes the confidence ratio of the warped image I 0?t . The confidence ratio has a value between 0 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Refinement network</head><p>The refinement network enhances the interpolated output given the blended intermediate frame, contextual feature, input images, and intermediate flows. Optical flow estimation can be failed in the occlusion, blurred, and brightness changed area. Most of the artifacts caused by flow estimation failure can be resolved by blending, but the artifacts may occur in the same area of I 0?t and I 1?t . To solve this case, we design the network adding the dilated convolutional block to grow the receptive field inspired by image inpainting algorithm <ref type="bibr" target="#b7">[8]</ref>. The network completes the occluded region and enforces the network to predict the residuals between the ground-truth and blended output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Functions</head><p>The proposed network illustrated in <ref type="figure">figure 4</ref> is trained using the following two loss functions; One is to measure how close the blended intermediate frame I blend t to the ground truth frame I GT t , and the other is to compare the refined intermediate frame I refine t with the frame I GT t .</p><formula xml:id="formula_3">L blend = x ||I blend t (x) ? I GT t (x)|| 1 , L refine = x ||I refine t (x) ? I GT t (x)|| 1 .<label>(3)</label></formula><p>The total-loss function is defined as:</p><formula xml:id="formula_4">L = L blend + 2 ? L refine ,<label>(4)</label></formula><p>where the weight for each loss is empirically determined. All the networks are trained in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Search range analysis in flow estimation</head><p>We can manage the range of receptive fields for flow estimation with the number of pyramid layers. At each pyramid level, the search range is determined with a radius r as mentioned in section 3.1. The totally covered search range is computed by</p><formula xml:id="formula_5">R 1 = D initial ? r, R L = R L?1 ? D L + r,<label>(5)</label></formula><p>where the R L means the maximum range of motion coverage, D L is a downsampling ratio according to the pyramid level in the encoder network, and L denotes the total number of pyramid levels. The proposed model determines the variables to have enough total search range and to have an efficient number of parameters. Since we first reduce the image size as a quarter, the downsampling ratio is 2 except for the D initial . The radius r is determined to 4. Therefore, the number of pyramid levels should be 6 to cover the 4K dataset, such as X4K1000FPS <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted experiments of video frame interpolation for four datasets: X4K1000FPS (4K) <ref type="bibr" target="#b17">[18]</ref>, VVC (Versatile Video Coding reference dataset, 4K), Vimeo90K (240p) <ref type="bibr" target="#b25">[26]</ref>, and UCF-101 (256 ? 256) <ref type="bibr" target="#b19">[20]</ref>.</p><p>X4K1000FPS <ref type="bibr" target="#b17">[18]</ref> The testset in X4K1000FPS contains 15 clips with a length of 33 consecutive frames and each frame is captured by 1000fps. We select 8 images with four frames interval in each sequence and conduct the 8 times interpolation to configure a similar test environment in the existing 4K VFI algorithm <ref type="bibr" target="#b17">[18]</ref>. 30.51 0.8719 33.20 0.8979 <ref type="table">Table 1</ref>: Quantitative comparison on X4K1000FPS (4K) and VVC. Note that the numbers in red denote the best performance and numbers in blue mean the second best performance. Subscript o is official pre-trained weight provided from the author, subscript f denotes fine-tuned model with X4K1000FPS training dataset from the pre-trained weight, and subscripts of Ours and XVFI are the number of pyramid layers for the test. The number of parameters #P(M) is expressed in million units.</p><p>VVC VVC contains six 4K videos captured by 30 to 60fps. To configure a similar comparison environment with X4K1000FPS, we randomly selected 150 test triplets from VVC with the interval of the input images as 1/30s.</p><p>Vimeo90K and UCF-101 <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref> Both datasets are standard triplet (240p) sequences to measure the performance of video frame interpolation. To validate the performance in the low resolution and low frame-rate videos, we also evaluate for 3,782 triplets in Vimeo90K testset and randomly selected 350 triplets from UCF-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We trained our model (Ours x4k ) for high resolution video using X4K1000FPS <ref type="bibr" target="#b17">[18]</ref> dataset which contains 4,408 clips. Each clip is composed with a lengths of 65 consecutive frames and image resolution of 768 ? 768 cropped from 4K size original image. During training, two input frames I 0 and I 1 , and one target intermediate frame I t were grouped for a training sample. We designed the training data loader to randomly determine the temporal distance between two inputs as from 2 to 32, and the temporal position of the target frame within the temporal distance of inputs for arbitrary temporal frame interpolation. The 384 ? 384 training image patches were randomly cropped from the original training data. We also performed flipping XVFI 5 XVFI <ref type="bibr" target="#b2">3</ref> Ours</p><formula xml:id="formula_6">GT CAIN f AdaCoF f Overlapped image Overlapped image XVFI 5</formula><p>Ours GT <ref type="figure">Figure 5</ref>: Visual comparisons of interpolation results from the X4K1000FPS dataset. Each result is zoomed from the yellow boxed part of the overlapped image. In particular, the purple boxed area is compared for the only two algorithms XVFI 5 and Ours. Prominent results are highlighted with red boxes. images horizontally and vertically, and swapping the order of frames for data augmentation.</p><p>To evaluate the low resolution and low frame-rate dataset, we also trained our another model(Ours vimeo ) using Vimeo90K <ref type="bibr" target="#b25">[26]</ref> dataset which contains 51,312 triplets of 256 ? 448 video frames. We applied the random cropping as 256 ? 256, flipping, and swapping on the triplet for the augmentation. As shown in Equation 5, we determined the pyramid level L according to the investigated flow magnitude <ref type="bibr" target="#b17">[18]</ref>. The L for Ours x4k was set to 4, and for ours vimeo was set to 1.</p><p>The weights of our model were initialized with the method of He initialization <ref type="bibr" target="#b6">[7]</ref>. We adopted ADAM <ref type="bibr" target="#b9">[10]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative and qualitative analysis</head><p>We conducted the quantitative evaluation for 4K video frame interpolation. For this experiment, we evaluate the methods on X4K1000FPS testset <ref type="bibr" target="#b17">[18]</ref> and randomly selected 150 test triplets in VVC. We compared our model (Ours x4k ) with previous video frame interpolation models, CAIN <ref type="bibr" target="#b4">[5]</ref>, AdaCoF <ref type="bibr" target="#b10">[11]</ref>, and XVFI <ref type="bibr" target="#b17">[18]</ref>. We evaluate both results of methods gathered by using pre-trained weights provided by the author and fine-tuned weight starting from the pre-trained model. Note that RRPN <ref type="bibr" target="#b26">[27]</ref> is excluded in the experiment because the official code has not been published yet. <ref type="table">Table 1</ref> compares the PSNR and the SSIM measures between our method and other ones. The results show that our model outperforms the other methods on both 4K datasets with the fewest # of parameters. Since the maximum flow magnitude in the X4K1000FPS testset is over 400, our method shows the best accuracy with the pyramid level 6 on the experiment. The qualitative results described in figure 5 for X4K1000FPS show that our interpolation results capture the objects which are failed in other ones.</p><p>We also evaluate our model(Ours vimeo ) compared to other previous methods on 3,782 Vimeo90K testset and randomly selected 350 test triplets from UCF-101.</p><p>In contrast to the 4K video frame interpolation, the testset in Vimeo90K and UCF-101 contains similar statistics of optical flow magnitude compared to the training environment. Therefore, we evaluate our model with the pyramid level to 1 which is the same as the level in the training process.</p><p>For this experiment, we compare our method and other VFI methods pre-trained with vimeo90K dataset. Quantitative comparisons of the video frame interpolation results are shown in <ref type="table" target="#tab_3">Table 2</ref>. These comparisons show our results are on par with the state-of-the-art method in <ref type="bibr" target="#b17">[18]</ref>, while showing much better results compared to the other video frame interpolation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation studies</head><p>Optical flow estimation The key contribution of our algorithm is accurate optical flow estimation for highresolution with fast motion video. To validate the performance of our flow estimation module, We conduct the ablation study for the model using two existing flow estimation methods, PWC-Net <ref type="bibr" target="#b20">[21]</ref> and RAFT <ref type="bibr" target="#b21">[22]</ref>. Experiments are performed with two conditions for each algorithm. One is freezing the flow estimation layers while training the interpolation module, and the other is training the entire network from the pre-trained weights. The training is performed under the same conditions in X4K1000FPS dataset <ref type="bibr" target="#b17">[18]</ref>. <ref type="table">Table 3</ref> shows the results of ablation studies. Since the existing methods are not targeting the large motion and are not familiar with the X4K1000FPS dataset, the fine-tuned model shows improved results compared to each frozen weight model. Although the results of fine-tuning have improved, the proposed forward warping based method has better results. From the results, accurate flow estimation and update modules with large receptive fields are necessary for the 4K VFI.  <ref type="table">Table 3</ref>: Ablation studies for optical flow estimation and refinement network. subscript 'fr' denotes freezed pre-trained weight for optical flow estimation network, subscript 'fi' is the model fine-tuned the network starting from the pretrained weight in the training process, and 'synth' denotes ours frame synthesis network + refinement network.</p><p>Refinement module To verify the performance of the refinement module, we evaluate our algorithm with and without refinement module for the X4K1000FPS test dataset. The optical flow estimation can be failed in high-resolution video with fast object motion. Furthermore, our interpolation module is designed simply blending two warped frames than other kernel based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>, which can cause missing regions in the blended output. We integrate the dilated convolution layer for the refinement module which is expected to fill the missing parts. As shown in <ref type="table">Table 3</ref>, both PSNR and SSIM are noticeably improved with the refinement module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed ECMNet to support frame interpolation for high resolution with fast motion video. Our model proposed enhanced correlation matching based on bi-directional optical flow estimation from consecutive frames and adopts the advantage of shared weights at each pyramid layer which enables to adjust the number of pyramid levels according to the target video. Furthermore, the frame synthesis and refinement networks are designed to synthesize the intermediate frame efficiently. The proposed network is composed of the integration of convolution layer and differentiable operators which facilitates end-to-end training. Our experimental results support the robust performance of the proposed network qualitatively and quantitatively. Especially, for the 4K datasets that we targeted, ECMNet achieve outperformed results against the state-of-the-art algorithms with fewer parameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the network. Our framework consists of three sub-networks; Bi-directional flow estimation network, frame synthesis network, and refinement network. Shared pyramid network for flow estimation is denoted to yellow boxes. Green boxes indicates frame synthesis network which computes intermediate flow and initial intermediate frame for the arbitrary temporal position t ? (0, 1). Finally, refinement network corrects the intermediate frame using estimated residual.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Backward and forward warping examples<ref type="bibr" target="#b0">[1]</ref>. As shown in I 1?0 , backward warping creates duplication where the flow fail to map pixels to the shifted location such as occlusion region. The duplication makes difficulties to compare the similarity between the warped image and original image. On the other hand, forward warping doesn't cause confusion because it does not create unknown parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of correlation matching method between existing and the proposed one. Note that the local grid (2r + 1) ? (2r + 1) in the features indicates the search range to find motion flows through correlation comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>The flows are corrected by selecting one having the highest correlation value from the candidates. The details of the flow update process are shown in figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>AdaCoF o 24.35 0.7296 31.89 0.8909 21.8 AdaCoF f 25.86 0.7691 32.67 0.8841</figDesc><table><row><cell>Method</cell><cell>X4K1000FPS PSNR SSIM PSNR SSIM VVC</cell><cell>#P(M)</cell></row><row><cell>CAIN o CAIN f</cell><cell cols="2">24.50 0.7458 32.43 0.8910 42.8 27.36 0.8182 32.12 0.8958</cell></row><row><cell>XVFI 3 XVFI 5</cell><cell>28.98 0.8573 33.17 0.8962 30.12 0.8707 33.08 0.8960</cell><cell>5.5</cell></row><row><cell>Ours x4k 4</cell><cell>28.81 0.8430 33.15 0.8977</cell><cell></cell></row><row><cell>Ours x4k 5</cell><cell>30.47 0.8719 33.19 0.8979</cell><cell>4.7</cell></row><row><cell>Ours x4k 6</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>optimizer with learning rate 1e ? 4, and reduced the learning rate factor of 4 at [100, 150, 180]-th epoch. We set the batch size as 14 for train Ours x4k , and 50 for Ours vimeo . Training takes about two days for Ours x4k , and four days for Ours vimeo to fully converge on NVIDIA Tesla V100 GPU.</figDesc><table><row><cell>Method</cell><cell>Vimeo90K PSNR SSIM PSNR SSIM UCF-101</cell><cell>#P(M)</cell></row><row><cell>AdaCoF</cell><cell cols="2">34.27 0.9714 34.95 0.9504 21.8</cell></row><row><cell>CAIN</cell><cell cols="2">34.65 0.9730 34.83 0.9507 42.8</cell></row><row><cell cols="2">XVFI vimeo 35.07 0.9760 34.95 0.9505</cell><cell>5.5</cell></row><row><cell cols="2">Ours vimeo 34.95 0.9749 34.97 0.9506</cell><cell>4.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison on Vimeo90K(240p) and UCF-101(256 ? 256) with the number of parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Method PSNR SSIM #P(M)PWC-Net fr + synth 27.39 0.8090 11.7 PWC-Net fi + synth 28.40 0.8183</figDesc><table><row><cell>RAFT fr + synth RAFT fi + synth</cell><cell>29.17 0.8519 29.67 0.8570</cell><cell>7.7</cell></row><row><cell>Ours W.O.refine</cell><cell>30.02 0.8609</cell><cell>3.7</cell></row><row><cell>Ours full</cell><cell>30.51 0.8719</cell><cell>4.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth-aware video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3703" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Memc-net: Motion estimation and motion compensation driven neural network for video interpolation and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lucas/kanade meets horn/schunck: Combining local and global optic flow methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schn?rr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Channel attention is all you need for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungsub</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10663" to="10671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepstereo: Learning to predict new views from the world&apos;s imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Neulander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5515" to="5524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9000" to="9008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adacof: Adaptive collaboration of flows for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeoh</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehyun</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyoun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5316" to="5325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Video frame synthesis using deep voxel flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4463" to="4471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning image matching by simply watching video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gucan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="434" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Phase-based frame interpolation for video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1410" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Softmax splatting for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive separable convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Xvfi: extreme video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonjun</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihyong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munchurl</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16206</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1289" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Siyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00627</idno>
		<title level="m">Quadratic video interpolation</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video enhancement with taskoriented flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A flexible recurrent residual pyramid network for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="474" to="491" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
