<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
							<email>albanie@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliane</forename><surname>Momeni</surname></persName>
							<email>liliane@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
							<email>afourast@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Fox</surname></persName>
							<email>neil.fox@ucl.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Deafness, Cognition and Language Research Centre</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Sign Language Recognition, Visual Keyword Spotting</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent progress in fine-grained gesture and action classification, and machine translation, point to the possibility of automated sign language recognition becoming a reality. A key stumbling block in making progress towards this goal is a lack of appropriate training data, stemming from the high complexity of sign annotation and a limited supply of qualified annotators. In this work, we introduce a new scalable approach to data collection for sign recognition in continuous videos. We make use of weakly-aligned subtitles for broadcast footage together with a keyword spotting method to automatically localise sign-instances for a vocabulary of 1,000 signs in 1,000 hours of video. We make the following contributions: (1) We show how to use mouthing cues from signers to obtain high-quality annotations from video data-the result is the BSL-1K dataset, a collection of British Sign Language (BSL) signs of unprecedented scale; (2) We show that we can use BSL-1K to train strong sign recognition models for co-articulated signs in BSL and that these models additionally form excellent pretraining for other sign languages and benchmarks-we exceed the state of the art on both the MSASL and WLASL benchmarks. Finally, (3) we propose new large-scale evaluation sets for the tasks of sign recognition and sign spotting and provide baselines which we hope will serve to stimulate research in this area.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the continual increase in the performance of human action recognition there has been a renewed interest in the challenge of recognising sign languages such as American Sign Language (ASL), British Sign Language (BSL), and Chinese Sign Language (CSL). Although in the past isolated sign recognition has seen some progress, recognition of continuous sign language remains extremely challenging <ref type="bibr">[10]</ref>. Isolated signs, as in dictionary examples, do not suffer from the naturally occurring complication of co-articulation (i.e. transition motions) between preceding and subsequent signs, making them visually very different from continuous signing. If we are to recognise ASL and BSL performed naturally by signers, then we need to recognise co-articulated signs.</p><p>Similar problems were faced by Automatic Speech Recognition (ASR) and the solution, as always, was to learn from very large scale datasets, using a parallel corpus of speech and text. In the vision community, a related path was taken with the modern development of automatic lip reading: first isolated words were recognised <ref type="bibr">[16]</ref>, and later sentences were recognised [15]-in both cases tied to the release of large datasets. The objective of this paper is to design a scalable method to generate large-scale datasets of continuous signing, for training and testing sign language recognition, and we demonstrate this for BSL. We start from the perhaps counter-intuitive observation that signers often mouth the word they sign simultaneously, as an additional signal [5, <ref type="bibr" target="#b20">55,</ref><ref type="bibr" target="#b21">56]</ref>, performing similar lip movements as for the spoken word. This differs from mouth gestures which are not derived from the spoken language <ref type="bibr">[21]</ref>. The mouthing helps disambiguate between different meanings of the same manual sign <ref type="bibr" target="#b27">[62]</ref> or in some cases simply provides redundancy. In this way, a sign is not only defined by the hand movements and hand shapes, but also by facial expressions and mouth movements <ref type="bibr">[20]</ref>.</p><p>We harness word mouthings to provide a method of automatically annotating continuous signing. The key idea is to exploit the readily available and abundant supply of sign-language translated TV broadcasts that consist of an overlaid interpreter performing signs and subtitles that correspond to the audio content. The availability of subtitles means that the annotation task is in essence one of alignment between the words in the subtitle and the mouthings of the overlaid signer. Nevertheless, this is a very challenging task: a continuous sign may last for only a fraction (e.g. 0.5) of a second, whilst the subtitles may last for several seconds and are not synchronised with the signs produced by the signer; the word order of the English need not be the same as the word order of the sign language; the sign may not be mouthed; and furthermore, words may not be signed or may be signed in different ways depending on the context. For example, the word "fish" has a different visual sign depending on referring to the animal or the food, introducing additional challenges when associating subtitle words to signs.</p><p>To detect the mouthings we use visual keyword spotting-the task of determining whether and when a keyword of interest is uttered by a talking face using only visual information-to address the alignment problem described above. Two factors motivate its use: (1) direct lip reading of arbitrary isolated mouthings is a fundamentally difficult task, but searching for a particular known word within a short temporal window is considerably less challenging; (2) the recent availability of large scale video datasets with aligned audio transcriptions [1, 17] now allows for the training of powerful visual keyword spotting models [33, <ref type="bibr" target="#b18">53,</ref><ref type="bibr" target="#b29">64]</ref> that, as we show in the experiments, work well for this application.</p><p>We make the following contributions: (1) we show how to use visual keyword spotting to recognise the mouthing cues from signers to obtain high-quality annotations from video data-the result is the BSL-1K dataset, a large-scale collection of BSL (British Sign Language) signs with a 1K sign vocabulary; (2) <ref type="table" target="#tab_7">Table 1</ref>. Summary of previous public sign language datasets: The BSL-1K dataset contains, to the best of our knowledge, the largest source of annotated sign data in any dataset. It comprises of co-articulated signs outside a lab setting. We show the value of BSL-1K by using it to train strong sign recognition models for co-articulated signs in BSL and demonstrate that these models additionally form excellent pretraining for other sign languages and benchmarks-we exceed the state of the art on both the MSASL and WLASL benchmarks with this approach;</p><p>(3) We propose new evaluation datasets for sign recognition and sign spotting and provide baselines for each of these tasks to provide a foundation for future research 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Sign language datasets. We begin by briefly reviewing public benchmarks for studying automatic sign language recognition. Several benchmarks have been proposed for American <ref type="bibr">[4,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b5">40,</ref><ref type="bibr" target="#b26">61]</ref>, German <ref type="bibr" target="#b0">[35,</ref><ref type="bibr" target="#b25">60]</ref>, Chinese [14, 32], and Finnish <ref type="bibr" target="#b24">[59]</ref> sign languages. BSL datasets, on the other hand, are scarce. One exception is the ongoing development of the linguistic corpus <ref type="bibr" target="#b16">[51,</ref><ref type="bibr" target="#b17">52]</ref> which provides fine-grained annotations for the atomic elements of sign production. Whilst its high annotation quality provides an excellent resource for sign linguists, the annotations span only a fraction of the source videos so it is less appropriate for training current state-of-the-art data-hungry computer vision pipelines. Tab. 1 presents an overview of publicly available datasets, grouped according to their provision of isolated signs or co-articulated signs. Earlier datasets have been limited in the size of their video instances, vocabularies, and signers. Within the isolated sign datasets, Purdue RVL-SLLL <ref type="bibr" target="#b26">[61]</ref> has a limited vocabulary of 104 signs (ASL comprises more than 3K signs in total <ref type="bibr" target="#b23">[58]</ref> Due to the difficulty of annotating co-articulated signs in long videos, continuous datasets have been limited in their vocabulary, and most of them have been recorded in lab settings [32, <ref type="bibr" target="#b25">60,</ref><ref type="bibr" target="#b26">61]</ref>. RWTH-Phoenix <ref type="bibr" target="#b0">[35]</ref> is one of the few realistic datasets that supports training complex models based on deep neural networks. A recent extension also allows studying sign language translation [10]. However, the videos in <ref type="bibr">[10,</ref><ref type="bibr" target="#b0">35]</ref> are only from weather broadcasts, restricting the domain of discourse. In summary, the main constraints of the previous datasets are one or more of the following: (i) they are limited in size, (ii) they have a large total vocabulary but only of isolated signs, or (iii) they consist of natural coarticulated signs but cover a limited domain of discourse. The BSL-1K dataset provides a considerably greater number of annotations than all previous public sign language datasets, and it does so in the co-articulated setting for a large domain of discourse.</p><p>Sign language recognition. Early work on sign language recognition focused on hand-crafted features computed for hand shape and motion <ref type="bibr">[24,</ref><ref type="bibr">25,</ref><ref type="bibr" target="#b19">54,</ref><ref type="bibr" target="#b22">57]</ref>. Upper body and hand pose have then been widely used as part of the recognition pipelines <ref type="bibr">[7,</ref><ref type="bibr">9,</ref><ref type="bibr">19,</ref><ref type="bibr" target="#b13">48,</ref><ref type="bibr" target="#b15">50]</ref>. Non-manual features such as face [24, <ref type="bibr" target="#b0">35,</ref><ref type="bibr" target="#b12">47]</ref>, and mouth [3, <ref type="bibr" target="#b1">36,</ref><ref type="bibr" target="#b3">38]</ref> shapes are relatively less considered. For sequence modelling of signs, HMMs <ref type="bibr">[2,</ref><ref type="bibr">23,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b19">54]</ref>, and more recently LSTMs [9, 32, 65, 67], have been utilised. Koller et al. <ref type="bibr" target="#b4">[39]</ref> present a hybrid approach based on CNN-RNN-HMM to iteratively re-align sign language videos to the sequence of sign annotations. More recently 3D CNNs have been adopted due to their representation capacity for spatio-temporal data <ref type="bibr">[6,</ref><ref type="bibr">8,</ref><ref type="bibr">31,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b5">40]</ref>. Two recent concurrent works <ref type="bibr">[34,</ref><ref type="bibr" target="#b5">40]</ref> showed that I3D models [13] significantly outperform their pose-based counterparts. In this paper, we confirm the success of I3D models, while also showing improvements using pose distillation as pretraining. There have been efforts to use sequence-to-sequence translation models for sign language translation [10], though this has been limited to the weather discourse of RWTH-Phoenix, and the method is limited by the size of the training set. The recent work of <ref type="bibr" target="#b6">[41]</ref> localises signs in continuous news footage to improve an isolated sign classifier.</p><p>In this work, we utilise mouthings to localise signs in weakly-supervised videos. Previous work <ref type="bibr">[7,</ref><ref type="bibr">17,</ref><ref type="bibr">18,</ref><ref type="bibr" target="#b15">50]</ref> has used weakly aligned subtitles as a source of training data, and both one-shot <ref type="bibr" target="#b15">[50]</ref> (from a visual dictionary) and zero-shot [6] (from a textual description) have also been used. Though no previous work, to our knowledge, has put these ideas together. The sign spotting problem was formulated in <ref type="bibr">[22,</ref><ref type="bibr" target="#b24">59]</ref>.</p><p>Using the mouth patterns. The mouth has several roles in sign language that can be grouped into spoken components (mouthings) and oral components (mouth gestures) <ref type="bibr" target="#b27">[62]</ref>. Several works focus on recognising mouth shapes <ref type="bibr">[3,</ref><ref type="bibr" target="#b3">38]</ref> to recover mouth gestures. Few works <ref type="bibr" target="#b1">[36,</ref><ref type="bibr" target="#b2">37]</ref> attempt to recognise mouthings in sign language data by focusing on a few categories of visemes, i.e., visual correspondences of phonemes in the lip region <ref type="bibr">[26]</ref>. Most closely related to our work, <ref type="bibr" target="#b14">[49]</ref> similarly searches subtitles of broadcast footage and uses the mouth as a cue to improve alignment between the subtitles and the signing. Two key differences between our work and theirs are: (1) we achieve precise localisation through keyword spotting, whereas they only use an open/closed mouth classifier to reduce the number of candidates for a given sign; (2) scale-we gather signs over 1,000 hours of signing (in contrast to the 30 hours considered in <ref type="bibr" target="#b14">[49]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Sign Recognition with Automatic Labels</head><p>In this section, we describe the process used to collect BSL-1K, a large-scale dataset of BSL signs. An overview of the approach is provided in <ref type="figure" target="#fig_1">Fig. 1</ref>. In Sec. 3.1, we describe how large numbers of video clips that are likely to contain a given sign are sourced from public broadcast footage using subtitles; in Sec. 3.2, we show how automatic keyword spotting can be used to precisely localise specific signs to within a fraction of a second; in Sec. 3.3, we apply this technique to efficiently annotate a large-scale dataset with a vocabulary of 1K signs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Finding probable signing windows in public broadcast footage</head><p>The source material for the dataset comprises 1,412 episodes of publicly broadcast TV programs produced by the BBC which contains 1,060 hours of continuous BSL signing. The episodes cover a wide range of topics: medical dramas, history and nature documentaries, cooking shows and programs covering gardening, business and travel. The signing represents a translation (rather than a transcription) of the content and is produced by a total of forty professional BSL interpreters. The signer occupies a fixed region of the screen and is cropped directly from the footage. A full list of the TV shows that form BSL-1K can be found in Appendix C.2. In addition to videos, these episodes are accompanied by subtitles (numbering approximately 9.5 million words in total). To locate temporal windows in which instances of signs are likely to occur within the source footage, we first identify a candidate list of words that: (i) are present in the subtitles; (ii) have entries in both BSL signbank 2 and sign BSL 3 , two online dictionaries of isolated signs (to ensure that we query words that have valid mappings to signs). The result is an initial vocabulary of 1,350 words, which are used as queries for the keyword spotting model to perform sign localisation-this process is described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Precise sign localisation through visual keyword spotting</head><p>By searching the content of the subtitle tracks for instances of words in the initial vocabulary, we obtain a set of candidate temporal windows in which instances of signs may occur. However, two factors render these temporal proposals extremely noisy: (1) the presence of a word in the subtitles does not guarantee its presence in the signing; (2) even for subtitled words that are signed, we find through inspection that their appearance in the subtitles can be misaligned with the sign itself by several seconds.</p><p>To address this challenge, we turn to visual keyword spotting. Our goal is to detect and precisely localise the presence of a sign by identifying its "spoken components" <ref type="bibr" target="#b21">[56]</ref> within a temporal sequence of mouthing patterns. Two hypotheses underpin this approach: (a) that mouthing provides a strong localisation signal for signs as they are produced; (b) that this mouthing occurs with sufficient frequency to form a useful localisation cue. Our method is motivated by studies in the Sign Linguistics literature which find that spoken components frequently serve to identify signs-this occurs most prominently when the mouth pattern is used to distinguish between manual homonyms 4 (see <ref type="bibr" target="#b21">[56]</ref> for a detailed discussion). However, even if these hypotheses hold, the task remains extremely challenging-signers typically do not mouth continuously and the mouthings that are produced may only correspond to a portion of the word <ref type="bibr" target="#b21">[56]</ref>. For this reason, existing lip reading approaches cannot be used directly (indeed, an initial exploratory experiment we conducted with the state-of-the-art lip reading model of [1] achieved zero recall on five-hundred randomly sampled sentences of signer mouthings from the BBC source footage).</p><p>The key to the effectiveness of visual keyword spotting is that rather than solving the general problem of lip reading, it solves the much easier problem of identifying a single token from a small collection of candidates within a short temporal window. In this work, we use the subtitles to construct such windows. The pipeline for automatic sign annotations therefore consists of two stages ( <ref type="figure" target="#fig_1">Fig. 1, left)</ref>: (1) For a given target sign e.g. "happy", determine the times of all occurrences of this sign in the subtitles accompanying the video footage. The subtitle time provides a short window during which the word was spoken, but not necessarily when its corresponding sign is produced in the translation. We therefore extend this candidate window by several seconds to increase the likelihood that the sign is present in the sequence. We include ablations to assess the influence of this padding process in Sec. 5 and determine empirically that padding by four seconds on each side of the subtitle represents a good choice.</p><p>(2) The resulting temporal window is then provided, together with the target word, to a keyword spotting model (described in detail in Sec. 4.1) which estimates the probability that the sign was mouthed at each time step (we apply the keyword spotter with a stride of 0.04 seconds-this choice is motivated by the fact that the source footage has a frame rate of 25fps). When the keyword spotter asserts with high confidence that it has located a sign, we take the location of the peak posterior probability as an anchoring point for one endpoint of a 0.6 second window (this value was determined by visual inspection to be sufficient for capturing individual signs). The peak probability is then converted into a decision about whether a sign is present using a threshold parameter. To build the BSL-1K dataset, we select a value of 0.5 for this parameter after conducting experiments (reported in Tab. 3) to assess its influence on the downstream task of sign recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">BSL-1K dataset construction and validation</head><p>Following the sign localisation process described above, we obtain approximately 280k localised signs from a set of 2.4 million candidate subtitles. To ensure that the dataset supports study of signer-independent sign recognition, we then compute face embeddings (using an SENet-50 [30] architecture trained for verification on the VGGFace2 dataset [11]) to group the episodes according to which of the forty signers they were translated by. We partition the data into three splits, assigning thirty-two signers for training, four signers for validation and four signers for testing. We further sought to include an equal number of hearing and non-hearing signers (the validation and test sets both contain an equal number of each, the training set is approximately balanced with 13 hearing, 17 non-hearing and 2 signers whose deafness is unknown). We then perform a further filtering step on the vocabulary to ensure that each word included in the dataset is represented with high confidence (at least one instance with confidence 0.8) in the training partition, which produces a final dataset vocabulary of 1,064 words (see <ref type="figure" target="#fig_2">Fig. 2</ref> for the distribution and Appendix C.3 for the full word list).</p><p>Validating the automatic annotation pipeline. One of the key hypotheses underpinning this work is that keyword spotting is capable of correctly locating signs. We first verify this hypothesis by presenting a randomly sampled subset of the test partition to a native BSL signer, who was asked to assess whether the short temporal windows produced by the keyword spotting model with high confidence (each 0.6 seconds in duration) contained correct instances of the target sign. A screenshot of the annotation tool developed for this task is provided in <ref type="figure">Fig. A.4</ref>. A total of 1k signs were included in this initial assessment, of which 70% were marked as correct, 28% were marked as incorrect and 2% were marked as uncertain, validating the key idea behind the annotation pipeline. Possible reasons for incorrect marks include: BSL mouthing patterns are not always identical to spoken English and mouthings many times do not represent the full word (e.g., "fsh" for "finish") <ref type="bibr" target="#b21">[56]</ref>.</p><p>Constructing a manually verified test set. To construct a high quality, human verified test set and to maximise yield from the annotators, we started from a collection of sign predictions where the keyword model was highly confident (assigning a peak probability of greater than 0.9) yielding 5,826 sign predictions.</p><p>Then, in addition to the validated 980 signs (corrections were provided as labels for the signs marked as incorrect and uncertain signs were removed), we further expanded the verified test set with non-native (BSL level 2 or above) signers who annotated a further 2k signs. We found that signers with lower levels of fluency were able to confidently assert that a sign was correct for a portion of the signs (at a rate of around 60%), but also annotated a large number of signs as "unsure", making it challenging to use these annotations as part of the validation test for the effectiveness of the pipeline. Only signs marked as correct were included into the final verified test set, which ultimately comprised 2,103 annotations covering 334 signs from the 1,064 sign vocabulary. The statistics of each partition of the dataset are provided in Tab. 2. All experimental test set results in this paper refer to performance on the verified test set (but we retain the full automatic test set, which we found to be useful for development).</p><p>In addition to the keyword spotting approach described above, we explore techniques for further dataset expansion based on other cues in Appendix A.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models and Implementation Details</head><p>In this section, we first describe the visual keyword spotting model used to collect signs from mouthings (Sec. 4.1). Next, we provide details of the model architecture for sign recognition and spotting (Sec. 4.2). Lastly, we describe a method for obtaining a good initialisation for the sign recognition model (Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual keyword spotting model</head><p>We use the improved visual-only keyword spotting model of Stafylakis et al. <ref type="bibr" target="#b18">[53]</ref> from <ref type="bibr" target="#b11">[46]</ref> (referred to in their paper as "P2G <ref type="bibr" target="#b18">[53]</ref> baseline"), provided by the authors. The model of <ref type="bibr" target="#b18">[53]</ref> combines visual features with a fixed-length keyword embedding to determine whether a user-defined keyword is present in an input video clip. The performance of <ref type="bibr" target="#b18">[53]</ref> is improved in <ref type="bibr" target="#b11">[46]</ref> by switching the keyword encoder-decoder from grapheme-to-phoneme (G2P) to phonemeto-grapheme (P2G).</p><p>In more detail, the model consists of four stages: (i) visual features are first extracted from the sequence of face-cropped image frames from a clip (this is performed using a 512 ? 512 SSD architecture <ref type="bibr" target="#b8">[43]</ref> trained for face detection on WIDER faces <ref type="bibr" target="#b28">[63]</ref>), (ii) a fixed-length keyword representation is built using a P2G encoder-decoder, (iii) the visual and keyword embeddings are concatenated and passed through BiLSTMs, (iv) finally, a sigmoid activation is applied on the output to approximate the posterior probability that the keyword occurs in the video clip for each input frame. If the maximum posterior over all frames is greater than a threshold, the clip is predicted to contain the keyword. The predicted location of the keyword is the position of the maximum posterior. Finally, non-maximum suppression is run with a temporal window of 0.6 seconds over the untrimmed source videos to remove duplicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sign recognition model</head><p>We employ a spatio-temporal convolutional neural network architecture that takes a multiple-frame video as input, and outputs class probabilities over sign categories. Specifically, we follow the I3D architecture [13] due to its success on action recognition benchmarks, as well as its recently observed success on sign recognition datasets <ref type="bibr">[34,</ref><ref type="bibr" target="#b5">40]</ref>. To retain computational efficiency, we only use an RGB stream. The model is trained on 16-frame consecutive frames (i.e., 0.64 sec for 25fps), as [7, <ref type="bibr" target="#b14">49,</ref><ref type="bibr" target="#b24">59]</ref> observed that co-articulated signs last roughly for 13 frames. We resize our videos to have a spatial resolution of 224 ? 224. For training, we randomly subsample a fixed-size, temporally contiguous input from the spatio-temporal volume to have 16 ? 224 ? 224 resolution in terms of number of frames, width, and height, respectively. We minimise the cross-entropy loss using SGD with momentum (0.9) with mini-batches of size 4, and an initial learning rate of 10 ?2 with a fixed schedule. The learning rate is decreased twice with a factor of 10 ?1 at epochs 20 and 40. We train for 50 epochs. Colour, scale, and horizontal flip augmentations are applied on the input video. When pretraining is used (e.g. on Kinetics-400 <ref type="bibr">[13]</ref> or on other data where specified), we replace the last linear layer with the dimensionality of our classes, and fine-tune all network parameters (we observed that freezing part of the model is suboptimal). Finally, we apply dropout on the classification layer with a probability of 0.5.</p><p>At test time, we perform centre-cropping and apply a sliding window with a stride of 8 frames before averaging the classification scores to obtain a video-level prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Video pose distillation</head><p>Given the significant focus on pose estimation in the sign language recognition literature, we investigate how explicit pose modelling can be used to improve the I3D model. To this end, we define a pose distillation network that takes in a sequence of 16 consecutive frames, but rather than predicting sign categories, the 1024-dimensional (following average pooling) embedding produced by the network is used to regress the poses of individuals appearing in each of the frames of its input. In more detail, we assume a single individual per-frame (as is the case in cropped sign translation footage) and task the network with predicting 130 human pose keypoints (18 body, 21 per hand, and 70 facial) produced by an OpenPose [12] model (trained on COCO <ref type="bibr" target="#b7">[42]</ref>) that is evaluated per-frame. The key idea is that, in order to effectively predict pose across multiple frames from a single video embedding, the model is encouraged to encode information not only about pose, but also descriptions of relevant dynamic gestures. The model is trained on a portion of the BSL-1K training set (due to space constraints, further details of the model architecture and training procedure are provided in Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first provide several ablations on our sign recognition model to answer questions such as which cues are important, and how to best use human pose. Then, we present baseline results for sign recognition and sign spotting, with our best model. Finally, we compare to the state of the art on ASL benchmarks to illustrate the benefits of pretraining on our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablations for the sign recognition model</head><p>In this section, we evaluate our sign language recognition approach and investigate (i) the effect of mouthing score threshold, (ii) the comparison to pose-based approaches, (iii) the contribution of multi-modal cues, and (iv) the video pose distillation. Additional ablations about the influence of the search window size for the keyword spotting (Appendix A.2) and the temporal extent of the automatic annotations (Appendix A.3) can be found in the appendix. Evaluation metrics. Following [34, 40], we report both top-1 and top-5 classification accuracy, mainly due to ambiguities in signs which can be resolved in context. Furthermore, we adopt both per-instance and per-class accuracy metrics. Per-instance accuracy is computed over all test instances. Per-class accuracy refers to the average over the sign categories present in the test set. We use this metric due to the unbalanced nature of the datasets.</p><p>The effect of the mouthing score threshold. The keyword spotting method, being a binary classification model, provides a confidence score, which we threshold to obtain our automatically annotated video clips. Reducing this threshold yields an increased number of sign instances at the cost of a potentially noisier set of annotations. We denote the training set defined by a mouthing threshold 0.8 as BSL-1K m.8 . In Tab. 3, we show the effect of changing this hyper-parameter between a low-and high-confidence model with 0.5 and 0.8 mouthing thresholds, respectively. The larger set of training samples obtained with a threshold of 0.5 provide the best performance. For the remaining ablations, we use the smaller BSL-1K m.8 training set for faster iterations, and return to the larger BSL-1K m.5 set for training the final model. Pose-based model versus I3D. We next verify that I3D is a suitable model for sign language recognition by comparing it to a pose-based approach. We implement Pose?Sign, which follows a 2D ResNet architecture [29] that operates on 3 ? 16 ? P dimensional dynamic pose images, where P is the number of keypoints. In our experiments, we use OpenPose [12] (pretrained on COCO <ref type="bibr" target="#b7">[42]</ref>) to extract 18 body, 21 per hand, and 70 facial keypoints. We use 16-frame inputs to make it comparable to the I3D counterpart. We concatenate the estimated normalised xy coordinates of a keypoint with its confidence score to obtain the 3 channels. In Tab. 4, we see that I3D significantly outperforms the explicit 2D pose-based method (65.57% vs 49.66% per-instance accuracy). This conclusion is in accordance with the recent findings of <ref type="bibr">[34,</ref><ref type="bibr" target="#b5">40]</ref>. Contribution of individual cues. We carry out two set of experiments to determine how much our sign recognition model relies on signals from the mouth and face region versus the manual features from the body and hands: (i) using Pose?Sign, which takes as input the 2D keypoint locations over several frames, (ii) using I3D, which takes as input raw video frames. For the pose-based model, we train with only 70 facial keypoints, 60 body&amp;hand keypoints, or with the combination. For I3D, we use the pose estimations to mask the pixels outside <ref type="table" target="#tab_7">Table 4</ref>. Contribution of individual cues: We compare I3D (pretrained on Kinetics) with a keypoint-based baseline both trained and evaluated on a subset of BSL-1Km.8, where we have the pose estimates. We also quantify the contribution of the body&amp;hands and the face regions. We see that significant information can be attributed to both types of cues, and the combination performs the best. of the face bounding box, to mask the mouth region, or use all the pixels from the videos. The results are summarised in Tab. 4. We observe that using only the face provides a strong baseline, suggesting that mouthing is a strong cue for recognising signs, e.g., <ref type="bibr" target="#b7">42</ref>.23% for I3D. However, using all the cues, including body and hands (65.57%), significantly outperforms using individual modalities. Pretraining for sign recognition. Next we investigate different forms of pretraining for the I3D model. In Tab. 5, we compare the performance of a model trained with random initialisation (39.80%), fine-tuning from gesture recognition (46.93%), sign recognition (69.90%), and action recognition (69.00%). Video pose distillation provides a small boost over the other pretraining strategies (70.38%), suggesting that it is an effective way to force the I3D model to pay attention to the dynamics of the human keypoints, which is relevant for sign recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Benchmarking sign recognition and sign spotting</head><p>Next, we combine the parameter choices suggested by each of our ablations to establish baseline performances on the BSL-1K dataset for two tasks: (i) sign recognition, (ii) sign spotting. Specifically, the model comprises an I3D architecture trained on BSL-1K m.5 with pose-distillation as initialisation and random temporal offsets of up to 4 frames around the sign during training (the ablation studies for this temporal augmentation parameter are included in Appendix A.3). The sign recognition evaluation protocol follows the experiments conducted in the ablations, the sign spotting protocol is described next.  Sign spotting. Differently from sign recognition, in which the objective is to classify a pre-defined temporal segment into a category from a given vocabulary, sign spotting aims to locate all instances of a particular sign within long sequences of untrimmed footage, enabling applications such as content-based search and efficient indexing of signing videos for which subtitles are not available. The evaluation protocol for assessing sign spotting on BSL-1K is defined as follows: for each sign category present amongst the human-verified test set annotations (334 in total), windows of 0.6-second centred on each verified instance are marked as positive and all other times within the subset of episodes that contain at least one instance of the sign are marked as negative. To avoid false penalties at signs that were not discovered by the automatic annotation process, we exclude windows of 8 seconds of footage centred at each location in the original footage at which the target keyword appears in the subtitles, but was not detected by the visual keyword spotting pipeline. In aggregate this corresponds to locating approximately one positive instance of a sign in every 1.5 hours of continuous signing negatives. A sign is considered to have been correctly spotted if its temporal overlap with the model prediction exceeds an IoU (intersectionover-union) of 0.5, and we report mean Average Precision (mAP) over the 334 sign classes as the metric for performance. We report the performance of our strongest model for both the sign recognition and sign spotting benchmarks in Tab. 6. In <ref type="figure" target="#fig_3">Fig. 3</ref>, we provide some qualita- <ref type="table" target="#tab_7">Table 7</ref>. Transfer to ASL: Performance on American Sign Language (ASL) datasets with and without pretraining on our data. I3D results are reported from the original papers for MSASL <ref type="bibr">[34]</ref> and WLASL <ref type="bibr" target="#b5">[40]</ref>. I3D ? denotes our implementation and training, adopting the hyper-parameters from <ref type="bibr">[34]</ref>. We show that our features provide good initialisation, even if it is trained on BSL.</p><p>WLASL <ref type="bibr" target="#b5">[40]</ref> MSASL tive results from our sign recognition method and observe some modes of failure which are driven by strong visual similarity in sign production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with the state of the art on ASL benchmarks</head><p>BSL-1K, being significantly larger than the recent WLASL <ref type="bibr" target="#b5">[40]</ref> and MSASL [34] benchmarks, can be used for pretraining I3D models to provide strong initialisation for other datasets. Here, we transfer the features from BSL to ASL, which are two distinct sign languages. As models from <ref type="bibr">[34]</ref> were not publicly available, we first reproduce the I3D Kinetics pretraining baseline with our implementation to achieve fair comparisons. We use 64-frame inputs as isolated signs in these datasets are significantly slower than co-articulated signs. We then train I3D from BSL-1K pretrained features. Tab. 7 compares pretraining on Kinetics versus our BSL-1K data. BSL-1K provides a significant boost in the performance, outperforming the state-of-theart results (46.82% and 64.71% top-1 accuracy). Find additional details, as well as similar experiments on co-articulated datasets in Appendix A.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have demonstrated the advantages of using visual keyword spotting to automatically annotate continuous sign language videos with weakly-aligned subtitles. We have presented BSL-1K, a large-scale dataset of co-articulated signs that, coupled with a 3D CNN training, allows high-performance recognition of signs from a large-vocabulary. Our model has further shown beneficial as initialisation for ASL benchmarks. Finally, we have provided ablations and baselines for sign recognition and sign spotting tasks. A potential future direction is leveraging our automatic annotations and recognition model for sign language translation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Results</head><p>In this section, we present complementary results to the main paper. Section A.1 provides a qualitative analysis. Additional experiments investigate the search window size for mouthing (Section A.2), the number of frames for sign recognition (Section A.3), the effect of masking the mouth at test time (Section A.4), ensembling part-specific models (Section A.5), the transfer to co-articulated datasets (Section A.6), and the baselines using other cues (Section A.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Qualitative analysis</head><p>We provide a video on our project page 1 to illustrate the automatically annotated training samples in our BSL-1K dataset, as well as the results of our sign recognition model on the manually verified test set. <ref type="figure" target="#fig_1">Figures A.1</ref> and A.2 present some of these results. In <ref type="figure">Figure A</ref>.1, we provide training samples localised using mouthing cues. In <ref type="figure">Figure A.</ref>2, we provide test samples classified by our I3D model trained on the automatic annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Size of the search window for visual keyword spotting</head><p>We investigate the influence of varying the extent of the temporal window around a given subtitle during the visual keyword spotting phase of dataset collection. For this experiment, we run the visual keyword spotting model with different search window sizes (centring these windows on the subtitle locations), and train sign recognition models (following the protocol described in the main paper, using Kinetics initialisation) on the resulting annotations. We find (Table A.1) that 8-second extended search windows yield the strongest performance on the test set (which is fixed across each run)-we therefore use these for all experiments used in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Temporal extent of the automatic annotations</head><p>Keyword spotting provides a precise localisation in time, but does not determine the duration of the sign. We observe that the highest mouthing confidence is obtained at the end of mouthing. We therefore take a certain number of frames before this peak to include in our sign classification training. In <ref type="table" target="#tab_7">Table A</ref>.2, we Mouthing results: Qualitative samples for the visual keyword spotting method for the keywords "happy" and "important". We visualise the top 24 videos with the most confident mouthing scores for each word. We note the visual similarity among manual features which suggests that mouthing cues can be a good starting point to automatically annotate training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. A.2. Sign recognition results:</head><p>Qualitative samples for our sign language recognition I3D model on the BSL-1K test set. We visualise the top 24 videos with the highest classification scores for the signs "orange" and "business", which appear to be all correctly classified. experiment with this hyper-parameter and see that 20 frames is a good compromise for creating variation in training, while not including too many irrelevant frames. In all of our experiments, we used 24 frames, except in <ref type="table" target="#tab_5">Table 6</ref> which combines the best parameters from each ablation, where we used 20 frames.</p><p>Note that our I3D model takes in 16 consecutive frames as input, which is sliced randomly during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Masking the mouth region at test time</head><p>In  <ref type="figure" target="#fig_3">Fig. A.3</ref>. Masking the mouth: Sample visualisations for the inputs described in <ref type="table" target="#tab_7">Table 4</ref> (for the signs "important" and "library"). We experiment with masking the mouth region or cropping only the face region using the detected pose keypoints. presenting the full-frames (65.57%). This procedure; however, involves additional complexity of computing the human pose and training two separate models. It is therefore only used for experimental purposes. <ref type="figure">Figure A.</ref>3 presents sample visualisations for the masking procedure. For mouth-masking, we replace the box covering the mouth region with the average pixel of the region. For face-cropping, we set pixels outside of the face region to zero (we observed divergence of training if the mean value was used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Transferring BSL-1K pretrained model to other datasets</head><p>As explained in Section 5.3, we use our model pretrained on BSL-1K as initialisation for transferring to other datasets. Additional details on fine-tuning for ASL. For MSASL [34] and WLASL <ref type="bibr" target="#b5">[40]</ref> isolated datasets on ASL, we have used the pretraining with the mouth-masking to force the model to entirely pay attention to manual features. We also observed that some signs are identical between ASL and BSL; therefore, instead of randomly initialising the last classification layer, we have kept the weights corresponding to common words between BSL-1K and the ASL dataset. We observed slight improvements with both of these choices. Results on co-articulated datasets. Here, we report the results of training sign language recognition on two co-articulated datasets: (i) RWTH-PHOENIX-Weather-2014-T [10, <ref type="bibr" target="#b0">35]</ref> and (ii) BSL-Corpus <ref type="bibr" target="#b16">[51,</ref><ref type="bibr" target="#b17">52]</ref>, with and without pretraining on BSL-1K.</p><p>Phoenix dataset is not directly applicable to our model due to the lack of signgloss alignment to train I3D with short clips of individual signs. We therefore implemented a simple CTC loss [28] to adapt I3D for Phoenix and obtained 5.6 WER improvement with BSL-1K pretraining over Kinetics pretraining.</p><p>BSL-Corpus is a linguistic dataset, and has not been used for computer vision research so far. We defined a train/val/test split (8:1:1 ratio) for a subset of 6k annotations of 966 signs and obtained 24.4% vs 12.8% accuracy with/without BSL-1K pretraining. In this case, we have also kept the last-layer classification weights for which the words are in common between BSL-Corpus and BSL-1K signs. We observed this to provide small gains over completely random initialisation of classification weights.</p><p>We conclude that our large-scale BSL-1K dataset provides a strong initialisation for both co-articulated and isolated datasets; for a variety of sign languages: ASL (American), BSL (British), and DGS (German).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Dataset expansion through other cues and additional baselines</head><p>In addition to the experiments reported in the paper, we further implemented the dataset labelling technique described in <ref type="bibr" target="#b14">[49]</ref> which searches subtitles for signs and picks candidate temporal windows that maximise the area under the ROC curve for positively and negatively labelled bags (here, a positive bag refers to temporal windows that occur within an approximately 400 frame interval centred on the target word). However, we found that without the use of the keyword spotting model for localisation, the annotations collected with this technique were extremely noisy, and the resulting model significantly under-performed all baselines reported in the main paper (that were instead trained on BSL-1K). We also experimented with dataset expansion through training ensembles of exemplar SVMs <ref type="bibr" target="#b9">[44]</ref> for each episode on signs that were predicted as confident positives (greater than 0.8) by our strongest pretrained model (and using all temporal windows that did not include the keyword as negatives). In this case, we found it challenging to calibrate SVM confidences (we explored both the parameters of the original paper, who discuss the difficulties of this process <ref type="bibr" target="#b9">[44]</ref> and a range of other parameters) and expanded the dataset by a factor of three, but did not achieve a boost in model performance when training on the expanded data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Video Pose Distillation</head><p>In this section, we give additional details of the video pose distillation model described in Section 4.3. The model architecture uses an I3D backbone [13] and takes as input a sequence of 16 frames at 224?224 pixels. We remove the classification head used in the original architecture and replace it with a linear layer that projects the 1024-dimensional embedding to 4160 dimensions-this corresponds to a set of 16 per-frame predictions of the xy coordinates of the 130 human pose keypoints produced by an OpenPose [12] model (trained on COCO <ref type="bibr" target="#b7">[42]</ref>). The coordinates are normalised (with respect to the dimensions of the input image) to lie in the range [0, 1] and an L2 loss is used to penalise inaccurate predictions. The training data for the pose distillation model comprises one-minute segments from each episode used in the BSL-1K training set. The model is trained for 100 epochs using the Lookahead optimizer <ref type="bibr" target="#b31">[66]</ref> with minibatches of 32 clips using a learning rate of 0.1 (reduced by a factor of 10 after 50 epochs) and a weight decay of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C BSL-1K Dataset Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Sign verification tool</head><p>In <ref type="figure">Figure A</ref>.4, we show a screenshot of the verification tool used by annotators to verify or reject signs found by the proposed keyword spotting method in the test set. Annotators have the ability to view the sign at reduced speed and indicate whether the sign is correct, incorrect, or they are unsure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Dataset source material</head><p>The BBC broadcast TV shows, together with their respective number of occurrences in the source material used to construct the dataset are: </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Equal contribution arXiv:2007.12131v2 [cs.CV] 13 Oct 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Keyword-driven sign annotation: (Left, the annotation pipeline): Stage 1: for a given target sign (e.g. "happy") each occurrence of the word in the subtitles provides a candidate temporal window when the sign may occur (this is further padded by several seconds on either side to account for misalignment of subtitles and signs); Stage 2: a keyword spotter uses the mouthing of the signer to perform precise localisation of the sign within this window. (Right): Examples from the BSL-1K datasetproduced by applying keyword spotting for a vocabulary of 1K words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>BSL-1K sign frequencies: Log-histogram of instance counts for the 1,064 words constituting the BSL-1K vocabulary, together with example signs. The long-tail distribution reflects the real setting in which some signs are more frequent than others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Qualitative analysis: We present results of our sign recognition model on BSL-1K for success (top) and failure (bottom) cases, together with their confidence scores in parentheses. To the right of each example, we show a random training sample for the predicted sign (in small). We observe that failure modes are commonly due to high visual similarity in the gesture (bottom-left) and mouthing (bottom-right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Bibliography [ 1 ]</head><label>1</label><figDesc>Afouras, T., Chung, J.S., Senior, A., Vinyals, O., Zisserman, A.: Deep audiovisual speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2019) 2, 6 [2] Agris, U., Zieren, J., Canzler, U., Bauer, B., Kraiss, K.F.: Recent developments in visual sign language recognition. Universal Access in the Information Society 6, 323-362 (2008) 4 [3] Antonakos, E., Roussos, A., Zafeiriou, S.: A survey on mouth modeling and analysis for sign language recognition. In: IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (2015) 4 [4] Athitsos, V., Neidle, C., Sclaroff, S., Nash, J., Stefan, A., Quan Yuan, Thangali, A.: The american sign language lexicon video dataset. In: CVPRW (2008) 3 [5] Bank, R., Crasborn, O., Hout, R.: Variation in mouth actions with manual signs in sign language of the Netherlands (ngt). Sign Language &amp; Linguistics 14, 248-270 (2011) 2 [6] Bilge, Y.C., Ikizler, N., Cinbis, R.: Zero-shot sign language recognition: Can textual data uncover sign languages? In: BMVC (2019) 4 [7] Buehler, P., Zisserman, A., Everingham, M.: Learning sign language by watching TV (using weakly aligned subtitles). In: CVPR (2009) 4, 9 [8] Camgoz, N.C., Hadfield, S., Koller, O., Bowden, R.: Using convolutional 3D neural networks for user-independent continuous gesture recognition. In: IEEE International Conference of Pattern Recognition, ChaLearn Workshop (2016) 4 [9] Camgoz, N.C., Hadfield, S., Koller, O., Bowden, R.: SubUNets: End-to-end hand shape and continuous sign language recognition. In: ICCV (2017) 4 [10] Camgoz, N.C., Hadfield, S., Koller, O., Ney, H., Bowden, R.: Neural sign language translation. In: CVPR (2018) 1, 3, 4, 24 [11] Cao, Q., Shen, L., Xie, W., Parkhi, O.M., Zisserman, A.: VGGFace2: A dataset for recognising faces across pose and age. In: International Conference on Automatic Face and Gesture Recognition (2018) 7 [12] Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields. In: arXiv preprint arXiv:1812.08008 (2018) 10, 11, 25 [13] Carreira, J., Zisserman, A.: Quo vadis, action recognition? A new model and the Kinetics dataset. In: CVPR (2017) 4, 9, 10, 12, 25 [14] Chai, X., Wang, H., Chen, X.: The devisign large vocabulary of chinese sign language database and baseline evaluations. Technical report VIPL-TR-14-SLR-001. Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS), Institute of Computing Technology, CAS (2014) 3 [15] Chung, J.S., Senior, A., Vinyals, O., Zisserman, A.: Lip reading sentences in the wild. In: CVPR (2017) 2 [16] Chung, J.S., Zisserman, A.: Lip reading in the wild. In: ACCV (2016) 2 [17] Chung, J.S., Zisserman, A.: Signs in time: Encoding human motion as a temporal image. In: Workshop on Brave New Ideas for Motion Representations, ECCV (2016) 2, 4 [18] Cooper, H., Bowden, R.: Learning signs from subtitles: A weakly supervised approach to sign language recognition. In: CVPR (2009) 4 [19] Cooper, H., Pugeault, N., Bowden, R.: Reading the signs: A video based sign dictionary. In: ICCVW (2011) 4 [20] Cooper, H., Holt, B., Bowden, R.: Sign language recognition. In: Visual Analysis of Humans: Looking at People, chap. 27, pp. 539 -562. Springer (2011) 2 [21] Crasborn, O.A., Van Der Kooij, E., Waters, D., Woll, B., Mesch, J.: Frequency distribution and spreading behavior of different types of mouth actions in three sign languages. Sign Language &amp; Linguistics (2008) 2 [22] Eng-Jon Ong, Koller, O., Pugeault, N., Bowden, R.: Sign spotting using hierarchical sequential patterns with temporal intervals. In: CVPR (2014) 4 [23] Farhadi, A., Forsyth, D.: Aligning ASL for statistical translation using a discriminative word model. In: CVPR (2006) 4 [24] Farhadi, A., Forsyth, D.A., White, R.: Transfer learning in sign language. In: CVPR (2007) 4 [25] Fillbrandt, H., Akyol, S., Kraiss, K..: Extraction of 3D hand shape and posture from image sequences for sign language recognition. In: IEEE International SOI Conference (2003) 4 [26] Fisher, C.G.: Confusions among visually perceived consonants. Journal of Speech and Hearing Research 11(4), 796-804 (1968) 4 [27] Forster, J., Oberd?rfer, C., Koller, O., Ney, H.: Modality combination techniques for continuous sign language recognition. In: Pattern Recognition and Image Analysis (2013) 4 [28] Graves, A., Fern?ndez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In: ICML (2006) 24 [29] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016) 11 [30] Hu, J., Shen, L., Albanie, S., Sun, G., Wu, E.: Squeeze-and-excitation networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (2019) 7 [31] Huang, J., Zhou, W., Li, H., Li, W.: Sign language recognition using 3D convolutional neural networks. In: International Conference on Multimedia and Expo (ICME) (2015) 4 [32] Huang, J., Zhou, W., Zhang, Q., Li, H., Li, W.: Video-based sign language recognition without temporal segmentation. In: AAAI (2018) 3, 4 [33] Jha, A., Namboodiri, V.P., Jawahar, C.V.: Word spotting in silent lip videos. In: WACV (2018) 2 [34] Joze, H.R.V., Koller, O.: MS-ASL: A large-scale data set and benchmark for understanding american sign language. In: BMVC (2019) 3, 4, 9, 11, 14, 24This document provides additional results (Section A), details about the video pose distillation model (Section B), and about the BSL-1K dataset (Section C).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig</head><label></label><figDesc>Fig. A.1. Mouthing results: Qualitative samples for the visual keyword spotting method for the keywords "happy" and "important". We visualise the top 24 videos with the most confident mouthing scores for each word. We note the visual similarity among manual features which suggests that mouthing cues can be a good starting point to automatically annotate training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). ASLLVD [4] has only 6 signers. Recently, MSASL [34] and WLASL [40] large-vocabulary isolated sign datasets have been released with 1K and 2K signs, respectively. The videos are collected from lexicon databases and other instructional videos on the web.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Statistics of the proposed BSL-1K dataset: The Test-(manually verified) split represents a sample from the Test-(automatic) split annotations that have been verified by human annotators (see Sec. 3.3 for details).</figDesc><table><row><cell>Set</cell><cell>sign vocabulary</cell><cell>sign annotations</cell><cell>number of signers</cell></row><row><cell>Train</cell><cell>1,064</cell><cell>173K</cell><cell>32</cell></row><row><cell>Val</cell><cell>1,049</cell><cell>36K</cell><cell>4</cell></row><row><cell>Test-(automatic)</cell><cell>1,059</cell><cell>63K</cell><cell>4</cell></row><row><cell>Test-(manually verified)</cell><cell>334</cell><cell>2103</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Trade-off between training noise vs. size: Training (with Kinetics initialisation) on the full training set BSL-1Km.5 versus the subset BSL-1Km.8, which correspond to a mouthing score threshold of 0.5 and 0.8, respectively. Even when noisy, with the 0.5 threshold, mouthings provide automatic annotations that allow supervised training at scale, resulting in 70.61% accuracy on the manually validated test set.</figDesc><table><row><cell>per-instance</cell><cell>per-class</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">per-instance</cell><cell cols="2">per-class</cell><cell></cell><cell>mAP</cell></row><row><cell></cell><cell>top-1</cell><cell>top-5</cell><cell>top-1</cell><cell>top-5</cell><cell></cell><cell>(334 sign classes)</cell></row><row><cell>Sign Recognition</cell><cell>75.51</cell><cell>88.83</cell><cell>52.76</cell><cell>72.14</cell><cell>Sign Spotting</cell><cell>0.159</cell></row></table><note>Benchmarking: We benchmark our best sign recognition model (trained on BSL-1Km.5, initialised with pose distillation, with 4-frame temporal offsets) for sign recognition and sign spotting task to establish strong baselines on BSL-1K.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A .</head><label>A</label><figDesc>1. The effect of the temporal window where we apply the visual keyword spotting model. Networks are trained on BSL-1Km.8 with Kinetics initialisation. Decreasing the window size increases the chance of missing the word, resulting in less training data and lower performance. Increasing too much makes the keyword spotting task difficult, reducing the annotation quality. We found 8 seconds to be a good compromise, which we used in all other experiments in this paper.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">per-instance</cell><cell></cell><cell>per-class</cell></row><row><cell>Keyword search window</cell><cell>#videos</cell><cell>top-1</cell><cell></cell><cell cols="2">top-5</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell>1 sec</cell><cell>25.0K</cell><cell>60.10</cell><cell></cell><cell cols="2">75.42</cell><cell>36.62</cell><cell>53.83</cell></row><row><cell>2 sec</cell><cell>33.9K</cell><cell>64.91</cell><cell></cell><cell cols="2">80.98</cell><cell>40.29</cell><cell>59.63</cell></row><row><cell>4 sec</cell><cell>37.6K</cell><cell>68.09</cell><cell></cell><cell cols="2">82.79</cell><cell>45.35</cell><cell>63.64</cell></row><row><cell>8 sec</cell><cell>38.9K</cell><cell cols="2">69.00</cell><cell cols="2">83.79</cell><cell>45.86</cell><cell>64.42</cell></row><row><cell>16 sec</cell><cell>39.0K</cell><cell>65.91</cell><cell></cell><cell cols="2">81.84</cell><cell>39.51</cell><cell>59.03</cell></row><row><cell cols="7">Table A.2. The effect of the number of frames before the mouthing peak used for</cell></row><row><cell cols="7">training. Networks are trained on BSL-1Km.8 with Kinetics initialisation.</cell></row><row><cell></cell><cell cols="2">per-instance</cell><cell></cell><cell cols="2">per-class</cell></row><row><cell>#frames</cell><cell>top-1</cell><cell>top-5</cell><cell cols="2">top-1</cell><cell cols="2">top-5</cell></row><row><cell>16</cell><cell>59.53</cell><cell>77.08</cell><cell cols="2">36.16</cell><cell cols="2">58.43</cell></row><row><cell>20</cell><cell>71.71</cell><cell>85.73</cell><cell cols="2">49.64</cell><cell cols="2">69.23</cell></row><row><cell>24</cell><cell>69.00</cell><cell>83.79</cell><cell cols="2">45.86</cell><cell cols="2">64.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table A.3, we experiment with the test modes for the networks trained with (i) full-frames including the mouth, versus (ii) masking the mouth region. If the mouth is masked only at test time, the performance drops from 65.57% to 34.74% suggesting the model's significant reliance on the mouth cues. The model can be improved to 46.75% if it is forced to focus on other regions by training with masked mouth.A.5 Late fusion of part-specific models.We further experiment with ensembling two I3D networks, each specialising on different parts of the human body, by averaging the classification scores, i.e., late fusion. The results are summarised inTable A.4. We observe significant improvements when combining a mouth-specific model (face-crop) with a bodyspecific model (mouth-masked), which suggests that forcing the network to focus on separate, but complementary signing cues (68.55%) can be more effective than</figDesc><table><row><cell>full-frame</cell><cell>mouth-masked</cell><cell>face-crop</cell><cell>full-frame</cell><cell>mouth-masked</cell><cell>face-crop</cell></row><row><cell></cell><cell>important</cell><cell></cell><cell></cell><cell>library</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A .</head><label>A</label><figDesc>3. We complementTable 4by investigating different test modes for I3D, when trained with or without the mouth pixels. The model trained with full-frames relies significantly on the mouth, whose performance drops from 65.57% to 34.74% when the mouth is masked. The models are trained on the subset of BSL-1Km.8 where pose estimates are available. Ensembling part-specific models fromTable 4. We observe that combining the I3D model trained only with the face and another model without the mouth (last row) achieves superior performance than using one model that inputs the full-frame. This suggests that disentangling manual and non-manual features, which are complementary, for sign recognition is a promising direction. The models are trained on the subset of BSL-1Km.8 where pose estimates are available.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Test mouth-masked</cell><cell></cell><cell cols="2">Test full-frame</cell><cell></cell></row><row><cell></cell><cell cols="2">per-instance</cell><cell cols="2">per-class</cell><cell cols="2">per-instance</cell><cell cols="2">per-class</cell></row><row><cell></cell><cell>top-1</cell><cell>top-5</cell><cell>top-1</cell><cell>top-5</cell><cell>top-1</cell><cell>top-5</cell><cell>top-1</cell><cell>top-5</cell></row><row><cell>Train mouth-masked</cell><cell>46.75</cell><cell>66.34</cell><cell>25.85</cell><cell>48.02</cell><cell>46.21</cell><cell>65.34</cell><cell>25.83</cell><cell>46.23</cell></row><row><cell>Train full-frame</cell><cell>34.74</cell><cell>51.42</cell><cell>13.62</cell><cell>29.80</cell><cell>65.57</cell><cell>81.33</cell><cell>44.90</cell><cell>64.91</cell></row><row><cell cols="6">Table A.4. per-instance</cell><cell cols="2">per-class</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>top-1</cell><cell>top-5</cell><cell>top-1</cell><cell>top-5</cell><cell></cell></row><row><cell>face-crop</cell><cell></cell><cell></cell><cell></cell><cell>42.23</cell><cell>69.70</cell><cell>21.66</cell><cell>50.51</cell><cell></cell></row><row><cell cols="2">mouth-masked</cell><cell></cell><cell></cell><cell>46.75</cell><cell>66.34</cell><cell>25.85</cell><cell>48.02</cell><cell></cell></row><row><cell>full-frame</cell><cell></cell><cell></cell><cell></cell><cell>65.57</cell><cell>81.33</cell><cell>44.90</cell><cell>64.91</cell><cell></cell></row><row><cell>full-frame</cell><cell cols="2">&amp; face-crop</cell><cell></cell><cell>64.50</cell><cell>83.01</cell><cell>42.30</cell><cell>65.58</cell><cell></cell></row><row><cell>full-frame</cell><cell cols="3">&amp; mouth-masked</cell><cell>68.09</cell><cell>81.33</cell><cell>46.29</cell><cell>65.41</cell><cell></cell></row><row><cell cols="3">mouth-masked &amp; face-crop</cell><cell></cell><cell>68.55</cell><cell>83.63</cell><cell>45.29</cell><cell>67.47</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Countryfile: 266, Natural World: 70, Great British Railway Journeys: 122, Holby City: 261, Junior Masterchef: 24, Junior Bake Off: 22, Hairy Bikers Bakeation: 6, Masterchef The Professionals: 37, Doctor Who Sci Fi: 23, Great British Menu: 110, A To Z Of Cooking: 24, Raymond Blanc Kitchen Secrets: 9, The Apprentice: 88, Country Show Cook Off: 18, A Taste Of Britain: 20, Lorraine Pascale How To Be A: 6, Chefs Put Your Menu Where Your: 13, Simply Nigella: 7, The Restaurant Man: 5, Hairy Bikers Best Of British: 27, Rip Off Britain Food: 20, Our Food Uk 4: 3, Disaster Chefs: 8, Terry And Mason Great Food Trip: 19, Gardeners World: 70, Paul Hollywood Pies Puds: 20, James Martin Food Map Of Britain: 10, Baking Made Easy: 6, Hairy Bikers Northern: 7, Nigel Slater</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The project page is at: https://www.robots.ox.ac.uk/~vgg/research/bsl1k/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://bslsignbank.ucl.ac.uk/ 3 https://www.signbsl.com/ 4 These are signs that use identical hand movements (e.g. "king" and "queen") whose meanings are distinguished by mouthings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.robots.ox.ac.uk/~vgg/research/bsl1k/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by EPSRC grant ExTol. We also thank T. Stafylakis </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Read my lips: Continuous signer independent weakly supervised viseme recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised automatic transcription of mouthings for gloss-based sign language corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC Workshop on the Representation and Processing of Sign Languages: Beyond the Manual Channel</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning of mouth shapes for sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Assistive Computer Vision and Robotics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Re-Sign: Re-aligned end-to-end sequence modelling with deep recurrent CNN-HMMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Opazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Transferring cross-domain knowledge for video sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The Jester dataset: A large-scale video dataset of human gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Momeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Seeing wake words: Audio-visual keyword spotting. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tracking facial features under occlusions and recognizing facial expressions in sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ranganath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Sign language recognition using sequential pattern trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale learning of sign language by watching tv (using co-occurrences)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: BMVC</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Domain-adaptive discriminative oneshot learning of gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">British Sign Language Corpus Project: A corpus of digital video data and annotations of British Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schembri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fenlon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rentelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cormier</surname></persName>
		</author>
		<ptr target="http://www.bslcorpusproject.org" />
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note>Third Edition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schembri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fenlon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rentelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cormier</surname></persName>
		</author>
		<title level="m">Building the British sign language corpus. Language Documentation &amp; Conservation</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zero-shot keyword spotting for visual speech recognition in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual Recognition of American Sign Language Using Hidden Markov Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mouthings and simultaneity in British sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton-Spence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Simultaneity in Signed Languages: Form and Function</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="147" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutton-Spence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Woll</surname></persName>
		</author>
		<title level="m">The Linguistics of British Sign Language: An Introduction</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognition of sign language motion images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kawasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>University</surname></persName>
		</author>
		<title level="m">The Gallaudet Dictionary of American Sign Language</title>
		<imprint>
			<publisher>Gallaudet University Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">S-pot -a benchmark in spotting signs within continuous signing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Viitaniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jantunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Savolainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laaksonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>LREC</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The significance of facial features for automatic sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Agris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Knorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kraiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th IEEE International Conference on Automatic Face Gesture Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Purdue RVL-SLLL American sign language database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<idno>TR-06-12</idno>
	</analytic>
	<monogr>
		<title level="j">School of Electrical and Computer Engineering Technical Report</title>
		<imprint>
			<biblScope unit="volume">47906</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Purdue University, W. Lafayette</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The sign that dares to speak its name: Echo phonology in British sign language (BSL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Woll</surname></persName>
		</author>
		<editor>Boyes-Braem, P., Sutton-Spence, R.</editor>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Signum Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<pubPlace>Hamburg</pubPlace>
		</imprint>
	</monogr>
	<note>The hands are the head of the mouth: The mouth as articulator in sign languages</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spotting visual keywords from temporal sliding windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mandarin Audio-Visual Speech Recognition Challenge</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Recognizing american sign language gestures from within continuous videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Spatial-temporal multi-cue network for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/2002.03187</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Eating Together: 6, Raymond Blanc How To Cook Well: 6, Great British Food Revival: 17, Great British Bake Off: 28, Two Greedy Italians: 4, Food Fighters: 10, Hairy Bikers Mums Know Best: 9</title>
		<idno>Fig. A.4</idno>
	</analytic>
	<monogr>
		<title level="j">Hairy Bikers Meals On Wheels</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Paul Hollywood Bread</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">cracker, crash, crazy, create, cricket, crisps, cross, cruel, culture, cup, cupboard, curriculum, custard, daddy, damage, dance, danger, daughter, deaf, debate, december, decide, decline, deep, degree, deliver, denmark, dentist, depend, deposit, depressed, depth, derby, desire, desk, detail, detective, devil, different, dig, disabled, disagree, disappear, disappoint, discuss, disk, distract, divide, dizzy, doctor, dog, dolphin, donkey, double, downhill, drawer, dream, drink, drip, drive, drop, drunk, dry, dublin, dvd, each, early, east, easter, easy, edinburgh, egypt, eight, elastic, electricity, elephant, eleven, embarrassed, emotion, empty, encourage, end, engaged, england, enjoy, equal, escalator, escape, ethnic, europe, evening, everything, evidence, exact, exchange, excited, excuse, exeter, exhausted, expand, expect, expensive, experience, explain, express, extract, face, factory, fairy, fall, false, family, famous, fantastic, far, farm, fast, fat, father, fault, fax, february, feed, feel, fence, fifteen, fifty, fight, film, final, finance, find, fine, finish, finland, fire, fireman, first, fish, fishing, five, flag, flat, flock, flood, flower, fog, follow, football, for, foreign, forever, forget, fork, formal, forward, four, fourteen, fox, france, free, freeze, fresh, friday, friend, frog, from, front, fruit, frustrated, fry, full, furniture, future, game, garden, general, generous, geography, germany, ginger, girl, give, glasgow, glass, gold, golf, gorilla, gossip, government, grab, grandfather, grandmother, greedy, green, group, grow, guarantee, guess, guilty, gym, hair, half, hall, hamburger, hammer, hamster, handshake, hang, happen, happy, hard, hat, have, headache, hearing, heart, heavy, helicopter, hello, help, hide, history, holiday, holland, home, hope, hopeless, horrible, horse, hospital, hot, hotel, hour, house, how, hundred, hungry, hypocrite, idea, if, ignore, imagine, impact, important, impossible, improve, income, increase, independent, india, influence, inform, information, injection, insert, instant, insurance, international, internet, interrupt, interview, introduce, involve, ireland, iron, italy, jamaica, january, japan, jealous, jelly, jersey, join, joke, jumper, just, kangaroo, karate, keep, kitchen, label, language, laptop, last, late, later, laugh, leaf, leave, leeds, left, lemon, library, lighthouse, lightning, like, line, link</title>
		<imprint>
			<pubPlace>list, little, live, liverpool, london, lonely, lost, love, lovely, machine, madrid, magic, make, man, manage, manchester, many, march, mark,</pubPlace>
		</imprint>
	</monogr>
	<note>pull, pulse, punch, purple, push, pyramid, quality, quarter, question, quick, quiet, quit, rabbit, race, radio, rain, rather, read, reading, ready, really, receipt, receive, recommend, red, reduce, region, regular, relationship, relax, release, relief, remember, remind, remove, rent, repair, replace, research, resign, respect, retire, review, rhubarb, rich, ride, right, river, rocket, roll, roman, room, roots, rough, round, rub, rubbish, rugby, run, russia, sack, sad, safe, same, sand, sandwich, satellite, saturday, sauce, sausage, school, science, scissors, scotland, scratch, search, second, seed, seem, self, sell, send, sense, sensitive, sentence, separate, september, sequence, service, settle, seven, sex, shadow, shakespeare, shame, shark, sharp, sheep, sheet, sheffield, shine, shirt, shop, should, shoulder, shout, show, shower, sick, sight, sign, silver, similar, since, sister, sit, six, size, skeleton, skin, sleep, sleepy, small, smell, smile, snake, soft, some, sometimes, son, soon, sorry, south, spain, specific, speech, spend, spicy, spider, spirit, split, sport, spray, spread, squash, squirrel, staff, stand, star, start, station, still, story, straight, strange, stranger, strawberry, stress, stretch, strict, string, strong, structure, stubborn, stuck, student, study, stupid, success, sugar, summer, sun, sunday, sunset, support, suppose, surprise, swan, swap, sweden, sweep, swim, swing. switzerland, sympathy, take, talk, tap, taste, tax, taxi, teacher, team, technology, television, temperature, temporary, ten, tennis, tent, terminate, terrified, that, theory, therefore, thing, think, thirsty, thousand, three, through, thursday, ticket, tidy, tiger, time, tired, title, toast, together, toilet, tomato, tomorrow, toothbrush, toothpaste, top, torch, total, touch, tough, tournament, towel, town, train, training, tram, trap, travel, tree, trouble, trousers, true, try, tube, tuesday, turkey, twelve, twenty, twice, two, ugly, ultrasound, umbrella, uncle, under, understand, unemployed, union, unit, university, until, up, upset, valley, vegetarian, video, vinegar, visit, vodka, volcano, volunteer, vote, wait, wales, walk, wall, want, war, warm, wash, waste, watch, water, weak, weather, wednesday, week, weekend, well, west, wet, what, wheelchair, when, where, which, whistle, white, who, why, wicked, width, wild, will, win, wind, window, wine, with, without, witness, wolf, woman, wonder, wonderful, wood, wool, work, world, worry, worship, worth, wow, write, wrong, yes, yesterday</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
