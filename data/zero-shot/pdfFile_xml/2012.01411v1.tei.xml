<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PatchmatchNet: Learned Multi-View Patchmatch Stereo</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangjinhua</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Mixed Reality &amp; AI Zurich Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Vogel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Mixed Reality &amp; AI Zurich Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Speciale</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Mixed Reality &amp; AI Zurich Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Mixed Reality &amp; AI Zurich Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PatchmatchNet: Learned Multi-View Patchmatch Stereo</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present PatchmatchNet, a novel and learnable cascade formulation of Patchmatch for high-resolution multiview stereo. With high computation speed and low memory requirement, PatchmatchNet can process higher resolution imagery and is more suited to run on resource limited devices than competitors that employ 3D cost volume regularization. For the first time we introduce an iterative multiscale Patchmatch in an end-to-end trainable architecture and improve the Patchmatch core algorithm with a novel and learned adaptive propagation and evaluation scheme for each iteration. Extensive experiments show a very competitive performance and generalization for our method on DTU, Tanks &amp; Temples and ETH3D, but at a significantly higher efficiency than all existing top-performing models: at least two and a half times faster than state-of-the-art methods with twice less memory usage. Code is available at https://github.com/FangjinhuaWang/ PatchmatchNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given a collection of images with known camera parameters, multi-view stereo (MVS) describes the task of reconstructing the dense geometry of the observed scene. Despite being a fundamental problem of geometric computer vision that has been studied for several decades, MVS is still a challenge. This is due to a variety of de-facto unsolved problems occurring in practice such as occlusion, illumination changes, untextured areas and non-Lambertian surfaces <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">32]</ref>.</p><p>The success of Convolutional Neural Networks (CNN) in almost any field of computer vision ignites the hope that data driven models can solve some of these issues that classical MVS models struggle with. Indeed, many learningbased methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43]</ref> appear to fulfill such promise and outperform some traditional methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">31]</ref> on MVS benchmarks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24]</ref>. While being successful at the benchmark level, most of them do only pay limited attention to scalability, memory and run-time. Currently, most  <ref type="figure">Figure 1</ref>: Comparison with state-of-the-art learning-based multi-view stereo methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref> on DTU <ref type="bibr" target="#b0">[1]</ref>. Relationship between error, GPU memory and run-time with image size 1152?864. learning-based MVS methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42]</ref> construct a 3D cost volume, regularize it with a 3D CNN and regress the depth. As 3D CNNs are usually time and memory consuming, some methods <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b42">42]</ref> down-sample the input during feature extraction and compute both, the cost volume and the depth map at low-resolution. Yet, according to <ref type="figure">Fig. 1</ref>, delivering depth maps at low resolution can harm accuracy. Methods that do not scale up well to realistic image sizes of several mega-pixel cannot exploit the full resolution due to memory limitations. Evidently, low memory and time consumption are key to enable processing on memory and computational restricted devices such as phones or mixed reality headsets, as well as in time critical applications. Recently, researchers tried to alleviate these limitations. For example, R-MVSNet <ref type="bibr" target="#b43">[43]</ref> decouples the memory requirements from the depth range and sequentially processes the cost volume at the cost of an additional runtime penalty. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">41]</ref> include cascade 3D cost volumes to predict high-resolution depth map from coarse to fine with high efficiency in time and memory. Several traditional MVS methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b45">45]</ref> abandon the idea of holding a structured cost volume completely and instead are based on the seminal Patchmatch <ref type="bibr" target="#b1">[2]</ref> algorithm. Patchmatch adopts a randomized, iterative algorithm for approximate nearest neighbor field computation <ref type="bibr" target="#b1">[2]</ref>. In particular, the inherent spatial coherence of depth maps is exploited to quickly find a good solution without the need to look through all possibilities. Low memory requirements -independent of the disparity range -and an implicit smoothing effect make this method very attractive for our deep learning based MVS setup.</p><p>In this work, we propose PatchmatchNet, a novel cascade formulation of learning-based Patchmatch, which aims at decreasing memory consumption and run-time for highresolution multi-view stereo. It inherits the advantages in efficiency from classical Patchmatch, but also aims to improve the performance with the power of deep learning.</p><p>Contributions: (i) We introduce the Patchmatch idea into an end-to-end trainable deep learning based MVS framework. Going one step further, we embed the model into a coarse-to-fine framework to speed up computation. (ii) We augment the traditional propagation and cost evaluation steps of Patchmatch with learnable, adaptive modules that improve accuracy and base both steps on deep features. We estimate visibility information during cost aggregation for the source views. Moreover, we propose a robust training strategy to introduce randomness into training for improved robustness in visibility estimation and generalization. (iii) We verify the effectiveness of our method on various MVS datasets, e.g. DTU <ref type="bibr" target="#b0">[1]</ref>, Tanks &amp; Temples <ref type="bibr" target="#b23">[24]</ref> and ETH3D <ref type="bibr" target="#b32">[32]</ref>. The results demonstrate that our Patch-matchNet achieves competitive performance, while reducing memory consumption and run-time compared to most learning-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Traditional MVS. Traditional MVS methods can be divided into four categories: voxel-based <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b36">36]</ref>, surface evolution based <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>, patch-based <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref> and depth map based <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b38">38]</ref>. Comparatively, depth map based methods are more concise and flexible. Here, we discuss Patchmatch Stereo methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b38">38]</ref> in this category. Galliani et al. <ref type="bibr" target="#b15">[16]</ref> present Gipuma, a massively parallel multi-view extension of Patchmatch stereo. It uses a red-black checkerboard pattern to parallelize message-passing during propagation. Sch?nberger et al. <ref type="bibr" target="#b31">[31]</ref> present COLMAP, which jointly estimates pixel-wise view selection, depth map and surface normal. ACMM <ref type="bibr" target="#b38">[38]</ref> adopts adaptive checkerboard sampling, multi-hypothesis joint view selection and multiscale geometric consistency guidance. Based on the idea of Patchmatch, we propose our learning-based Patchmatch, which inherits the efficiency from classical Patchmatch, but also improves the performance leveraging deep learning.</p><p>Learning-based stereo. GCNet <ref type="bibr" target="#b21">[22]</ref> introduces 3D cost volume regularization for stereo estimation and regresses the final disparity map with a soft argmin operation. PSM-Net <ref type="bibr" target="#b4">[5]</ref> adds spatial pyramid pooling (SPP) and applies a 3D hour-glass network for regularization. DeepPruner <ref type="bibr" target="#b10">[11]</ref> develops a differentiable Patchmatch module, without learnable parameters, to discard most disparities and then builds a lightweight cost volume, which is regularized by a 3D CNN. In contrast, we do not apply any cost volume regularization but extend the original Patchmatch idea into the deep learning era. Xu et al. <ref type="bibr" target="#b37">[37]</ref> propose a sparse point based intra-scale cost aggregation method with deformable convolution <ref type="bibr" target="#b9">[10]</ref>. Likewise, we propose a strategy to adaptively sample points for spatial cost aggregation.</p><p>Learning-based MVS. Voxel-based methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> are restricted to small-scale reconstructions, due to the drawbacks of a volumetric representation. In contrast, based on plane-sweep stereo <ref type="bibr" target="#b8">[9]</ref>, many recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42]</ref> use depth maps to reconstruct the scene. They build cost volumes with warped features from multiple views, regularize them with 3D CNNs and regress the depth. As 3D CNNs are time and memory consuming, they usually use downsampled cost volumes. To reduce memory, R-MVSNet <ref type="bibr" target="#b43">[43]</ref> sequentially regularizes 2D cost maps with GRU <ref type="bibr" target="#b7">[8]</ref> but sacrifices run-time. Current research targets to improve efficiency and also estimate high-resolution depth maps. Cas-MVSNet <ref type="bibr" target="#b16">[17]</ref> proposes cascade cost volumes based on a feature pyramid and estimates the depth map in a coarse-tofine manner. UCS-Net <ref type="bibr" target="#b6">[7]</ref> proposes cascade adaptive thin volumes, which use variance-based uncertainty estimates for an adaptive construction. CVP-MVSNet <ref type="bibr" target="#b41">[41]</ref> forms an image pyramid and also constructs a cost volume pyramid. To accelerate propagation in Patchmatch, we likewise employ a cascade formulation. In addition to cascade cost volumes, PVSNet <ref type="bibr" target="#b40">[40]</ref> learns to predict visibility for each source image. An anti-noise training strategy is used to introduce disturbing views. We also learn a strategy to adaptively combine the information of multiple views based on visibility information. Moreover, we propose a robust training strategy to include randomness into the training to improve robustness in visibility estimation and generalization. Fast-MVSNet <ref type="bibr" target="#b44">[44]</ref> constructs a sparse cost volume to learn a sparse depth map and then use high-resolution RGB image and 2D CNN to densify it. We build a refinement module and use the RGB image to guide the up-sampling of the depth map based on MSG-Net <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce the structure of Patchmatch-Net, illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. It consists of multi-scale feature extraction, learning-based Patchmatch included iteratively in a coarse-to-fine framework, and a spatial refinement module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multi-scale Feature Extraction</head><p>Given N input images of size W ? H, we use I 0 and</p><formula xml:id="formula_0">{I i } N ?1 i=1</formula><p>to denote reference and source images respectively. Before we apply our learning-based Patchmatch algorithm, we extract pixel-wise features from our inputs, Patchmatch is applied for multiple iterations on several stages to predict the depth map in a coarse-to-fine manner. Refinement uses the input to guide upsampling of the final depth map. On stage k, the resolution of the depth maps is</p><formula xml:id="formula_1">W 2 k ? H 2 k , with input images of size W ?H.</formula><p>similar to Feature Pyramid Network (FPN) <ref type="bibr" target="#b25">[26]</ref>. Features are extracted hierarchically at multiple resolutions, which allows us to advance our depth map estimation in a coarseto-fine manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning-based Patchmatch</head><p>Following traditional Patchmatch <ref type="bibr" target="#b1">[2]</ref> and subsequent adaptations to depth map estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>, our learnable Patchmatch consists of the following three main steps:</p><p>1. Initialization: generate random hypotheses; 2. Propagation: propagate hypotheses to neighbors; 3. Evaluation: compute the matching costs for all the hypotheses and choose best solutions.</p><p>After initialization, the approach iterates between propagation and evaluation until convergence. Leveraging deep learning, we propose an adaptive version of the propagation (Sec. 3.2.2) and evaluation (Sec. 3.2.3) module and also adjust the initialization (Sec. 3.2.1). The detailed structure of our Patchmatch pipeline is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. In a nutshell, the propagation module adaptively samples the points for propagation based on the extracted deep features. Our adaptive evaluation learns to estimate visibility information for cost computation and adaptively samples the spatial neighbors to aggregate the costs again based on deep features. Unlike <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">38]</ref>, we refrain from parameterizing the perpixel hypothesis as a slanted plane, due to heavy memory penalties. Instead, we rely on our learned adaptive evaluation to organize the spatial pattern within the window over which matching costs are computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Initialization and Local Perturbation</head><p>In the very first iteration of Patchmatch, the initialization is performed in a random manner to promote diversification. Based on a pre-defined depth range [d min , d max ], we sample per pixel D f depth hypotheses in the inverse depth range, corresponding to uniform sampling in image space. This helps our model be applicable to complex and largescale scenes <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b43">43]</ref>. To ensure we cover the depth range evenly, we divide the (inverse) range into D f intervals and ensure that each interval is covered by one hypothesis.</p><p>For subsequent iterations on stage k, we perform local perturbation by generating per pixel N k hypotheses uniformly in the normalized inverse depth range R k and gradually decrease R k for finer stages. To define the center of R k , we utilize the estimation from previous iteration, possibly up-sampled from a coarser stage. This delivers a more diverse set of hypotheses than just using propagation. Sampling around the previous estimation can refine the result locally and correct wrong estimates (see supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Adaptive Propagation</head><p>Spatial coherence of depth values does in general only exist for pixel from the same physical surface. Hence, instead of propagating depth hypotheses naively from a static set of neighbors as done for Gipuma <ref type="bibr" target="#b15">[16]</ref> and DeepPruner <ref type="bibr" target="#b10">[11]</ref>, we want to perform the propagation in an adaptive manner, which gathers hypotheses from the same surface. This helps Patchmatch converge faster and deliver more accurate depth maps. <ref type="figure" target="#fig_3">Fig. 4</ref> illustrates the idea and functionality of our strategy. Our adaptive scheme tends to gather hypotheses from pixels of the same surface -for both the textured object and the textureless region -enabling us to effectively collect more promising depth hypotheses compared to using just a static pattern.</p><p>We base our implementation of the adaptive propagation on Deformable Convolution Networks <ref type="bibr" target="#b9">[10]</ref>. As the approach is identical for each resolution stage, we omit subindices denoting the stage. To gather K p depth hypotheses for pixel p in the reference image, our model learns ad-</p><formula xml:id="formula_2">ditional 2D offsets {?o i (p)} Kp i=1 that are applied on top of fixed 2D offsets {o i } Kp i=1</formula><p>, organized as a grid. We apply a 2D CNN on the reference feature map F 0 to learn additional 2D offsets for each pixel p and get the depth hypotheses D p (p) via bilinear interpolation as follows:</p><formula xml:id="formula_3">D p (p) = {D(p + o i + ?o i (p))} Kp i=1 ,<label>(1)</label></formula><p>where D is the depth map from previous iteration, possibly up-sampled from a coarser stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Adaptive Evaluation</head><p>The adaptive evaluation module performs the following steps: differentiable warping, matching cost computation,  adaptive spatial cost aggregation and depth regression. As the approach is identical at each resolution stage, we omit subindices to ease notation.</p><p>Differentiable Warping. Following plane sweep stereo <ref type="bibr" target="#b8">[9]</ref>, most learning-based MVS methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43]</ref> establish front-to-parallel planes at sampled depth hypotheses and warp the feature maps of source images into them. Equipped with intrinsic matrices</p><formula xml:id="formula_4">{K i } K i=0 and rel- ative transformations {[R 0,i |t 0,i ]} K i=1</formula><p>of reference view 0 and source view i, we compute the corresponding pixel p i,j := p i (d j ) in the source for a pixel p in the reference, given in homogeneous coordinates, and depth hypothesis d j := d j (p) as follows:</p><formula xml:id="formula_5">p i,j = K i ? (R 0,i ? (K ?1 0 ? p ? d j ) + t 0,i ).<label>(2)</label></formula><p>We obtain the warped source feature maps of view i and the j-th set of (per pixel different) depth hypotheses, F i (p i,j ), via differentiable bilinear interpolation.</p><p>Matching Cost Computation. For multi-view stereo, this step has to integrate information from an arbitrary number of source views into a single cost per pixel p and depth hypothesis d j . To that end, we compute the cost per hypothesis via group-wise correlation <ref type="bibr" target="#b39">[39]</ref> and aggregate over the views with a pixel-wise view weight <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b40">40]</ref>. In that manner we can employ visibility information during cost aggregation and gain robustness. Finally, the per group costs are projected into a single number, per reference pixel and hypothesis, by a small network. Let F 0 (p), F i (p i,j ) ? R C be the features in the reference and source feature maps respectively. After dividing their feature channels evenly into G groups, F 0 (p) g and</p><formula xml:id="formula_6">F i (p i,j ) g , the g-th group similarity S i (p, j) g ? R is com- puted as: S i (p, j) g = G C F 0 (p) g , F i (p i,j ) g ,<label>(3)</label></formula><p>where ?, ? is the inner product. We use S i (p, j) ? R G to denote the respective group similarity vector. Agglomeration over hypotheses and pixels delivers the tensor S i ? R W ?H?D?G . To find pixel-wise view weights,</p><formula xml:id="formula_7">{w i (p)} N ?1 i=1</formula><p>, we exploit the diversity of our initial set of depth hypotheses in the first iteration on stage 3 (Sec. 3.2.1). We intend w i (p) to represent the visibility information of pixel p in the source image I i . The weights are computed once and kept fixed and up-sampled for finer stages.</p><p>A simple pixel-wise view weight network, composed of 3D convolution layers with 1?1?1 kernels and sigmoid nonlinearities, takes the initial set of similarities S i to output a number between 0 and 1 per pixel and depth hypothesis to produce P i ? R W ?H?D . The view weights for pixel p and source image I i are given by:</p><formula xml:id="formula_8">w i (p) = max {P i (p, j)|j = 0, 1, . . . , D ? 1} ,<label>(4)</label></formula><p>where P i (p, j) intuitively represents confidence of visibility for the range covered by the j-th depth hypothesis at p. The final per group similaritiesS(p, j) for pixel p and the j-th hypothesis are the weighted sum of S i (p, j) and the view weight w i (p):</p><formula xml:id="formula_9">S(p, j) = N ?1 i=1 w i (p) ? S i (p, j) N ?1 i=1 w i (p) .<label>(5)</label></formula><p>Finally, we composeS(p, j) for all pixels and hypothe- ses intoS ? R W ?H?D?G and apply a small network with 3D convolution and 1?1?1 kernels to obtain a single cost, C ? R W ?H?D , per pixel and depth hypothesis.</p><p>Adaptive Spatial Cost Aggregation. Traditional MVS matching algorithms often aggregate costs over a spatial window (i.e. in our case a front-to-parallel plane) for increased matching robustness and an implicit smoothing effect. Arguably, our multi-scale feature extractor already aggregates neighboring information from a large receptive field in the spatial domain. Nevertheless, we propose to look into spatial cost aggregation. To prevent the problem of aggregating across surface boundaries, we propose an adaptive spatial aggregation strategy based on Patchmatch stereo <ref type="bibr" target="#b2">[3]</ref> and AANet <ref type="bibr" target="#b37">[37]</ref>. For a spatial window of K e pixels {p k } Ke k=1 are organized as a grid, we learn per pixel additional offsets {?p k } Ke k=1 . The aggregated spatial cost C(p, j) is defined as:</p><formula xml:id="formula_10">C(p, j) = 1 Ke k=1 w k d k Ke k=1 w k d k C(p+p k +?p k , j),<label>(6)</label></formula><p>where w k and d k weight the cost C based on feature and depth similarity (details in supplementary). Similar to adaptive propagation, the per pixel sets of displacements {?p k } Ke k=1 are found by applying a 2D CNN on the reference feature map F 0 . <ref type="figure" target="#fig_4">Fig. 5</ref> exemplifies the learned adaptive aggregation window. Sampled locations stay within object boundaries, while for the textureless region, the sampling points aggregate over a larger spatial context, which can potentially reduce the ambiguity of estimation.</p><p>Depth Regression. Using softmax we turn the (negative) costC to a probability P, which is used for sub-pixel depth regression and measure estimation confidence <ref type="bibr" target="#b42">[42]</ref>. The regressed depth value D(p) at pixel p is found as the expectation w.r.t. P of the hypotheses:</p><formula xml:id="formula_11">D(p) = D?1 j=0 d j ? P(p, j).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Depth Map Refinement</head><p>Instead of using Patchmatch also on the finest resolution level (stage 0), we find it sufficient to directly up-sample (from resolution W 2 ? H 2 to W ?H) and refine our estimation with the RGB image. Based on MSG-Net <ref type="bibr" target="#b18">[19]</ref>, we design a depth residual network. To avoid being biased for a certain depth scale, we pre-scale the input depth map into the range [0, 1] and convert it back after refinement. Our refinement network learns to output a residual that is added to the (up-sampled) estimation from Patchmatch, D, to get the refined depth map D ref . This network independently extracts feature maps F D and F I from D and I 0 and applies deconvolution on F D to up-sample the feature map to the image size. Multiple 2D convolution layers are applied on top of the concatenation of both feature maps -depth map and image -to deliver the depth residual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Function</head><p>Loss function L total considers the losses among all the depth estimation and rendered ground truth with same resolution as a sum:</p><formula xml:id="formula_12">L total = 3 k=1 n k i=1 L k i + L 0 ref .<label>(8)</label></formula><p>We adopt the smooth L1 loss for L k i , the loss of the i-th iteration of Patchmatch on stage k (k = 1, 2, 3) and L 0 ref , the loss for final refined depth map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our work on multiple datasets, such as DTU <ref type="bibr" target="#b0">[1]</ref>, Tanks &amp; Temples <ref type="bibr" target="#b23">[24]</ref> and ETH3D <ref type="bibr" target="#b32">[32]</ref> and analyze each new component with an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The DTU dataset <ref type="bibr" target="#b0">[1]</ref> is an indoor multi-view stereo dataset with 124 different scenes where all scenes share the same camera trajectory. We use the training, testing and validation split introduced in <ref type="bibr" target="#b19">[20]</ref>. The Tanks &amp; Temples dataset <ref type="bibr" target="#b23">[24]</ref> is provided as a set of video sequences in realistic environments. It is divided into intermediate and advanced datasets. ETH3D benchmark <ref type="bibr" target="#b32">[32]</ref> consists of calibrated high-resolution images of scenes with strong viewpoint variations. It is divided into training and test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Robust Training Strategy</head><p>Many learning-based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref> select two best source views based on view selection scores <ref type="bibr" target="#b42">[42]</ref> to train models on DTU <ref type="bibr" target="#b0">[1]</ref>. However, the selected source views have a strong visibility correlation with the reference view, which may affect the training of the pixel-wise view weight network. Instead, we propose a robust training strategy based on PVSNet <ref type="bibr" target="#b40">[40]</ref>. For each reference view, we Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acc.(mm)</head><p>Comp.(mm) Overall(mm) Camp <ref type="bibr" target="#b3">[4]</ref> 0.835 0.554 0.695 Furu <ref type="bibr" target="#b14">[15]</ref> 0.613 0.941 0.777 Tola <ref type="bibr" target="#b35">[35]</ref> 0.342 1.190 0.766 Gipuma <ref type="bibr" target="#b15">[16]</ref> 0.283 0.873 0.578 SurfaceNet <ref type="bibr" target="#b19">[20]</ref> 0.450 1.040 0.745 MVSNet <ref type="bibr" target="#b42">[42]</ref> 0.396 0.527 0.462 R-MVSNet <ref type="bibr" target="#b43">[43]</ref> 0.383 0.452 0.417 CIDER <ref type="bibr" target="#b39">[39]</ref> 0.417 0.437 0.427 P-MVSNet <ref type="bibr" target="#b27">[28]</ref> 0.406 0.434 0.420 Point-MVSNet <ref type="bibr" target="#b5">[6]</ref> 0.342 0.411 0.376 Fast-MVSNet <ref type="bibr" target="#b44">[44]</ref> 0.336 0.403 0.370 CasMVSNet <ref type="bibr" target="#b16">[17]</ref> 0.325 0.385 0.355 UCS-Net <ref type="bibr" target="#b6">[7]</ref> 0.338 0.349 0.344 CVP-MVSNet <ref type="bibr" target="#b41">[41]</ref> 0.296 0.406 0.351 Ours 0.427 0.277 0.352 <ref type="table">Table 1</ref>: Quantitative results of different methods on DTU's evaluation set <ref type="bibr" target="#b0">[1]</ref> (lower is better).</p><p>randomly choose four from the ten best source views for training. This strategy increases the diversity at training time and augments the dataset on the fly, which improves the generalization performance. In addition, training on those random source views with weak visibility correlation generates further robustness for our visibility estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>We implement the model with PyTorch <ref type="bibr" target="#b29">[30]</ref> and train it on DTU's training set <ref type="bibr" target="#b0">[1]</ref>. We set the image resolution to 640 ? 512 and the number of input images to N = 5. The selection of source images is based on the proposed robust training strategy. We set the iteration number of Patchmatch on stages 3, 2, 1 as 2, 2, 1. For initialization, we set D f = 48. For local perturbation, we set R 3 = 0.38, R 2 = 0.09, R 1 = 0.04 (see supplementary), N 3 = 16, N 2 = 8, N 1 = 8. In the adaptive propagation, we set K p on stages 3, 2, 1 to 16, 8, 0 (no propagation for last iteration on stage 1, see supplementary). For the adaptive evaluation, we use K e = 9 on all stages. We train our model with Adam <ref type="bibr" target="#b22">[23]</ref> (? 1 = 0.9, ? 2 = 0.999) for 8 epochs with a learning rate of 0.001. Here, we use a batch size of 4 and train on 2 Nvidia GTX 1080Ti GPUs. After depth estimation, we reconstruct point clouds similar to MVSNet <ref type="bibr" target="#b42">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Benchmark Performance</head><p>Evaluation on DTU Dataset. We input images at their original size (1600?1200) and set the number of views N to 5. The depth range for sampling depth hypotheses is fixed to [425 mm, 935 mm]. We follow the evaluation metrics provided by the DTU dataset <ref type="bibr" target="#b0">[1]</ref>. As shown in <ref type="table">Table 1</ref>, while Gipuma <ref type="bibr" target="#b15">[16]</ref> performs best in accuracy, our method outperforms others in completeness and achieves competitive performance in overall quality. <ref type="figure">Fig. 6</ref> shows qualitative results. Our solution reconstructs a denser point cloud with finer details, which reflects in a high completeness. Further, <ref type="figure">Figure 6</ref>: Qualitative comparison of scan 9 of DTU <ref type="bibr" target="#b0">[1]</ref>. Top: Reconstructed and ground truth point clouds. Our method preserves the thin structures on the roof better than CasMVSNet <ref type="bibr" target="#b16">[17]</ref> and delivers accurate boundaries. Bottom: Zoom in. Capable of handling a high input resolution, our result is much denser, with finer details at doors, windows and logos. our reconstruction at boundaries and thin structures appears better than of CasMVSNet <ref type="bibr" target="#b16">[17]</ref>. Our adaptive propagation can recover from errors at boundaries, induced at coarser resolutions, by using the information of neighbors inside the boundary (c.f . <ref type="figure" target="#fig_3">Fig. 4</ref>), while solely relying on sampling around a previous estimation, as CasMVSNet, can fail.</p><p>Memory and Run-time Comparison. We compare the memory consumption and run-time with several state-ofthe-art learning-based methods that achieve competing performance with low memory consumption and run-time: CasMVSNet <ref type="bibr" target="#b16">[17]</ref>, UCS-Net <ref type="bibr" target="#b6">[7]</ref> and CVP-MVSNet <ref type="bibr" target="#b41">[41]</ref>. These methods propose a cascade formulation of 3D cost volumes and output depth maps at the same resolution as the input images. As shown in <ref type="figure">Fig. 7</ref>, memory and run-time of all the methods increase almost linearly w.r.t. the resolution as the number of depth hypotheses is fixed (notably this will lead to under-sampling of the enlarged image space for methods using a naive cost volume approach). Note that at higher resolutions other methods could not fit into the memory of the GPU used for evaluation. We further observe that memory consumption and run-time increase much slower for PatchmatchNet than for other methods. For example, at a resolution of 1152 ? 864 (51.8%), memory consumption and run-time are reduced by 67.1% and 66.9% compared to CasMVSNet, by 55.8% and 63.9% compared to UCS-Net and by 68.5% and 83.4% compared to CVP-MVSNet. Combining the results in <ref type="table">Table 1</ref>, we conclude that our method is much more efficient in memory consumption and run-time than most state-of-the-art learning-based methods, at a very competitive performance.</p><p>Evaluation on Tanks &amp; Temples Dataset. We use the model trained on DTU <ref type="bibr" target="#b0">[1]</ref> without any fine-tuning. For evaluation, we set the input image size to 1920 ? 1056 and the number of views N to 7. The camera parameters and sparse point cloud are recovered with OpenMVG <ref type="bibr" target="#b28">[29]</ref>. During evaluation, the GPU memory and run-time for each depth map are 2887 MB and 0.505 s respectively. As shown in <ref type="table">Table 2</ref>, the performance of our method on the interme-  <ref type="figure">Figure 7</ref>: Relating GPU memory and run-time to the input resolution on DTU's evaluation set <ref type="bibr" target="#b0">[1]</ref>. The original image resolution is 1600?1200 (100%). Note that at higher resolutions other methods could not fit into the memory of a Nvidia RTX 2080 GPU, which is used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Intermediate Advanced COLMAP <ref type="bibr" target="#b31">[31]</ref> 42.14 27.24 MVSNet <ref type="bibr" target="#b42">[42]</ref> 43.48 -R-MVSNet <ref type="bibr" target="#b43">[43]</ref> 48. <ref type="bibr" target="#b40">40</ref> 24.91 CIDER <ref type="bibr" target="#b39">[39]</ref> 46.76 23.12 P-MVSNet <ref type="bibr" target="#b27">[28]</ref> 55.62 -Point-MVSNet <ref type="bibr" target="#b5">[6]</ref> 48.27 -Fast-MVSNet <ref type="bibr" target="#b44">[44]</ref> 47.39 -CasMVSNet <ref type="bibr" target="#b16">[17]</ref> 56.42 31.12 UCS-Net <ref type="bibr" target="#b6">[7]</ref> 54.83 -CVP-MVSNet <ref type="bibr" target="#b41">[41]</ref> 54.03 -Ours 53.15 32.31 <ref type="table">Table 2</ref>: Results of different methods on Tanks &amp; Temples <ref type="bibr" target="#b23">[24]</ref> (F score, higher is better). Note that most methods refrain to evaluate on the more challenging Advanced dataset.</p><p>diate dataset is comparable to CasMVSNet <ref type="bibr" target="#b16">[17]</ref>, which has the highest score. For the more complex advanced dataset, our method performs best among all the methods. Overall, due to its simple, scalable structure, our PatchmatchNet demonstrates competitive generalization performance, low memory consumption and low run-time compared to stateof-the-art learning-based methods that commonly use 3D cost volume regularization.</p><p>Evaluation on ETH3D Benchmark. We use the model trained on DTU <ref type="bibr" target="#b0">[1]</ref> without any fine-tuning. For evaluation, we set the input image size as 2688 ? 1792. Due to the strong viewpoint changes in ETH3D, we also use N = 7 views to utilize more multi-view information. Camera parameters and the sparse point cloud are recovered with COLMAP <ref type="bibr" target="#b31">[31]</ref>. During evaluation, the GPU memory consumption and run-time for the estimation of each depth map are 5529 MB and 1.250 s respectively. As shown in <ref type="table">Table 3</ref>, on the training dataset, the performance of our method is comparable to COLMAP <ref type="bibr" target="#b31">[31]</ref> and PVSNet <ref type="bibr" target="#b40">[40]</ref>. On the particularly challenging test dataset, our method performs   <ref type="table">Table 3</ref>: Results of different methods on ETH3D <ref type="bibr" target="#b32">[32]</ref> (F 1 score, higher is better). Due to strong viewpoint variations, currently, the only competitive pure learning-based method submitted on ETH3D is PVSNet <ref type="bibr" target="#b40">[40]</ref>.</p><p>best among all methods. Furthermore, our method is the fastest one evaluated so far (November 16th, 2020) on the ETH3D benchmark. Noting that PVSNet is a state-of-theart learning-based method, the quantitative results demonstrate the effectiveness, efficiency and generalization capabilities of our method. We visualize the pixel-wise view weight in <ref type="figure" target="#fig_6">Fig. 8</ref>. Brighter colors indicate co-visible areas, i.e. regions in the reference image that are also (well) visible in the source images. Conversely, areas that are not visible in the source images receive a dark color, corresponding to a low weight. Also pixel near depth discontinuities appear slightly darker than surrounding areas. By inspection, we conclude that our pixel-wise view weight is indeed capable to determine co-visible areas between the reference and source views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>We conduct an ablation study to analyze the components. Unless specified, all the following studies are done on DTU's evaluation set <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adaptive Propagation (AP) &amp; Adaptive Evaluation (AE).</head><p>We compare our base model with versions that employ fixed 2D offsets, similar to Gipuma <ref type="bibr" target="#b15">[16]</ref>, for propagation (w/o AP), and, fixed 2D offsets to sample the neighbors for spatial cost aggregation in the evaluation step (w/o AE). As shown in     <ref type="bibr" target="#b0">[1]</ref>. 'Stage k, n th iter' denotes the result of the n th iteration of Patchmatch on stage k. Already after stage 3 our estimate is close to the solution while stage 2 and 1 refine it even more. well as completeness.</p><p>Number of Iterations of Patchmatch. Recall that, during training (Sec. 4.3), we do not include adaptive propagation for Patchmatch on stage 1. Consequently, we also keep the number of iterations on stage 1 as 1 for this investigation. More iterations of Patchmatch generally improve the performance, yet, the improvements stagnate after '2,2,1' iterations, <ref type="table" target="#tab_3">Table 5</ref>. We further visualize the distribution of normalized absolute error in the inverse depth range for the setting '2,2,1' in <ref type="figure" target="#fig_7">Fig. 9</ref>. We observe that the error converges after only 5 iterations of Patchmatch across all stages. Compared to Gipuma <ref type="bibr" target="#b15">[16]</ref> that employs a large number of neighbors for propagation, we have embedded Patchmatch in a coarse-to-fine framework to speed up long range interactions. Apart from that, our learned adaptive propagation, diverse initialization and local perturbation all contribute towards a faster converge.    <ref type="bibr" target="#b23">[24]</ref> and ETH3D <ref type="bibr" target="#b32">[32]</ref>. <ref type="table" target="#tab_5">Table 6</ref> shows a similar performance on DTU <ref type="bibr" target="#b0">[1]</ref> for all the models, yet, we observe a drop in performance on the other datasets without pixel-wise view weight or the robust training strategy. This proves that these two modules lead to improved robustness and a better generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel</head><p>Number of Views. Apart form our standard setting of N = 5 views for DTU's evaluation set <ref type="bibr" target="#b0">[1]</ref>, we also evaluate the performance when N = 2, 3, 6. Using more views is known to improve performance, e.g. by alleviating the occlusion problem, which coincides with our findings summarized in <ref type="table" target="#tab_6">Table 7</ref>. With more input views, the reconstruction quality tends to improve in both accuracy and completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present PatchmatchNet, a novel cascade formulation of learning-based Patchmatch, augmented with learned adaptive propagation and evaluation modules based on deep features. Inherited from its name-giving ancestor, Patch-matchNet naturally possesses low memory requirements, independent of the disparity range and unlike most learningbased methods, PatchmatchNet does not rely on 3D cost volume regularization. Embedded into a cascade formulation, PatchmatchNet further shows a high processing speed. Despite its simple structure, extensive experiments on DTU, Tanks &amp; Temples and ETH3D demonstrate a remarkably low computation time, low memory consumption, favorable generalization properties and competitive performance compared to the state-of-the-art. PatchmatchNet makes learning-based MVS more efficient and more applicable to memory restricted devices or time critical applications. For the future, we hope to apply it on movable platforms such as mobile phones and head mounted displays, where the computation resource is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Why not use 3D cost volume regularization?</head><p>The adaptive evaluation of our learning-based Patchmatch utilizes 3D convolution layers with 1 ? 1 ? 1 kernels for the matching cost computation as well as the pixelwise view weight estimation. This is in contrast to common previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref> where a 3D U-Net regularizes the cost volume. Similarly, arguing that the distribution of cost volume itself being not discriminative enough <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">33]</ref>, PVSNet <ref type="bibr" target="#b40">[40]</ref> also applies a 3D U-Net for predicting the visibility per source view.</p><p>The problem with such regularization framework is that it requires a regular spatial structure in the volume. Although we concatenate the matching costs per pixel and depth hypothesis into a volume-like shape as other works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42]</ref>, we do not possess such a regular structure: (i) the depth hypotheses for each pixel and its spatial neighbors are different, which makes it difficult to aggregate cost information in the spatial domain; (ii) the depth hypotheses of each pixel are not uniformly distributed in the inverse depth range as CIDER <ref type="bibr" target="#b39">[39]</ref>, which makes it difficult to aggregate cost information along depth dimension.</p><p>Recall that during the computation of the pixel-wise view weights in the initial iteration of Patchmatch, depth hypotheses are randomly distributed in the inverse depth range, i.e. the hypotheses are spatially different per pixel. In each subsequent iteration (on stage k), we perform local perturbation by generating per pixel N k depth hypotheses uniformly in the normalized inverse depth range R k , which is centered at the previous estimate. Consequently, the hypotheses of spatial neighbors can differ significantly, especially at depth discontinuities and thin structures. Including the hypotheses obtained by adaptive propagation, that are, moreover, not uniform in the inverse depth range, will increase these effects further.</p><p>In the end, however, the main reason for our approach to avoid 3D cost volume regularization altogether is efficiency. In a coarse-to-fine framework, running such regularization frameworks over multiple iterations of Patchmatch on each stage would increase memory consumption and run-time significantly and mitigate our main contribution of building a high-performance, but particularly lightweight framework that can operate with a high computation speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">How to set the normalized inverse depth</head><p>range R k in the local perturbation step of Patchmatch?</p><p>After the initial iteration, our set of hypothesis is obtained by adaptive propagation and by local perturbation of the previous estimation. Recall that our local perturbation procedure enriches the set of hypothesis by generating  per pixel N k depth hypotheses uniformly in the normalized inverse depth range R k . The objective is two-fold. Especially at the beginning, at low resolution, this helps to further explore the search space. More importantly, our adaptive propagation implicitly assumes front-to-parallel surfaces, since we do not explicitly include tangential surface information (due to an implied heavy memory consumption) like <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">38]</ref>. Sampling in the local vicinity of the previous estimation will refine the solution locally and mitigate potential disadvantages from not explicitly modeling tangential surface information. We find it helpful to apply these perturbations already at an early stage to inject the positive effects into hypothesis propagation and note that aposteriori refinement at the finest level alone cannot recover the same quality. In practice, we again operate in coarse-tofine manner and set R k accordingly, based on the hierarchy level. <ref type="figure" target="#fig_9">Fig.10</ref> shows the cumulative distribution function of the normalized absolute error in the inverse depth range on DTU's evaluation set <ref type="bibr" target="#b0">[1]</ref>. After the first iteration of Patchmatch on stage 3, the estimation error decreases remarkably: the normalized error is already smaller than 0.1 for 90.0% percent of the cases. Visibly, the performance keeps improving after each iteration. To correct errors in estimation and refine the results on stage k, we set R k to compensate most of estimation errors. For example, we set R 3 = 0.38 for Patchmatch on stage 3 after first iteration so that we can cover most ground truth depth in the hypothesis range and then refine the results. Besides, adaptive propagation will further correct those wrong estimations with the depth hypotheses from neighbors when sampling in R k fails in refinement (c.f . <ref type="figure">Fig. 6</ref> from the paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Why not include propagation for last iteration of Patchmatch on stage 1?</head><p>Similar to MVSNet <ref type="bibr" target="#b42">[42]</ref>, the point cloud reconstruction mainly consists of photometric consistency filtering, geometric consistency filtering and depth fusion. Photometric consistency filtering is used to filter out those depth hypotheses that have low confidence. Based on MVSNet <ref type="bibr" target="#b42">[42]</ref>, we define the confidence as the probability sum of the depth hypotheses that fall in a small range near the estimation. We use the probability P (c.f . Eq. 7 from the paper) from the last iteration of Patchmatch on stage 1 for filtering. In this iteration, we only perform local perturbation, without adaptive propagation. At stage 1, operating at a quarter the image resolution and with the algorithm almost converged, the hypotheses obtained via propagation from spatial neighbors are usually very similar to the current solution. Such irregular sampling of the probability space causes bias in the regression (c.f . Eq. 7 from the paper) and the estimate becomes over-confident at the current solution, where most propagated samples are located. In contrast, by performing only the local perturbation, the depth hypotheses are uniformly distributed in the inverse depth range. Contrary to previous iterations, we compute the estimated depth at pixel p, D(p), by utilizing the inverse depth regression <ref type="bibr" target="#b39">[39]</ref>, which is based on the soft argmin operation <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_13">D(p) = ( D?1 j=0 1 d j ? P(p, j)) ?1 ,<label>(9)</label></formula><p>where P(p, j) is the probability for pixel p at the j-th depth hypothesis. Then we compute the probability sum of four depth hypotheses that are nearest to the estimation to measure the confidence <ref type="bibr" target="#b42">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Weighting in the Adaptive Spatial Cost Aggregation</head><p>Recall that in Eq. 6 of the paper we utilize two weights to aggregate our spatial costs, {w k } Ke k=1 based on spatial feature similarity and {d k } Ke k=1 based on the similarity of depth hypotheses. The feature weights {w k } Ke k=1 at a pixel p are based on the feature similarity at the sampling locations around p, measured in the reference feature map F 0 . Given the sampling positions {p + p k + ?p k } Ke k=1 , we extract the corresponding features from F 0 via bilinear interpolation. Then we apply group-wise correlation <ref type="bibr" target="#b17">[18]</ref> between the features at each sampling location and p. The results are concatenated into a volume on which we apply 3D convolution layers with 1 ? 1 ? 1 kernels and sigmoid non-linearities to output normalized weights that describe the similarity between each sampling point and p.</p><p>As discussed in Sec. 1, neighboring pixels will be assigned different depth values throughout the estimation pro-  <ref type="table">Table 8</ref>: Quantitative results of different stages on DTU's evaluation set <ref type="bibr" target="#b0">[1]</ref> (lower is better). The depth maps on stages 3, 2 and 1 are upsampled to reach the same resolution as input images and then used to reconstruct point clouds. cess. For pixel p and the j-th depth hypothesis, our depth weights {d k } Ke k=1 take this into account and downweight the influence of samples with large depth difference, especially when located across depth discontinuities. To that end, we collect the absolute difference in inverse depth between each sampling point and pixel p with their j-th hypotheses, and obtain the weights by applying a sigmoid function on the, again, inverted differences for normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation of Multi-stage Depth Estimation</head><p>We use multiple stages to estimate the depth map in a coarse-to-fine manner. Here, we analyze the effectiveness of our multi-stage framework. We upsample the estimated depth maps on stages 3, 2 and 1, to the same resolution as the input and then reconstruct the point clouds. As shown in <ref type="table">Table 8</ref>, the reconstruction quality gradually increases from coarser stages to finer ones. This shows that our multistage framework can reconstruct the scene geometry with increasing accuracy and completeness. <ref type="figure" target="#fig_1">Figure 12</ref>: Visualization of sampling locations in adaptive evaluation for two typical situations: object boundary and textureless region. The center points and sampling points are shown in red and blue respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Visualization of Adaptive Propagation</head><p>We visualize the sampling locations in two typical situations, at an object boundary and a textureless region. As shown in <ref type="figure" target="#fig_10">Fig. 11</ref>, for the pixel p at the object boundary, all sampling points tend to be located on the same surface as p. In contrast, for the pixel q in the textureless region, the sampling points are spread over a larger region. By sampling from a large region, a more diverse set of depth hypotheses can be propagated to q and reduce the local ambiguity for depth estimation in the textureless area. The visualization shows two examples how the adaptive propagation successfully adapts the sampling to different challenging situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Visualization of Adaptive Evaluation</head><p>Here, we again visualize the sampling locations for two situations, at an object boundary and a textureless region. <ref type="figure" target="#fig_1">Fig.12</ref> demonstrates that for the pixel p at the object boundary, sampling points tend to stay within the boundaries of the object, such that they focus on similar depth regions. For the pixel q in the textureless region, the points are distributed sparsely to sample from a large context, which helps to obtain reliable matching and to reduce the ambiguity. Again, the visualization demonstrates how our adaptive evaluation adapts the sampling for the spatial cost aggregation to different situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Visualization of Point Clouds</head><p>We visualize reconstructed point clouds from DTU's evaluation set <ref type="bibr" target="#b0">[1]</ref>, Tanks &amp; Temples dataset <ref type="bibr" target="#b23">[24]</ref> and ETH3D benchmark <ref type="bibr" target="#b32">[32]</ref> in <ref type="figure" target="#fig_2">Fig. 13, 14</ref>, 15.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Structure of PatchmatchNet: multi-scale feature extractor, learning-based Patchmatch and refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Detailed structure of learned Patchmatch. At the initial iteration of coarsest stage 3 only random depth hypotheses in initialization are used. Afterwards, hypotheses are obtained from adaptive propagation and local perturbation, the latter providing depth samples around the previous estimate. The learned pixel-wise view weight is estimated in the first iteration of Patchmatch and kept fixed in the matching cost computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Sampled locations with adaptive propagation. Pixels located at the object boundary (yellow) and a textureless region (red) receive depth hypotheses from sampled neighbors (green and orange). (a) Reference image. (b) Fixed sampling locations of classic propagation. (c) Adaptive sampling locations with adaptive propagation. The grayscale image in (b) and (c) is the ground truth depth map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Sampled locations in adaptive spatial cost aggregation. Pixels located at an object boundary (yellow) and a textureless area (red) aggregate matching costs from sampled neighbors (blue and orange). (a) Reference image for depth prediction. (b) Fixed sampling locations. (c) Adaptive sampling locations of our method. The grayscale image in (b) and (c) is the ground truth depth map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Visualization of our pixel-wise view weight on a scene from ETH3D<ref type="bibr" target="#b32">[32]</ref>. Areas marked with boxes in source images and reference image are co-visible. Right: The corresponding pixel-wise view weights, bright colors (large values) indicate co-visibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Distribution of normalized absolute errors in the inverse depth range on DTU's evaluation set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Cumulative distribution function of normalized absolute errors in the inverse depth range on DTU's evaluation set<ref type="bibr" target="#b0">[1]</ref>. 'Stage k, n th iter' denotes the result of the n th iteration of Patchmatch on stage k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Visualization of sampling locations in adaptive propagation for two typical situations: object boundary and textureless region. The center points and sampling points are shown in red and blue respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Reconstruction results on DTU's evaluation set<ref type="bibr" target="#b0">[1]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Reconstruction results on Tanks &amp; Temples dataset<ref type="bibr" target="#b23">[24]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 ,</head><label>4</label><figDesc>our adaptive propagation and adaptive evaluation modules each improve results w.r.t. accuracy as</figDesc><table><row><cell>Methods</cell><cell>Acc.(mm)</cell><cell>Comp.(mm)</cell><cell>Overall(mm)</cell></row><row><cell>w/o AP &amp; AE</cell><cell>0.453</cell><cell>0.339</cell><cell>0.396</cell></row><row><cell>w/o AP</cell><cell>0.437</cell><cell>0.285</cell><cell>0.361</cell></row><row><cell>w/o AE</cell><cell>0.437</cell><cell>0.324</cell><cell>0.380</cell></row><row><cell>Ours</cell><cell>0.427</cell><cell>0.277</cell><cell>0.352</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Parameter sensitivity on DTU<ref type="bibr" target="#b0">[1]</ref> for Adaptive Propagation (AP) and Adaptive Evaluation (AE).</figDesc><table><row><cell>Iterations</cell><cell>Acc.(mm)</cell><cell>Comp.(mm)</cell><cell>Overall(mm)</cell></row><row><cell>1,1,1</cell><cell>0.446</cell><cell>0.278</cell><cell>0.362</cell></row><row><cell>2,2,1</cell><cell>0.427</cell><cell>0.277</cell><cell>0.352</cell></row><row><cell>3,3,1</cell><cell>0.425</cell><cell>0.277</cell><cell>0.351</cell></row><row><cell>4,4,1</cell><cell>0.425</cell><cell>0.277</cell><cell>0.351</cell></row><row><cell>5,5,1</cell><cell>0.425</cell><cell>0.277</cell><cell>0.351</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Ablation</figDesc><table><row><cell></cell><cell>study of the number of Patchmatch it-</cell></row><row><cell cols="2">erations on DTU [1]. Iterations, 'a,b,c' means that there</cell></row><row><cell cols="2">are a, b and c iterations on stage 3, 2 and 1. Thanks to the</cell></row><row><cell cols="2">coarse-to-fine framework, learned adaptive propagation, di-</cell></row><row><cell cols="2">verse initialization and local perturbation, our method con-</cell></row><row><cell cols="2">verges after only 5 iterations combined on all stages.</cell></row><row><cell>Percentage</cell><cell>Normalized Absolute Error in Inverse Depth Range 0.000 0.005 0.010 0.015 0.020 0.025 0.00 0.05 0.10 0.15 0.20 0.25 0.30 stage 3: 1st iter stage 3: 2nd iter stage 2: 1st iter stage 2: 2nd iter stage 1: 1st iter</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation study concerning the pixel-wise view weight (VW) and the robust training strategy (RT).</figDesc><table><row><cell>N</cell><cell>Acc.(mm)</cell><cell>Comp.(mm)</cell><cell>Overall(mm)</cell></row><row><cell>2</cell><cell>0.439</cell><cell>0.332</cell><cell>0.385</cell></row><row><cell>3</cell><cell>0.428</cell><cell>0.284</cell><cell>0.356</cell></row><row><cell>5</cell><cell>0.427</cell><cell>0.277</cell><cell>0.352</cell></row><row><cell>6</cell><cell>0.429</cell><cell>0.278</cell><cell>0.353</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of the number of input views</figDesc><table><row><cell>N on</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 15</ref><p>: Reconstruction results on ETH3D benchmark <ref type="bibr" target="#b32">[32]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale data for multiple-view stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Rasmus Ramsb?l Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders Bjorholm</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Patchmatch stereo -stereo matching with slanted support windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using multiple hypotheses to improve depth-maps for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Hern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Ren</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Sheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Point-based multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep stereo using adaptive thin volume representation with uncertainty awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A space-sweep approach to true multi-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeppruner: Learning efficient stereo matching via differentiable patchmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Duggal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning confidence measures by multi-modal convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Fu And Mohsen Ardabilian Fard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conf. on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mve-a multi-view reconstruction environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Langguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goesele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCH</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Carved visual hulls for image-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasutaka</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Massively parallel multiview stereopsis by surface normal diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Group-wise correlation stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wukui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Depth map super-resolution by deep multi-scale guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Surfacenet: An end-to-end 3D neural network for multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a multi-view stereo machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>H?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayk</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitro</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Tanks and temples: Benchmarking large-scale scene reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Knapitsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detail-preserving and content-aware variational multi-view stereo reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuanquan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Image Processing (TIP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Progressive prioritized multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Locher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning patch-wise matching confidence aggregation for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. P-Mvsnet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Openmvg: Open multiple view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Moulon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Monasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romuald</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Reproducible Research in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Adam Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De-Vito</surname></persName>
		</author>
		<imprint>
			<publisher>Sasank Chilamkurthy</publisher>
			<pubPlace>Martin Raison</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structure-from-Motion Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A multi-view stereo benchmark with highresolution images and multi-camera videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sch?ps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvano</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Patch based confidence prediction for dense disparity map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihito</forename><surname>Seki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-view stereo via graph cuts on the dual of an adaptive tetrahedral mesh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippos</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient large-scale multi-view stereo for ultra high-resolution image sets. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic multi-view stereo: Jointly estimating objects and voxels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">AANet: Adaptive aggregation network for efficient stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR), 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multi-scale geometric consistency guided multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning inverse depth regression for multi-view stereo with correlation cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">PVSNet: Pixelwise visibility-aware multi-view stereo network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cost volume pyramid based depth inference for multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MVSNet: Depth inference for unstructured multiview stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recurrent MVSNet for high-resolution multi-view stereo depth inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fast-MVSNet: Sparse-todense multi-view stereo with learned propagation and gaussnewton refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Patchmatch based joint view selection and depthmap estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
