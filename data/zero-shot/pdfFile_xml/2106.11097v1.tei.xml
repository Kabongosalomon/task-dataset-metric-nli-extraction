<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CLIP2Video: Mastering Video-Text Retrieval via Image CLIP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
							<email>fanghan@bupt.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
							<email>xiongpengfei2019@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luhui</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>PCG</roleName><forename type="first">Yu</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">CLIP2Video: Mastering Video-Text Retrieval via Image CLIP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present CLIP2Video network to transfer the imagelanguage pre-training model to video-text retrieval in an end-to-end manner. Leading approaches in the domain of video-and-language learning try to distill the spatiotemporal video features and multi-modal interaction between videos and languages from a large-scale video-text dataset. Different from them, we leverage pretrained imagelanguage model, simplify it as a two-stage framework with co-learning of image-text and enhancing temporal relations between video frames and video-text respectively, make it able to train on comparatively small datasets. Specifically, based on the spatial semantics captured by Contrastive Language-Image Pretraining (CLIP) model, our model involves a Temporal Difference Block to capture motions at fine temporal video frames, and a Temporal Alignment Block to re-align the tokens of video clips and phrases and enhance the multi-modal correlation. We conduct thorough ablation studies, and achieve state-of-the-art performance on major text-to-video and video-to-text retrieval benchmarks, including new records of retrieval accuracy on MSR-VTT, MSVD and VATEX.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video-text retrieval, which aims to return for a given textual query the most relevant videos, is a fundamental research task for multi-modal video-and-language understanding. It becomes an emerging requirement with the increasing of web videos. In the past years, remarkable progress <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3]</ref> has occurred across many video-text benchmarks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Most such approaches focus on two critical issues. The first is the visual feature representation in the video domain. * This work is done when Han Fang is an intern at Tencent ? Corresponding author Different from image, video feature representation considers both spatial and temporal dimensions. Multi-path 2D or 3D convolutional networks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37]</ref> are still the core operators for feature learning, while both the spatial and temporal representations are considered in the same convolution operation for semantic and motion modalities. The other one is multi-modal interaction between video and languages. Based on a large-scale video-text dataset, singlestream or two-stream methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b8">9]</ref> are adopted to jointly train video features and text features inside the same embedding space. Nevertheless, these two problems are complex enough to make it difficult to achieve both goals in the same network. Some massive pre-training video-text datasets are sorted out to solve this problem, e.g. Howto100M <ref type="bibr" target="#b25">[26]</ref>. However, the pretrained models show limited performance gain for video-text retrieval, while annotated video data is hard to collect.</p><p>To address these challenges, we rethink the video-text retrieval task from a more macroscopic point of view. While videos and sentences are both sequential, the meaning of a word can be reflected in an image or a sequence frames. For example, atomic actions need to be contextualized with short-term segments, while object is described in single image. Thus, the video-and-language understanding is divided into two independent problems, spatial representation of multi-modal image-text training, and temporal relationships of video frames and video-language. Compared with the video-text pre-training model, the learning of imagetext model is much easier. The prominent success of the CLIP <ref type="bibr" target="#b29">[30]</ref> (Contrastive Language-Image Pre-training) has demonstrated its capability of learning SOTA image representations from linguistic supervision with pre-training on large-scale image and text pairs.</p><p>Based on the spatial semantics captured by CLIP <ref type="bibr" target="#b29">[30]</ref>, we present CLIP2Video network to transfer the imagelanguage pre-training model to video-text retrieval with two parts: Temporal Difference Block (TDB) and Temporal Alignment Block (TAB). As their names imply, the two components are devised to confront with the temporal rela-  <ref type="figure">Figure 1</ref>. Overview of CLIP2Video. It consists of two key components: Temporal Difference Block (TDB), which is used to enhance temporal interaction between frames; Temporal Alignment Block (TAB), which is adopted to align video clips and contextual words in the same space, capturing the motion change by cross-modal understanding.</p><p>tions of video frames and video-language respectively. We transform the video feature into a feature sequence. For Temporal Difference Block, we add the difference of image frames to the sequence to simulate the motion change. In respect of Temporal Alignment Block, the video sequence and text sequence are aligned to the same space to enhance the correlation between video clips and phrases. <ref type="figure">Fig.1</ref> shows the structure of these two components. Similar to our work, the concurrent works from <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b22">23]</ref> are also built on CLIP for video-text retrieval. However, both of these two works only analyze a series of experiments to verify the effects of CLIP model for pre-training. Instead, we further study how to better model the temporal dependency between video frames and video-text, taking advantage of existing remarkable image pretrained model. In summary, there are three main contributions in our paper:</p><p>? We put forward a new perspective of video-language learning with two independent modules, image-text multi-modal learning and temporal relationships between video frames and video-text, which respectively solves the multi-modal learning problems in the spatial and temporal aspects.</p><p>? We introduce two modules, Temporal Difference Block and Temporal Alignment Block, handling with temporal relationship of video frame and video text respectively, which can be used for any video language problem.</p><p>? We report new records of retrieval accuracy on several text-to-video and video-to-text retrieval benchmarks, including MSR-VTT <ref type="bibr" target="#b38">[39]</ref>, MSVD <ref type="bibr" target="#b6">[7]</ref> and VATEX <ref type="bibr" target="#b37">[38]</ref>. Accompanied by thorough ablation studies, the large improvements are pin-pointed as contributions by our divided concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Representation Learning. Previous works mainly focus on 2D/3D spatial-temporal convolution for video representation. SlowFast <ref type="bibr" target="#b11">[12]</ref> explores a network architecture that operates in two pathways and different frame rates. Recently ViT <ref type="bibr" target="#b9">[10]</ref>, a transformer-based image encoder, which is shown to deliver impressive performance on image categorization, has attracted much attention. While introduced into video domain, ViViT <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref> and time transformer <ref type="bibr" target="#b5">[6]</ref> propose several variants of ViT, including those that are more efficient by factorising the spatial and temporal dimensions of the input video. Similar to our work, they have been carried out this idea that adopt separately attentions on temporal and spatial with two-path transformer models. However, their methods still focus on designing an end-to-end network structure to decouple the two problems. We mainly investigate the effective temporal relationships based on multi-modal image-text learning.</p><p>Video-language Learning. Learning visual representation from text representation is an emerging research topic with the benefit of large-scale visual and language pairs collection. Howto100M <ref type="bibr" target="#b25">[26]</ref> is one of the largest datasets for video-text multi-modal pretraining. However, there exists much ambiguity noises between text semantics and video content. MIL-NCE <ref type="bibr" target="#b23">[24]</ref> mainly investigates to leverage this noisy instructional videos to learn a better video encoder in an multi-modal learning manner. Others <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b18">19]</ref> collect videos and accompanied text information from YouTube and Instagram to to learn spatio-temporal features in an efficient weakly-supervised manner. However, compared with image text pretraining data set, the collection of video text data set is much more complex, and its noise is also much larger. This makes the video pretraining model difficult to play a very big role.</p><p>Video-Text Retrieval. Early works on video-text retrieval designed intensive fusion mechanisms for crossmodal learning. Based on a large-scale annotated videotext dataset, single-stream or two-stream methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b8">9]</ref> are adopted to jointly extract video features and language features and project them into the same embedding space. Recently, the pre-trained models have dominated the leaderboard of the video-text retrieval with noticeable results on zero-shot retrieval. Concurrent to our work, <ref type="bibr" target="#b28">[29]</ref> apply CLIP for zero-shot prediction. We propose to directly transfer the powerful knowledge from the pretrained CLIP and continue learn the designed video-based CLIP2video model on a video-language dataset. Empirical studies present the effectiveness of the CLIP2video model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Given a set of captions as queries, our goal is to search for the corresponding videos by mapping video and text into joint embedding space. Inspired by the success of transferring image-text pre-training knowledge into video-text learning <ref type="bibr" target="#b17">[18]</ref>, we directly adopt CLIP <ref type="bibr" target="#b29">[30]</ref> for initialization to extend the ability in text-to-video retrieval. Different from image-to-text retrieval, temporal correlations of visual clues fully reflect the semantics of video, which helps to facilitate cross-modal understanding. So, a temporal difference block is proposed to excite the motion-sensitive interactions explicitly. Meanwhile, we propose temporal alignment block to fully exploit the alignment between context of text and content of key frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Temporal Difference Block</head><p>To obtain video embedding, vision transformer (ViT) <ref type="bibr" target="#b9">[10]</ref> is adopted firstly to encode every frame into feature. In particular, ViT extracts N non-overlapping image patches and perform linear projection to map every patch into 1D token. With injection of positional embedding and extra [CLS] token, the sequence of tokens z are input into L slayer transformer to model the correlation of each patch, where each layer l s comprises of Multi-Head Self-Attention (M SA) <ref type="bibr" target="#b35">[36]</ref>, layer normalization (LN ) <ref type="bibr" target="#b3">[4]</ref>, and Multi-layer Perception (M LP ). Then, a linear projection is adopted to encode z Ls cls into embedding of the same dimension as text embedding for frame representation.</p><p>However, as shown in <ref type="figure">Fig.1</ref>, the spatial ViT models correlation within each frame without consideration of temporal indices. So, to exploit interactions between different frames, we propose a L t -layer temporal transformer to encode video representation. Frame embeddings output by ViT are concatenated as frame tokens. Since two successive frames contain content displacement, which reflects the actual actions, we explicitly propose temporal difference block to extend the input and guide temporal transformer to encode more motion-related representations. The structure is shown in <ref type="figure" target="#fig_0">Fig.2</ref>. Specifically, we adopt transformed difference of frame embedding between adjacent time stamps to describe the motion change, which is formulated as:</p><formula xml:id="formula_0">F d = 2?(?({f 1 f ? f 0 f , f 2 f ? f 1 f , ..., f m?1 f ? f m?2 f } + P)) ? 1<label>(1)</label></formula><p>where P is the positional embedding, f m?1 f and f m?2 f are the two adjacent frame embeddings, ? is the sigmoid function, ? is 1-layer transformer, and F d is the differenceenhanced token. Instead of adopting subtraction directly to represent difference, we propose to perform difference-level attention ? with sigmoid transformation. By employing attention transformation on the whole subtraction, the subtraction of successive frame embedding can be encoded to mode long-term relationship of all segments and normalized into [-1, 1] to indicate difference. Then we insert differenceenhanced tokens between every adjacent frames as:</p><formula xml:id="formula_1">F te = {f 0 f , f 1 d , f 1 f , f 2 d , f 2 f , ..., f m?1 d , f m?1 f } + P + T (2)</formula><p>F te is the final temporal token output from temporal difference block, which is added with positional (P) and type (T) information. So, the frame tokens inserted with difference-enhanced tokens are input into temporal transformer, further promoting the sensitivity to capture motionrelated information. Since F d only describe the motion between frames, we only adopt output of frame tokens</p><formula xml:id="formula_2">F v = {f 0 v , f 1 v , f 2 v , ..., f m?1 v</formula><p>} as video embedding, which consist of both spatial and temporal information. Then global average pooling is adopted to encode final video representation</p><formula xml:id="formula_3">f g v . ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame Token</head><p>Word Token</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Difference-enhanced Token</head><p>Temporal Alignment Block (TAB) <ref type="figure">Figure 3</ref>. Temporal Alignment Block. Word tokens and frame tokens with temporal-enhanced correlation are aligned into the shared centers C for measuring the aggregated distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Alignment Block</head><p>In common text-video retrieval, the modal representation is firstly calculated in individual domain, and then measure the similarity in joint space. By adopting temporal difference block with temporal transformer, the video embedding can be finely encoded. For text representation, we directly adopt CLIP's text encoder to generate the text representation, which is based on 12-layer transformer <ref type="bibr" target="#b35">[36]</ref> modified by Radford et al. <ref type="bibr" target="#b30">[31]</ref>. Following CLIP <ref type="bibr" target="#b29">[30]</ref>, lowercased byte pair encoding (BPE) with a 49152 vocab size <ref type="bibr" target="#b32">[33]</ref> is employed to tokenize input caption ?. The tokenized captions are bracketed with [CLS] and [SEP] token to indicate the start and end. Then text embedding is computed by the text encoder and representation can be seen</p><formula xml:id="formula_4">as F t = {f cls t , f 0 t , f 1 t , ..., f n?1 t },</formula><p>where n is the sequence length. So, the output of [CLS] token which named as f cls t is utilized as overall representation f g t to minimize the distance with f g v for global matching. However, since the existence of abundant contextual information in F t , which fully indicates the entire semantics, all word tokens can be adopted as auxiliary supervision to align key frames with apparent motion change.</p><p>Inspired by Netvlad <ref type="bibr" target="#b1">[2]</ref>, we propose temporal alignment block to aggregate token embeddings of different modalities with shared centers and re-emphasise context-related representation. Specifically, K shared centers {c 1 , c 2 , ..., c k } are learned to align frame and word embedding jointly. Following <ref type="bibr" target="#b1">[2]</ref>, we calculate the confidence between modal features and shared centers by using dot-product, which is employed to assign weight for each cluster to measure the distribution. The formula is seen as follows:</p><formula xml:id="formula_5">w ij = exp(? i c T j ) K k=1 exp(? i c T k ) ,<label>(3)</label></formula><p>where ? i indicates i-th modal feature, c j is j-th shared center and w ij represents the normalized similarity. Then the aggregated embedding aligned with center c j can be obtained as:</p><formula xml:id="formula_6">? j = ? i=1 w ij (? i ?c j ) ? i=1 w ij (? i ?c j ) 2 .<label>(4)</label></formula><p>? is the max length of modal feature, and ? j is j-th aligned center embedding.c j is the trainable weight that has the same size as c j to increase adaption <ref type="bibr" target="#b1">[2]</ref>. Then center representation can be obtained as ? = {? 1 , ? 2 , ..., ? K }. Since the video and text are aggregated with shared centers of the same content, the overall semantic context in every modality token can be fully aligned into joint space before calculating the similarity. To further emphasize the weight of motion-related frame tokens toward action-described centers, we re-sample frame embedding sparsely with double frame rate from F f as:</p><formula xml:id="formula_7">F fl = {f 0 f , f 2 f , ..., f m?1 f }.</formula><p>Although, F fl sampled in large frame rate loses semantic coherence, it highlights changes in motion, which is beneficial as complementary information to re-adjust the weight distribution of motion-related center. To excite the temporal relationship, the shared temporal difference block is adopted to encode temporal tokens. We adopt a 1-layer transformer to correlate each temporal token and use the output of F fl as</p><formula xml:id="formula_8">F dl . Then F dl is concatenated with F f as F ml = [F f , F dl ].</formula><p>Finally, aligned video and text representations can be seen as follows:</p><formula xml:id="formula_9">? v j = 1 ? v 1.5(m?1) i=0 exp(f i ml c T j ) K k=1 exp(f i ml c T k ) (f i ml ?c j )<label>(5)</label></formula><formula xml:id="formula_10">? t j = 1 ? t n i=0 exp(f i t c T j ) K k=1 exp(f i t c T k ) (f i t ?c j ),<label>(6)</label></formula><p>where ? v and ? t indicates the l 2 normalization. By adding the difference-enhanced frame tokens in large frame rate, the weight of action-described centers can be readjust for better alignment. Then global average pooling is also adopted to obtain final aligned representation f a v and f a t for video and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss function</head><p>To train CLIP2Video, we adopt symmetric cross entropy loss. Each training batch ? consists of B video-text pairs, which is discriminated in training:</p><formula xml:id="formula_11">L t2v = ? 1 B i?? log exp(&lt; f i t , f i v &gt;) j?? exp(&lt; f i t , f j v &gt;) ,<label>(7)</label></formula><formula xml:id="formula_12">L v2t = ? 1 B i?? log exp(&lt; f i v , f i t &gt;) j?? exp(&lt; f i v , f j t &gt;) ,<label>(8)</label></formula><formula xml:id="formula_13">L o = 1 2 (L t2v + L v2t )<label>(9)</label></formula><p>where &lt; f, f &gt; indicates cosine similarity, and L o is symmetric loss. And, we adopt f g and f a to calculate L o respectively. So, the overall loss function can be seen as</p><formula xml:id="formula_14">L o = 1 2 (L g o + L a o )</formula><p>. The similarity during inference is formulated as:</p><formula xml:id="formula_15">&lt; f t , f v &gt;= 1 2 (&lt; f g t , f g v &gt; + &lt; f a t , f a v &gt;).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Datasets. We conduct experiment on three benchmarks for video-to-text retrieval and text-to-video retrieval tasks including MSR-VTT <ref type="bibr" target="#b38">[39]</ref>, MSVD <ref type="bibr" target="#b6">[7]</ref> and VATEX <ref type="bibr" target="#b37">[38]</ref>.</p><p>? MSR-VTT [39] contains 10,000 videos, where each video contains 20 captions. We report result on 1k-A <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21]</ref> and full protocol <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> in our paper. Full protocol <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> is the standard split which includes 6513 videos for train, 497 videos for val and 2990 videos for test. In this protocol, each video contains multiple independent captions, which are all used in text-video retrieval. Besides, 1k-A protocol adopts 9,000 videos with all corresponding captions for training and utilizes 1,000 video-text pairs as test. When reporting the results of video-to-text retrieval, we adopt the maximum similarity among all corresponding captions for a given video query.</p><p>? MSVD <ref type="bibr" target="#b6">[7]</ref> dataset includes 1,970 videos with approximately 80,000 captions, where train, validation and test are splited into 1200, 100 and 670 videos. In this paper, we report the results of test split with multiple captions per video.</p><p>? Besides, VATEX <ref type="bibr" target="#b37">[38]</ref> includes 34,991 videos with multilingual annotations. The training split contains 25,991 videos. Since it is inaccessible to obtain test annotation, we report the results following HGR's <ref type="bibr" target="#b7">[8]</ref> validation protocol, which includes 1500 videos for validation and 1500 videos for test. For fair comparison, we adopt the English annotations.</p><p>Evaluation Metric. We follow the standard retrieval task <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b40">41]</ref> and report Recall at rank K (R@K), median rank (MdR) and mean rank (MnR) as metric, where the higher R@K, and lower median rank and mean rank indicates better performance.</p><p>Implementation Details. We initialize the basic text transformer and spatial transformer (ViT) with CLIP (ViT-B/32). To initialize the other proposed transformers such as temporal transformer, we reuse parameters of similar dimensions in CLIP. The dimension of all representations f g and f a in video and text is 512. The model is finetuned with Adam optimizer. The caption token length is 32 and frame length is 12 in our settings. Meanwhile the length of layer in video transformer are 12 and 4 for L s and L t . Besides, the shared centers c andc are the trainable weight of K ? 512 dimension. And, we adopt K=5 for training in MSVD and MSR-VTT and set K=7 in VATEX for temporal alignment block. More thorough implementation details can be found in supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Effects of Temporal Difference Block.</head><p>Compared with mean pooling, the usage of temporal transformer achieves better performance by interacting and aggregating frames. To further enhance the temporal correlation, we adopt the difference of adjacent frame embedding as description of action to insert into the frame tokens. Specifically, difference-level attention (1-layer transformer) is adopted to encode the subtraction as difference. For fair comparison, we report the results of different settings in Tab.1, exploiting the best type of video representation. It can be seen that inserting subtraction directly (TDB-Sub) or just employing MLP (TDB-MLP) to encode correlations, the whole frame representation with positional indices will be damaged. The implicit difference with weak transformation increase the difficulty of temporal transformer to capture the motion-related information. Instead, the usage of difference-level attention (TDB) provides the explicit interaction before inserting, which significantly improves the performance. Besides, we also exploit to whether adopt global average pooling on all the output of tokens. Since the lack of complete spatial information, adopting all the output of tokens (TDB-All) including difference-enhanced tokens, inevitably downgrades the retrieval performance evidently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effects of Temporal Alignment Block.</head><p>We adopt frame embedding concatenated with differenceenhanced embedding in double frame rate to aggregate text embedding by aligning the shared centers. In this section, we compare different types of difference-enhanced embedding for alignment and also report the results in Tab.1. By introducing basic alignment (TDB+TAB-base), the performance of video-to-text retrieval has achieved the evident improvement. However, since each token embedding of text contain abundant contextual information, it is hard to aggregate independently modeled frame representation, due to the semantic gap. One way to solve that is to utilize the shared temporal output (TDB+TAB-Temporal) as difference-enhanced embedding, but the weight of motionrelated frames can not be strengthened, since every token contains the whole interaction. Instead, we adopt frame embedding for basic alignment, and add extra frame embedding with 1-layer transformer (TDB+TAB-Transformer). The representations with large frame rate encode the apparent motion change in less frames, re-distributing the weight of motion-related center to align with the context of text. Meanwhile, adopting TDB (TDB+TAB-TDB) to further encode the temporal relationship in large frame rate, we have achieve the best performance.</p><p>We also give more ablation studies to exploit the settings of hyper-parameters. In Tab.2, we compare the results of different center numbers K on MSR-VTT for alignment, based on the results of TDB+TAB-base. The results in Tab.2 shows that performance degrades with the evidently increase of K. Since the limited number of videos in MSR-VTT, it is hard for convergence when aligning with large number of centers K. So, we choose K = 5 as the best settings in MSVD and MSR-VTT. Besides, due to the more number of videos in VATEX, we adopt K = 7 to provide more centers to finely discriminate the key frames. When calculating the loss during training, we adopt</p><formula xml:id="formula_16">L o = w(L g o ) + (1 ? w)L a o ,</formula><p>where w is 0.5 in our paper. During inference, the similarity between video and text is formulated as:</p><formula xml:id="formula_17">&lt; f t , f v &gt;= w(&lt; f g t , f g v &gt;) + (1 ? w)(&lt; f a t , f a v &gt;), where w is 1 2 .</formula><p>To exploit the weight of two similarities, we give the results of different w and report in Tab.2. With the increase of weight, the confidence of aligned similarity has been weakened, which damages the whole representation performance. Since the alignment is not sufficiently trained with low weight w, the retrieval performance of only adopting TDB is better than adopting TDB and TAB simultaneously. However, with the large weight of alignment, loss also can not be converged well, due to the limited initializa-tion. Since both f g and f l are equally important, we adopt w=0.5 in our paper to achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Other Methods</head><p>We compare our proposed method against the state-ofthe-art. All the results of video-to-text and text-to-video retrieval are reported in Tab.3,4,5. We achieve the SOTA results on all three datasets compared with baselines, where the visible growth of performance can be found by employing CLIP <ref type="bibr" target="#b28">[29]</ref> as pre-trained model. Benefit from the largescale image-text pre-training, zero-shot retrieval of CLIP with mean pooling has surpassed most of the fine-tuning work. Although, the usage of CLIP doesn't fully exploit the temporal relationship of video, the powerful spatial representations in short video alleviate the lack of information. Meanwhile, adopting CLIP for fine-tuning and modeling interaction between frames with temporal transformer, the performance can be further increased, which is shown by CLIP4clip <ref type="bibr" target="#b22">[23]</ref> on Tab.3. However, when give a small dataset such as MSVD, it is hard to learn temporal representation with temporal transformer by only relying positional embedding to indicate temporal indices. Instead, we insert temporal-enhanced tokens between frame tokens to guide temporal transformer to caption dynamic motion patterns. And, the whole contextual words are utilized to align with key frame with abundant semantics by the shared centers. So, the optimization can be supervised to focus on fusing temporal representation, achieving better performance even in small dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>We show two kinds of videos retrieved by our proposed method. As depicted in <ref type="figure">Fig.4</ref>, The left two queries demonstrates easy results with large margin of similarity. Since, the scene of video conveys the evident difference, our CLIP2video can will retrieve them by introducing more temporal information to describe the action precisely. Besides, we also give some hard samples, which are shown in the right of <ref type="figure">Fig.4</ref>, where it is hard to discriminate them due to the similar frame. For example, when give query: "an animated horse is in a barn and the maker asks for comments", the searched videos with high confidence both contain the animated horse in the barn. However, the half of capitation emphasizes that " the maker asks for comments", which is aligned to the specific centers to aggregate target frames. So, the weight of frames including subtitles can be enhanced with temporal alignment block and help to retrieve the video that best meets the description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We redefine the video-text retrieval from a macroscopic view of point, dividing it into a image-text multi-modal learning and temporal relationships between video frames and video-text. Aiming to consider both sides, we propose CLIP2Video network to transfer the image-language pre-training model to video-text retrieval, which based on a image-language pretraining model and two Temporal Blocks to capture motions at fine temporal frames and re-align the tokens between video and languages respectively. Our experimental results show that the proposed approach can significantly improve the performance on several text-video retrieval benchmarks, including new records on MSR-VTT, MSVD, VATEX.    <ref type="figure">Figure 4</ref>. The text-to-video retrieval results on MSR-VTT, MSVD and VATEX. The upper-left is the query caption for each group. And each two frames are sampled from the target video. Besides, the correct caption of videos in rank 2 is also given in the bottom.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Temporal Difference Block. By inserting TDB in adjacent frames, the motion can be explicitly provided for temporal transformer to capture temporal representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>two men walking and talking about the road one of two guys walking on a carton of eggs with bare?</head><label></label><figDesc>details on what comes with it by carfaxsomeone is talking about a car multi colored horses in a barn and outside in the snow an animated horse is in a barn and the maker asks for comments girl talks to a man with a mustache a man dressed as a woman in a spanish language tv program a man catches the ball in a game of cricket eating a small wedge of watermelon a watermelon is being sliced with a cleaver someone is slicing two uncooked racks of ribs apart in an office and juggling a small ball off his feet and head a man jumping rope A small pile of vegetation is actively burning in a paved area of a field.A man spraying water into a flaming wheelbarrow, effectively distinguishing the fire. young men are gathered together to watch the guy perform a breakdanceA crowd of people mosh together in a circle at a loud concert.A woman performs a backwards bending exercise on a black exercise ball.A young girl is doing push ups using a soft semi-round piece of exerciseThree people stand on a capsized rubber dinghy, lean back while pulling on a rope and right the dinghy, as they fll into the water People are hanging from a boat and then falling off with the sail</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Jun 2021 Spatial Transformer(ViT) Temporal Transformer Text Transformer TDB TDB Temporal Transformer TAB ? ? ? Matching A woman is describing some fears you may have while rock climbing. Token Token Token Token Token Token Token Token Token Sharing Weight</head><label></label><figDesc></figDesc><table /><note>arXiv:2106.11097v1 [cs.CV] 21</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Retrieval result on MSR-VTT. 1k-A indicates test set of 1000 pairs used by<ref type="bibr" target="#b39">[40]</ref>, while full represents the standard test set. CLIP4Clip-meanP and CLIP4Clip-seqTransf indicate the version with mean pooling and temporal transformer for frame aggregation. Retrieval results on MSVD.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Text =? Video</cell><cell></cell><cell></cell><cell cols="3">Video =? Text</cell><cell></cell></row><row><cell>Method</cell><cell cols="11">Test-set R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR</cell></row><row><cell>JSFusion [40]</cell><cell>1k-A</cell><cell>10.2</cell><cell>31.2</cell><cell>43.2</cell><cell>13.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HT-pretrained [26]</cell><cell>1k-A</cell><cell>14.9</cell><cell>40.2</cell><cell>52.8</cell><cell>9.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE [21]</cell><cell>1k-A</cell><cell>20.9</cell><cell>48.8</cell><cell>62.4</cell><cell>6.0</cell><cell>28.2</cell><cell>20.6</cell><cell>50.3</cell><cell>64.0</cell><cell>5.3</cell><cell>25.1</cell></row><row><cell>MMT-Pretrained [13]</cell><cell>1k-A</cell><cell>26.6</cell><cell>57.1</cell><cell>69.6</cell><cell>4.0</cell><cell>24.0</cell><cell>27.0</cell><cell>57.5</cell><cell>69.7</cell><cell>3.7</cell><cell>21.3</cell></row><row><cell>SUPPORT-SET [28]</cell><cell>1k-A</cell><cell>27.4</cell><cell>56.3</cell><cell>67.7</cell><cell>3.0</cell><cell>-</cell><cell>26.6</cell><cell>55.1</cell><cell>67.5</cell><cell>3.0</cell><cell>-</cell></row><row><cell>FROZEN [5]</cell><cell>1k-A</cell><cell>31.0</cell><cell>59.5</cell><cell>70.5</cell><cell>3.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLIP [29]</cell><cell>1k-A</cell><cell>31.2</cell><cell>53.7</cell><cell>64.2</cell><cell>4.0</cell><cell>-</cell><cell>27.2</cell><cell>51.7</cell><cell>62.6</cell><cell>5.0</cell><cell>-</cell></row><row><cell>HIT-pretrained [20]</cell><cell>1k-A</cell><cell>30.7</cell><cell>60.9</cell><cell>73.2</cell><cell>2.6</cell><cell>-</cell><cell>32.1</cell><cell>62.7</cell><cell>74.1</cell><cell>3.0</cell><cell>-</cell></row><row><cell>MDMMT [11]</cell><cell>1k-A</cell><cell>38.9</cell><cell>69.0</cell><cell>79.7</cell><cell>2.0</cell><cell>16.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLIP4Clip-meanP [23]</cell><cell>1k-A</cell><cell>43.1</cell><cell>70.4</cell><cell>80.8</cell><cell>2.0</cell><cell>16.2</cell><cell>43.1</cell><cell>70.5</cell><cell>81.2</cell><cell>2.0</cell><cell>12.4</cell></row><row><cell>CLIP4Clip-seqTransf [23]</cell><cell>1k-A</cell><cell>44.5</cell><cell>71.4</cell><cell>81.6</cell><cell>2.0</cell><cell>15.3</cell><cell>42.7</cell><cell>70.9</cell><cell>80.6</cell><cell>2.0</cell><cell>11.6</cell></row><row><cell>ours</cell><cell>1k-A</cell><cell>45.6</cell><cell>72.6</cell><cell>81.7</cell><cell>2.0</cell><cell>14.6</cell><cell>43.5</cell><cell>72.3</cell><cell>82.1</cell><cell>2.0</cell><cell>10.2</cell></row><row><cell>Dual Enc. [9]</cell><cell>Full</cell><cell>7.7</cell><cell>22.0</cell><cell>31.8</cell><cell>32.0</cell><cell>-</cell><cell>13.0</cell><cell>30.8</cell><cell>43.3</cell><cell>15.0</cell><cell>-</cell></row><row><cell>E2E [24]</cell><cell>Full</cell><cell>9.9</cell><cell>24.0</cell><cell>32.4</cell><cell>29.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE [21]</cell><cell>Full</cell><cell>10.0</cell><cell>29.0</cell><cell>41.2</cell><cell>16.0</cell><cell>86.8</cell><cell>15.6</cell><cell>40.9</cell><cell>55.2</cell><cell>8.3</cell><cell>38.1</cell></row><row><cell>HT-pretrained [26]</cell><cell>Full</cell><cell>14.9</cell><cell>40.2</cell><cell>52.8</cell><cell>9.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLIP [29]</cell><cell>Full</cell><cell>21.4</cell><cell>41.1</cell><cell>50.4</cell><cell>10.0</cell><cell>-</cell><cell>40.3</cell><cell>69.7</cell><cell>79.2</cell><cell>2.0</cell><cell>-</cell></row><row><cell>UNiVL [22]</cell><cell>Full</cell><cell>21.2</cell><cell>49.6</cell><cell>63.1</cell><cell>6.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MDMMT [11]</cell><cell>Full</cell><cell>23.1</cell><cell>49.8</cell><cell>61.8</cell><cell>6.0</cell><cell>52.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ours</cell><cell>1k-A</cell><cell>29.8</cell><cell>55.5</cell><cell>66.2</cell><cell>4.0</cell><cell>45.5</cell><cell>54.6</cell><cell>82.1</cell><cell>90.8</cell><cell>1.0</cell><cell>5.3</cell></row><row><cell></cell><cell></cell><cell cols="3">Text =? Video</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Video =? Text</cell><cell></cell></row><row><cell>Method</cell><cell cols="11">R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR</cell></row><row><cell>VSE [17]</cell><cell>12.3</cell><cell>30.1</cell><cell>42.3</cell><cell>14.0</cell><cell>-</cell><cell></cell><cell>34.7</cell><cell>59.9</cell><cell>70.0</cell><cell>3.0</cell><cell>-</cell></row><row><cell>CE [21]</cell><cell>19.8</cell><cell>49.0</cell><cell>63.8</cell><cell>6.0</cell><cell cols="2">23.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSML [1]</cell><cell>20.3</cell><cell>49.0</cell><cell>63.3</cell><cell>6.0</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SUPPORT-SET [28]</cell><cell>28.4</cell><cell>60.0</cell><cell>72.9</cell><cell>4.0</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FROZEN [5]</cell><cell>33.7</cell><cell>64.7</cell><cell>76.3</cell><cell>3.0</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLIP [29]</cell><cell>37.0</cell><cell>64.1</cell><cell>73.8</cell><cell>3.0</cell><cell>-</cell><cell></cell><cell>59.9</cell><cell>85.2</cell><cell>90.7</cell><cell>1.0</cell><cell>-</cell></row><row><cell cols="2">CLIP4Clip-seqTransf [23] 45.2</cell><cell>75.5</cell><cell>84.3</cell><cell>2.0</cell><cell cols="2">10.0</cell><cell>62.0</cell><cell>87.3</cell><cell>92.6</cell><cell>1.0</cell><cell>4.3</cell></row><row><cell>CLIP4Clip-meanP [23]</cell><cell cols="2">46.t2 76.1</cell><cell>84.6</cell><cell>2.0</cell><cell cols="2">10.0</cell><cell>56.6</cell><cell>79.7</cell><cell>84.3</cell><cell>1.0</cell><cell>7.6</cell></row><row><cell>ours</cell><cell>47.0</cell><cell>76.8</cell><cell>85.9</cell><cell>2.0</cell><cell>9.6</cell><cell></cell><cell>58.7</cell><cell>85.6</cell><cell>91.6</cell><cell>1.0</cell><cell>4.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Retrieval results on VATEX.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Noise estimation using density estimation for self-supervised multimodal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rotman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03186</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Hinton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">G?l Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00650</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10638" to="10647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9346" to="9355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mdmmt: Multidomain multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepan</forename><surname>Komkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Petiushko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10699</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale weaklysupervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Revor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06183</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features via video and text pair discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05691</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hit: Hierarchical transformer with momentum contrast for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengsheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15049</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Univilm: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for cross-modal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><forename type="middle">K</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A straightforward framework for video retrieval using clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s Andr?s</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Carlos</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12443</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The long-short story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="209" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13915</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">C</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14937</idno>
		<title level="m">Learning video representations from textual web supervision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hallucinating idt descriptors and i3d optical flow features for action recognition with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8698" to="8708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, highquality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4581" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08069</idno>
		<title level="m">S3d: single shot multi-span detector via fully 3d convolutional networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
