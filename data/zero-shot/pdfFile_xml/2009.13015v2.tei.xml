<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cloud Removal for Remote Sensing Imagery vai Spatial Attention Generative Adversarial Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Pan</surname></persName>
							<email>hengpan@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="laboratory">State Key Lab of CAD&amp;CG</orgName>
								<orgName type="institution">Zhejiang University Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cloud Removal for Remote Sensing Imagery vai Spatial Attention Generative Adversarial Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>High-resolution Remote Sensing Imagery</term>
					<term>cloud removal</term>
					<term>generative adversarial networks</term>
					<term>spatial attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optical remote sensing imagery has been widely used in many fields due to its high resolution and stable geometric properties. However, remote sensing imagery is inevitably affected by climate, especially clouds. Removing the cloud in the highresolution remote sensing satellite image is an indispensable preprocessing step before analyzing it. For the sake of large-scale training data? neural networks have been successful in many image processing tasks, but the use of neural networks to remove cloud in remote sensing imagery is still relatively small. We adopt generative adversarial network to solve this task and introduce the spatial attention mechanism into the remote sensing imagery cloud removal task, proposes a model named spatial attention generative adversarial network (SpA GAN), which imitates the human visual mechanism, and recognizes and focuses the cloud area with localto-global spatial attention, thereby enhancing the information recovery of these areas and generating cloudless images with better quality. In the comparison experiment with the existing cloud removal models <ref type="figure">(conditional GAN, cycle GAN)</ref> on the open source RICE dataset, SpA GAN achieved the best performance on both peak signal to noise ratio (PSNR) and structural similarity index (SSIM). It proved that the spatial attention mechanism is effective for improving the quality of the cloud removal image and the superior performance of the model on the cloud removal task. The code of SpA GAN is https://github.com/Penn000/SpA-GAN_for_cloud_removal.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Optical remote sensing imagery has been widely used in many fields such as national defense security, environmental science, and weather monitoring due to its high resolution and stable geometric properties. However, when the remote sensing sensor carried by artificial satellite captures land information, it will inevitably be affected by the climate, especially clouds. Thus, removing the cloud in the high-resolution remote sensing imagery is an indispensable pre-processing step before analyzing it. The clouds to be removed from remote sensing imagery can be specifically classified into three categories, namely thin clouds, thick clouds, and cloud shadows, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The area covered by the thin clouds still keeps part of the ground feature captured by the remote sensing sensor, and the original information can be recovered from a single image. The information covered by the thick clouds is completely lost, which makes removing thick clouds from a single image become a condition-limited problem. Therefore, solving this problem often requires multitemporal data. Cloud shadows are usually caused by thick clouds blocking sunlight, they often appear with thick clouds at the same time. In this paper, we focus on removing thin clouds from a single optical remote sensing imagery as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.  The goal of cloud removal work is to recover the cloudless feature information from satellite images contaminated by clouds. Looking at this problem from a generalized perspective, it can also be understood as a kind of image denoising that clouds are the noise with regard to the surface objects. Recent years, relevant scholars have proposed their own solutions for this problem, including traditional image processing methods and deep learning methods. And in the field of computer vision, researches of removing fog, rain drop <ref type="bibr" target="#b17">[17]</ref>, watermark <ref type="bibr" target="#b18">[18]</ref> and shade <ref type="bibr" target="#b19">[19]</ref> have achieved a series of impressive results. These works have a use for reference to design the model that can remove clouds better.</p><p>Based on extensive investigation and previous work, we adopt generative adversarial network <ref type="bibr" target="#b0">[1]</ref> to solve this task and introduce the spatial attention mechanism <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> into the remote sensing imagery cloud removal field, and proposes a model named spatial attention generative adversarial networks or SpA GAN, which imitates the human visual mechanism, and recognizes and focuses the cloud area with local-to-global spatial attention, thereby enhancing the information recovery of these areas and generating cloudless images with better quality. In the comparison experiment with the existing cloud removal models on the open source RICE dataset <ref type="bibr" target="#b3">[4]</ref>, SpA GAN achieved the best performance on both peak signal to noise ratio (PSNR) <ref type="bibr" target="#b4">[5]</ref> and structural similarity index (SSIM) <ref type="bibr" target="#b5">[6]</ref>. It proved that the spatial attention mechanism is effective for improving the quality of the cloud removal image and the superior performance of the model on the cloud removal task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cloud removal</head><p>The existing cloud removal methods can be divided into two categories: traditional image processing methods and deep learning methods. Traditional image processing methods hope to remove the cloud through pixel correction. Haze Optimized Transformation or HOT <ref type="bibr" target="#b6">[7]</ref> is a classic thin cloud removal method which utilizes the high correlation between the blue and red bands in multispectral remote sensing imagery. However, HOT is sensitive to ground objects, easy to overcorrect and color distortion of RGB composite image. Improvement methods of HOT are then proposed <ref type="bibr" target="#b7">[8]</ref>. Dark Channel Prior or DCP is also a classic thin cloud removal method which was first applied for image dehazing by <ref type="bibr" target="#b9">[9]</ref> and achieved great success. DCP finds the prior knowledge of dark channel distribution through statistical analysis of the clear image library, and then uses it to infer the cloud model <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>. Homomorphic filtering <ref type="bibr" target="#b12">[12]</ref> is an image processing method that combines frequency filtering and grayscale transformation. It transforms the image into frequency domain via Fourier transform, and then uses a high-pass filter to filter the image and remove the thin clouds.</p><p>Traditional image processing methods mainly use the lowlevel features of the image, and the designed models often have limited performance. With the improvement of computing power, deep neural networks have made great progress in computer vision tasks, such as image restoration, image denoising and image super-resolution reconstruction. Some scholars try to use deep learning methods to solve the problem of high-resolution remote sensing imagery cloud removal. <ref type="bibr" target="#b13">[13]</ref> applied the conditional generative adversarial networks <ref type="bibr" target="#b20">[20]</ref> to the cloud removal field and proposed a model named multispectral conditional generative adversarial nets or McGANs. McGANs uses synthetic cloud satellite images and near-infrared band images to train the network so that it can automatically generate cloudless images from cloud images. In order to solve the trouble of the shortage of paired cloudless/cloudy data sets, <ref type="bibr" target="#b14">[14]</ref> introduced the cycle generative adversarial network <ref type="bibr" target="#b15">[15]</ref> and proposed a cloud removal model named cloud GAN. The core idea of cloud GAN is that model maps the cloudy image to the cloudless image through the generative network, and then maps it back to the cloudy image, the output should be similar to the original input. In addition, <ref type="bibr" target="#b16">[16]</ref> proposes a cloud removal method by fusing synthetic aperture radar or SAR image data, which is a high-resolution imaging radar that can obtain high-resolution radar images similar to optical photography without being affected by climatic conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generative adversarial networks</head><p>Generative Adversarial Networks <ref type="bibr" target="#b0">[1]</ref> (GAN) is a deep learning model proposed by Goodfellow Ian at the 2014 Conference and Workshop on Neural Information Processing Systems (NeurIPS). GAN consists of two parts, the generator and the discriminator. The generator learns to obtain the data distribution of the target sample space, and the discriminator is used to evaluate the probability that a sample comes from the real data space instead of the generator generating data. Once proposed, GAN has attracted much attention. It is considered by the industry to be one of the most promising methods for learning complex data distribution. It currently plays an important role in many visual tasks like image generation, superresolution reconstruction, data augmentation, and semantic segmentation. We believe the strong image generation power of GAN can transfer in cloud removal field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>GAN trains the generator and the discriminator in a game with each other. The generator is dedicated to generating data that makes the discriminator unable to distinguish whether the data comes from the training sample or the generator; while the discriminator is committed to learning to distinguish between true and false data. For generator G and discriminator D, GAN can be defined as a minimax problem as:</p><formula xml:id="formula_0">~( ) ( ) min max ( , ) [log( ( ))] [log(1 ( ( )))] data z x p x G D z p z V D G E D x E D G z = + ? (1), Here x is real samples, data p is the distribution of x, z is random noise, z p is the data distribution of z, D(x)</formula><p>is the output of discriminator that represents the probability of x is a real sample, G(z) is the generated output of generator. Regard to cloud removal problem, we modify the primal GAN and propose the spatial attention GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generator</head><p>The generative network of SpA GAN is a convolutional neural network called spatial attentive network (SPANet). Its overall structure is shown in <ref type="figure">Fig. 3</ref>. The input image first passes through a convolutional layer to extract the feature map, then passes through three standard residual blocks and four spatial attentive blocks (SAB), and then passes through two standard residual blocks and one convolutional layer, finally output the generated result which is the image with cloud removed. The SAB is shown in <ref type="figure">Fig. 3b</ref>, which includes three spatial attentive residual blocks (SARB) and one spatial attentive module (SAM) connected in parallel. The SAB module is used to discover and generate attention maps from the input feature maps. The attention map is a two-dimensional matrix, where the value of each element is a continuous value that indicates how much attention should be allocated to the pixel. The larger the value, the more attention should be given. It indicates the spatial distribution of the cloud which can guide the subsequent steps for cloud removal . The visualization of the attention map can be seen in <ref type="figure" target="#fig_8">Fig. 7</ref>. The SAM module is shown in <ref type="figure">Fig. 3d</ref>. It is a tworound, four-direction (up, down, left, right) recurrent neural networks with ReLU and identity matrix initialization (IRNN). The first round of IRNN is dedicated to generating a feature map that summarizes the contextual information of the location points from the input image; the second round of IRNN further collects non-local context information to generate a global perceptual feature map. The SARB module is shown in <ref type="figure">Fig. 3c</ref>. It removes clouds through negative residuals under the guidance of the attention map. <ref type="figure">Fig. 3</ref>. Generator (a) of SpA GAN. It adopts three standard residual blocks to extract features, four spatial attentive blocks (SAB) (b) to identify cloud progressively in four stages, and two residual blocks to reconstruct a clean background. A SAB contains three spatial attentive residual blocks (SARBs) (c) and one spatial attentive module (SAM) (d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discriminator</head><p>The discriminant network of SpA GAN is an ordinary convolutional neural network whose structure is shown in <ref type="figure" target="#fig_2">Fig.  4</ref>. Here C represents the convolutional layer, B represents the batch normalization layer, and R represents the Leaky ReLU layer. The input of the network is a three-channel image, and the output is a flag of true or false, which means whether the input image is a real image or an image generated by the generator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss</head><p>The total loss fo SpA GAN is:</p><formula xml:id="formula_1">1 arg min max ( , ) ( ) SpA cGAN L Att G D L L G D L G L = + +<label>(2)</label></formula><p>The loss function consists of three parts where the first part is the loss function of conditional GAN, as shown in <ref type="formula" target="#formula_2">(3)</ref> </p><p>The second part is a standard L1 loss, which is used to measure the accuracy of each reconstructed pixel, as shown in (4), here in I is the input cloudy image, gt I is the ground truth, </p><p>The third part is attention loss, which is defined as <ref type="bibr" target="#b4">(5)</ref>. The matrix A is the attention map generated by the spatial attentive module, and the matrix M is the binary image of the cloud area, which is calculated by the difference between the cloudy and the cloudless image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discuss</head><p>SpA GAN uses the spatial attention mechanism in the part of generative network. The attention map accumulates the global information of image during the generative process. Each location point will learn the information from its four-direction connected pixels, so do these pixels learn, spread in sequence, and finally learn the global information of the image. The attention map is not only used to guide the spatial attention residual block for cloud removal, its contribution is also reflected in the loss function which guides the training process of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Cloud removal is an indispensable preprocessing step for high-resolution remote sensing imagery analysis, but deep learning methods are rarely used in the field of cloud removal. An important reason is the lack of data sets for training. Therefore, <ref type="bibr" target="#b3">[4]</ref> provides a open source dataset named Remote sensing Image Cloud rEmoving (RICE) for cloud removal researching. The RICE dataset consists of two sub sets called RICE1 and RICE2, which is available on https://github.com/BUPTLdy/RICE_DATASET. The RICE1 dataset contains 500 data samples with each sample having a cloudy image and a cloudless image under 512?512 resolution. The dataset is collected by Google Earth, and the cloudy/cloudless images are obtained by setting the cloud layer whether to display.</p><p>The RICE2 is constructed from Landsat 8 OLI/TIRS data by using LandsatLook images with georeferenced in Earth Explorer. LandsatLook images are full-resolution files derived from Landsat Level-1 data products. LandsatLook images include Natural Color Image, Thermal Image and Quality Image, here Natural Color Image and Quality Image are used in RICE2. <ref type="bibr" target="#b3">[4]</ref> manually selected a cloudless image at the same location with a cloud image time less than 15 days apart to get the cloudless reference image. Finally, there are 736 groups of 512 ? 512 images in the RICE2, and each group contains 1 cloudy, 1 cloudless, and 1 cloud mask image. The data samples of RICE are shown in <ref type="figure">Fig. 5</ref>. <ref type="figure">Fig. 5</ref>. The data samples of RICE, the first row is cloudy image, the second row is cloudless image. The first two columns of images belong to RICE1, the last two columns of images belong to RICE2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metric</head><p>The input of model is a remote sensing image contaminate by cloud, and the output is a cloudless image after cloud removal. In order to measure the quality of the generated cloudless image and the cloud removal ability of the neural network model, peak signal to noise ratio (PSNR) <ref type="bibr" target="#b4">[5]</ref> and structural similarity index (SSIM) <ref type="bibr" target="#b5">[6]</ref> are widely used as image quality evaluation metrics. a) PSNR: PSNR is the most widely and most common used objective measurement for evaluating image quality, whose calculation formula is: </p><formula xml:id="formula_5">? ? ? = ? ? ? ?<label>(6)</label></formula><p>where n is the bits of pixel value that n is 8 for grayscale images. MSE is the mean square error between the image X and Y, calculated as:</p><formula xml:id="formula_6">2 1 1 1 ( ( , ) ( , )) H W i j MSE X i j Y i j H W = = = ? ? ??<label>(7)</label></formula><p>The value of PSNR generally situates in 20 to 40, the larger value represents the closer distance between predict image and ground truth image and the better prediction quality. b) SSIM: SSIM is an evaluation metirc that measures the similarity of two images through three aspects: brightness, contrast, and structure, whose formula are (8), (9), (10) respectively: , , C C C are constants for the sake of avoiding divide zero error. , ? ? are the mean and variance of image, XY ? is the covariance of image X and Y. Thus, the formula of SSIM is:</p><formula xml:id="formula_7">1 2 2 1 2 ( , ) X Y X Y C l X Y C ? ? ? ? + = + + (8) 2 2 2 2 2 ( , ) X Y X Y C c X Y C ? ? ? ? + = + +<label>(9)</label></formula><formula xml:id="formula_8">( , ) ( , ) ( , ) SSIM l X Y c X Y s X Y = ? ? (11)</formula><p>The value range of SSIM is between 0 and 1, larger value means more similar between two images. If the value is 1, the two images are exactly same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Settings</head><p>For RICE dataset, we choose 400 images for training and 100 images for testing in RICE1, choose 588 images for training and 148 images for testing in RICE2, and set learning rate to 0.0004, minibatch to 1, epoch to 200 when training the model. In addition, we compare our SpA GAN with the existing cloud removal models including conditional GAN <ref type="bibr" target="#b20">[20]</ref> and cycle GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results and analysis</head><p>The results of conditional GAN, cycle GAN and SpA GAN on the RICE1 dataset are shown in <ref type="figure">Fig. 6</ref>. From left to right, each column represents cloudy image, conditional GAN generated result, cycle GAN generated result, SpA GAN generated result, and ground truth cloudless image respectively. Since almost all the clouds in RICE1 are thin clouds, the information of ground objects is not completely lost, every generated images after cloud removal retain the correct geometric structure and spatial information of the ground objects from visual perspective. <ref type="figure">Fig. 6</ref>. The generated result on RICE1, from left to right, each column represents cloudy image, conditional GAN generated result, cycle GAN generated result, SpA GAN generated result, and ground truth cloudless image respectively.</p><p>The quantitative analysis results of conditional GAN, cycle GAN and SpA GAN on the test set are shown in <ref type="table">Table 1</ref>. From <ref type="table">Table 1</ref>, we can see that the PSNR and SSIM metrics of SpA GAN are 30.232dB and 0.954, respectively. Both have achieved the best performance and significantly outperform the other two models. It shows that the attention mechanism can observe cloud areas effectively and improve the performance of cloud removal. Cycle GAN has the lowest PSNR and SSIM values that demonstrates the necessity of paired discriminate information for high-resolution remote sensing imagery cloud removal. The results of conditional GAN, cycle GAN and SpA GAN on the RICE2 dataset are shown in <ref type="figure">Fig. 8</ref>. From left to right, each column represents cloudy image, conditional GAN generative result, cycle GAN generative result, SpA GAN generative result, and ground truth cloudless image respectively. The RICE2 dataset contains a large number of images with thick clouds, where the ground objects information in the cloud-covered area is completely lost. The reconstruction of these pixels needs to learn from a large amount of similar data. As see in <ref type="figure">Fig. 8</ref>, cycle GAN removes the white area of thick cloud, but the corresponding ground objects is not well restored and image spatial continuity is disturbed. Although the image generated by conditional GAN ensures the spatial continuity, the predict cloudless image has fuzzy areas, and the details of the ground objects are not well restored. Compared with the aforementioned models, the cloudless image generated by SpA GAN retains more details and more consistency, which is visually closest to the ground truth. In addition, to the third row sample, we can found that the lake's shape and area between cloudy image and ground truth are inconsistent, but all three generative adversarial network models generate the correct lake that are consist with cloudy image. This shows the robustness and anti-noise ability of the generative adversarial network model on the task of cloud removal. On the other hand, it also shows the difficulty of obtaining cloudy/cloudless paired data at the same time and same place. The attention map generated by the SpA GAN during the cloud removal process is shown in <ref type="figure" target="#fig_8">Fig. 7</ref>. In the heatmap, the redder area means more attention are allocated, on the contrary the bluer area means less attention are allocated.</p><p>The quantitative analysis results of conditional GAN, cycle GAN and SpA GAN on the test set are shown in <ref type="table">Table 2</ref>. The PSNR and SSIM of SpA GAN are 28.368dB and 0.906, respectively, both achieved the best performance during three models. PSNR increases 2.982dB compared to conditional GAN and increases 4.458dB compared to cycle GAN. SSIM increases 0.095 compared to conditional GAN and increases 0.113 compared to cycle GAN. The results illustrate the effectiveness of the attention mechanism in observing cloud areas and improving the performance of cloud removal once again. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>Remote sensing sensors are susceptible to interfered by climate, especially clouds when capture optical satellite images. It greatly reduces the availability of the satellite image data. Thus, cloud removal is a necessary preprocessing step before image analysis. Thanks to large-scale training data and powerful computing power, neural networks have been successful in many visual tasks, but the use of neural networks for remote sensing satellite cloud removal is still relatively small. This paper is the first that introduces spatial attention mechanism into the remote sensing imagery cloud removal task, and based on generative adversarial network, proposes the Spatial Attention Generative Adversarial Network or SpA GAN. Compare with conditional GAN and cycle GAN on the public RICE dataset, SpA GAN shows the best cloud removal ability that evaluated by PSNR and SSIM, which proves spatial attention mechanism is effective in improving the quality of cloud removal images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Different type of clouds, from left to right is thin cloud, thick cloud, cloud shadow respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Thin cloud removal for high-resolution remote sensing imagery, the cloud removal model takes a cloudy image as input and output a cloudless image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The discriminator of SpA GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>c?</head><label></label><figDesc>is the weight of each channel contributes to the loss, which is set to 1 in our model. ( ) in G I is the predicted result of generator, C, H, W represent the number of channels, height and width of the image respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>The attention heatmap of SpA GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell>.</cell><cell cols="2">QUANTITATIVE ANALYSIS ON RICE1 DATASET</cell></row><row><cell>Model</cell><cell cols="2">Quantitative Metrics PNSR SSIM</cell></row><row><cell>Conditional GAN</cell><cell>26.547</cell><cell>0.903</cell></row><row><cell>Cycle GAN</cell><cell>25.880</cell><cell>0.893</cell></row><row><cell>SpA GAN</cell><cell>30.232</cell><cell>0.954</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II .</head><label>II</label><figDesc>QUANTITATIVE ANALYSIS ON RICE2 DATASET The generated result on RICE2, from left to right, each column represents cloudy image, conditional GAN generated result, cycle GAN generated result, SpA GAN generated result, and ground truth cloudless image respectively.</figDesc><table><row><cell>Model</cell><cell cols="2">Quantitative Metrics PNSR SSIM</cell></row><row><cell>Conditional GAN</cell><cell>25.386</cell><cell>0.811</cell></row><row><cell>Cycle GAN</cell><cell>23.910</cell><cell>0.793</cell></row><row><cell>SpA GAN</cell><cell>28.368</cell><cell>0.906</cell></row><row><cell>Fig. 8.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<title level="m">Generative Adversarial Nets. International Conference on Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoyu</forename><surname>Lin1</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00600v1</idno>
		<title level="m">Yang Wang1, Xian Sun1, Kun Fu1? 2019. A Remote Sensing Image Dataset for Cloud Removal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Guangluan Xu1, Xiaoke Wang1</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scope of validity of PSNR in image/video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huynh-Thu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghanbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics Letters</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="800" to="801" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image Quality Metrics: PSNR vs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Hor?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSIM. 2010 International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic detection and removal of thin haze based on own features of Landsat image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L I</forename><surname>Cun Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Zhejiang University</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="13" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">removal based on advanced hazeoptimized transformation (AHOT) for multispectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="5331" to="5348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single Image Haze Removal Using Dark Channel Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Haze Removal for a Single Remote Sensing Image Based on Deformed Haze Imaging Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1806" to="1810" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Haze and Thin Cloud Removal via Sphere Model Improved Dark Channel Prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingwu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyao</forename><surname>Ai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>IEEE Geoscience and Remote Sensing Letters</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Modified Homomorphism Filtering Algorithm for Cloud Removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X W X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Intelligence &amp; Software Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Filmy Cloud Removal on Satellite Imagery with Multispectral Conditional Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Enomoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cloud-Gan: Cloud Removal for Sentinel-2 Imagery Using a Cyclic Consistent Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cycle GAN: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Synthesis of Multispectral Optical Images From SAR/Optical Multitemporal Data Using Conditional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bermudez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<title level="m">Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset. 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the Effectiveness of Visible Watermarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single-image shadow detection and removal using paired regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2033" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conditional Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
