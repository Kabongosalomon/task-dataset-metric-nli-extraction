<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MPViT : Multi-Path Vision Transformer for Dense Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghee</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electronics and Telecommunications Research Institute (ETRI)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Willette</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">AITRICS</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MPViT : Multi-Path Vision Transformer for Dense Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense computer vision tasks such as object detection and segmentation require effective multi-scale feature representation for detecting or classifying objects or regions with varying sizes. While Convolutional Neural Networks (CNNs) have been the dominant architectures for such tasks, recently introduced Vision Transformers (ViTs) aim to replace them as a backbone. Similar to CNNs, ViTs build a simple multi-stage structure (i.e., fine-to-coarse) for multi-scale representation with single-scale patches. In this work, with a different perspective from existing Transformers, we explore multi-scale patch embedding and multi-path structure, constructing the Multi-Path Vision Transformer (MPViT). MPViT embeds features of the same size (i.e., sequence length) with patches of different scales simultaneously by using overlapping convolutional patch embedding. Tokens of different scales are then independently fed into the Transformer encoders via multiple paths and the resulting features are aggregated, enabling both fine and coarse feature representations at the same feature level. Thanks to the diverse, multi-scale feature representations, our MPViTs scaling from tiny (5M) to base (73M) consistently achieve superior performance over state-of-theart Vision Transformers on ImageNet classification, object detection, instance segmentation, and semantic segmentation. These extensive results demonstrate that MPViT can serve as a versatile backbone network for various vision tasks. Code will be made publicly available at https: //git.io/MPViT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since its introduction, the Transformer <ref type="bibr" target="#b52">[53]</ref> has had a huge impact on natural language processing (NLP) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b42">43]</ref>. Likewise, the advent of Vision Transformer (ViT) <ref type="bibr" target="#b15">[16]</ref> has moved the computer vision community forward. As a result, there has been a recent explosion in Transformerbased vision works, spanning tasks such as static image . . . . . .  classification <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67]</ref>, object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b73">74]</ref>, and semantic segmentation <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b62">63]</ref> to temporal tasks such as video classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b17">18]</ref> and object tracking <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b55">56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Scale</head><p>It is crucial for dense prediction tasks such as object detection and segmentation to represent features at multiple scales for discriminating between objects or regions of varying sizes. Modern CNN backbones which show better performance for dense prediction leverage multiple scales at the convolutional kernel level <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, or feature level <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55]</ref>. Inception Network <ref type="bibr" target="#b45">[46]</ref> or VoVNet <ref type="bibr" target="#b31">[32]</ref> exploits multi-grained convolution kernels at the same feature level, yielding diverse receptive fields and in turn boosting detection performance. HRNet <ref type="bibr" target="#b54">[55]</ref> represents multi-scale features by simultaneously aggregating fine and coarse features throughout the convolutional layers.</p><p>Although CNN models are widely utilized as feature extractors for dense predictions, the current state-of-theart (SOTA) Vision Transformers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71]</ref> have surpassed the performance of CNNs. While the ViTvariants <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71]</ref> focus on how to address the quadratic complexity of self-attention when applied to dense prediction with a high-resolution, they pay less attention to building effective multi-scale representations. For example, following conventional CNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">44]</ref>, recent Vision Transformer backbones <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71]</ref> build a simple multi-stage structure (e.g., fine-to-coarse structure) with single-scale patches (i.e., tokens). CoaT <ref type="bibr" target="#b64">[65]</ref> simultaneously represents fine and coarse features by using a co-scale mechanism allowing cross-layer attention in parallel, boosting detection performance. However, the co-scale mechanism requires heavy computation and memory overhead as it adds extra cross-layer attention to the base models (e.g., CoaT-Lite). Thus, there is still room for improvement in multi-scale feature representation for ViT architectures.</p><p>In this work, we focus on how to effectively represent multi-scale features with Vision Transformers for dense prediction tasks. Inspired by CNN models exploiting the multi-grained convolution kernels for multiple receptive fields <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b45">46]</ref>, we propose a multi-scale patch embedding and multi-path structure scheme for Transformers, called Multi-Path Vision Transformer (MPViT). As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, the multi-scale patch embedding tokenizes the visual patches of different sizes at the same time by overlapping convolution operations, yielding features having the same sequence length (i.e., feature resolution) after properly adjusting the padding/stride of the convolution. Then, tokens from different scales are independently fed into Transformer encoders in parallel. Each Transformer encoder with different-sized patches performs global self-attention. Resulting features are then aggregated, enabling both fine and coarse feature representations at the same feature level. In the feature aggregation step, we introduce a global-to-local feature interaction (GLI) process which concatenates convolutional local features to the transformer's global features, taking advantage of both the local connectivity of convolutions and the global context of the transformer.</p><p>Following the standard training recipe as in DeiT <ref type="bibr" target="#b49">[50]</ref>, we train MPViTs on ImageNet-1K <ref type="bibr" target="#b12">[13]</ref>, which consistently achieve superior performance compared to recent SOTA Vision Transformers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67]</ref>. Furthermore, We validate MPViT as a backbone on object detection and instance segmentation on the COCO dataset and semantic segmentation on the ADE20K dataset, achieving statethe-art performance. In particular, MPViT-Small (22M &amp; 4GFLOPs) surpasses the recent, and much larger, SOTA  <ref type="table">Table 3</ref>.</p><p>Focal-Base <ref type="bibr" target="#b66">[67]</ref> (89M &amp; 16GFLOPs) as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>To summarize, our main contributions are as follows:</p><p>? We propose a multi-scale embedding with a multipath structure for simultaneously representing fine and coarse features for dense prediction tasks.</p><p>? We introduce global-to-local feature interaction (GLI) to take advantage of both the local connectivity of convolutions and the global context of the transformer.</p><p>? We provide ablation studies and qualitative analysis, analyzing the effects of different path dimensions and patch scales, discovering efficient and effective configurations.</p><p>? We verify the effectiveness of MPViT as a backbone of dense prediction tasks, achieving state-of-the-art performance on ImageNet classification, COCO detection and ADE20K segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Vision Transformers for dense predictions. Current SOTA Vision Transformers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71]</ref> have focused on reducing the quadratic complexity of self-attention when applied to dense prediction with a high-resolution. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71]</ref> constrain the attention range with fine-grained patches in local regions and combine this with sliding windows or sparse global attention. <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b59">60]</ref>   MPViT consists of multi-scale patch embedding (MS-PatchEmbed) and multi-path transformer (MP-Transformer) blocks, which output features from each of the four stages for dense prediction tasks. Transformer encoders utilize factorized multi-head self-attention (MHSA) <ref type="bibr" target="#b64">[65]</ref>. We omit the convoultional position encodings for simplicity.</p><p>with spatial reduction (i.e., pooling). <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b64">65]</ref> realizes linear complexity by operating the self-attention across feature channels rather than tokens. While <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b70">71]</ref> has a simple pyramid structure (fine-to-coarse), XCiT <ref type="bibr" target="#b16">[17]</ref> has a single-stage structure as ViT <ref type="bibr" target="#b15">[16]</ref>. When applied to dense prediction tasks, XCiT adds down-/up-sampling layers to extract multi-scale features after pre-training on ImageNet. Xu et al. <ref type="bibr" target="#b64">[65]</ref> introduce both CoaT-Lite with a simple pyramid structure and CoaT with cross-layer attention on top of CoaT-Lite. The cross-layer attention allows CoaT to outperform CoaT-Lite, but requires heavy memory and computation overhead, which limits scaling of the model. Comparison to Concurrent work. CrossViT <ref type="bibr" target="#b5">[6]</ref> also utilizes different patch sizes (e.g., small and large) and dual-paths in a single-stage structure as ViT <ref type="bibr" target="#b15">[16]</ref> and XCiT <ref type="bibr" target="#b16">[17]</ref>. However, CrossViT's interactions between branches only occur through [CLS] tokens, while MPViT allows all patches of different scales to interact. Also, unlike CrossViT (classification only), MPViT explores larger path dimensions (e.g., over two) more generally and adopts multi-stage structure for dense predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-Path Vision Transformer</head><p>3.1. Architecture <ref type="figure" target="#fig_3">Fig. 3</ref> shows the Multi-Path Vision Transformer (MPViT) architecture.</p><p>Since our aim is to explore a powerful backbone network for dense predictions, we construct a multi-stage architecture <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b66">67]</ref> instead of a single-stage (i.e., monolithic) one such as ViT <ref type="bibr" target="#b15">[16]</ref> and XCiT <ref type="bibr" target="#b16">[17]</ref>. Specifically, we build a four-stage feature hierarchy for generating feature maps of different scales. As a multi-stage architecture has features with higher resolutions, it requires inherently more computation. Thus, we use Transformer encoders including factorized self-attention as done in CoaT <ref type="bibr" target="#b64">[65]</ref> for the entire model due to its linear complexity. In LeViT <ref type="bibr" target="#b19">[20]</ref>, a convolutional stem block shows better low-level representation (i.e., without losing salient information) than non-overlapping patch embedding. Inspired by LeViT, given an input image with the size of H ? W ? 3, we also adopt a stem block which consists of two 3 ? 3 convolutional layers with channels of C 2 /2, C 2 and stride of 2 which generates a feature with the size of H/4 ? W/4 ? C 2 where C 2 is the channel size at stage 2. Each convolution is followed by Batch Normalization <ref type="bibr" target="#b28">[29]</ref> and a Hardswish <ref type="bibr" target="#b24">[25]</ref> activation function. From stage 2 to stage 5, we stack the proposed multi-scale patch embedding (MS-PatchEmbed) and multipath Transformer (MP-Transformer) blocks in each stage. Many works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b57">58]</ref> have proved that replacing the [CLS] token with a global average pooling (GAP) of the final feature map does not affect performance, so we also remove the [CLS] token and use GAP for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Scale Patch Embedding</head><p>We devise a multi-scale patch embedding (MS-PatchEmbed) layer that exploits both fine-and coarsegrained visual tokens at the same feature level. To this end, we use convolution operations with overlapping patches, similar to CNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">44]</ref> and CvT <ref type="bibr" target="#b59">[60]</ref>. Specifically, given a 2D-reshaped output feature map (i.e., token map) from a previous stage X i ? R Hi?1?Wi?1?Ci?1 as the input to stage i, we learn a function F k?k (?) that maps X i into new tokens F k?k (X i ) with a channel size C i , where F (?) is 2D convolution operation of kernel size (i.e., patch size) k ? k, stride s and padding p. The output 2D token map F k?k (X i ) ? R Hi?Wi?Ci has height and width as below:</p><formula xml:id="formula_0">H i = H i?1 ? k + 2p s + 1 , W i = W i?1 ? k + 2p s + 1 .</formula><p>(1) The convolutional patch embedding layer enables us to adjust the sequence length of tokens by changing stride and padding. i.e., it is possible to output the features of the same size (i.e., resolution) with different patch sizes. Thus, we form several convolutional patch embedding layers with different kernel sizes in parallel. For example, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, we can generate various-sized visual tokens of the same sequence length with 3 ? 3, 5 ? 5, 7 ? 7 patch sizes.</p><p>Since stacking consecutive convolution operations with the same channel and filter sizes enlarges receptive field (e.g., two 3 ? 3 are equivalent to 5 ? 5) and requires fewer parameters (e.g., 2 ? 3 2 &lt; 5 2 ), we choose consecutive 3 ? 3 convolution layers in practice. For the triple-path structure, we use three consecutive 3 ? 3 convolutions with the same channel size C , padding of 1 and stride of s where s is 2 when reducing spatial resolution otherwise 1. Thus, given a feature X i ? R Hi?Wi?Ci at stage i, we can get</p><formula xml:id="formula_1">F 3?3 (X i ), F 5?5 (X i ), F 7?7 (X i ) features with the same size of Hi s ? Ci s ? C .</formula><p>Since MPViT has more embedding layers due to the multi-path structure, we reduce model parameters and computational overhead by adopting 3 ? 3 depthwise separable convolutions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref> which consist of 3 ? 3 depthwise convolution followed by 1 ? 1 pointwise convolution in embedding layers. All convolution layers are followed by Batch Normalization <ref type="bibr" target="#b28">[29]</ref> and Hardswish <ref type="bibr" target="#b24">[25]</ref> activation functions. Finally, the different sized token embedding features are separately fed into each transformer encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Global-to-Local Feature Interaction</head><p>Although self-attention in Transformers can capture long-range dependencies (i.e., global context), it is likely to ignore structural information <ref type="bibr" target="#b29">[30]</ref> and local relationships <ref type="bibr" target="#b38">[39]</ref> within each patch. Additionally, Transformers benefit from a shape bias <ref type="bibr" target="#b51">[52]</ref>, allowing them to focus on important parts of the image. On the contrary, CNNs can exploit local connectivity from translation invariance <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b51">52]</ref> -each patch in an image is processed by the same weights. This inductive bias encourages CNN to have a stronger dependency on texture rather than shape when categorizing visual objects <ref type="bibr" target="#b1">[2]</ref>. Thus, MPViT combines the local connectivity of CNNs with the global context transformers in a complementary manner. To this end, we introduce a globalto-local feature interaction module that learns to interact between local and global features for enriched representations. Specifically, to represent local feature L i ? R Hi?Wi?Ci at stage i, we adopt a depthwise residual bottleneck block which consists of 1 ? 1 convolution, 3 ? 3 depthwise convolution, and 1 ? 1 convolution with the same channel size of C i and residual connection <ref type="bibr" target="#b22">[23]</ref>. With the 2D-reshaped global features from each transformer G i,j ? R Hi?Wi?Ci . Aggregation of the local and global features is performed by concatenation,</p><formula xml:id="formula_2">A i = Concat([L i , G i,0 , G i,1 , ..., G i,j ]) (2) X i+1 = H(A i ),<label>(3)</label></formula><p>where j is the index of the path, A i ? R Hi?Wi?(j+1)Ci is the aggregated feature and H(?) is a function which learns to interact with features, yielding the final feature X i+1 ? R Hi?Wi?Ci+1 with the size of next stage channel dimension C i+1 . We use 1 ? 1 convolution with channel of C i+1 for H(?). The final feature X i+1 is used as input for the next stage's the multi-scale patch embedding layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Configuration</head><p>To alleviate the computational burden of multi path structure, we use the efficient factorized self-attention proposed in CoaT <ref type="bibr" target="#b64">[65]</ref>:</p><formula xml:id="formula_3">FactorAtt(Q, K, V ) = Q ? C (softmax(K) V ),<label>(4)</label></formula><p>where Q, K, V ? R N ?C are linearly projected queries, keys, values and N, C denote the number of tokens and the embedding dimension respectively. To maintain comparable parameters and FLOPs, increasing the number of paths requires a reduction of the channel C or the number of layers L (i.e., the number of transformer encoders). L factorized self-attention layers <ref type="bibr" target="#b64">[65]</ref> with N tokens and h transformer encoder heads have a total time complexity of O(LhN C 2 ) and memory complexity of O(LhC 2 + LhN C). The complexities are quadratic w.r.t. to the channel C while linear w.r.t. the number of layers L. Accordingly, we expand the number of paths from single-path (i.e., CoaT-Lite <ref type="bibr" target="#b64">[65]</ref> baseline) to triple-path by a reduction in C rather than L. We verify that reducing C achieves better performance than reducing L in the ablation study (see <ref type="table">Table 5</ref>). As the computation cost of stage 2 is relatively high due to a higher feature resolution, we also set the number of paths to 2 at stage 2 for triple-path models. Thus, from stage 3, triple-path models have 3 paths. Interestingly, we also found that while triple-path and dual-path yield similar accuracy on ImageNet classification, the triple-path model shows better performance in dense prediction tasks. This indicates that the diverse features from expanding the path dimension are useful for dense prediction tasks. Therefore, we build MPViT models based on the triple-path structure. We scale-up the MPViT models from the small-scale MPViT-Tiny (5M) corresponding to CoaT-Lite Tiny (5M) <ref type="bibr" target="#b64">[65]</ref> or DeiT-Tiny(5.7M) <ref type="bibr" target="#b49">[50]</ref>, to the large-scale MPViT-Base (74M) corresponding to Swin-Base (88M) <ref type="bibr" target="#b36">[37]</ref>. All MPViT models use 8 transformer encoder heads, and the expansion ratio of the MLPs are set to 2 and 4 for Tiny and the other models, respectively. The details of MPViTs are described in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the effectiveness and versatility of MPViT as a vision backbone on image classification (ImageNet-1K <ref type="bibr" target="#b12">[13]</ref>), dense predictions such as object detection and instance segmentation (COCO <ref type="bibr" target="#b35">[36]</ref>), and semantic segmentation (ADE20K <ref type="bibr" target="#b72">[73]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ImageNet Classification</head><p>Setting. We perform classification on the ImageNet-1K <ref type="bibr" target="#b12">[13]</ref> dataset. For fair comparison with recent works, we follow the training recipe in DeiT <ref type="bibr" target="#b49">[50]</ref> as do other baseline Transformers <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67]</ref>. We train for 300 epochs with the AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer, a batch size of 1024, weight decay of 0.05, five warm-up epochs, and an initial learning rate of 0.001, which is scaled by a cosine decay learning rate scheduler. We crop each image to 224 ? 224 and use the same data augmentations as in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b64">65]</ref>. The stochastic depth drop <ref type="bibr" target="#b26">[27]</ref> is only used in the Small and Base sized models, where we set the rates to 0.05 and 0.3, respectively. More details are described in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>. <ref type="table" target="#tab_3">Table 2</ref> summarizes performance comparisons according to model size. For fair comparison, we compare the models only using 224 ? 224 input resolution and without distillation <ref type="bibr" target="#b49">[50]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Detection and Instance Segmentation</head><p>Setting. We validate MPViT as an effective feature extractor for object detection and instance segmentation with RetinaNet <ref type="bibr" target="#b34">[35]</ref> and Mask R-CNN <ref type="bibr" target="#b21">[22]</ref>, respectively. We benchmark our models on the COCO <ref type="bibr" target="#b35">[36]</ref> dataset. We pretrain the backbones on the ImageNet-1K and plug the pretrained backbones into RetinaNet and Mask R-CNN. Following common settings <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b60">61]</ref> and the training recipe of Swin-Transformer <ref type="bibr" target="#b36">[37]</ref>, we train models for 3? schedule (36 epochs) <ref type="bibr" target="#b60">[61]</ref> with a multi-scale training strategy <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>. We use AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer with an initial learning rate of 0.0001 and weight decay of 0.05. We implement models based on the detectron2 [61] library. More details are described in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Params. (M) GFLOPs  <ref type="table">Table 3</ref>. COCO detection and instance segmentation with RetinaNet <ref type="bibr" target="#b34">[35]</ref> and Mask R-CNN <ref type="bibr" target="#b21">[22]</ref>. Models are trained for 3? schedule <ref type="bibr" target="#b60">[61]</ref> with multi-scale training inputs (MS) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b44">45]</ref>. All backbones are pretrained on ImageNet-1K. We omit models pretrained on larger-datasets (e.g., ImageNet-21K). Mask R-CNN's parameters/FLOPs are followed by RetinaNet in parentheses.</p><formula xml:id="formula_4">Mask R-CNN 3? schedule + MS RetinaNet 3? schedule + MS AP b AP b 50 AP b 75 AP m AP m 50 AP m 75 AP b AP b 50 AP b 75 AP b S AP b M AP b L</formula><p>Results. <ref type="table">Table 3</ref> shows MPViT-models consistently outperform recent, comparably sized SOTA Transformers on both object detection and instance segmentation. For Reti-naNet, MPViT-S achieves 47.6%, which improves over Swin-T <ref type="bibr" target="#b36">[37]</ref> and Focal-T <ref type="bibr" target="#b66">[67]</ref>, by large margins of over 2.1 -2.6%. Interestingly, MPViT-S (32M) shows superior performance compared to the much larger Swin-S (59M) / B (98M) and Focal-S (61M) / B (100M), which have higher classification accuracies in <ref type="table" target="#tab_3">Table 2</ref>. These results demonstrate the proposed multi-scale patch embedding and multipath structure can represent more diverse multi-scale features than simpler multi-scale structured models for object detection, which requires scale-invariance. Notably   ploy UperNet <ref type="bibr" target="#b61">[62]</ref> as a segmentation method and integrate the ImageNet-1k pre-trained MPViTs into the UperNet. Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref>, for fair comparison, we train models for 160k iterations with a batch size of 16, the AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer, a learning rate of 6e-5, and a weight decay of 0.01. We report the performance using the standard single-scale protocol. We implement MPViTs using mmseg <ref type="bibr" target="#b10">[11]</ref> library. More details are described in the Appendix.</p><p>Results. As shown in <ref type="table">Table 4</ref>, our MPViT models consistently outperform recent SOTA architectures of similar size. MPViT-S achieves higher performance (48.3%) over other Swin-T, Focal-T and XCiT-S12/16 by large margins of +3.8%, +2.5%, and +2.4%. Interestingly, MPViT-S also surpasses much larger models, e.g., Swin-S/B, XCiT-S24/16, -M24/16, -S24/8, and Focal-S. Furthermore, MPViT-B outperforms the recent (and larger) SOTA Transformer, Focal-B <ref type="bibr" target="#b66">[67]</ref>. These results demonstrate the diverse feature representation power of MPViT, which stems from its multi-scale embedding and multi-path structure, makes MPViT effective on pixel-wise dense prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>We conduct ablation studies on each component of MPViT-XS to investigate the effectiveness of the proposed multi-path structure on image classification and object detection with Mask R-CNN <ref type="bibr" target="#b21">[22]</ref> using 1? schedule <ref type="bibr" target="#b21">[22]</ref> and single-scale input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exploring path dimension.</head><p>We investigate the effect of differing path dimensions, and how the path dimension could be effectively extended in <ref type="table">Table 5</ref>. We conduct experiments using various metrics such as model size (i.e., model parameter), computation cost (GFLOPs), GPU peak memory, and GPU throughput (img/sec). We use Coat-Lite Mini <ref type="bibr" target="#b64">[65]</ref> as a single-path baseline because it also leverages the same factorized self-attention mechanism as MPViT.</p><p>For a fair comparison with the baseline, we do not use a stem block, stochastic depth drop path, and the convolutional local features introduced in Sec. 3.3. For dual-path, higher feature resolution at stage 2 requires more computation, so we decrease the number of layers L (i.e., the number of transformer encoders). At stage 5, a higher embedding dimension results in a larger model size, thus we also reduce L and the embedding dimension C, increasing L at stage 4 instead. As multiple paths lead to higher computation cost, we curtail C at stages 3 and 4 to compensate. As a result, dual-path (a) in <ref type="table">Table 5</ref> improves over the single-path while having a similar model size and slightly higher FLOPs.</p><p>When expanding dual-path to triple-path, we ablate the embedding dimension C and the number of layers L, respectively. For the embedding dimension of (b) in <ref type="table">Table 5</ref>, we maintain C but reduce L to maintain a similar model size and FLOPs, which leads to worse accuracy than the dual-path. Conversely, when we decrease C and maintain L, (c) achieves similar classification accuracy but higher detection performance than the dual-path. Lastly, we further expand the path to quad-path (d), keeping L and reducing C. The quad-path achieves similar classification accuracy, but detection performance is not better than the triple-path of (c). These results teach us three lessons : i) the number of layers (i.e., deeper) is more important than the embedding dimension (i.e., wider), which means deeper and thinner structure is better in terms of performance. ii) the multigrained token embedding and multi-path structure can provide object detectors with richer feature representations. iii) Under the constraint of the same model size and FLOPs, triple-path is the best choice.</p><p>We note that our strategy of expanding the path dimension does not increase the memory burden as shown in Table 5. dual-path (a) and triple-path (b,c) consume less memory than the single-path. Also, (a) and (b) consume more memory than (c) because (a) and (b) have bigger C at stages 3 and 4. This is because C (quadratic) is a bigger factor in memory usage than L (linear) as described in sec.3.4. Therefore, our strategy of reducing the embedding dimension and expanding the path dimension and layers (deeper) leads to a memory-efficient model. However, the growth of the total number of layers due to multi-path structure decreases inference speed as compared the single-path. This issue is addressed in detail in sec. 5.  <ref type="table">Table 7</ref>. Model Capacity Analysis. We measure inference throughput and peak GPU memory usage (GB) for MPViT-S and comparable models. All models are tested on V100 GPU with batch size of 256 and 224 ? 224 resolution.</p><p>Multi-Scale Embedding. In <ref type="table" target="#tab_6">Table 6</ref>, we investigate the effects of patch size and structure in the multi-scale embedding, as outlined in Section 3.2. We use three convolution layers in parallel with the same stride of 2 and patch sizes of 3, 5, and 7, respectively. i.e., each path embedding layer operates independently using the previous input feature. For parameter efficiency, we also use three convolution layers in series with the same kernel size of 3 and strides of 2, 1, 1, respectively. We note that the latter has equivalent receptive fields (e.g., 3, 5, 7) as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. The series version improves over parallel while reducing the model size and FLOPs. Intuitively, this performance gain likely comes from the fact that the series version actually contains small 3 layer CNNs with non-linearities, which allows for more complex representations.</p><p>Global-to-Local feature Interaction. We experiment with different aggregation schemes in the GLI module, which aggregates convolutional local feature and the global transformer features, we test two types of operations: addition and concatenation. As shown in <ref type="table" target="#tab_6">Table 6</ref>, the sum operation shows no performance gain while concatenation shows improvement on both classification and detection tasks. Intuitively, summing features before the 1 ? 1 convolution naively mixes the features, while concatenation preserves them, allowing the 1 ? 1 convolution to learn more complex interactions between the features. This result demonstrates that the GLI module effectively learns to interact between local and global features for enriching representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Model Capacity Analysis.</p><p>Measuring actual GPU throughput and memory usage, we analyze the model capacity of MPViT-S, comparing with recent SOTA Transformers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67]</ref> in <ref type="table">Table 7</ref>. We test all models on the same Nvidia V100 GPU with a batch size of 256. Although CoaT Small <ref type="bibr" target="#b64">[65]</ref> achieves the best detection performance thanks its additional cross-layer attention, it exhibits heavier memory usage and GPU computation than CoaT-Lite Small with a simple multi-stage structure similar to Swin-T <ref type="bibr" target="#b36">[37]</ref> and Focal-T <ref type="bibr" target="#b66">[67]</ref>. Compared to CoaT Small, MPViT-S consumes much less memory and runs 4? faster with comparable detection performance, which means MPViT can perform efficiently and its multi-scale representations are effective without the additional cross layer attention of CoaT. Moreover, CoaT has limitations in scaling up models due to its exhaustive memory usage, but MPViT can scale to larger models. For XCiT <ref type="bibr" target="#b16">[17]</ref> having single-stage structure, XCiT-S12/16 (16x16 patches : scale 4) shows faster speed and less memory usage, while XCiT-S12/8 requires more computation and memory than MPViT-S due to its higher feature resolution. We note that XCiT-S12/8 shows higher classification accuracy (83.4%) than MPViT-S (83.0%), whereas detection performance is the opposite (47.0 vs. <ref type="bibr">48.4)</ref>. This result demonstrates that for dense prediction tasks, the mutli-scale embedding and multi-path structure of MPViT is both more efficient and effective than the single-stage structure of XCiT equipped with additional up-/down-sampling layers. MPViT also has a relatively smaller memory footprint than most models.</p><p>Qualitative Analysis. In <ref type="figure">Fig. 4</ref>, we visualize the attention maps, comparing the triple-path (c in <ref type="table">Table 5</ref>) with the single-path (CoaT-Lite Mini). Since the triple-path embeds different patch sizes, we visualize attention maps for each path. The attention maps from CoaT-Lite and path-1 have similar patch sizes and show similar attention maps. Interestingly, we observe that attention maps from path-3, which attends to larger patches with higher-level representations, are more object centric, precisely capturing the extents of the objects, as shown in the rightmost column of <ref type="figure">Fig. 4</ref>. However, at the same time, path-3 suppresses small objects and noise. Contrarily, path-1 attends to small objects due to fine patches, but does not precisely capture largeobject boundaries due to its usage of low-level representa-tions. This is especially apparent in the 3rd-row of <ref type="figure">Fig. 4</ref>, where path-1 captures a smaller ball, while path-3 attends to a larger person. These results demonstrate that combining fine and coarse features via a multi-path structure can capture objects of varying scales in the given visual inputs.</p><p>Limitation and Future works. The extensive experimental results have demonstrated that MPViT significantly outperforms current SOTA Vision Transformers not only on image-level prediction, but also on dense predictions tasks. A possible limitation of our MPViT model is the latency at inference time. As shown in <ref type="table">Table 7</ref>, the inference time of MPViT-S is slower than Swin-T and XCiT-S12/16. We hypothesize that the multi-path structure leads to suboptimal GPU utilization as similar observations have been made for grouped convolution <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b63">64]</ref> (e.g., GPU context switching, kernel synchronization, etc.). To alleviate this issue, we are currently working on an efficient implementation of our model to speed-up the inference of MPViT, which integrates the features with different scales into one tensor and then performing multi-head self attention with the tensor. This will improve parallelization and GPU utilization. Moreover, to strike the balanced tradeoff between accuracy/speed, we will further consider path dimensions in a compound scaling strategy <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b48">49]</ref>, to consider the optimal combination of depths, widths, and resolutions when increasing/decreasing the model capacity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>In this appendix, Section A.1 first describe the training details of our experiments for ImageNet classification, COCO detection/instance segmentation, and ADE20K semantic segmentation. Second, in Section A.2, we show further experimental analyses for ImageNet classification and COCO object detection. Finally, in Section A.3, we provide more qualitative analysis on the learned attention maps and failure cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Detailed Experimental Settings</head><p>ImageNet classification. Following the training recipe as in CoaT <ref type="bibr" target="#b64">[65]</ref> and DeiT <ref type="bibr" target="#b13">[14]</ref>, we perform the same data  augmentations such as MixUP <ref type="bibr" target="#b27">[28]</ref>, CutMix <ref type="bibr" target="#b69">[70]</ref>, random erasing <ref type="bibr" target="#b71">[72]</ref>, repeated augmentation <ref type="bibr" target="#b23">[24]</ref>, and label smoothing <ref type="bibr" target="#b47">[48]</ref>. We train MPViTs for 300 epochs with the AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer, a batch size of 1024, weight decay of 0.05, five warm-up epochs, and an initial learning rate of 0.001, which is scaled by a cosine decay learning rate scheduler. We implement MPViTs based on CoaT official code 1 and timm library <ref type="bibr" target="#b58">[59]</ref>. For fair comparison, we do not include models that are distilled <ref type="bibr" target="#b49">[50]</ref> or use 384 ? 384 resolution.</p><p>Object detection and Instance segmentation. For fair comparison, we follow the training recipe as in CoaT <ref type="bibr" target="#b64">[65]</ref> and Swin Transformer <ref type="bibr" target="#b36">[37]</ref> for RetinaNet <ref type="bibr" target="#b34">[35]</ref> and Mask R-CNN <ref type="bibr" target="#b21">[22]</ref>. Specifically, we train all models for 3? schedule (36 epochs) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b60">61]</ref> with multi-scale inputs (MS) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">45]</ref> which resizes the input such that the shorter side is between 480 and 800 while the longer side is at most 1333). We use the AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer, a weight decay of 0.05, a batch size of 16, and an initial learning rate of 0.0001 which is decayed by 10? at epochs 27 and 33. We set stochastic depth drop rates <ref type="bibr" target="#b26">[27]</ref> to 0.1, 0.1, 0.2, and 0.4 for Tiny, XSmall, Small, and Base, respectively. We implement all models based on the detectron2 library <ref type="bibr" target="#b60">[61]</ref>.</p><p>Semantic segmentation. Following the same training recipe as in Swin Transformer <ref type="bibr" target="#b36">[37]</ref> and XCiT <ref type="bibr" target="#b16">[17]</ref>, we deploy UperNet <ref type="bibr" target="#b61">[62]</ref> with the AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer, a weight decay of 0.01, an initial learning rate of 6 ? 10 ?5 which is scaled using a linear learning rate decay, and linear warmup of 1,500 iterations. We train models for 160K iterations with a batch size of 16 and an input size of 512 ? 512. We use the same data augmentations as <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37]</ref>, utilizing random horizontal flipping, a random re-scaling ratio in the range [0,5, 2.0] and random photometric distortions. We set stochastic depth drop rates <ref type="bibr" target="#b26">[27]</ref> to 0.2 and 0.4 for Small and Base, respectively. We implement all models based on the mmseg library <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. More Experimental Analysis</head><p>ImageNet classification. We provide a full summary of comparisons on ImageNet-1K classification in <ref type="table" target="#tab_9">Table 8</ref> by adding more recent Vision Transformers including ViL <ref type="bibr" target="#b70">[71]</ref>, TnT <ref type="bibr" target="#b20">[21]</ref>, ViTAE <ref type="bibr" target="#b65">[66]</ref>, HRFormer <ref type="bibr" target="#b68">[69]</ref>, and Twins <ref type="bibr" target="#b9">[10]</ref>. We can observe that MPViTs consistently achieve state-the-art performance compared to SOTA mod-  <ref type="table">Table 9</ref>. COCO Object Detection results on Deformable DETR <ref type="bibr" target="#b73">[74]</ref>. These all models are trained using the same codebase. els with similar model capacity. Notably, the smaller MPViT variants often outperform their larger baseline counterparts even when the baselines use significantly more parameters, as shown in <ref type="table" target="#tab_9">Table 8</ref> and <ref type="figure" target="#fig_7">Fig. 5 (right)</ref>. Furthermore, <ref type="figure" target="#fig_7">Fig. 5</ref> demonstrates that MPViT is a more efficient and effective Vision Transformer architecture in terms of computation and model parameters.</p><p>Deformable-DETR. Additionally, we compare our MPViT-Small with baselines, CoaT-Lite Small <ref type="bibr" target="#b64">[65]</ref> and CoaT Small <ref type="bibr" target="#b64">[65]</ref>, on the Deformable DETR (DD) <ref type="bibr" target="#b73">[74]</ref>. For fair comparison, we train MPViT for 50 epochs with the same training recipe 2 as in CoaT <ref type="bibr" target="#b64">[65]</ref>. We use the AdamW <ref type="bibr" target="#b37">[38]</ref> optimizer with a batch size of 16, a weight decay of 10 ?4 , and an initial learning rate of 2 ? 10 ?4 , which is decayed by a factor of 10 at 40 epoch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. More Qualitative Results</head><p>Visualization of Attention Maps. As shown in Eq.(4), the factorized self-attention in <ref type="bibr" target="#b64">[65]</ref> first extracts channel-wise attention softmax(K) by applying a softmax over spatial dimensions (x, y). Then, softmax(K) T V is computed as below:</p><formula xml:id="formula_5">(softmax(K) T V )(c i , c j ) = (x,y) softmax(K)(x, y, c i )V (x, y, c j ),<label>(5)</label></formula><p>where x and y are position of tokens. c i and c j indicate channel indices of K and V , respectively. It can be interpreted as multiplying V by the channel-wise spatial attention in a pixel-wise manner followed by the sum over spatial dimension. In other words, softmax(K) T V represents the weighted sum of V where the weight of each position (x, y) is the channel-wise spatial attention. Therefore, to obtain the importance of each position, we employ the mean of softmax(K) over the channel dimension, resulting in spatial attention. Then, the spatial attention is overlaid to the original input image for better visualization, as shown in <ref type="figure">Fig. 6</ref>. In detail, we resize the spatial attention to the size of the original image, normalize the value to [0,1], and then multiply the attention map by the image.</p><p>To validate the effectiveness of our attention map qualitatively, we compare attention maps of MPViT and CoaT-Lite <ref type="bibr" target="#b64">[65]</ref> in <ref type="figure">Fig. 6</ref>. We compare the attention maps of each method generated from the 4th stage in the same way. For a fair comparison, we pick the best qualitative attention map of each method since both CoaT-Lite and MPViT have eight heads for each layer. Furthermore, we visualize attention maps extracted from all three paths of MPViT to observe the individual effects of each path.</p><p>As mentioned in Section 5, the three paths of MPViT can capture objects of varying sizes due to the multi-scale embedding of MPViT as the similar effect of multiple receptive fields. In other words, path-1 concentrates on small objects or textures while path-3 focuses on large objects or high-level semantic concepts. We support this intuition by observing more examples shown in <ref type="figure">Fig. 6</ref>. Attention maps of path-1 (3rd column) capture small objects such as small ducks (4th row), an orange (5th row), a small ball (6th row), and an antelope (8th row). In addition, since path-1 also captures textures due to a smaller receptive field, a relatively low level of attention is present in the background. In contrast, we can observe different behavior for path-3, which can be seen in the rightmost column. Path-3 accentuates large objects while suppressing the background and smaller objects. For example, the ducks (4th row), orange (5th row), and ball (8th row) are masked out in the rightmost column since path-3 concentrates on larger objects. The attention maps of path-2 (4th column) showcase the changing behavior between paths-1 and 3 since the scale of path-2 is in-between the scales of paths-1 and 3, and accordingly, the attention maps also begin to transition from smaller to larger objects. In other words, although the attention map of path-2 attends similar regions as path-1, it is also more likely to emphasize larger objects, as path-3 does. For example, in the last row, path-2 attends to similar regions as path-1 while emphasizing the large giraffes more than path-1. Therefore, although the three paths independently deal with different scales, they act in a complementary manner, which is beneficial for dense prediction tasks.</p><p>Since Coat-Lite has a single-path architecture, the singular path needs to deal with objects of varying sizes. Therefore, attention maps from CoaT-Lite (2nd column) simultaneously attend to large and small objects, as shown in the 4th row. However, it is difficult to capture all objects with a single path, as CoaT-Lite misses the orange (5th row) and ball (7th row). In addition, Coat-Lite cannot capture object boundaries as precisely as path-3 of MPViT since path-3 need not attend to small objects or textures. As a result, MPViT shows superior results compared to Coat-Lite on classification, detection, and segmentation tasks.</p><p>Failure case. In order to verify the effects of attention from a different perspective, we further analyze failure cases on the ImageNet validation images. We show attention maps of each path corresponding to the input image along with the ground truth and the predicted labels of MPViT in <ref type="figure">Fig. 7</ref>. For example, in the first row, the ground truth of the input image is a forklift, while the predicted label is a trailer truck. Although the attention map from path-1 places light emphasis on the forklift, the attention maps from all paths commonly accentuate the trailer truck rather than the forklift, which leads to classifying the image as a trailer truck and not a forklift. Other classification results in <ref type="figure">Fig. 7</ref> fail in similar circumstances, except for the last row. In the last row, MPViTs attention maps correctly capture the beer bottle. However, the attention maps also attend to the face near the bottle. Therefore, the bottle is misunderstood as a microphone since the image of "drinking a bottle of beer" and "using a microphone" are semantically similar. From the above, we can observe that the attention maps and the predicted results are highly correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoaT-Lite</head><p>Path-1 ( ? ) Path-2 ( ? ) Path-3 ( ? ) MPViT <ref type="figure">Figure 6</ref>. Additional Attention Maps generated by CoaT-Lite <ref type="bibr" target="#b64">[65]</ref> and our MPViT. MPViT has a triple-path structure with patches of various sizes (e.g., 3 ? 3, 5 ? 5, 7 ? 7), leading to fine and coarse features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path-1 ( ? )</head><p>Path-2 <ref type="bibr">(</ref>   <ref type="bibr">Figure 7</ref>. Attention Maps of failure cases on ImageNet validation images. The input image and corresponding attention maps from each path are illustrated. In the rightmost column, we show the ground truth labels and predicted labels colored with red and blue, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Top: The state-of-the-art ViT-variants [37, 60, 67] use single-scale patches and single-path Transformer encoders. Bottom: MPViT uses multi-scale patch embedding, each embedded patch following a path to an independent Transformer encoder, allowing simulatneous representations of fine and coarse features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>FLOPs vs. COCO mask AP on Mask R-CNN. MPViTs outperform state-of-the-art Vision Transformers while having fewer parameters and FLOPs. B, S, XS, and T at the end of the model names denote base, small, extra-small and tiny respectively. Complete results are in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Overview of Multi-Path Vision Transformer (MPViT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, Swin-B and Focal-B show a performance drop compared to Swin-S and Focal-S, while MPViT-B improves over MPViT-S, showing MPViT scales well to large models. For Mask R-CNN, MPViT-XS and MPViT-S outperform the single-path baselines CoaT [65]-Lite Mini and Small by significant margins. Compared to CoaT which adds parallel blocks to CoaT-Lite with additional cross-layer attention, MPViT-XS improves over CoaT Mini, while MPViT-S shows lower box AP b but higher mask AP m . We note that although CoaT-S and MPViT-S show comparable performance, MPViT-S requires much less computation. This result suggests that MPViT can efficiently represent multiscale features without the additional cross-layer attention of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Input CoaT-Lite Path- 1 (Figure 4 .</head><label>14</label><figDesc>? ) Path-2 ( ? ) Path-3 ( ? ) MPViT Attention maps generated by CoaT-Lite and MPViT at stage4. MPViT has a triple-path structure with patches of various sizes (e.g., 3 ? 3, 5 ? 5, 7 ? 7), leading to fine and coarse features. See Appendix for more visualization results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>This work was supported by Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korean government (MSIT) (No. 2020-0-00004, Development of Previsional Intelligence based on Long-term Visual Memory Network and No. 2014-3-00123, Development of High Performance Visual BigData Discovery Platform for Large-Scale Realtime Data Analysis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Performance comparisons with respect to FLOPs and model parameters on ImageNet-1K classification. These models are trained with 224 ? 224 single-crop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>MPViT Configurations. MPViT models use paths<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3]</ref> in each of the 4 stages. #Layers and Channels denote the number of transformer encoders and the embedding dimension in each stage, respectively. We use 8 transformer heads in all models. The MLP expansion ratio is 2 and 4 for Tiny and other models, respectively. FLOPs are measured using 224 ? 224 input image.</figDesc><table><row><cell>MPViT</cell><cell>#Layers</cell><cell>Channels</cell><cell cols="2">Param. GFLOPs</cell></row><row><cell>Tiny (T)</cell><cell cols="2">[1, 2, 4, 1] [ 64, 96, 176, 216]</cell><cell>5.7M</cell><cell>1.5</cell></row><row><cell cols="4">XSmall (XS) [1, 2, 4, 1] [ 64, 128, 192, 256] 10.5M</cell><cell>2.9</cell></row><row><cell>Small (S)</cell><cell cols="3">[1, 3, 6, 3] [ 64, 128, 216, 288] 22.8M</cell><cell>4.7</cell></row><row><cell>Base (B)</cell><cell cols="3">[1, 3, 8, 3] [128, 224, 368, 480] 74.8M</cell><cell>16.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>or a larger resolution of 384 ? 384. MPViT models consistently outperform SOTA Vision Transformer architectures with similar parameter counts and computational complexity. Both MPViT-XS and Small improve over the single-path baselines, CoaT-Lite Mini and Small by a large margin of 2.0% and 1.1%, respectively. MPViT-Small also outperforms CoaT Small, while having about 3? fewer GFLOPs. Furthermore, MPViT-Small outperforms the larger models such as PVT-L, DeiT-B/16, and XCiT-M24/16. MPViT-Base (74M) achieves 84.3%, surpassing</figDesc><table><row><cell>Model</cell><cell cols="3">Param.(M) GFLOPs Top-1</cell><cell>Reference</cell></row><row><cell>DeiT-T [50]</cell><cell>5.7</cell><cell>1.3</cell><cell>72.2</cell><cell>ICML21</cell></row><row><cell>XCiT-T12/16 [17]</cell><cell>7.0</cell><cell>1.2</cell><cell>77.1</cell><cell>NeurIPS21</cell></row><row><cell>CoaT-Lite T [65]</cell><cell>5.7</cell><cell>1.6</cell><cell>76.6</cell><cell>ICCV21</cell></row><row><cell>MPViT-T</cell><cell>5.8</cell><cell>1.6</cell><cell>78.2 (+1.6)</cell><cell></cell></row><row><cell>ResNet-18 [23]</cell><cell>11.7</cell><cell>1.8</cell><cell>69.8</cell><cell>CVPR16</cell></row><row><cell>PVT-T [58]</cell><cell>13.2</cell><cell>1.9</cell><cell>75.1</cell><cell>ICCV21</cell></row><row><cell>XCiT-T24/16 [17]</cell><cell>12.0</cell><cell>2.3</cell><cell>79.4</cell><cell>NeurIPS21</cell></row><row><cell>CoaT Mi [65]</cell><cell>10.0</cell><cell>6.8</cell><cell>80.8</cell><cell>ICCV21</cell></row><row><cell>CoaT-Lite Mi [65]</cell><cell>11.0</cell><cell>2.0</cell><cell>78.9</cell><cell>ICCV21</cell></row><row><cell>MPViT-XS</cell><cell>10.5</cell><cell>2.9</cell><cell>80.9 (+2.0)</cell><cell></cell></row><row><cell>ResNet-50 [23]</cell><cell>25.6</cell><cell>4.1</cell><cell>76.1</cell><cell>CVPR16</cell></row><row><cell>PVT-S [58]</cell><cell>24.5</cell><cell>3.8</cell><cell>79.8</cell><cell>ICCV21</cell></row><row><cell>DeiT-S/16 [50]</cell><cell>22.1</cell><cell>4.6</cell><cell>79.9</cell><cell>ICML21</cell></row><row><cell>Swin-T [37]</cell><cell>29.0</cell><cell>4.5</cell><cell>81.3</cell><cell>ICCV21</cell></row><row><cell>CvT-13 [60]</cell><cell>20.0</cell><cell>4.5</cell><cell>81.6</cell><cell>ICCV21</cell></row><row><cell>XCiT-S12/16 [17]</cell><cell>26.0</cell><cell>4.8</cell><cell>82.0</cell><cell>NeurIPS21</cell></row><row><cell>Focal-T [67]</cell><cell>29.1</cell><cell>4.9</cell><cell>82.2</cell><cell>NeurIPS21</cell></row><row><cell>CoaT S [65]</cell><cell>22.0</cell><cell>12.6</cell><cell>82.1</cell><cell>ICCV21</cell></row><row><cell>CrossViT-15 [6]</cell><cell>28.2</cell><cell>6.1</cell><cell>82.3</cell><cell>ICCV21</cell></row><row><cell>CvT-21 [60]</cell><cell>32.0</cell><cell>7.1</cell><cell>82.5</cell><cell>ICCV21</cell></row><row><cell>CrossViT-18 [6]</cell><cell>43.3</cell><cell>9.5</cell><cell>82.8</cell><cell>ICCV21</cell></row><row><cell>CoaT-Lite S [65]</cell><cell>20.0</cell><cell>4.0</cell><cell>81.9</cell><cell>ICCV21</cell></row><row><cell>MPViT-S</cell><cell>22.8</cell><cell>4.7</cell><cell>83.0 (+1.1)</cell><cell></cell></row><row><cell>ResNeXt-101 [64]</cell><cell>83.5</cell><cell>15.6</cell><cell>79.6</cell><cell>CVPR17</cell></row><row><cell>PVT-L [58]</cell><cell>61.4</cell><cell>9.8</cell><cell>81.7</cell><cell>ICCV21</cell></row><row><cell>DeiT-B/16 [50]</cell><cell>86.6</cell><cell>17.6</cell><cell>81.8</cell><cell>ICML21</cell></row><row><cell>XCiT-M24/16 [17]</cell><cell>84.0</cell><cell>16.2</cell><cell>82.7</cell><cell>NeurIPS21</cell></row><row><cell>Swin-B [37]</cell><cell>88.0</cell><cell>15.4</cell><cell>83.3</cell><cell>ICCV21</cell></row><row><cell>XCiT-S12/8 [17]</cell><cell>26.0</cell><cell>18.9</cell><cell>83.4</cell><cell>NeurIPS21</cell></row><row><cell>Focal-B [67]</cell><cell>89.8</cell><cell>16.0</cell><cell>83.8</cell><cell>NeurIPS21</cell></row><row><cell>MPViT-B</cell><cell>74.8</cell><cell>16.4</cell><cell>84.3</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>ImageNet-1K classification. These models are trained with 224 ? 224 resolution. For fair comparison, we do not include models that are distilled<ref type="bibr" target="#b49">[50]</ref> or use 384 ? 384 resolution. Note that CoaT-Lite<ref type="bibr" target="#b64">[65]</ref> models are our single-path baselines.</figDesc><table><row><cell>the recent SOTA models which use more parameters such as</cell></row><row><cell>Swin-Base (88M) and Focal-Base (89M). Interestingly, the</cell></row><row><cell>MPViT-Base outperforms XCiT-M24/16 which is trained</cell></row><row><cell>with a more sophisticated training recipe [17, 51] using</cell></row><row><cell>more epochs (400), LayerScale, and a different crop ratio.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>ADE20k semantic segmentation results using Uper-Net<ref type="bibr" target="#b61">[62]</ref>. For fair comparison, We do not include models that are pre-trained on larger datasets (i.e., ImageNet-21K).CoaT. Notably, the mask AP (43.9%) of MPViT-S is higher than those of larger models such as XCiT-M24/8 or Focal-B, while having much less FLOPs. 7+1.<ref type="bibr" target="#b7">8</ref> 42.6+2.4 39.1+1.8 (b) Triple [2,3,3,3]P [1,1,2,1]L [64, 128, 256, 320]C 10.8M 2.3 6000 1080 79.8+0.9 41.4+1.2 38.0+0.7 (c) Triple [2,3,3,3]P [1,2,4,1]L [64, 128, 192, 256]C 10.1M 2.7 5954 803 80.5+1.6 43.0+2.8 39.4+2.1 (d) Quad [2,4,4,4]P [1,2,4,1]L [64, 96, 176, 224]C 10.5M 2.6 5990 709 80.5+1.6 42.4+2.2 38.8+1.5 Exploring the path dimension. Spec means [#path per stage]P, [#layer per stage]L and [dimension perstage]C. We measure inference throughput and peak GPU memory usage on V100 GPU with batch size of 256. Note that the single-path is CoaT-Lite Mini<ref type="bibr" target="#b64">[65]</ref>.</figDesc><table><row><cell>Backbone</cell><cell cols="3">Params. GFLOPs mIoU</cell></row><row><cell>Swin-T [37]</cell><cell>59M</cell><cell>945</cell><cell>44.5</cell></row><row><cell>Focal-T [67]</cell><cell>62M</cell><cell>998</cell><cell>45.8</cell></row><row><cell>XCiT-S12/16 [17]</cell><cell>54M</cell><cell>966</cell><cell>45.9</cell></row><row><cell>XCiT-S12/8 [17]</cell><cell>53M</cell><cell>1237</cell><cell>46.6</cell></row><row><cell>MPViT-S</cell><cell>52M</cell><cell>943</cell><cell>48.3</cell></row><row><cell>XCiT-S24/16 [17]</cell><cell>76M</cell><cell>1053</cell><cell>46.9</cell></row><row><cell>Swin-S [37]</cell><cell>81M</cell><cell>1038</cell><cell>47.6</cell></row><row><cell>XCiT-M24/16 [17]</cell><cell>112M</cell><cell>1213</cell><cell>47.6</cell></row><row><cell>Focal-S [67]</cell><cell>85M</cell><cell>1130</cell><cell>48.0</cell></row><row><cell>Swin-B [37]</cell><cell>121M</cell><cell>1841</cell><cell>48.1</cell></row><row><cell>XCiT-S24/8 [17]</cell><cell>74M</cell><cell>1587</cell><cell>48.1</cell></row><row><cell>XCiT-M24/8 [17]</cell><cell>110M</cell><cell>2161</cell><cell>48.4</cell></row><row><cell>Focal-B [67]</cell><cell>126M</cell><cell>1354</cell><cell>49.0</cell></row><row><cell>MPViT-B</cell><cell>105M</cell><cell>1186</cell><cell>50.3</cell></row></table><note>4.3. Semantic segmentation Setting. We further evaluate the capability of MPViT for se- mantic segmentation on the ADE20K [73] dataset. We de-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Component Analysis.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Full comparison on ImageNet-1K classification. These models are trained with 224?224 resolution. For fair comparison, we do not include models that are distilled<ref type="bibr" target="#b49">[50]</ref> or use 384 ? 384 resolution. Note that CoaT-Lite<ref type="bibr" target="#b64">[65]</ref> models are our single-path baselines.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>BackboneAP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell>ResNet-50 [23]</cell><cell>44.5 63.7</cell><cell>48.7</cell><cell>26.8</cell><cell>47.6</cell><cell>59.6</cell></row><row><cell cols="2">CoaT-Lite small [65] 47.0 66.5</cell><cell>51.2</cell><cell>28.8</cell><cell>50.3</cell><cell>63.3</cell></row><row><cell>CoaT Small [65]</cell><cell>48.4 68.5</cell><cell>52.4</cell><cell>30.2</cell><cell>51.8</cell><cell>63.8</cell></row><row><cell>MPViT-Small</cell><cell>49.0 68.7</cell><cell>53.7</cell><cell>31.7</cell><cell>52.4</cell><cell>64.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>5% AP S ) as compared to others (i.e., AP M or AP L ). COCO detection and instance segmentation with RetinaNet<ref type="bibr" target="#b34">[35]</ref> and Mask R-CNN<ref type="bibr" target="#b21">[22]</ref>. Models are trained for 1? schedule<ref type="bibr" target="#b60">[61]</ref> with single-scale training inputs. All backbones are pretrained on ImageNet-1K. We omit models pretrained on larger-datasets (e.g., ImageNet-21K). The GFLOPs are measured at resolution 800 ? 1280. Mask R-CNN's parameters/FLOPs are followed by RetinaNet in parentheses.on RetinaNet<ref type="bibr" target="#b34">[35]</ref> and Mask R-CNN<ref type="bibr" target="#b21">[22]</ref> with 1? schedule (12 epochs)<ref type="bibr" target="#b60">[61]</ref> using single-scale inputs. Tab. 10 shows result comparisons with state-of-the-art methods. In the results of 3? schedule + multi-scale (MS), we can also observe that MPViTs consistently outperform on both Reti-naNet and Mask R-CNN. We note that MPViTs surpass the most recent improved PVTv2<ref type="bibr" target="#b56">[57]</ref> models.</figDesc><table><row><cell>Tab. 9</cell></row></table><note>COCO with 1? schedule.. In addition to the 3? sched- ule + multi-scale (MS) setting, we also evaluate MPViT</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>) Input Forklift Trailer truck Laptop Fire screen Sandbar Pelican Eel Plate Beer bottle Microphone Doormat Ping-pong ball Quilt Tabby Classification Results (GT/Pred)</head><label></label><figDesc></figDesc><table><row><cell>? )</cell><cell>Path-3 ( ?</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/mlpc-ucsd/CoaT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/mlpc-ucsd/CoaT/tree/main/ tasks/Deformable-DETR</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep convolutional networks do not classify based on global object shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gennady</forename><surname>Erlikhman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">J</forename><surname>Kellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1006613</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visformer: The vision-friendly transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengsu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Twins: Revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation,2020" />
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT (1)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and accurate model scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Xcit: Cross-covariance image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Res2net: A new multi-scale backbone architecture. TPAMI</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Data augmentation by pairing samples for images classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Inoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Md Amirul Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil Db</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08248,2020.4</idno>
		<title level="m">How much position information do convolutional neural networks encode? arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On translation invariance in cnns: Convolutional layers can exploit absolute spatial location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan C Van</forename><surname>Osman Semih Kayhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemert</surname></persName>
		</author>
		<idno>CVPR, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An energy and gpu-computation efficient backbone network for real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joong-Won</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangrok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuseok</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wide-residual-inception networks for real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huieun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuenan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakil</forename><surname>Kim</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Trackformer: Multi-object tracking with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02702</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sparse r-cnn: End-to-end object detection with learnable proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Are convolutional neural networks or transformers more like human vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikhar</forename><surname>Tuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishita</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas L</forename><surname>Griffiths</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07197,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Transformer meets tracker: Exploiting temporal context for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13797</idno>
		<title level="m">Pvtv2: Improved baselines with pyramid vision transformer</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Coscale conv-attentional image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Vitae: Vision transformer advanced by exploring intrinsic inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Focal selfattention for local-global interactions in vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Glance-and-gaze vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Hrformer: Highresolution transformer for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
