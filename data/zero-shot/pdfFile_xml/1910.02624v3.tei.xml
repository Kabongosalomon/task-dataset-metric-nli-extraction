<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Label-PEnet: Sequential Label Propagation and Enhancement Networks for Weakly Supervised Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
							<email>whuang@malong.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
							<email>mscott@malong.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Malong Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Malong Artificial Intelligence Research Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Label-PEnet: Sequential Label Propagation and Enhancement Networks for Weakly Supervised Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-supervised instance segmentation aims to detect and segment object instances precisely, given imagelevel labels only. Unlike previous methods which are composed of multiple offline stages, we propose Sequential Label Propagation and Enhancement Networks (referred as Label-PEnet) that progressively transform image-level labels to pixel-wise labels in a coarse-to-fine manner. We design four cascaded modules including multi-label classification, object detection, instance refinement and instance segmentation, which are implemented sequentially by sharing the same backbone. The cascaded pipeline is trained alternatively with a curriculum learning strategy that generalizes labels from high-level images to low-level pixels gradually with increasing accuracy. In addition, we design a proposal calibration module to explore the ability of classification networks to find key pixels that identify object parts, which serves as a post validation strategy running in the inverse order. We evaluate the efficiency of our Label-PEnet in mining instance masks on standard benchmarks: PASCAL VOC 2007 and 2012. Experimental results show that Label-PEnet outperforms the state-of-the-art algorithms by a clear margin, and obtains comparable performance even with the fully-supervised approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks (CNNs) have made a series of breakthroughs in computer vision, by using large-scale manually-labeled data for training. By designing strong network architectures, CNNs can detect object locations and segment object instances precisely. However, the performance on object detection or segmentation will drop considerably due to lack of strong annotation provided * Weilin Huang is the corresponding author. at the object level or pixel level <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43]</ref>, i.e. when there are only image-level labels available.</p><p>To investigate the ability of CNNs to estimate pixel-wise labels when only image-level supervision is given, various weakly-supervised approaches have been developed for object detection or instance segmentation. A number of methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> exploit a bottom-up approach to group pixels into proposals, and then evaluate the proposals repetitively in an effort to search exact object locations. Several algorithms dissect the classification process of CNNs in a topdown <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b23">24]</ref> or bottom-up manner <ref type="bibr" target="#b41">[42]</ref>, with the goal of generating seeds for instance segmentation <ref type="bibr" target="#b42">[43]</ref>. There are also some hybrid approaches that combine both bottom-up and top-down cues <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Existing weakly-supervised methods can achieve competitive results, but the performance is still significantly lower than that of fully-supervised counterparts. Although we can roughly identify an object using a classification network, it is particularly challenging to precisely infer pixelwise labels from a classification model, even using multiple post-processing methods. This inspired us to re-think the ability of CNNs for various vision tasks, such as image classification, object detection and instance segmentation. We observed that full supervision with accurate annotations is the key to success. Therefore, the central issue for weakly-supervised detection and segmentation is to transfer image-level supervision to pixel-wise labels gradually and smoothly, in a coarse-to-fine manner by designing multiple cascaded modules.</p><p>The 2-D structure of convolutional kernels allows CNNs to grasp local information accurately, and enlarge the size of receptive fields gradually with the increase of convolutional layers, which enable the CNN model to memorize and classify objects accurately. Our goal is to enable CNNs to segment objects by just providing image-level labels. We design CNNs with such ability by introducing four new modules: (1) multi-label classification module, <ref type="bibr" target="#b1">(2)</ref> object detection module, (3) instance refinement module, and (4) instance segmentation module, which are cascaded sequentially.</p><p>Multi-Label Classification Module. In this module, an image is first partitioned into a number of patches, generating a set of object proposals. We employ an unsupervised method, selective search <ref type="bibr" target="#b37">[38]</ref> or edge box <ref type="bibr" target="#b43">[44]</ref>, where pixels are organized by low-level statistics for generating object candidates. Then a classification branch and a weight branch are incorporated to perform multi-label classification. In addition, we propose a proposal calibration module able to identify more accurate object locations and predict pixel-wise labels in object proposals.</p><p>Object Detection Module. The rough object locations generated are used to train a standard object detection with Faster-RCNN <ref type="bibr" target="#b29">[30]</ref>. But it can be unstable with direct training as we implemented. Thus we explore object scores generated from the classification module to guide the training of current object detection, and infer object locations with the model during sequential learning. Similarly, we perform proposal calibration to identify pixels belonging to the corresponding objects, which further improve the detection accuracy.</p><p>Instance Refinement Module. With the generated object locations and instance masks, we perform instance segmentation using a standard Mask-RCNN <ref type="bibr" target="#b16">[17]</ref>. However, current supervised information is still not accurate enough, so that we need to further explore object scores generated from the detection module to guide the training of current instance segmentation. Furthermore, a new instance branch is explored to perform instance segmentation, because the previous instance masks are generated based on individual samples, and can be rectified gradually with increasing accuracy when used as supervision.</p><p>Instance Segmentation Module. In this module, we obtain relatively strong supervision from the previous modules, which are used to guide the training of current instance segmentation, where final results are generated.</p><p>The main contributions of this work are summarized as: First, we introduce Sequential Label Propagation and Enhancement Networks (Label-PEnet) for weaklysupervised instance segmentation. Our framework is composed of four cascaded modules that mine, summarize and rectify the appearance of objects repetitively. A two-stage training scheme is developed to train Label-PEnet effectively. It is an important step forward in exploiting the ability of CNNs to recognize objects from image level to pixel level, and thus boost up the performance of weakly-supervised instance segmentation.</p><p>Second, we propose a proposal calibration module to uncover the classification process of CNNs, and then mine the pixel-wise labels from image-level and object-level supervision. In this module, both top-down and bottom-up methods are explored and combined to identify object pixels with increasing accuracy.</p><p>Third, to validate the effectiveness of the proposed Label-PEnet, we conduct experiments on standard benchmarks: PASCAL VOC 2007 and PASCAL VOC 2012. Experimental results show that Label-PEnet outperforms stateof-art approaches by a clear margin, and obtains comparable performance even compared with fully supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly review the related studies on weaklysupervised object detection and segmentation, along with recent neural attention methods and applications of curriculum learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weakly-Supervised Object Detection and Segmentation.</head><p>Weakly-supervised object detection and segmentation is very challenging but is important to image understanding. They aim to locate and segment objects using image-level labels only <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7]</ref>. There are usually three kinds of methods: bottom-up manner, top-down manner, or the combination of two. For example, methods in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref> treat the weakly-supervised object localization as a multi-label classification problem, and locate objects by using specific pooling layers. On the other hand, approaches in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref> extract and select object instances from images using selective search <ref type="bibr" target="#b37">[38]</ref> or edge boxes <ref type="bibr" target="#b43">[44]</ref>, and handle the weakly-supervised detection problem with multi-instance learning <ref type="bibr" target="#b7">[8]</ref>. The method in <ref type="bibr" target="#b42">[43]</ref> attempted to find peaks in the class activation map, and then propagate the peaks to identify the corresponding object proposals generated by MCG <ref type="bibr" target="#b27">[28]</ref>. In this paper, we decompose the instance segmentation task into multiple simpler problems, and utilize the ability of CNNs to identify object pixels progressively.</p><p>Neural Attention. Neural attention aims to understand the classification process of CNNs, and learn the relationship between the pixels in the input image and the neural activations in convolutional layers. Recent effort has been made to explain how neural networks work <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, Lapuschkin et al. extended a layer-wise relevance propagation (LRP) <ref type="bibr" target="#b0">[1]</ref> to visualize inherent structured reasoning of deep neural networks. To identify the important regions producing final classification results, Zhang et al. <ref type="bibr" target="#b40">[41]</ref> proposed a positive neural attention back-propagation scheme, called excitation back-propagation (Excitation BP). Other related methods include Grad-CAM <ref type="bibr" target="#b33">[34]</ref> and network dissection <ref type="bibr" target="#b1">[2]</ref>. Neural attention obtains pixel-wise   <ref type="figure">Figure 1</ref>. The proposed Label-PEnet for weakly-supervised instance segmentation. (a) Overview: the training pipeline contains two different stages. One is curriculum learning stage which learns from image-level labels to pixel-wise labels. The other one learns in an inverse order to validate the results generated from the previous modules. (b) Shared backbone: the backbone is shared by all modules. (c) The details of different modules for multi-label classification, object detection, instance refinement, and instance segmentation. We develop a two-stage training scheme for learning Label-PEnet: a cascaded pre-training stage and a forward-backward learning stage. The backbone is fixed during the cascaded pre-training, and then is trained in forward-backward learning stage. class probabilities using image-level labels in a top-down manner on a well trained network. In our pipeline, we propose a forward network that computes pixel-wise class probability map for each individual proposal. This allows us to transfer image-level labels to pixel-wise ones, providing richer supervision for subsequent object detection and instance segmentation.</p><p>Curriculum Learning. Curriculum learning <ref type="bibr" target="#b2">[3]</ref> is set of machine learning methods that decompose a complicated learning task into multiple sub-tasks with gradually increasing learning difficulty. In <ref type="bibr" target="#b2">[3]</ref>, Yoshua et al. described the concept of curriculum learning, and used a toy classification problem to show the advantage of decomposing a complex problem into multiple simpler ones. Various machine learning algorithms <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b13">14]</ref> follow a similar divide-andconquer strategy in curriculum learning. Recently, Sheng et al. <ref type="bibr" target="#b14">[15]</ref> proposed CurriculumNet for large-scale weaklysupervised image classification. CurriculumNet is able to learn high-performance CNNs from an image dataset containing a large amount of noisy images and labels, which were collected rawly from the Internet without any human annotation <ref type="bibr" target="#b25">[26]</ref>. In this paper, we adopt this strategy to decompose the instance segmentation problem into multilabel image classification, object detection and instance segmentation sequentially. All the learning tasks in these mod-ules are relatively simple by using the training data with the refined labels generated from previous stages 3. Label-PEnet: Sequential Label Propagation and Enhancement Networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary and Overview</head><p>Given an image I associated with an image-level label y I = [y 1 , y 2 , ..., y C ] T , our goal is to estimate pixel-wise labels Y I = [y 1 , y 2 , ..., y P ] T for each object instance. C is the number of object classes, P is the number of pixels in I. y l is a binary value, where y l = 1 means the image I contains the l-th object category, and otherwise, y l = 0. The label of a pixel p is denoted by a C-dimensional binary vector y p . In this work, we propose a weakly-supervised learning approach for instance segmentation, which is inspired by the divide-and-conquer idea in curriculum learning <ref type="bibr" target="#b2">[3]</ref>. This allows us to train our model with increasingly stronger supervision which is learned automatically by propagating object information from image level to pixel level via four cascaded modules: multi-label classification module, object detection module, instance refinement module, and instance segmentation module. The proposed Label-PEnet is described in <ref type="figure">Fig. 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiple Cascaded Modules</head><p>Multi-Label Classification Module. This module aims to generate a set of rough object proposals with corresponding class confident values and proposal weights, by just using image-level category labels. To identify rough regions of objects, we exploit selective search <ref type="bibr" target="#b37">[38]</ref> to generate a set of object proposals R = (R 1 , R 2 , ..., R n ). These object candidates are then used as input to our multi-label classification module for collecting the proposals with higher confidence, and learning to identify the pixels which paly the key role in the classification task.</p><p>For an image I of W ? H, given a deep neural network ? d (?, ?; ?) with a convolutional stride of ? s , we have convolutional feature maps with a spatial size of H/? s ?W/? s in the last convolutional layer. Then ROI pooling <ref type="bibr" target="#b12">[13]</ref> is performed on the convolutional feature maps to compute the features for each object proposals in R, resulting in |R| regional features for image I. Two fully-connected layers are applied separately to the computed regional features, generating classification results, x c,1 ? R |R|?C , and weight vectors, x p,1 ? R |R|?C , for the |R| object proposals. The proposal weights indicate the contribution of each proposal to the C categories in image-level multi-label classification. A softmax function is applied to normalize the weights as,</p><formula xml:id="formula_0">w p,1 ij = e x p,1 ij |R| i=1 e x p,1 ij .<label>(1)</label></formula><p>where x p,1 ij stands for the weight of the i-th proposal on the j-th class. We can have a normalized weight matrix w p,1 ? R |R|?C . Then the final score for each proposal on different classes is calculated by taking an element-wise product, x 1 = x c,1 w p,1 , and the final image-level multilabel classification results are computed by summing over all the proposals associated to each class,</p><formula xml:id="formula_1">s 1 c = |R| i=1 x 1 ic .</formula><p>This results in a final score vector for the input image I, s 1 = s 1 1 , s 1 2 , ..., s 1 C , indicating a confident value for each class. A probability vectorp 1 = p 1 1 ,p 1 2 , ...,p 1 C can be computed by applying a softmax function to s 1 , and the loss function for image-level multi-label classification is,</p><formula xml:id="formula_2">L 1 (I, y I ) = ? C k=1 y k logp 1 k .<label>(2)</label></formula><p>Proposal Calibration. The generated object proposals, with their classification scores, x c,1 , are further processed by proposal calibration, which is a proposal refinement submodule able to refine the generated proposals. The goal is to improve the prediction accuracy on object bounding boxes, and generate object masks, providing stronger and more accurate supervision for next modules. Recent work of <ref type="bibr" target="#b40">[41]</ref> introduces a new Excitation Back-Propagation (Excitation BP) able to generate a discriminative object-based attention map by using the predicted image-level class labels, which inspired us to compute an attention map for each proposal by using the predicted classification score. We explore a same network architecture as the classification module. Specifically, given a proposal R i , we apply a softmax function on its class prediction x c,1 i ? R C to have a normalized vector, w c,1 i , and predict an object class c i by using the highest value. Then we get a class activation vector, a c,1 i ? R C , by setting all other elements to 0, except for the c i -th one in w c,1 i . We perform the Excitation BP <ref type="bibr" target="#b40">[41]</ref> in a feed forward manner from the classification layer to the ROI pooling layer by using the activation vector, generating a proposal attention map, A i , for proposal R i , as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Then for the proposals with label c in the image I, we perform non-maximum suppression (NMS) by using the classification scores, x c,1 , and generate an object candidate R c having the highest score. For those proposals (with label c) which are suppressed by R c , we add their proposal attention maps by projecting them into the corresponding locations in the image, and generate a class-specific attention map A c , referred as instance at-tention map for object class c, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Finally, we can compute a set of object instance attention maps:</p><formula xml:id="formula_3">A = [A 1 , A 2 , ..., A C ] ? R C?H?W , with a background map, A 0 = max(0, 1 ? ? C l=1 y l A l )</formula><p>. We further compute an object heat-map for each instance class. The object heat-map for instance class c is generated by computing pixel-wise sum over all proposals with class c, using the corresponding classification scores in x c,1 . Then we combine instance attention maps and object heatmaps to generate final instance confident maps, where a conditional random field (CRF) <ref type="bibr" target="#b22">[23]</ref> is further implemented to segment object instances more accurately. This results in a set of segmentation masks, S 1 ? R K?H?W , with corresponding object bounding boxes, B 1 ? R K?4 . Meanwhile, for each pair of bounding box and segmentation mask, we simply use the classification score of the identified object candidate (e.g., R c ) as a weight, obtaining the predicted instance weights W 1 ? R K which are used to guide the training of next object detection module.</p><p>Object Detection Module. With the generated proposal bounding boxes B 1 and the corresponding weights W 1 , we train a standard object detection model by using them as ground truth. The main difference is that we provide a learned weight for each generated proposal during training. By following Faster-RCNN <ref type="bibr" target="#b30">[31]</ref>, we sample positive and negative proposals around a ground truth bounding box, and each proposal sampled has a same weight with the corresponding ground truth. Then the optimization objective of region proposal network (RPN) is modified as,</p><formula xml:id="formula_4">L (w i , t i ) rpn = 1 N rpn i L obj (w i , w * i ) + ? 1 N rpn i w * i L reg (t i , t * i ),<label>(3)</label></formula><p>where N rpn is the number of candidate proposals, w i is the predicted object score, t i is the predicted location offset, w * i is the proposal weight, t * i is the pseudo object location, ? is a constant value. L obj , L cls and L reg are the binary object or non-object loss, classification loss, and boundingbox regression loss respectively. For the RCNN part, the optimization objective is computed as,</p><formula xml:id="formula_5">L (p i , t i ) rcnn = 1 N rcnn i w * i L cls (p i , p * i ) + ? 1 N rcnn i w * i L reg (t i , t * i ).<label>(4)</label></formula><p>where p i is the classification score, and p * i indicates the object class. N rcnn is the number of proposals generated by RPN, and L cls is the classification loss. On the head of Faster-RCNN architecture, we perform proposal calibration to refine object proposals, which is similar to that of multilabel classification module. This enables the model to generate dense proposal attention maps. In inference, multiple object candidates can be generated for multiple labels, which are different from the proposal calibration in classification module that outputs one candidate for each label. Finally, we can obtain multiple instance marks, S 2 , with corresponding bounding boxes, T 2 , and weights, W 2 ? R J , where J is the number of object instances.</p><p>Instance Refinement Module. With the generated instance masks S 2 and object bounding boxes T 2 , we can train an instance segmentation task having a joint detection branch and mask branch similar to that of Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>. In this module, we implement instance inference for dense pixel-wise prediction rather than proposal calibration, by following the feed forward inference as <ref type="bibr" target="#b16">[17]</ref>. Object instances are learnt and modeled in the module by collecting part of the information hidden in the results generated from previous modules. We perform object instance segmentation with the learned weights W 2 , and our training process follows that of Mask-RCNN <ref type="bibr" target="#b16">[17]</ref>. As in the proposal calibration, object masks affiliated with the predicted object location are summed together to generate a new instance confident map. Similarly, we perform CRF <ref type="bibr" target="#b22">[23]</ref> to obtain more accurate results of instance segmentation.</p><p>Instance Segmentation Module. In this module, imagelevel labels have been successfully transferred into dense pixel-wise labels. We perform standard instance segmentation in a fully supervised manner, by simply following the training strategies implemented in the instance refinement module. Final results can be generated during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training with Label Propagation</head><p>To better train multiple sequential models and avoid local minima, we initialize the backbone network with an ImageNet pre-trained model. The training is implemented sequentially by using the output of previous module, with gradually enhanced supervision. We develop a two-stage training process containing cascaded pre-training and forward-backward learning with curriculum.</p><p>Cascaded Pre-Training. The backbone networks are fixed during cascaded pre-training. We pre-train four cascaded modules sequentially, from multi-label classification to instance segmentation. When the training of current module is converged, with model outputs well regularized and refined, such outputs are then used as supervision for the next module. With the cascaded pre-training, we decompose a weakly-supervised instance segmentation task into four sequential sub-tasks where image-level supervision is propagated gradually and efficiently to dense pixelwise predictions.  Forward-Backward Learning with Curriculum Training four sequential models is challenging, because networks might get into local minima easily with sequential label propagation. To overcome this problem, we propose a forward-backward learning method by leveraging curriculum learning, which has a forward curriculum learning phase and backward validation phase, as shown in <ref type="figure">Fig. 1</ref>.</p><p>In the forward curriculum learning, the four modules are trained sequentially where the supervised information is enhanced gradually. While in the backward validation, training is performed in an inverse order. The backward validation starts from instance segmentation module, where we just perform inference at the module, and generate object locations and instance masks for instance refinement module. Then the instance refinement module is trained in a fully supervised manner, providing object locations for object detection module. In multi-label classification module, we set the proposals, which have an overlap of &gt; ? (= 0.5) with the objects detected by the detection module, with a label of the corresponding objects or background. Then we perform single-label classification on these proposals, and at the same time, keep training multi-label classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>Our methods were implemented using Caffe <ref type="bibr" target="#b18">[19]</ref> and run on a NVIDIA TITAN RTX GPU with 24GB memory. The parameters of object detection and instance segmentation modules are the same with Faster R-CNN <ref type="bibr" target="#b29">[30]</ref> and Mask R-CNN <ref type="bibr" target="#b16">[17]</ref>. Several examples are illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network Structures</head><p>Backbone Network. The backbone network is based on VGG-16, where the layers after relu4 3 are removed. As shown in <ref type="figure">Fig 1,</ref> only the first four convolutional blocks are preserved. All the parameters are initialized from an Ima-geNet pre-trained model.</p><p>Multi-label Classification Module. Following the backbone network, the fifth convolution block contains conv5 1, conv5 2, and conv5 3. We set dilations in the three layers to 2. The feature stride ? s at layer relu5 3 is 8. A ROI pooling <ref type="bibr" target="#b12">[13]</ref> is added to generate a set of 512 ? 7 ? 7 feature maps, followed by f c6 and f c7 layers. The classification branch and proposal weight branch are initialized randomly using a Gaussian initializer as in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Object Detection Module. As in multi-label classification module, dilations in conv5 1, conv5 2, and conv5 3 are set to 2. The RPN <ref type="bibr" target="#b29">[30]</ref> contains three convolutional layers which are all initialized with Gaussian distributions with 0 mean and standard deviation of 0.01. It generates proposals where ROI pooling <ref type="bibr" target="#b12">[13]</ref> is conducted on the feature maps relu5 3. A proposal classification branch and a  bounding box regression branch are presented by following two fully-connected layers f c6 and f c7.</p><p>Instance Refinement Module and Instance Segmentation Module. The two modules have the same network architecture, which contains an object detection part and an instance segmentation part. The object detection part is similar to that of object detection module, with only one difference that the RPN and ROI pooling are computed on the feature maps of the pool4 layer, not the relu5 3. For the instance segmentation part, we adopt the atrous spatial pyramid pooling as that of DeepLab V3 <ref type="bibr" target="#b4">[5]</ref> after layer relu5 3, with dilations set to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Cascaded Pre-Training. In the cascaded pre-training stage, we train the four cascaded modules in a forward order, but keep the parameters in the backbone network fixed. For data augmentation, we use five image scales, {480, 576, 688, 864, 1024} (for the shorter side), and horizontal flip, and cap the longer side at 1,200. The mini-batch size for SGD is set to 2, and the learning rate is set to 0.001 in the first 40K iterations, and then is decreased to 0.0001 in the following 10K iterations. The weight decay is 0.0005, with a momenta of 0.9. These settings are used in all the four modules. We start training the next module only when the training of previous one is finished. Selective Search (SS) <ref type="bibr" target="#b37">[38]</ref> is adopted in the multi-label classification module to generate about 1,600 object proposals per image. For RPN in object detection module and instance segmentation module, we follow <ref type="bibr" target="#b29">[30]</ref> to use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. The sizes of convolutional feature maps after ROI pooling in the detection branch and segmentation branch are 7 ? 7 and 14 ? 14.</p><p>Forward-Backward Learning with Curriculum. As shown in <ref type="figure">Fig 1,</ref> there are two sub-stages for training: a forward curriculum learning stage and an inverse backward validation stage, which are implemented alternatively at each iteration. All layers with learnable parameters are trained in an end-to-end manner. The training starts from the cascaded pre-trained model, with a learning rate of 0.0001 in the following 80K iterations. In inference, an image with original size is used as input.  -   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Weakly Supervised Object Detection</head><formula xml:id="formula_6">- - - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Weakly-Supervised Semantic Segmentation</head><p>Dataset and Evaluation. Pascal VOC 2012 dataset <ref type="bibr" target="#b10">[11]</ref> is the standard benchmark for the task of weakly-supervised semantic segmentation. It contains 21 classes with 10,582 images for training (including VOC 2012 training set and additional data annotated in <ref type="bibr" target="#b15">[16]</ref>), 1,449 images for validation and 1,456 for test. Only image-level labels are used for training. We do not use any additional data annotated in <ref type="bibr" target="#b15">[16]</ref>, and report the results on the test set in <ref type="table" target="#tab_7">Table 5</ref>.</p><p>Results. As shown in <ref type="table" target="#tab_7">Table 5</ref>, our method achieves a mean IoU of 57.2%, and outperforms the previous state-of-the-art AE-SPL <ref type="bibr" target="#b39">[40]</ref> and MCOF [39] by 1.6% and 1% respectively. Compared with recent algorithms, including AE-SPL <ref type="bibr" target="#b39">[40]</ref>, F-B <ref type="bibr" target="#b32">[33]</ref>, FCL <ref type="bibr" target="#b31">[32]</ref>, and SEC <ref type="bibr" target="#b21">[22]</ref>, our Label-PEnet cast the semantic segmentation problem into multiple easier tasks, which allows us to propagate high-level image labels to pixel-wise labels gradually with enhanced accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Weakly-Supervised Instance Segmentation</head><p>Dataset and Evaluation. We follow the experimental settings in <ref type="bibr" target="#b42">[43]</ref> by using Pascal VOC 2012 dataset <ref type="bibr" target="#b10">[11]</ref> for weakly-supervised instance segmentation. Experimental results are evaluated with mAP r at IoU threshold of 0.25, 0.5 and 0.75, and the Average Best Overlap (ABO) <ref type="bibr" target="#b28">[29]</ref>. We report results on the test set in <ref type="table" target="#tab_8">Table 6</ref>.</p><p>Results. We use VGG16 as our backbone, and report the performance in the term of four metrics, while most existing methods used ResNet50. Only PRM-VGG16 applied VGG16 and obtained a mAP r 0.5 of 22.0%. Obviously, our method outperforms PRM-VGG16 by 8.2% on mAP r 0.5 .</p><p>Even compared with PRM-ResNet50, our method can obtain large improvements on all four metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Evaluation on Individual Modules</head><p>We further compare the effect of each individual modules on the test set of Pascal VOC 2007 detection, as shown in <ref type="table" target="#tab_2">Table 2</ref>. In the cascaded pre-training, the multi-label classification can only have a mAP of 26.9%, which is improved to 39.1% when we refine object locations with the proposal calibration module and detection module. Furthermore, instance refinement module further improves the object detection results considerably by 10.6%, reaching to 49.7%. Finally, the instance segmentation module can achieve a mAP of 51.3%. The results suggest that with more accurate results provided as guidance and supervision, the object detection results can be improved gradually and significantly with four cascaded modules. When we perform the forward-backward learning, our Label-PEnet can have a mAP of 53.1%, which is 1.8% higher than that of the cascaded pre-training, and also outperforms previous methods, such as MEFF+FRCNN <ref type="bibr" target="#b11">[12]</ref> and OICR-Ens+FRCNN <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented new Sequential Label Propagation and Enhancement Networks (referred as Label-PEnet) for weakly-supervised object detection and instance segmentation. Label-PEnet is able to progressively transform imagelevel labels to pixel-wise predictions in a coarse-to-fine manner, by designing four cascaded modules, from multilabel classification, object detection, instance refinement to instance segmentation. In addition, we design a proposal calibration module to explore the ability of classification CNNs to identify key pixels of objects, which further improves detection and segmentation accuracy. Our Label-PEnet is evaluated on the standard benchmarks for weaklysupervised object detection and segmentation, where it outperformed the state-of-the-art methods by a clear margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Proposal attention maps: "Person"-"Horse" (d) Object mask generation Proposal calibration module. (a) Object proposals: all candidate object proposals suppressed by NMS are taken to generate a set of proposal attention maps. (b) Excitation BP: the process of Excitation BP implemented on each proposal. (c) The generated proposal attention maps for "Person" and "Horse", and all proposal attention maps for a same instance are combined to generate a single instance attention map. (d) Instance mask generation: instance attention map and object heat-map are combined to compute an instance confident map, where CRF [23] is implemented to generate a final instance mask.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Instance detection and segmentation results on Pascal VOC 2012 (the first row) and Pascal VOC 2007 (the second row). The proposals with the highest confidence are selected and visualized. The segmentation results are post-processed by CRF [23].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>method</cell><cell cols="4">aero bike bird boat bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell cols="6">chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mAP</cell></row><row><cell>OM+MIL+FRCNN[25]</cell><cell cols="3">54.5 47.4 41.3 20.8</cell><cell>17.7</cell><cell cols="5">51.9 63.5 46.1 21.8 57.1 22.1 34.4</cell><cell>50.5</cell><cell>61.8</cell><cell>16.2</cell><cell>29.9</cell><cell>40.7</cell><cell>15.9 55.3 40.2 39.5</cell></row><row><cell cols="4">HCP+DSD+OSSH3[20] 54.2 52.0 35.2 25.9</cell><cell>15.0</cell><cell cols="5">59.6 67.9 58.7 10.1 67.4 27.3 37.8</cell><cell>54.8</cell><cell>67.3</cell><cell>5.1</cell><cell>19.7</cell><cell>52.6</cell><cell>43.5 56.9 62.5 43.7</cell></row><row><cell cols="4">OICR-Ens+FRCNN[36] 65.5 67.2 47.2 21.6</cell><cell>22.1</cell><cell cols="3">68.0 68.5 35.9</cell><cell>5.7</cell><cell>63.1 49.5 30.3</cell><cell>64.7</cell><cell>66.1</cell><cell>13.0</cell><cell>25.6</cell><cell>50.0</cell><cell>57.1 60.2 59.0 47.0</cell></row><row><cell>MEFF+FRCNN[12]</cell><cell cols="3">64.3 68.0 56.2 36.4</cell><cell>23.1</cell><cell cols="3">68.5 67.2 64.9</cell><cell>7.1</cell><cell>54.1 47.0 57.0</cell><cell>69.3</cell><cell>65.4</cell><cell>20.8</cell><cell>23.2</cell><cell>50.7</cell><cell>59.6 65.2 57.0 51.2</cell></row><row><cell cols="2">Multi-label Cls Module  ? 41.2 42.0</cell><cell>6.5</cell><cell>17.1</cell><cell>7.1</cell><cell cols="2">54.1 40.5</cell><cell>8.5</cell><cell cols="2">17.3 33.0 13.2 10.3</cell><cell>24.4</cell><cell>54.0</cell><cell>5.5</cell><cell>7.5</cell><cell>20.0</cell><cell>39.2 49.9 47.3 26.9</cell></row><row><cell>Object Det Module  ?</cell><cell cols="3">49.1 61.3 24.8 15.9</cell><cell>46.9</cell><cell cols="5">58.9 25.3 17.7 23.3 41.8 28.9 42.4</cell><cell>67.1</cell><cell>25.3</cell><cell>6.7</cell><cell>50.4</cell><cell>40.9</cell><cell>62.4 50.4 42.3 39.1</cell></row><row><cell>Instance Ref Module  ?</cell><cell cols="3">62.3 68.3 47.2 27.9</cell><cell>53.8</cell><cell cols="5">69.1 39.9 41.9 25.9 56.5 40.1 53.0</cell><cell>70.0</cell><cell>44.9</cell><cell>13.3</cell><cell>53.5</cell><cell>51.1</cell><cell>68.6 60.9 45.2 49.7</cell></row><row><cell>Instance Seg Module  ?</cell><cell cols="3">63.8 69.0 47.9 35.3</cell><cell>56.1</cell><cell cols="5">68.9 41.5 42.7 25.9 58.3 44.3 52.5</cell><cell>70.3</cell><cell>44.4</cell><cell>13.8</cell><cell>56.9</cell><cell>52.9</cell><cell>70.0 62.3 49.9 51.3</cell></row><row><cell cols="2">Multi-label Cls Module  ? 42.4 43.8</cell><cell>8.9</cell><cell>18.7</cell><cell>6.5</cell><cell cols="5">55.7 42.0 10.0 18.3 34.3 14.5 11.4</cell><cell>24.8</cell><cell>56.2</cell><cell>3.7</cell><cell>9.1</cell><cell>22.1</cell><cell>40.5 51.1 46.5 28.0</cell></row><row><cell>Object Det Module  ?</cell><cell cols="3">51.2 63.0 28.8 17.5</cell><cell>51.1</cell><cell cols="5">60.3 28.9 20.7 25.9 41.0 31.2 46.4</cell><cell>68.1</cell><cell>27.1</cell><cell>6.0</cell><cell>50.9</cell><cell>43.6</cell><cell>65.8 50.6 40.3 40.3</cell></row><row><cell>Instance Ref Module  ?</cell><cell cols="3">63.2 67.5 48.3 29.8</cell><cell>54.8</cell><cell cols="5">70.4 40.9 42.6 27.9 55.0 41.5 54.3</cell><cell>70.0</cell><cell>43.2</cell><cell>15.3</cell><cell>55.4</cell><cell>52.4</cell><cell>69.0 62.2 46.8 50.5</cell></row><row><cell>Instance Seg Module  ?</cell><cell cols="3">65.7 69.4 50.6 35.8</cell><cell>55.5</cell><cell cols="5">71.9 43.6 45.3 27.5 58.5 45.4 55.4</cell><cell>71.7</cell><cell>45.8</cell><cell>18.2</cell><cell>56.6</cell><cell>56.1</cell><cell>72.0 64.6 51.4 53.1</cell></row></table><note>Average precision (in %) of weakly-supervised methods on PASCAL VOC 2007 detection test set.? stands for the results of the cascaded pre-training.? stands for the results of the recurrent mixed fine-tuning.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Average precision (in %) of weakly-supervised methods on PASCAL VOC 2012 detection test set.</figDesc><table><row><cell>method</cell><cell cols="5">aero bike bird boat bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell cols="8">chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mAP</cell></row><row><cell>OICR-VGG16[36]</cell><cell cols="3">67.7 61.2 41.5 25.6</cell><cell cols="2">22.2</cell><cell cols="7">54.6 49.7 25.4 19.9 47.0 18.1 26.0</cell><cell>38.9</cell><cell>67.7</cell><cell>2.0</cell><cell>22.6</cell><cell>41.1</cell><cell>34.3 37.9 55.3 37.9</cell></row><row><cell>WSDDN+context[7]</cell><cell cols="2">64.0 54.9 36.4</cell><cell>8.1</cell><cell cols="2">12.6</cell><cell cols="3">53.1 40.5 28.4</cell><cell>6.6</cell><cell cols="3">35.3 34.4 49.1</cell><cell>42.6</cell><cell>62.4</cell><cell>19.8</cell><cell>15.2</cell><cell>27.0</cell><cell>33.1 33.0 50.0 35.3</cell></row><row><cell cols="4">HCP+DSD+OSSH3+NR[20] 60.8 54.2 34.1 14.9</cell><cell cols="2">13.1</cell><cell cols="3">54.3 53.4 58.6</cell><cell>3.7</cell><cell>53.1</cell><cell>8.3</cell><cell>43.4</cell><cell>49.8</cell><cell>69.2</cell><cell>4.1</cell><cell>17.5</cell><cell>43.8</cell><cell>25.6 55.0 50.1 38.3</cell></row><row><cell>OICR-Ens+FRCNN[36]</cell><cell cols="3">71.4 69.4 55.1 29.8</cell><cell cols="2">28.1</cell><cell cols="7">55.0 57.9 24.4 17.2 59.1 21.8 26.6</cell><cell>57.8</cell><cell>71.3</cell><cell>1.0</cell><cell>23.1</cell><cell>52.7</cell><cell>37.5 33.5 56.6 42.5</cell></row><row><cell>MEFF+FRCNN[12]</cell><cell cols="3">71.0 66.9 55.9 33.8</cell><cell cols="2">24.0</cell><cell cols="7">57.6 58.0 61.4 22.5 58.4 19.2 58.7</cell><cell>61.9</cell><cell>75.0</cell><cell>11.2</cell><cell>23.9</cell><cell>50.3</cell><cell>44.9 41.3 54.3 47.5</cell></row><row><cell>Multi-label Cls Module  ?</cell><cell>37.1 40.0</cell><cell>5.9</cell><cell>11.7</cell><cell>5.5</cell><cell></cell><cell cols="2">48.3 40.5</cell><cell>7.0</cell><cell cols="2">16.3 29.2</cell><cell>9.9</cell><cell>8.3</cell><cell>19.3</cell><cell>51.1</cell><cell>3.0</cell><cell>6.1</cell><cell>17.0</cell><cell>36.3 46.4 39.1 23.9</cell></row><row><cell>Object Det Module  ?</cell><cell cols="3">49.2 57.0 25.1 13.9</cell><cell cols="2">49.5</cell><cell cols="7">53.3 25.3 15.9 20.0 36.5 29.1 42.1</cell><cell>60.9</cell><cell>22.9</cell><cell>5.5</cell><cell>43.5</cell><cell>37.8</cell><cell>63.4 48.7 35.8 36.8</cell></row><row><cell>Instance Ref Module  ?</cell><cell cols="3">57.9 65.5 43.9 26.9</cell><cell cols="2">50.9</cell><cell cols="7">64.7 35.9 38.7 22.8 50.9 38.9 50.9</cell><cell>65.5</cell><cell>39.5</cell><cell>13.6</cell><cell>52.9</cell><cell>48.9</cell><cell>65.7 57.9 41.9 46.7</cell></row><row><cell>Instance Seg Module  ?</cell><cell cols="3">60.8 65.4 46.2 31.4</cell><cell cols="2">50.3</cell><cell cols="7">68.3 40.7 39.9 25.3 52.8 43.4 53.9</cell><cell>68.2</cell><cell>40.8</cell><cell>15.9</cell><cell>53.1</cell><cell>50.0</cell><cell>68.1 59.8 49.0 49.2</cell></row><row><cell>method</cell><cell cols="4">aero bike bird boat bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell cols="9">chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mCorLoc</cell></row><row><cell>OICR-VGG16[36]</cell><cell cols="2">81.7 80.4 48.7 49.5</cell><cell>32.8</cell><cell cols="8">81.7 85.4 40.1 40.6 79.5 35.7 33.7</cell><cell>60.5</cell><cell>88.8</cell><cell>21.8</cell><cell>57.9</cell><cell>76.3</cell><cell>59.9 75.3 81.4</cell><cell>60.6</cell></row><row><cell>WSDDN-Ens[7]</cell><cell cols="2">68.9 68.7 65.2 42.5</cell><cell>40.6</cell><cell cols="8">72.6 75.2 53.7 29.7 68.1 33.5 45.6</cell><cell>65.9</cell><cell>86.1</cell><cell>27.5</cell><cell>44.9</cell><cell>76.0</cell><cell>62.4 66.3 66.8</cell><cell>58.0</cell></row><row><cell>OM+MIL+FRCNN[25]</cell><cell cols="2">78.2 67.1 61.8 38.1</cell><cell>36.1</cell><cell cols="8">61.8 78.8 55.2 28.5 68.8 18.5 49.2</cell><cell>64.1</cell><cell>73.5</cell><cell>21.4</cell><cell>47.4</cell><cell>64.6</cell><cell>22.3 60.9 52.3</cell><cell>52.4</cell></row><row><cell cols="3">HCP+DSD+OSSH3[20] 72.2 55.3 53.0 27.8</cell><cell>35.2</cell><cell cols="8">68.6 81.9 60.7 11.6 71.6 29.7 54.3</cell><cell>64.3</cell><cell>88.2</cell><cell>22.2</cell><cell>53.7</cell><cell>72.2</cell><cell>52.6 68.9 74.4</cell><cell>54.9</cell></row><row><cell cols="3">OICR-Ens+FRCNN[36] 85.8 82.7 62.8 45.2</cell><cell>43.5</cell><cell cols="8">84.8 87.0 46.8 15.7 82.2 51.0 45.6</cell><cell>83.7</cell><cell>91.2</cell><cell>22.2</cell><cell>59.7</cell><cell>75.3</cell><cell>65.1 76.8 78.1</cell><cell>64.3</cell></row><row><cell>MEFF+FRCNN[12]</cell><cell cols="2">88.3 77.6 74.8 63.3</cell><cell>37.8</cell><cell cols="8">78.2 83.6 72.7 19.4 79.5 46.4 78.1</cell><cell>84.7</cell><cell>90.4</cell><cell>28.6</cell><cell>43.6</cell><cell>76.3</cell><cell>68.3 77.9 70.6</cell><cell>67.0</cell></row><row><cell>Label-PEnet</cell><cell cols="2">89.8 82.6 75.3 65.7</cell><cell>39.2</cell><cell cols="8">80.2 81.6 77.7 18.4 82.7 49.3 75.0</cell><cell>86.9</cell><cell>85.9</cell><cell>30.7</cell><cell>49.6</cell><cell>75.3</cell><cell>71.5 76.1 70.6</cell><cell>68.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>CorLoc (in %) of weakly-supervised methods on PASCAL VOC 2007 detection trainval set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>CorLoc (in %) of weakly-supervised methods on PASCAL VOC 2012 detection trainval set.</figDesc><table><row><cell>method</cell><cell>bg</cell><cell cols="2">aero bike bird boat bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell cols="5">chair cow table dog horse mbike person plant sheep sofa train</cell><cell>tv</cell><cell>mIoU</cell></row><row><cell>SEC[22]</cell><cell cols="2">83.5 56.4 28.5 64.1 23.6</cell><cell>46.5</cell><cell cols="4">70.6 58.5 71.3 23.2 54.0 28.0 68.1</cell><cell>62.1</cell><cell>70.0</cell><cell>55.0</cell><cell>38.4</cell><cell>58.0</cell><cell>39.9 38.4 48.3</cell><cell>51.7</cell></row><row><cell>FCL[32]</cell><cell cols="2">85.7 58.8 30.5 67.6 24.7</cell><cell>44.7</cell><cell cols="4">74.8 61.8 73.7 22.9 57.4 27.5 71.3</cell><cell>64.8</cell><cell>72.4</cell><cell>57.3</cell><cell>37.0</cell><cell>60.4</cell><cell>42.8 42.2 50.6</cell><cell>53.7</cell></row><row><cell>TP-BM[21]</cell><cell cols="2">83.4 62.2 26.4 71.8 18.2</cell><cell>49.5</cell><cell cols="4">66.5 63.8 73.4 19.0 56.6 35.7 69.3</cell><cell>61.3</cell><cell>71.7</cell><cell>69.2</cell><cell>39.1</cell><cell>66.3</cell><cell>44.8 35.9 45.5</cell><cell>53.8</cell></row><row><cell>AE-PSL[40]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Comparisons of weakly-supervised semantic segmentation methods on PASCAL VOC 2012 segmentation test set.</figDesc><table><row><cell>method</cell><cell>mAP r 0.25</cell><cell>mAP r 0.5</cell><cell>mAP r 0.75</cell><cell>ABO</cell></row><row><cell>PRM-VGG16 [43]</cell><cell>-</cell><cell>22.0</cell><cell>-</cell><cell>-</cell></row><row><cell>PRM-ResNet50 [43]</cell><cell>44.3</cell><cell>26.8</cell><cell>9.0</cell><cell>37.6</cell></row><row><cell>Label-PEnet</cell><cell>49.1</cell><cell>30.2</cell><cell>12.9</cell><cell>41.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Comparisons of weakly-supervised instance segmentation methods on Pascal VOC 2012 validation set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6541" to="6549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08258</idno>
		<title level="m">Weakly supervised cascaded convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weldon: Weakly supervised learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge: A retrospective. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="98" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-evidence filtering and fusion for multi-label classification, object detection and semantic segmentation based on weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1277" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Automated curriculum learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03003</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Curriculumnet: Weakly supervised learning from large-scale web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinglong</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Donggeun Yoo, and In So Kweon. Two-phase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Analyzing classifiers: Fisher vectors and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2912" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Webvision database: Visual learning and understanding from web data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="128" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boosting object proposals: From pascal to coco</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Tuset</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1546" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining bottom-up, top-down, and smoothness cues for weakly supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3529" to="3538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Built-in foreground/background prior for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatemehsadat</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Sadegh Ali</forename><surname>Akbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="413" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A robust approach for text detection from natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2906" to="2920" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly supervised region proposal network and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angtian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongluan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="352" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iteratively mining common object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1354" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weakly supervised instance segmentation using class peak response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3791" to="3800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
