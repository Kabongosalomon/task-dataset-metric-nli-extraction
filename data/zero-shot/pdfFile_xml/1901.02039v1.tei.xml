<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPHERICAL CNNS ON UNSTRUCTURED GRIDS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyu</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><forename type="middle">&amp;quot;</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Kashinath</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Prabhat Lawrence Berkeley Nat&apos;l Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">Berkeley</forename><surname>Nat&amp;apos;l Lab</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Prabhat Lawrence Berkeley Nat&apos;l Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Marcus</surname></persName>
							<affiliation key="aff3">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SPHERICAL CNNS ON UNSTRUCTURED GRIDS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an efficient convolution kernel for Convolutional Neural Networks (CNNs) on unstructured grids using parameterized differential operators while focusing on spherical signals such as panorama images or planetary signals. To this end, we replace conventional convolution kernels with linear combinations of differential operators that are weighted by learnable parameters. Differential operators can be efficiently estimated on unstructured grids using one-ring neighbors, and learnable parameters can be optimized through standard back-propagation. As a result, we obtain extremely efficient neural networks that match or outperform state-of-the-art network architectures in terms of performance but with a significantly smaller number of network parameters. We evaluate our algorithm in an extensive series of experiments on a variety of computer vision and climate science tasks, including shape classification, climate pattern segmentation, and omnidirectional image semantic segmentation. Overall, we (1) present a novel CNN approach on unstructured grids using parameterized differential operators for spherical signals, and <ref type="formula">(2)</ref> show that our unique kernel parameterization allows our model to achieve the same or higher accuracy with significantly fewer network parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A wide range of machine learning problems in computer vision and related areas require processing signals in the spherical domain; for instance, omnidirectional RGBD images from commercially available panorama cameras, such as Matterport , panaramic videos coupled with LIDAR scans from self-driving cars <ref type="bibr" target="#b17">(Geiger et al., 2013)</ref>, or planetary signals in scientific domains such as climate science <ref type="bibr" target="#b31">(Racah et al., 2017)</ref>. Unfortunately, naively mapping spherical signals to planar domains results in undesirable distortions. Specifically, projection artifacts near polar regions and handling of boundaries makes learning with 2D convolutional neural networks (CNNs) particularly challenging and inefficient. Very recent work, such as <ref type="bibr" target="#b10">Cohen et al. (2018)</ref> and <ref type="bibr" target="#b16">Esteves et al. (2018)</ref>, propose network architectures that operate natively in the spherical domain, and are invariant to rotations in the SO(3) group. Such invariances are desirable in a set of problems -e.g., machine learning problems of molecules -where gravitational effects are negligible and orientation is arbitrary. However, for other different classes of problems at large, assumed orientation information is crucial to the predictive capability of the network. A good example of such problems is the MNIST digit recognition problem, where orientation plays an important role in distinguishing digits "6" and "9". Other examples include omnidirectional images, where images are naturally oriented by gravity; and planetary signals, where planets are naturally oriented by their axis of rotation.</p><p>In this work, we present a new convolution kernel for CNNs on arbitrary manifolds and topologies, discretized by an unstructured grid (i.e., mesh), and focus on its applications in the spherical domain approximated by an icosahedral spherical mesh. We propose and evaluate the use of a new parameterization scheme for CNN convolution kernels, which we call Parameterized Differential Operators (PDOs), which is easy to implement on unstructured grids. We call the resulting convolution operator that operates on the mesh using such kernels the MeshConv operator. This parameterization scheme utilizes only 4 parameters for each kernel, and achieves significantly better performance Published as a conference paper at ICLR 2019  <ref type="figure">Figure 1</ref>: Illustration for the MeshConv operator using parameterized differential operators to replace conventional learnable convolutional kernels. Similar to classic convolution kernels that establish patterns between neighboring values, differential operators computes "differences", and a linear combination of differential operators establishes similar patterns.</p><formula xml:id="formula_0">? 0 ? +? 1 ? +? 2 ?<label>+?</label></formula><p>than competing methods, with much fewer parameters. In particular, we illustrate its use in various machine learning problems in computer vision and climate science.</p><p>In summary, our contributions are as follows:</p><p>? We present a general approach for orientable CNNs on unstructured grids using parameterized differential operators. ? We show that our spherical model achieves significantly higher parameter efficiency compared to state-of-the-art network architectures for 3D classification tasks and spherical image semantic segmentation. ? We release and open-source the codes developed and used in this study for other potential extended applications 1 .</p><p>We organize the structure of the paper as follows. We first provide an overview of related studies in the literature in Sec. 2; we then introduce details of our methodology in Sec. 3, followed by an empirical assessment of the effectiveness of our model in Sec. 4. Finally, we evaluate the design choices of our kernel parameterization scheme in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Spherical CNNs The first and foremost concern for processing spherical signals is distortions introduced by projecting signals on curved surfaces to flat surfaces. <ref type="bibr" target="#b35">Su &amp; Grauman (2017)</ref>  Reparameterized Convolutional Kernel Related to our approach in using parameterized differential operators, several works utilize the diffusion kernel for efficient Machine Learning and CNNs. <ref type="bibr" target="#b20">Kondor &amp; Lafferty (2002)</ref> was among the first to suggest the use of diffusion kernel on graphs. <ref type="bibr" target="#b1">Atwood &amp; Towsley (2016)</ref> propose Diffusion-Convolutional Neural Networks (DCNN) for efficient convolution on graph structured data. <ref type="bibr" target="#b4">Boscaini et al. (2016)</ref> introduce a generalization of classic CNNs to non-Euclidean domains by using a set of oriented anisotropic diffusion kernels. Cohen &amp; Welling (2016) utilized a linear combination of filter banks to acquire equivariant convolution filters. <ref type="bibr" target="#b33">Ruthotto &amp; Haber (2018)</ref> explore the reparameterization of convolutional kernels using parabolic and hyperbolic differential basis with regular grid images.</p><p>Non-Euclidean Convolutions Related to our work on performing convolutions on manifolds represented by an unstructured grid (i.e., mesh), works in geometric deep learning address similar problems <ref type="bibr" target="#b6">(Bronstein et al., 2017)</ref>. Other methods perform graph convolution by parameterizing the convolution kernels in the spectral domain, thus converting the convolution step into a spectral dot product <ref type="bibr" target="#b7">(Bruna et al., 2014;</ref><ref type="bibr" target="#b15">Defferrard et al., 2016;</ref><ref type="bibr" target="#b19">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b40">Yi et al., 2017)</ref>. <ref type="bibr" target="#b24">Masci et al. (2015)</ref> perform convolutions directly on manifolds using cross-correlation based on geodesic distances and <ref type="bibr" target="#b23">Maron et al. (2017)</ref> use an optimal surface parameterization method (seamless toric covers) to parameterize genus-zero shapes into 2D signals for analysis using conventional planar CNNs.</p><p>Image Semantic Segmentation Image semantic segmentation is a classic problem in computer vision, and there has been an impressive body of literature studying semantic segmentation of planar images <ref type="bibr" target="#b32">(Ronneberger et al., 2015;</ref><ref type="bibr" target="#b2">Badrinarayanan et al., 2015;</ref><ref type="bibr" target="#b22">Long et al., 2015;</ref><ref type="bibr" target="#b18">J?gou et al., 2017;</ref><ref type="bibr" target="#b37">Wang et al., 2018a)</ref>.  study semantic segmentation of equirectangular omnidirectional images, but in the context of image inpainting, where only a partial view is given as input.  and  provide benchmarks for semantic segmentation of 360 panorama images. In the 3D learning literature, researchers have looked at 3D semantic segmentation on point clouds or voxels <ref type="bibr" target="#b13">(Dai et al., 2017a;</ref><ref type="bibr" target="#b29">Qi et al., 2017a;</ref><ref type="bibr" target="#b38">Wang et al., 2018b;</ref><ref type="bibr" target="#b36">Tchapmi et al., 2017;</ref><ref type="bibr" target="#b14">Dai et al., 2017b)</ref>. Our method also targets the application domain of image segmentation by providing a more efficient convolutional operator for spherical domains, for instance, focusing on panoramic images .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PARAMETERIZED DIFFERENTIAL OPERATORS</head><p>We present a novel scheme for efficiently performing convolutions on manifolds approximated by a given underlying mesh, using what we call Parameterized Differential Operators. To this end, we reparameterize the learnable convolution kernel as a linear combination of differential operators. Such reparameterization provides two distinct advantages: first, we can drastically reduce the number of parameters per given convolution kernel, allowing for an efficient and lean learning space; second, as opposed to the cross-correlation type convolution on mesh surfaces <ref type="bibr" target="#b24">(Masci et al., 2015)</ref>, which requires large amounts of geodesic computations and interpolations, first and second order differential operators can be efficiently estimated using only the one-ring neighborhood.</p><p>In order to illustrate the concept of PDOs, we draw comparisons to the conventional 3?3 convolution kernel in the regular grid domain. The 3 ? 3 kernel parameterized by parameters ?: G 3?3 ? can be written as a linear combination of basis kernels which can be viewed as delta functions at constant offsets:</p><formula xml:id="formula_1">G 3?3 ? (x, y) = 1 i=?1 1 j=?1 ? ij ?(x ? i, y ? j)<label>(1)</label></formula><p>where x and y refer to the spatial coordinates that correspond to the two spatial dimensions over which the convolution is performed. Due to the linearity of the cross-correlation operator ( * ), the output feature map can be expressed as a linear combination of the input function cross-correlated with different basis functions. Defining the linear operator ? ij to be the cross-correlation with a basis delta function, we have:  <ref type="figure">Figure 2</ref>: Schematics for model architecture for classification and semantic segmentation tasks, at a level-5 input resolution. Ln stands for spherical mesh of level-n as defined in Sec. 3.2. MeshConv is implemented according to Eqn. 4. MeshConv T first pads unknown values at the next level with 0, followed by a regular MeshConv. DownSamp samples the values at the nodes in the next mesh level. A ResBlock with bottleneck layers, consisting of Conv1x1 (1-by-1 convolutions) and MeshConv layers is detailed above. In the decoder, ResBlock is after each MeshConv T and Concat.</p><formula xml:id="formula_2">? ij F(x, y) :=F(x, y) * ?(x ? i, y ? j)<label>(2)</label></formula><formula xml:id="formula_3">F(x, y) * G 3?3 ? (x, y) = 1 i=?1 1 j=?1 ? ij ? ij F(x, y)<label>(3)</label></formula><formula xml:id="formula_4">Dropout MLP L0 L1 L2 L3 L4 L5 L5 output signal MeshConv MeshConv T ResBlock MeshConv T ResBlock MeshConv T ResBlock MeshConv T ResBlock MeshConv T ResBlock Concat Concat Concat Concat Concat DownSamp ResBlock ResBlock(a, b, c) Conv1x1(a, b) Bn, ReLU MeshConv(b, b) Bn, ReLU Conv1x1(b, c) Bn ReLU Element-wise Addition Conv1x1(a, c) Bn</formula><p>In our formulation of PDOs, we replace the cross-correlation linear operators ? ij with differential operators of varying orders. Similar to the linear operators resulting from cross-correlation with basis functions, differential operators are linear, and approximate local features. In contrast to crosscorrelations on manifolds, differential operators on meshes can be efficiently computed using Finite Element basis, or derived by Discrete Exterior Calculus. In the actual implementation below, we choose the identity (I, 0th order differential, same as ? 00 ), derivatives in two orthogonal spatial dimensions (? x , ? y , 1st order differential), and the Laplacian operator (? 2 , 2nd order differential):</p><formula xml:id="formula_5">F(x, y) * G dif f ? = ? 0 IF + ? 1 ? x F + ? 2 ? y F + ? 3 ? 2 F<label>(4)</label></formula><p>The identity (I) of the input function is trivial to obtain. The first derivative (? x , ? y ) can be obtained by first computing the per-face gradients, and then using area-weighted average to obtain per-vertex gradient. The dot product between the per-vertex gradient value and the corresponding x and y vector fields are then computed to acquire ? x F and ? y F. For the sphere, we choose the eastwest and north-south directions to be the x and y components, since the poles naturally orient the spherical signal. The Laplacian operator on the mesh can be discretized using the cotangent formula:</p><formula xml:id="formula_6">? 2 F ? 1 2A i j?N (i) (cot ? ij + cot ? ij )(F i ? F j )<label>(5)</label></formula><p>where N (i) is the nodes in the neighboring one-ring of i, A i is the area of the dual face corresponding to node i, and ? ij and ? ij are the two angles opposing edge ij. With this parameterization of the convolution kernel, the parameters can be similarly optimized via backpropagation using standard stochastic optimization routines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ICOSAHEDRAL SPHERICAL MESH</head><p>The icosahedral spherical mesh <ref type="bibr" target="#b3">(Baumgardner &amp; Frederickson, 1985)</ref> is among the most uniform and accurate discretizations of the sphere. A spherical mesh can be obtained by progressively subdividing each face of the unit icosahedron into four equal triangles and reprojecting each node to unit distance from the origin. Apart from the uniformity and accuracy of the icosahedral sphere, the subdivision scheme for the triangles provides a natural coarsening and refinement scheme for the grid that allows for easy implementations of pooling and unpooling routines associated with CNN architectures. See <ref type="figure">Fig. 1</ref> for a schematic of the level-3 icosahedral spherical mesh.</p><p>For the ease of discussion, we adopt the following naming convention for mesh resolution: starting with the unit icosahedron as the level-0 mesh, each progressive mesh resolution is one level above the previous. Hence, for a level-l mesh:</p><formula xml:id="formula_7">n f = 20 ? 4 l ; n e = 30 ? 4 l ; n v = n e ? n f + 2<label>(6)</label></formula><p>where n f , n e , n v stands for the number of faces, edges, and vertices of the spherical mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MODEL ARCHITECTURE DESIGN</head><p>A detailed schematic for the neural architectures in this study is presented in <ref type="figure">Fig. 2</ref>. The schematic includes architectures for both the classification and regression network, which share a common encoder architecture. The segmentation network consists of an additional decoder which features transpose convolutions and skip layers, inspired by the U-Net architecture <ref type="bibr" target="#b32">(Ronneberger et al., 2015)</ref>. Minor adjustments are made for different tasks, mainly surrounding adjusting the number of input and output layers to process signals at varied resolutions. A detailed breakdown for model architectures, as well as training details for each task in the Experiment section (Sec. 4), is provided in the appendix (Appendix Sec. B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SPHERICAL MNIST</head><p>To validate the use of parameterized differential operators to replace conventional convolution operators, we implemented such neural networks towards solving the classic computer vision benchmark task: the MNIST digit recognition problem <ref type="bibr" target="#b21">(LeCun, 1998)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Setup</head><p>We follow Cohen et al. <ref type="formula" target="#formula_1">(2018)</ref> by projecting the pixelated digits onto the surface of the unit sphere. We further move the digits to the equator to prevent coordinate singularity at the poles. We benchmark our model against two other implementations of spherical CNNs: a rotational-invariant model by <ref type="bibr" target="#b10">Cohen et al. (2018)</ref> and an orientable model by <ref type="bibr" target="#b11">Coors et al. (2018)</ref>.</p><p>All models are trained and tested with non-rotated digits to illustrate the performance gain from orientation information.</p><p>Results and Discussion Our model outperforms its counterparts by a significant margin, achieving the best performance among comparable algorithms, with comparable number of parameters. We attribute the success in our model to the gain in orientation information, which is indispensable for many vision tasks. In contrast, <ref type="bibr">S2CNN (Cohen et al., 2018)</ref> is rotational-invariant, and thus has difficulties distinguishing digits "6" and "9".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">3D OBJECT CLASSIFICATION</head><p>We use the ModelNet40 benchmark <ref type="bibr" target="#b39">(Wu et al., 2015)</ref>, a 40-class 3D classification problem, to illustrate the applicability of our spherical method to a wider set of problems in 3D learning. For this study, we look into two aspects of our model: peak performance and parameter efficiency.  Experiment Setup To use our spherical CNN model for the object classification task, we preprocess the 3D geometries into spherical signals. We follow <ref type="bibr" target="#b10">Cohen et al. (2018)</ref> for preprocessing the 3D CAD models. First, we normalize and translate each mesh to the coordinate origin. We then encapsulate each mesh with a bounding level-5 unit sphere and perform ray-tracing from each point to the origin. We record the distance from the spherical surface to the mesh, as well as the sin, cos of the incident angle. The data is further augmented with the 3 channels corresponding to the convex hull of the input mesh, forming a total of 6 input channels. An illustration of the data preprocessing process is presented in <ref type="figure">Fig. 5</ref>. For peak performance, we compare the best performance achievable by our model with other 3D learning algorithms. For the parameter efficiency study, we progressively reduce the number of feature layers in all models without changing the overall model architecture. Then, we evaluate the models after convergence in 250 epochs. We benchmark our results against PointNet++ <ref type="bibr" target="#b29">(Qi et al., 2017a</ref><ref type="bibr">), VoxNet (Qi et al., 2016</ref>, and S2CNN 2 . <ref type="figure" target="#fig_1">Fig. 3</ref> shows a comparison of model performance versus number of parameters. Our model achieves the best performance across all parameter ranges. In the lowparameter range, our model is able to achieve approximately 60% accuracy for the 40-class 3D classification task with a mere 2000+ parameters. <ref type="table">Table 2</ref> shows a comparison of peak performance between models. At peak performance, our model is on-par with comparable state-of-the-art models, and achieves the best performance among models consuming spherical input signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">OMNIDIRECTIONAL IMAGE SEGMENTATION</head><p>We illustrate the semantic segmentation capability of our network on the omnidirectional image segmentation task. We use the Stanford 2D3DS dataset  for this task. The 2D3DS dataset consists of 1,413 equirectangular images with RGB+depth channels, as well as semantic labels across 13 different classes. The panoramic images are taken in 6 different areas, and the dataset is officially split for a 3-fold cross validation. While we are unable to find reported results on the semantic segmentation of these omnidirectional images, we benchmark our spherical segmentation algorithm against classic 2D image semantic segmentation networks as well as a 3D point-based model, trained and evaluated on the same data.  <ref type="figure">Figure 5</ref>: Illustration of spherical signal rendering process for a given 3D CAD model.</p><p>Experiment Setup First, we preprocess the data into a spherical signal by sampling the original rectangular images at the latitude-longitudes of the spherical mesh vertex positions. Input RGB-D channels are interpolated using bilinear interpolation, while semantic labels are acquired using nearest-neighbor interpolation. We input and output spherical signals at the level-5 resolution. We use the official 3-fold cross validation to train and evaluate our results. We benchmark our semantic segmentation results against two classic semantic segmentation networks: the U-Net <ref type="bibr" target="#b32">(Ronneberger et al., 2015)</ref> and FCN8s <ref type="bibr" target="#b22">(Long et al., 2015)</ref>. We also compared our results with a modified version of spherical S2CNN, and 3D point-based method, PointNet++ (Qi et al., 2017b) using (x, y, z,r,g,b) inputs reconstructed from panoramic RGBD images. We provide additional details toward the implementation of these models in Appendix E. We evaluate the network performance under two standard metrics: mean Intersection-over-Union (mIoU), and pixel-accuracy. Similar to Sec. 4.2, we evaluate the models under two settings: peak performance and a parameter efficiency study by varying model parameters. We progressively decimate the number of feature layers uniformly for all models to study the effect of model complexity on performance. <ref type="figure" target="#fig_2">Fig. 4</ref> compares our model against state-of-the-art baselines. Our spherical segmentation outperforms the planar baselines for all parameter ranges, and more significantly so compared to the 3D PointNet++. We attribute PointNet++'s performance to the small amount of training data. <ref type="figure">Fig. 6</ref> shows a visualization of our semantic segmentation performance compared to the ground truth and the planar baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">CLIMATE PATTERN SEGMENTATION</head><p>To further illustrate the capabilities of our model, we evaluate our model on the climate pattern segmentation task. We follow <ref type="bibr" target="#b26">Mudigonda et al. (2017)</ref> for preprocessing the data and acquiring the ground-truth labels for this task. This task involves the segmentation of Atmospheric Rivers (AR) and Tropical Cyclones (TC) in global climate model simulations. Following <ref type="bibr" target="#b26">Mudigonda et al. (2017)</ref>, we analyze outputs from a 20-year run of the Community Atmospheric Model v5 (CAM5) <ref type="bibr" target="#b27">(Neale et al., 2010)</ref>. We benchmark our performance against <ref type="bibr" target="#b26">Mudigonda et al. (2017)</ref>    <ref type="figure">Figure 6</ref>: Visualization of semantic segmentation results on test set. Our results are generated on a level-5 spherical mesh and mapped to the equirectangular grid for visualization. Model underperforms in complex environments, and fails to predict ceiling lights due to incomplete RGB inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>Segmentation accuracy is presented in <ref type="table" target="#tab_5">Table 3</ref>. Our model achieves better segmentation accuracy as compared to the baseline models. The baseline model <ref type="bibr" target="#b26">(Mudigonda et al., 2017)</ref> trains and tests on random crops of the global data, whereas our model inputs the entire global data and predicts at the same output resolution as the input. Processing full global data allows the network to acquire better holistic understanding of the information, resulting in better overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ABLATION STUDY</head><p>We further perform an ablation study for justifying the choice of differential operators for our convolution kernel (as in Eqn. 4). We use the ModelNet40 classification problem as a toy example and use a 250k parameter model for evaluation. We choose various combinations of differential operators, and record the final classification accuracy. Results for the ablation study is presented in  <ref type="table" target="#tab_6">Table 4</ref>: Results for the ablation study. The choice of kernel that includes all differential operator components achieve the best accuracy, validating our choice of kernel in Eqn. 4. among other choices, and the network performance improves with increased differential operators, thus allowing for more degrees of freedom for the kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have presented a novel method for performing convolution on unstructured grids using parameterized differential operators as convolution kernels. Our results demonstrate its applicability to machine learning problems with spherical signals and show significant improvements in terms of overall performance and parameter efficiency. We believe that these advances are particularly valuable with the increasing relevance of omnidirectional signals, for instance, as captured by real-world 3D or LIDAR panorama sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A ADDITIONAL DETAILS FOR IMPLEMENTING MESHCONV OPERATOR</head><p>In this section we provide more mathematical details for the implementation of the MeshConv Operator as described in Sec 3.1. In particular, we will describe in details the implementation of the various differential operators in Eqn. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity Operator</head><p>The identity operator as suggested by its definition is just the identical replica of the original signal, hence no additional computation is required other than using the original signal as is.</p><p>Gradient Operator Using a triangular mesh for discretizing the surface manifold, scalar functions on the surface can be discritized as a piecewise linear function, where values are defined on each vertex. Denoting the spatial coodinate vector as x, the scalar function as f (x), the scalar function values stored on vertex i as f i , and the piecewise linear "hat" functions as ? i (x), we have:</p><formula xml:id="formula_8">f (x) = n i=1 ? i (x)f i<label>(7)</label></formula><p>the piecewise linear basis function ? i (x) takes the value of 1 on vertex i and takes the value of 0 on all other vertices. Hence, the gradient of this piecewise linear function can be computed as:</p><formula xml:id="formula_9">?f (x) = ? n i ? i (x)f i = n i=1 ?? i (x)f i<label>(8)</label></formula><p>Due to the linearity of the basis functions ? i , the gradient is constant within each individual triangle. The per-face gradient value can be computed with a single linear operator G on the per-vertex scalar function f :</p><formula xml:id="formula_10">?f (f ) = Gf (v)<label>(9)</label></formula><p>where the resulting per-face gradient is a 3-dimensional vector. We use the superscript (f ), (v) to distinguish per-face and per-vertex quantities. We refer the reader to <ref type="bibr" target="#b5">Botsch et al. (2010)</ref> for detailed derivations for the gradient operator G. Denoting the area of each face as a (f ) (which can be easily computed using Heron's formula given coordinates of three points), the resulting per-vertex gradient vector can be computed as an average of per-face gradients, weighted by face area:</p><formula xml:id="formula_11">?f (v) i = j?N (i) a (f ) j ?f (f ) j j?N (i) a (f ) j<label>(10)</label></formula><p>denote the per-vertex polar (east-west) and azimuthal (north-south) direction fields asx <ref type="bibr">(v)</ref> and? <ref type="bibr">(v)</ref> . They can be easily computed using the gradient operators detailed above, with the longitudinal and latitudinal values as the scalar function, followed by normalizing each vector to unit length. Hence two per-vertex gradient components can be computed as a dot-product against the unit directional fields:</p><formula xml:id="formula_12">? x f (v) = ?f (v) ?x (v) (11) ? y f (v) = ?f (v) ?? (v)<label>(12)</label></formula><p>Laplacian Operator The mesh Laplacian operator is a standard operator in computational and differential geometry. We consider its derivation beyond the scope of this study. We provide the cotangent formula for computing the mesh Laplacian in Eqn. 5. We refer the reader to Chapters 6.2 and 6.3 of <ref type="bibr" target="#b12">Crane (2015)</ref> for details of the derivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation Meaning</head><p>MeshConv(a, b) Mesh convolution layer with a input channels and producing b output channels MeshConv(a, b) T Mesh transpose convolution layer with a input channels and producing b output channels. BN Batch Normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReLU</head><p>Rectified Linear Unit activation function DownSamp</p><p>Downsampling spherical signal at the next resolution level. ResBlock(a, b, c) As illustrated in <ref type="figure">Fig. 1, where a, b</ref>, c stands for input channels, bottle neck channels, and output channels.</p><formula xml:id="formula_13">[ ] Li</formula><p>The layers therein is at a mesh resolution of Li.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat</head><p>Concatenate skip layers of same resolution. Training details We train our network with a batch size of 16, initial learning rate of 1 ? 10 ?2 , step decay of 0.5 per 10 epochs, and use the Adam optimizer. We use the cross-entropy loss for training the classification network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 3D OBJECT CLASSIFICATION</head><p>Architecture The input signal is at a level-5 resolution. The network architecture closely follows that in the schematics in <ref type="figure">Fig. 2</ref>. We present two network architectures, one that corresponds to the network architecture with the highest accuracy score (the full model), and another that scales well with low parameter counts (the lean model Training details We train our network with a batch size of 16, initial learning rate of 5 ? 10 ?3 , step decay of 0.7 per 25 epochs, and use the Adam optimizer. We use the cross-entropy loss for training the classification network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 OMNIDIRECTIONAL IMAGE SEGMENTATION</head><p>Architecture Input signal is sampled at a level-5 resolution. The network architecture is identical to the segmentation network in <ref type="figure">Fig. 2</ref> Training details Note that the number of output channels is 15, since the 2D3DS dataset has two additional classes (invalid and unknown) that are not evaluated for performance. We train our network with a batch size of 16, initial learning rate of 1 ? 10 ?2 , step decay of 0.7 per 20 epochs, and use the Adam optimizer. We use the weighted cross-entropy loss for training. We weight the loss for each class using the following weighting scheme:</p><formula xml:id="formula_14">w c = 1 1.02 + log(f c )<label>(13)</label></formula><p>where w c is the weight corresponding to class c, and f c is the frequency by which class c appears in the training set. We use zero weight for the two dropped classes (invalid and unknown).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 CLIMATE PATTERN SEGMENTATION</head><p>Architecture We use the same network architecture as the Omnidirectional Image Segmentation task in Sec. B.3. Minor difference being that all feature layers are cut by 1/4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Total number of parameters: 328339</head><p>Training details We train our network with a batch size of 256, initial learning rate of 1 ? 10 ?2 , step decay of 0.4 per 20 epochs, and use the Adam optimizer. We train using weighted cross-entropy loss, using the same weighting scheme as in Eqn. 13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C DETAILED STATISTICS FOR 2D3DS SEGMENTATION</head><p>We provide detailed statistics for the 2D3DS semantic segmentation task. We evaluate our model's per-class performance against the benchmark models. All statistics are mean over 3-fold cross validation.     <ref type="figure">Figure 8</ref>: Comparison of runtime for our model and the only other model of comparable peak accuracy: PointNet++. Our model achieves a significant speed gain (5x) over the baseline in high accuracy regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D RUNTIME ANAYLSIS</head><p>To further evaluate our model's runtime gain, we record the runtime of our model in inference mode. We tabulate the runtime of our model (across a range of model sizes) in <ref type="table" target="#tab_13">Table 8</ref>. We also compare our runtime with PointNet++, whose best performing model achieves a comparable accuracy. Inference is performed on a single NVIDIA GTX 1080 Ti GPU. We use a batch size of 8, and take the average runtime in 64 batches. Runtime for the first batch is disregarded due to extra initialization time. We observe that our model achieves fast and stable runtime, even with increased parameters, possibly limited by the serial computations due to network depth. We achieve a significant speedup compared to our baseline (PointNet++) of nearly 5x, particularly closer to the high-accuracy regime. A frame rate of over 339 fps is sufficient for real-time applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E IMPLEMENTION DETAILS FOR SEGMENTATION BASELINES</head><p>We provide further details for implementing the baseline models in the semantic segmentation task.</p><p>FCN8S and U-Net The two planar models are slightly modified by changing the first convolution to take in 4 channels (RGBD) instead of 3. No additional changes are made to the model, and the models are trained from scratch. We use the available open source implementation 3 for the two models.</p><p>PointNet++ We use the official implementation of PointNet++ 4 , and utilize the same code for the examplar ScanNet task. The number of points we use is 8192, same as the number of points used for the ScanNet task. We perform data-augmentation by rotating around the z-axis and take sub-regions for training.</p><p>S2CNN S2CNN was not initially designed and evaluated for semantic segmentation tasks. However, we provide a modified version of the S2CNN model for comparison. To produce scalar fields as outputs, we perform average pooling of the output signal only in the gamma dimension. Also since no transpose convolution operator is defined for S2CNN, we maintain its bandwidth of 64 throughout the network. The current implementations are not particularly memory efficient, hence we were only able to fit in low-resolution images of tiny batch sizes of 2 per GPU. Architecture overview:</p><p>[S2Conv(4, 64) +BN+ReLU] b64 + [SO3Conv(64, 15)] b64 + AvgPoolGamma</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Parameter efficiency study on Model-Net40, benchmarked against representative 3D learning models consuming different input data representations: PointNet++ using point clouds as input, VoxNet consuming binary-voxel inputs, S2CNN consuming the same input structure as our model (spherical signal). The abscissa is drawn based on log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Parameter efficiency study on 2D3DS semantic segmentation task. Our spherical segmentation model outperforms the planar and point-based counterparts by a significant margin across all parameter regimes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of segmentation for Atmospheric River (AR). Plotted in the background is Integrated Vapor Transport (IVT), whereas red masks indicates the existance of AR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>process equirectangular images with regular convolutions with increased kernel sizes near polar regions where greater distortions are introduced by the planar mapping. Coors et al. (2018) and Zhao et al. (2018) use a constant kernel that samples points on the tangent plane of the spherical image to reduce distortions. A slightly different line of literature explores rotational-equivariant implementations of spherical CNNs. Cohen et al. (2018) proposed spherical convolutions with intermediate feature maps in SO(3) that are rotational-equivariant. Esteves et al. (2018) used spherical harmonic basis to achieve similar results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on the Spherical MNIST dataset for validating the use of Parameterized Differential Operators. Our model achieves state-of-the-art performance with comparable number of training parameters.</figDesc><table><row><cell>Model</cell><cell cols="2">Accuracy(%) Number of Parameters</cell></row><row><cell>S2CNN (Cohen et al., 2018)</cell><cell>96.00</cell><cell>58k</cell></row><row><cell>SphereNet (Coors et al., 2018)</cell><cell>94.41</cell><cell>196k</cell></row><row><cell>Ours</cell><cell>99.23</cell><cell>62k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>for the climate segmentation task to highlight our model performance. We preprocess the data to level-5 resolution.</figDesc><table><row><cell>Model</cell><cell cols="4">Background (%) TC (%) AR (%) Mean (%)</cell></row><row><cell>Mudigonda et al. (2017)</cell><cell>97</cell><cell>74</cell><cell>65</cell><cell>78.67</cell></row><row><cell>Ours</cell><cell>97</cell><cell>94</cell><cell>93</cell><cell>94.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>We achieves better accuracy compared to our baseline for climate pattern segmentation.</figDesc><table><row><cell>FCN8s</cell><cell>UNet</cell><cell>Ours</cell><cell>Ground Truth</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Legend:</cell></row><row><cell></cell><cell></cell><cell></cell><cell>beam</cell></row><row><cell></cell><cell></cell><cell></cell><cell>board</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bookcase</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ceiling</cell></row><row><cell></cell><cell></cell><cell></cell><cell>chair</cell></row><row><cell></cell><cell></cell><cell></cell><cell>clutter</cell></row><row><cell></cell><cell></cell><cell></cell><cell>column</cell></row><row><cell></cell><cell></cell><cell></cell><cell>door</cell></row><row><cell></cell><cell></cell><cell></cell><cell>floor</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sofa</cell></row><row><cell></cell><cell></cell><cell></cell><cell>table</cell></row><row><cell></cell><cell></cell><cell></cell><cell>wall</cell></row><row><cell></cell><cell></cell><cell></cell><cell>window</cell></row><row><cell></cell><cell></cell><cell></cell><cell>?? Failure</cell></row><row><cell></cell><cell></cell><cell></cell><cell>mode</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Our choice of differential operator combinations in Eqn. 4 achieves the best performance</figDesc><table><row><cell cols="2">Convolution kernel Accuracy</cell></row><row><cell>I + ? ?y + ? 2</cell><cell>0.8748</cell></row><row><cell>I + ? ?x + ? 2 I + ? 2</cell><cell>0.8809 0.8801</cell></row><row><cell>I + ? ?x + ? ?y</cell><cell>0.8894</cell></row><row><cell>I + ? ?x + ? ?y + ? 2</cell><cell>0.8979</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Network architecture notation list</cell></row><row><cell>B NETWORK ARCHITECTURE AND TRAINING DETAILS</cell></row><row><cell>In this section we provide detailed network architecture and training parameters for reproducing our</cell></row><row><cell>results in Sec. 4. We use Fig. 2 as a reference.</cell></row><row><cell>B.1 SPHERICAL MNIST</cell></row><row><cell>Architecture Since the input signal for this experiment is on a level-4 mesh, the input pathway is</cell></row><row><cell>slightly altered. The network architecture is as follows:</cell></row><row><cell>[MeshConv(1,16) + BN + ReLU] L4 + [DownSamp + ResBlock(16, 16, 64)] L3 + [DownSamp +</cell></row><row><cell>ResBlock(64, 64, 256)] L2 + AvgPool + MLP(256, 10)</cell></row><row><cell>Total number of parameters: 61658</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Per-class accuracy comparison with baseline models</figDesc><table><row><cell>Model</cell><cell>Mean</cell><cell>beam</cell><cell cols="3">board bookcase ceiling</cell><cell>chair</cell><cell>clutter column</cell><cell>door</cell><cell>floor</cell><cell>sofa</cell><cell>table</cell><cell>wall</cell><cell>window</cell></row><row><cell>UNet</cell><cell cols="3">0.3587 0.0853 0.2721</cell><cell>0.3072</cell><cell cols="8">0.7857 0.3531 0.2883 0.0487 0.3377 0.8911 0.0817 0.3851 0.5878 0.2392</cell></row><row><cell>FCN8s</cell><cell cols="3">0.3560 0.0572 0.3139</cell><cell>0.2894</cell><cell cols="8">0.7981 0.3623 0.2973 0.0353 0.4081 0.8884 0.0263 0.3809 0.5849 0.1859</cell></row><row><cell cols="4">PointNet++ 0.2312 0.0909 0.1503</cell><cell>0.2210</cell><cell cols="8">0.4775 0.2981 0.1610 0.0782 0.1866 0.4426 0.1844 0.3332 0.3061 0.0755</cell></row><row><cell>Ours</cell><cell cols="3">0.3829 0.0869 0.3268</cell><cell>0.3344</cell><cell cols="8">0.8216 0.4197 0.2562 0.1012 0.4159 0.8702 0.0763 0.4170 0.6167 0.2349</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Mean IoU comparison with baseline models</figDesc><table><row><cell># of parameters</cell><cell>2558</cell><cell>6862</cell><cell cols="2">20828 70192 254648 960056 3737160</cell></row><row><cell>Runtime (ms)</cell><cell cols="3">2.9458 2.9170 2.8251 2.9222 3.0618 3.0072</cell><cell>2.9476</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Runtime analysis for classification network (as in ModelNet40 experiment in Sec. 4.2)</figDesc><table><row><cell></cell><cell>14</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>12</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Runtime (ms)</cell><cell>6 8 10</cell><cell cols="2">PointNet++</cell><cell></cell><cell>5x Speed Gain</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell>Ours</cell></row><row><cell></cell><cell>2</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our codes are available on Github: https://github.com/maxjiang93/ugscnn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use the author's open-source implementations: PointNet++, VoxNet, S2CNN. We run PointNet++ with standard input of 1000 points with xyz coordinates for the classification task.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/zijundeng/pytorch-semantic-segmentation 4 https://github.com/charlesq34/pointnet2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank <ref type="table">Taco</ref>  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Icosahedral discretization of the two-sphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baumgardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul O Frederickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1107" to="1115" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodol?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3189" to="3197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Polygon mesh processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Kobbelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Alliez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>L?vy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AK Peters/CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR2014)</title>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06158</idno>
		<title level="m">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08498</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Steerable cnns. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spherical CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Khler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hkbd5xZRb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spherenet: Learning spherical representations for detection and classification in omnidirectional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Coors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Paul Condurache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="518" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Discrete differential geometry: An applied introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenan</forename><surname>Crane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scancomplete: Large-scale scene completion and semantic segmentation for 3d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Bokeloh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning so (3) equivariant representations with spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allec-Blanchette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1175" to="1183" />
		</imprint>
	</monogr>
	<note>2017 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffusion kernels on graphs and other discrete structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Risi Imre Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on machine learning</title>
		<meeting>the 19th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2002</biblScope>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on surfaces via seamless toric covers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meirav</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miri</forename><surname>Aigerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">71</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Segmenting and tracking extreme climate events using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayur</forename><surname>Mudigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sookyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Mahesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincen</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Travis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mr</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prabhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chieh</forename><surname>Richard B Neale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gettelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsu</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolando</forename><surname>Conley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Kinnison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lamarque</surname></persName>
		</author>
		<title level="m">NCAR Tech. Note NCAR/TN-486+ STR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>et al. Description of the ncar community atmosphere model (cam 5.0</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Extremeweather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Racah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mr</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3402" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04272</idno>
		<title level="m">Deep neural networks motivated by partial differential equations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04569</idno>
		<title level="m">Extrapolating 360 structure and semantics beyond the field of view</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning spherical convolution for fast features from 360 imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyne</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="537" to="547" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Xiaodi Hou, and Garrison Cottrell. Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1451" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6584" to="6592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distortion-aware cnns for spherical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yike</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1198" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
