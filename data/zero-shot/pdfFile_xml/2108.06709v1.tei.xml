<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
							<email>qiangenx@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<addrLine>California 2 Waymo</addrLine>
									<region>LLC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
							<email>yinzhou@waymo.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<addrLine>California 2 Waymo</addrLine>
									<region>LLC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
							<email>weiyuewang@waymo.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<addrLine>California 2 Waymo</addrLine>
									<region>LLC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<addrLine>California 2 Waymo</addrLine>
									<region>LLC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
							<email>dragomir@waymo.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern</orgName>
								<address>
									<addrLine>California 2 Waymo</addrLine>
									<region>LLC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SPG: Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Our Semantic Point Generation (SPG) recovers the foreground regions by generating semantic points (red). Combined with the original cloud, these semantic points can be directly used by modern LiDAR-based detectors and help improve the detection results (green boxes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In autonomous driving, a LiDAR-based object detector should perform reliably at different geographic locations and under various weather conditions. While recent 3D detection research focuses on improving performance within a single domain, our study reveals that the performance of modern detectors can drop drastically cross-domain. In this paper, we investigate unsupervised domain adaptation (UDA) for LiDAR-based 3D object detection. On the Waymo Domain Adaptation [54] dataset, we identify the deteriorating point cloud quality as the root cause of the performance drop. To address this issue, we present Semantic Point Generation (SPG), a general approach to enhance the reliability of LiDAR detectors against domain shifts. Specifically, SPG generates semantic points at the predicted foreground regions and faithfully recovers missing parts of the foreground objects, which are caused by phenomena such as occlusions, low reflectance or weather interference. By merging the semantic points with the original points, we obtain an augmented point cloud, which can be directly con- ? Work done during internship at Waymo LLC. sumed by modern LiDAR-based detectors. To validate the wide applicability of SPG, we experiment with two representative detectors, PointPillars [25] and PV-RCNN [49]. On the UDA task, SPG significantly improves both detectors across all object categories of interest and at all difficulty levels. SPG can also benefit object detection in the original domain. On the Waymo Open Dataset [54] and KITTI [18], SPG improves 3D detection results of these two methods across all categories. Combined with PV-RCNN [49], SPG achieves state-of-the-art 3D detection results on KITTI.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A robust autonomous driving system requires its LiDAR-based detector to reliably handle different environmental conditions, e.g., geographic locations and weather conditions. While 3D detection has received increasing interest in recent years, most existing works <ref type="bibr" target="#b79">[79,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b65">65,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b78">78]</ref>   varies significantly. In this paper, we address the domain gap caused by the deteriorating point cloud quality and aim to improve 3D object detection in the setting of unsupervised domain adaptation (UDA). We use the Waymo Domain Adaptation dataset <ref type="bibr" target="#b54">[54]</ref> to analyze the domain gap and introduce semantic point generation (SPG), a general approach to enhance the reliability of LiDAR detectors against domain shift. SPG is able to improve detection quality in both the target domain and the source domain and can be naturally combined with modern LiDAR-based detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Understanding the Domain Gap</head><p>Waymo Open Dataset (OD) is mainly collected in California and Arizona, and Waymo Kirkland Dataset (Kirk) <ref type="bibr" target="#b54">[54]</ref> is collected in Kirkland. We consider OD as the source domain and Kirk as the target domain. To understand the possible domain gap, we take a PointPillars <ref type="bibr" target="#b25">[25]</ref> model trained on the OD training set and compare its 3D vehicle detection performance on OD validation set and those on Kirk validation set. We observe a drastic performance drop of 21.8 points in 3D average precision (AP) (see <ref type="table" target="#tab_0">Table 1</ref>).</p><p>We first confirm that there is no significant difference in object size between two domains. Then by investigating the meta data in the datasets, we find that only 0.5% of Li-DAR frames in OD are collected under rainy weather, but almost all frames in Kirk share the rainy weather attribute. To rule out other factors, we extract all dry weather frames in Kirk training set and form a "Kirk Dry" dataset. Because the the rain drop changes the surface property of objects, there are twice amount of missing LiDAR points per frame in Kirk validation set than in OD or Kirk Dry (see <ref type="table" target="#tab_0">Table 1</ref>). As a result, vehicles in Kirk receive around 27% fewer Li-DAR point observations than those in OD (see statistics and more details in the supplemental). In <ref type="figure" target="#fig_0">Figure 2</ref>, we visualize two range images from OD and Kirk, respectively. We can observe that in the rainy weather, a significant number of points are missing and the distribution of missing points is more irregular compared to the dry weather.</p><p>To conclude, the major domain gap between OD and Kirk is the deteriorating point cloud quality, which is caused by the rainy weather condition. In the target domain, we name this phenomenon as the "missing point" problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Previous Methods to Address the Domain Gap</head><p>Multiple studies propose to align the features across domains. Most of them focus on 2D tasks <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b13">14]</ref> or object-level 3D tasks <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b45">45]</ref>. Applying feature alignment <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b35">35]</ref> requires a redesign of the model or loss of a detector. Our goal is to seek a general solution to benefit recently reported LiDAR-based detectors <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b79">79,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b19">19]</ref>.</p><p>Another direction is to apply transformations to the data from one domain to match the data from another domain. A naive approach is to randomly down-sample the point cloud but this not only fails to satisfactorily simulate the pattern of missing points ( <ref type="figure" target="#fig_0">Figure 2d</ref>) but also hurts the performance on the source domain. Another approach is to up-sample the point cloud <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b28">28]</ref> in the target domain, which can increase point density around observed regions. However, those methods have a limited capability in recovering the 3D shape of very partially observed objects. Moreover, upsampling the entire point cloud will lead to a significantly higher latency. A third approach is to leverage style transfer techniques: <ref type="bibr" target="#b80">[80,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b47">47]</ref> render point clouds as 2D pseudo images and enforce the renderings from different domains to be resemblant in style. However, these methods introduce an information bottleneck during rasterization <ref type="bibr" target="#b79">[79]</ref> and they are not applicable to modern pointbased 3D detectors <ref type="bibr" target="#b49">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">SPG for Closing the Domain Gap</head><p>The "missing point" problem deteriorates the point cloud quality and reduces the number of point observations, thus undermining the detection performance. To address this issue, we propose Semantic Point Generation (SPG). Our approach aims to learn the semantic information of the point cloud and performs foreground region prediction to identify voxels that are inside foreground objects. Based on the predicted foreground voxels, SPG generates points to recover the foreground regions. Since these points are discriminatively generated at foreground objects, we denote them by semantic points. These semantic points are merged with the original points into an augmented point cloud, which is then fed to a 3D detector.</p><p>The contributions of this paper are two-fold: 1. We present an in-depth analysis of unsupervised domain adaptation (UDA) for LiDAR 3D detectors across different geographic locations and weather conditions. Our study reveals that the rainy weather can severely deteriorate the quality of LiDAR point clouds and lead to drastic performance drop for modern detectors. 2. We propose semantic point generation (SPG). To our best knowledge, it is the first learning-based model that targets UDA for point cloud 3D detection. Specifically, SPG has the following merits: ? SPG can generate semantic points that faithfully recover the foreground regions suffering from the "missing point" problem. SPG can significantly improve performance over poor-quality point clouds in the target domain while also benefiting source domain, for representative 3D detectors, including PointPillars <ref type="bibr" target="#b25">[25]</ref> and PV-RCNN <ref type="bibr" target="#b49">[49]</ref>. ? SPG also improves the performance for the general 3D object detection task. We verify its effectiveness on KITTI <ref type="bibr" target="#b18">[18]</ref> for the aforementioned 3D detectors. ? SPG is a general approach and can be easily combined with modern off-the-shelf LiDAR-based detectors. ? Our approach is light-weight and efficient. Introducing less than 6% additional points, SPG only adds a marginal complexity to a 3D detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unsupervised Domain Adaptation</head><p>Unsupervised domain adaptation (UDA) aims to generalize a model to a novel (target) domain by using label information only from the source domain. The two domains are generally related, but there exists a distribution shift (domain gap). Most methods focus on learning aligned feature representations across domains. To reach this goal, <ref type="bibr" target="#b1">[2]</ref> proposes Maximum Mean Discrepancy (MMD) while <ref type="bibr" target="#b38">[38]</ref> proposes Transfer Component Analysis (TCA). <ref type="bibr" target="#b33">[33]</ref> designs a Joint Distribution Adaptation to close the distribution shift while <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b34">34]</ref> utilize a shared Hilbert space. Without using explicit distance measures, deep learning models <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46]</ref> use adversarial training to get indistinguishable features between domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Domain Adaptation for 2D Detection</head><p>The object detection task is sensitive to local geometric features. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">20]</ref> hierarchically align the features between domains. Most of these works focus on UDA for 2D detection. With the current advances of unpaired style transfer methods <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b80">80]</ref>, studies such as <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b21">21]</ref> translate the image from source domain to target domain or vice versa.</p><p>Unsupervised Domain Adaptation for 3D Tasks Most of the UDA methods focus on 2D tasks, only a few stud-ies explore the UDA in 3D. <ref type="bibr" target="#b77">[77,</ref><ref type="bibr" target="#b45">45]</ref> align the global and local features for object-level tasks. To reduce the sparsity, <ref type="bibr" target="#b59">[59]</ref> projects the point cloud to 2D view, while <ref type="bibr" target="#b47">[47]</ref> projects the point cloud to birds-eye view (BEV). <ref type="bibr" target="#b14">[15]</ref> creates a car model set and adapts their features to the detection object features. However, this study targets general car 3D detection on a single point cloud domain. <ref type="bibr" target="#b57">[57]</ref> is the first published study targeting UDA for 3D LiDAR detection. They identify the vehicle size as the domain gap between KITTI <ref type="bibr" target="#b18">[18]</ref> and other datasets. So they resize the vehicles in the data. In contrast, we identify the point cloud quality as the major domain gap between Waymo's two datasets <ref type="bibr" target="#b54">[54]</ref>. We use a learning-based approach to close the domain gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Point Cloud Transformation</head><p>One way to improve point cloud quality is to suitably transform the point cloud. Studies of point cloud upsampling <ref type="bibr" target="#b73">[73,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b28">28]</ref> can transfer a low density point cloud to a high density one. However, they need high density point cloud ground truth during training. These networks can densify the point cloud in the observed regions. But in our case, we also need to recover regions with no point observation, caused by "missing points".</p><p>Point cloud completion networks <ref type="bibr" target="#b74">[74,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b61">61]</ref> aim to complete the point cloud. Specialized in object-level completion, these models assume a single object has been manually located and the input only consists of the points on this object. Therefore, these models do not fit our purpose of object detection. Point cloud style transfer models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref> can transfer the color theme and the object-level geometric style for the point cloud. However, these models do not focus on preserving local details with high-fidelity. Therefore, their transformation cannot directly help 3D detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semantic Point Generation</head><p>In the input point cloud P C raw = {p 1 , p 2 , ..., p N } ? R 3+F , each point has three channels of xyz and F properties (e.g., intensity, elongation). <ref type="figure">Figure 3</ref> illustrates the SPG-aided 3D detection pipeline. SPG takes raw point cloud P C raw as input and generates a set of semantic points in the predicted foreground regions. Then, these semantic points are combined with the original point cloud into an augmented point cloud P C aug , which is fed into a point cloud detector to obtain object detection results.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, SPG voxelizes P C raw into an evenly spaced 3D voxel grid, and learns the point cloud semantics for these voxels. For each voxel, the network predicts the probability confidenceP f of it being a foreground voxel (contained in a foreground object bounding box). In each foreground voxel, the network generates a semantic pointsp with point features? = [?,f ].? ? R 3 is the xyz coordinate ofsp andf ? R F is the point properties.</p><p>To faithfully recover the foreground regions of the ob-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Detector Foreground Generation</head><p>Probability Thresholding</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Point Cloud</head><p>Augmented Point Cloud Detection Result <ref type="figure">Figure 3</ref>: Illustration of SPG-aided 3D detection pipeline. SPG voxelizes the entire point cloud and generates prediction for each voxel (both occupied and empty) within the generation areas. After applying a probability thresholding, we take the top voxels with highest foreground probability and add a semantic point (red) at the predicted location in each of these voxels. These points are merged with the original point cloud and fed into the selected 3D point cloud detector.</p><p>served objects, we define a generation area. Only voxels occupied or neighbored by the observed points are considered within the generation area. We also filter out semantic points withP f less than P thresh , then take K semantic points {sp 1 ,sp 2 , ...,sp K } with the highestP f and merge them with the original point cloud P C raw to get P C aug . In practice, we use P thresh = 0.5.</p><p>To enable SPG to be directly used by modern LiDARbased detectors, we encode the augmented point cloud P C aug as {p 1 ,p 2 , ...,p N ,sp 1 ,sp 2 , ...,sp K } ? R 3+F +1 . Here we add another property channel to each point, indicating the confidence in foreground prediction:P f is used for the semantic points, and 1.0 for the original raw points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training Targets</head><p>To train SPG, we need to create two supervisions: 1) y f , the class label if a voxel (either occupied or empty) is a foreground voxel, which supervisesP f ; 2) ? ? R 3+F , the regression target for semantic point features?.</p><p>As visualized in <ref type="figure" target="#fig_2">Figure 4</ref>, we mark a point as a foreground point if it is inside an object bounding box. Voxels contained in a foreground bounding box are marked as foreground voxels V f . For voxel v i , we assign</p><formula xml:id="formula_0">y f i = 1 if v i ? V f and y f i = 0 otherwise. If v i is an occupied fore- ground voxel, we set ? i = [? i ,f i ] as the regression target, where? i ? R 3 is the centroid (xyz) of all foreground points in v i whilef i ? R F</formula><p>is the mean of their point properties (e.g. intensity, elongation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model Structure</head><p>The lower part of <ref type="figure" target="#fig_2">Figure 4</ref> illustrates the network architecture. SPG uses a light-weight encoder-decoder network <ref type="bibr" target="#b79">[79,</ref><ref type="bibr" target="#b25">25]</ref>, which is composed of three modules: 1) The Voxel Feature Encoding module <ref type="bibr" target="#b79">[79]</ref> aggregates points inside each voxel by using several MLPs. Similar to <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b49">49]</ref>, these voxel features are later stacked into pillars and projected onto a birds-eye view feature space;</p><p>2) The Information Propagation module applies 2D convolutions on the pillar features. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the semantic information in the occupied pillars (dark green) is populated into the neighboring empty pillars (light green), which enables SPG to recover the foreground regions in the empty space. 3. The Point Generation module maps the pillar features to the corresponding voxels. For each voxel v i in the generation area, the module creates a semantic pointsp i with encoding</p><formula xml:id="formula_1">[? i ,f i ,P f i ], in which? i is the point location,f i is the point properties, andP f i is the foreground probability.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Foreground Region Recovery</head><p>The above pipeline supervises SPG to generate semantic points in the occupied voxels. However, it is also crucial to recover the empty voxels caused by the "missing points" problem. To generate semantic points in the empty areas, SPG employs two strategies: ? "Hide and Predict", which produces the "missing points" on the source domain during training and guides SPG to recover the foreground object shape in the empty space. ? "Semantic Area Expansion", which leverages the foreground/background voxel labels derived from the bounding boxes and encourages SPG to recover more unobserved foreground regions in each bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Hide and Predict</head><formula xml:id="formula_2">SPG voxelizes P C raw ? R 3+F into a voxel set V = {v 1 , v 2 , ..., v M }.</formula><p>Before passing V to the network, we randomly select ?% of the occupied voxels V hide ? V and hide all their points. During training, SPG is required to predict the foreground/background label y f for all voxels in V , even though it only observes points in |V ? V hide |. The predicted point features? in V f hide should match the corresponding ground-truth ? calculated by these hidden points.</p><p>This strategy brings two benefits: 1. Hiding points region by region mimics the missing point pattern in the target domain; 2. The strategy naturally creates the training targets for semantic points in the empty space. Section 4.4 shows the effectiveness of this strategy. Here we set ? = 25.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Semantic Area Expansion</head><p>In section 1.1, we find the poor point cloud quality leads to insufficient points on each object and substantially degrades the detection performance. To remedy this problem, we allow SPG to expand the generation area to the empty space. <ref type="figure" target="#fig_3">Figure 5</ref> a and c show the examples of the generation area with and without the expansion, respectively. Without the expansion, we can use the ground-truth knowledge of foreground points to supervise SPG only on the occupied voxels ( <ref type="figure" target="#fig_3">Figure 5 b</ref>). However, with the expansion, there is no foreground point inside these empty voxels. Therefore, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>  4. We only impose point features supervision ? at occupied foreground voxels V f o . To investigate the effectiveness of the expansion, we train a model on the OD training set and evaluate it on the Kirk validation set. The expansion results in 510% more semantic points on foreground objects, which mitigates the "missing points" problem caused by environmental interference and occlusions. <ref type="figure" target="#fig_4">Figure 6</ref> shows the generation results with and without the expansion. The supervision scheme encourages SPG to learn the extended shape of vehicle parts and enables SPG to fill in more foreground space with semantic points. We also conduct ablation studies (Section 4.4) to show the effectiveness of the proposed strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objectives</head><p>We use two loss functions, i.e., foreground area classification loss L cls and feature regression loss L reg .</p><p>To superviseP f with label y f , we use Focal loss <ref type="bibr" target="#b31">[31]</ref> to mitigate the background-foreground class imbalance. L cls can be decomposed as focal losses on four categories of voxels: the occupied voxels V o , the empty background voxels V b e , the empty foreground voxels V f e and the hidden voxels V hide . The labeling strategy for these categories is described in Section 3.3.2.</p><formula xml:id="formula_3">L cls = 1 |V o ? V b e | Vo?V b e L f ocal + ? |V f e | V f e L f ocal + ? |V hide | V hide L f ocal (1)</formula><p>We use Smooth-L1 loss <ref type="bibr" target="#b20">[20]</ref> for point feature? regression, and supervise on the semantic points in occupied foreground voxels V f o and the hidden foreground voxels V f hide .</p><formula xml:id="formula_4">L reg = 1 |V f o | V f o L smooth?L1 (?, ?) + ? |V f hide | V f hide L smooth?L1 (?, ?)<label>(2)</label></formula><p>Please note that we are only interested in the L cls and L reg on voxels inside the generation area. We find ? = 0.5 and ? = 2.0 achieves the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first evaluate the effectiveness of SPG as a general UDA approach for 3D detection, based on the Waymo Domain Adaptation Dataset <ref type="bibr" target="#b54">[54]</ref>. In addition, we show that SPG can also improve results for top-performing 3D detectors on the source domain <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b18">18]</ref>. To demonstrate the wide applicability of SPG, we choose two representative detectors: 1) PointPillars <ref type="bibr" target="#b25">[25]</ref>, popular among industrialgrade autonomous driving systems; 2) PV-RCNN <ref type="bibr" target="#b49">[49]</ref>, a high performance LiDAR-based 3D detector <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b54">54]</ref>. We perform two groups of model comparisons under the setting of unsupervised domain adaptation (UDA) and general 3D object detection: group 1, PointPillars vs. SPG + Point-Pillars; group 2, PV-RCNN vs. SPG + PV-RCNN. SPG can also be combined with range image-based detectors <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr" target="#b0">1]</ref> by applying ray casting to the generated points. However, we leave this as future work.</p><p>Datasets The Waymo Domain Adaptation dataset 1.0 <ref type="bibr" target="#b54">[54]</ref> consists of two sub datasets, the Waymo Open Dataset (OD) and the Waymo Kirkland Dataset (Kirk). OD provides 798 training segments of 158,361 LiDAR frames and 202 validation segments of 40,077 frames. Captured across California and Arizona, 99.40% of its frames have dry weather. Kirk is a smaller dataset including 80 training segments of 15,797 frames and 20 validation segments of 3,933 frames. Captured in Kirkland, 97.99% its LiDAR frames have rainy weather. To examine a detector's reliability when entering a new environment, we conduct UDA experiments without using the data in Kirk during training.</p><p>KITTI <ref type="bibr" target="#b18">[18]</ref> contains 7481 training samples and 7518 testing samples. Following <ref type="bibr" target="#b7">[8]</ref>, we divide the training data into a train split and a val split containing 3721 and 3769 LiDAR frames, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation and Training Details</head><p>We use a single lightweight network architecture on all experiments. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, our Voxel Feature Encoding <ref type="bibr" target="#b79">[79]</ref> module includes a single layer point-wise MLP and a voxelwise max-pooling <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b79">79]</ref>. The Information Propagation module includes two levels of CNN layers. The first level includes three CNN layers with stride 1. The second level includes one CNN layer with stride 2 and four subsequent CNN layers with stride 1, then up-sampled back to the original resolution. Each layer has an output dimension of 128. From the BEV feature map, the Point Generation module uses one FC layer to produceP f and another FC layer to generate the features? for the voxels in each pillar. SPG and each detector are trained separately.</p><p>We implement PointPillars following <ref type="bibr" target="#b25">[25]</ref> and use the PV-RCNN code provided by <ref type="bibr" target="#b49">[49]</ref> (the training settings on OD 1.0 are obtained via direct communication with the author). On the Waymo Domain Adaptation Dataset <ref type="bibr" target="#b54">[54]</ref>, we set the voxel dimensions to (0.32m, 0.32m, 0.4m) for Point-Pillars and (0.2m, 0.2m, 0.3m) for PV-RCNN. On KITTI, we set the voxel dimensions to (0.16m, 0.16m, 0.2m) and (0.2m, 0.2m, 0.3m) for PointPillars and PV-RCNN, respectively. By default, the generation area includes voxels within 6 steps of any occupied voxel. After probability thresholding, we preserve up to 8000 semantic points for the Waymo Domain Adaptation Dataset and 6000 for KITTI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation on the Waymo Open Dataset</head><p>We perform two groups of model comparisons by training them on the OD training set and evaluating them on both the OD validation set and the Kirk validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>The Kirk 1.0 validation set only provides the evaluation labels for the vehicle and the pedestrian classes. We use the official evaluation tool released by <ref type="bibr" target="#b54">[54]</ref>. The IoU thresholds for vehicles and pedestrians are 0.7 and 0.5. In <ref type="table" target="#tab_2">Table 2</ref> we report both 3D and BEV AP on two difficulty levels. More results with distance breakdown are shown in the supplemental material.</p><p>Target Domain On Kirk, we observe that SPG brings remarkable improvements over both detectors across all object types. Averaged over two difficulty levels, SPG improves PointPillars on Kirk vehicle 3D AP by 6.7% and BEV AP by 8.8%. For PV-RCNN, SPG improves Kirk pedestrian 3D AP by 5.6% and BEV AP by 5.7%.</p><p>Source Domain Unlike most UDA methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b48">48]</ref> that only optimize the performance on the target domain, SPG also consistently improves the results on the source domain. Averaged across both difficulty levels, SPG improves OD vehicle 3D AP for PointPillars by 5.4% and improves OD pedestrian 3D AP for PV-RCNN by 1.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Alternative Strategies</head><p>We compare SPG with alternative strategies that also target the deteriorating point cloud quality. We employ PointPillars as the baseline and choose LEVEL 1 vehicle 3D AP as the main   <ref type="table" target="#tab_0">Table 1</ref>). 2. K-frames, where we use K consecutive historical frames in both the source domain and the target domain. The points in the first K ? 1 are transformed into the last frame according to the ground-truth ego-motion, so that the last frame has K times the number of points. 3. Adversarial Domain Adaptation (ADA), where we follow <ref type="bibr" target="#b17">[17]</ref> and add a domain classification loss on the pillar features of PointPillars. As shown in <ref type="table" target="#tab_4">Table 3</ref>, although "RndDrop" enforces the quantity of missing points in the source domain to match with that in the target domain, the pattern of missing points still differs from the reality (see <ref type="figure" target="#fig_0">Figure 2</ref>), which limits the improvement to only 0.80% in 3D AP. To remedy the "missing points" problem, "3-frames" contains real points from 3 frames and "5-frames" contains points from 5 frames. With around 800K points per scene, "5-frames" significantly improves the single-frame baseline. However, aggregating multiple frames inevitably increases the memory usage and the processing time. ADA improves 3D AP to 36.34 on the target domain, but we observe an AP drop of 1.52 in the source domain. Remarkably, SPG can outperform "5frames", by adding only 8000 semantic points, which is less than 6% of the points in a single frame.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on the KITTI Dataset</head><p>In this section, we show besides the usefulness in UDA (Sec. 4.1) the proposed SPG can also boost performance in another popular 3D detection benchmark (i.e. KITTI <ref type="bibr" target="#b18">[18]</ref>). We follow the training and evaluation protocols in <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b49">49]</ref>. <ref type="table" target="#tab_5">Table 4</ref>, SPG significantly improves PV-RCNN on Car 3D detection. As of Mar. 3rd, 2021, our method ranks the 1st on KITTI car 3D detection among all published methods (4th among all submitted approaches). Moreover, SPG demonstrates strong robustness in detecting hard objects (truncation up to 50%). Specifically, SPG surpasses all submitted methods on the hard category by a big margin and achieves the highest overall 3D AP of 83.84% (averaged over Easy, Mod. and Hard).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI Test Set As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KITTI Validation Set</head><p>We summarize the results in <ref type="table" target="#tab_7">Table  5</ref>. We train each group of models using the recommended settings of baseline detectors <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b49">49]</ref>.</p><p>SPG remarkably improves both PointPillars and PV-RCNN on all object types and difficulty levels. Specifically, for PointPillars, SPG improves the 3D AP of car detection by 2.02%, 2.97%, 3.67% on easy, moderate, and hard levels, respectively. For PV-RCNN, SPG improves the 3D AP  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Efficiency</head><p>We evaluate the efficiency of SPG on the KITTI val split ( <ref type="table" target="#tab_9">Table 6</ref>). SPG contains 0.39 million parameters while adding less than 17 milliseconds latency to the detectors. This indicates that SPG is highly efficient for industrialgrade deployment on a stringent computation budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detectors</head><p>PointPillars   <ref type="table">Table 7</ref>: Ablation studies of SPG. The models are trained on OD and evaluated on Kirk. The metric is LEVEL 1 Vehicle 3D AP. We use PointPillars <ref type="bibr" target="#b25">[25]</ref> as our baseline.</p><p>We conduct ablation studies on "Semantic Area Expansion", "Hide and Predict" and whether to add foreground confidence (P f ) as a point property and show all of them can benefit detection quality (see <ref type="table">Table 7</ref>). We also change the weighting factor ? on the empty foreground voxels V f e . A larger ? encourages more point generation in the empty foreground space. However, in reality, an object typically does not occupy the entire space within a bounding  box. Therefore, over-aggressively generating points does not help improve the performance (see ? = 1.0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability Thresholding</head><p>In <ref type="table" target="#tab_10">Table 8</ref>, we show the effect of choosing different thresholds during probability thresholding. While a higher P thresh only keeps semantic points with high foreground probability, a lower P thresh admits more points, but may introduce points to the background. We find the threshold of 0.5 achieves the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we investigate unsupervised domain adaptation for LiDAR-based 3D detectors across different geographic locations and weather conditions. We observe that rainy weather can severely deteriorate the point cloud quality and cause drastic performance drop for modern 3D detectors, based on the Waymo Domain Adaptation dataset. The proposed SPG method addresses this issue as a novel unsupervised domain adaptation (UDA) task without using any training data from the new domain. This setting allows us to rigorously test 3D detectors against real-world challenges autonomous vehicles may experience due to diverse conditions (e.g., different levels of fog/rain/snow beyond what one may effectively train for) during the trip.</p><p>Utilizing two strategies "Hide and Predict" and "Semantic Area Generation", SPG generates semantic points to recover the shape of foreground objects with a negligible overhead (only adding 6% extra points) and can be conveniently integrated with modern LiDAR-based detectors. We test SPG with two detectors: PointPillars and PV-RCNN. For unsupervised domain adaptation, SPG achieves significant performance gains on the challenging target domain. On Waymo Open dataset and KITTI, SPG also consistently benefits detection quality on the source domain.</p><p>In this supplementary material, we provide detailed analysis about the statistics of the Waymo Domain Adaptation Dataset in Section A; the robustness analysis of the foreground voxel classifier in Section B; the derivation of the dropout rate used in the RndDrop method in Section C; more results on the Waymo Domain Adaptation Dataset in Section D; more results on KITTI in Section E; and more visualization of the semantic point generation in Section F. <ref type="figure">Figure 7</ref>: The average number of raw points per vehicle across different ranges. On the x axis, the range value stands for the distance between the center of a bounding box and the LiDAR sensor. The y axis shows the value after applying log 10 on the number of points N . "Kirk Dry" is extracted from the Kirk Training set and contains frames captured in the dry weather.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Statistics of the Waymo Domain Adaptation Dataset</head><p>We collect the statistics about the average number of points in a vehicle bounding box across different ranges. The range value is calculated as the euclidean distance between the LiDAR sensor and the center of a bounding box. We investigate four sets of point clouds: ? The OD Validation set, in which 99.5% of the frames are collected in the dry weather. ? The Kirk Dry set, which consists of all the frames with the dry weather condition from the Kirk training set. ? The Kirk Training Rainy set, which consists of all the frames with the rainy weather condition from the Kirk training set. ? The Kirk Validation set, in which all the frames are collected in the rainy weather. As shown in <ref type="figure">Figure 7</ref>, the point clouds with similar weather conditions share similar numbers of points per ob-ject, even though they are collected at different locations. Specifically, the vehicle objects of the two "dry datasets", i.e., the Kirk Dry set and the OD Validation set, have similar numbers of points across all ranges. The vehicle objects of the two "rainy datasets" i.e., the Kirk Training Rainy set and the Kirk Validation set, share similar statistics.</p><p>In addition, the point clouds captured in the dry weather (the OD Validation set and the Kirk Dry set) have more points on each object than those collected in the rainy weather (the Kirk Training Rainy set and the Kirk Validation set). Please note that we have applied log 10 to the number of points for better visualization. The difference in the number of points is substantial between two weather conditions across all ranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Robustness of the Foreground Voxel Classifier</head><p>In order to generalize detectors to different domains, it is crucial to correctly classify foreground voxels so that semantic points can be reliably generated.  <ref type="table" target="#tab_11">Table 9</ref>: Foreground voxel classification results of our SPG. The model is trained on the OD training set and then it is evaluated on the OD validation set and Kirk validation set, respectively. The accuracy, precision and recall are evaluated by settingP f &gt; 0.5.</p><p>results in <ref type="table" target="#tab_11">Table 9</ref> are averaged among all voxels in the foreground regions. Our SPG is trained on the OD training set. Then it is evaluated on the OD validation set and the Kirk validation set, respectively. The classification of a voxel is correct if its prediction scoreP f &gt; 0.5 when y f = 1.0 or P f &lt; 0.5 when y f = 0.0. The accuracy, precision and recall are all calculated under this setting. The AP is calculated using 40 recall thresholds. The results show that SPG achieves high performance in both domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dropout Rate of the RndDrop Method</head><p>In the experiment section, we implement a baseline method RndDrop, where we randomly drop out 17% of points for point clouds from the source domain during training. This dropout ratio is chosen to match the ratio of missing points in the target domain. We calculate (N src ? N tgt )/N src = 17%, where N src = 121.2K is the average number of points per scene in the source domain and N tgt = 100.4K is the average number of points per scene in the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Results on the Waymo Domain Adaptation Dataset</head><p>The evaluation tool <ref type="bibr" target="#b54">[54]</ref> provides the average precision for three distance-based breakdowns: 0 to 30 meters, 30 to 50 meters, and beyond 50 meters. The AP is calculated using 100 recall thresholds.</p><p>We perform two groups of model comparisons in the setting of UDA: Group 1. PointPillars vs. SPG + PointPillars; Group 2. PV-RCNN vs. SPG + PV-RCNN. We train all models on the OD training set and evaluate them on both the OD validation set and the Kirk validation set. <ref type="table" target="#tab_0">Table 10</ref> and 11 show the comparisons on vehicle 3D AP and vehicle BEV AP, respectively. <ref type="table" target="#tab_0">Table 12</ref> and <ref type="table" target="#tab_0">Table 13</ref> show the comparisons in pedestrian 3D AP and pedestrian BEV AP, respectively. In most cases, SPG improves the detection performance across all ranges for both vehicles and pedestrians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Results on KITTI</head><p>We provide more 3D object detection results on KITTI. There are two commonly used metric standards for evaluating the detection performance: 1) R11, where the AP is evaluated with 11 recall positions; 2) R40, where the AP is evaluated with 40 recall positions. In addition to the improvement on car and pedestrian detection, SPG also significantly boosts the performance in cyclist detection. Based on R11, <ref type="table" target="#tab_0">Table 14</ref> and <ref type="table" target="#tab_0">Table 15</ref> show the results in 3D AP and BEV AP for three object types, respectively. Based on R40, <ref type="table" target="#tab_0">Table 16</ref> and <ref type="table" target="#tab_0">Table 17</ref> show the results in 3D AP and BEV AP for three object types, respectively.</p><p>We show more comparisons on the KITTI test set in <ref type="table" target="#tab_0">Table 18</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. More Visualization of Semantic Point Generation</head><p>In <ref type="figure">Figure 9</ref>, we illustrate more augmented point clouds, where the raw points are rendered in the grey color and the generated semantic points are highlighted in red.         <ref type="table" target="#tab_0">Table 18</ref>: Car detection result comparisons on the KITTI test set. The results are evaluated by the Average Precision with 40 recall positions on the KITTI benchmark website. We compare with the leader board front runner detectors that are associated with conferences or journals released before our submission. The Avg. AP is calculated by averaging over the APs of Easy, Mod. and Hard. difficulty levels. <ref type="figure">Figure 9</ref>: More visualization of generated semantic points. The grey points are original raw points. The red points are the generated semantic points. The green boxes are the predicted bounding boxes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Examples of RGB and range image (intensity channel) in OD validation set and Kirk validation set. The dark regions in the range images indicate missed LiDAR returns. The regions of "missing points" are irregular in shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Training targets construction and SPG model architecture. Three steps to create the semantic point training targets: 1.Voxelization; 2. Foreground points searching 3. Label assignment and ground-truth point feature calculation. SPG includes: the Voxel Feature Encoding module (VFE), the Information Propagation module, and the Point Generation module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of "Semantic Area Expansion". (a) and (c) show the occupied voxels and the generation area, respectively. (b) and (d) show the supervision strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>d, we design a supervision scheme as follows: 1. For both occupied and empty background voxels V b o and V b e , we impose negative supervision and set label y f = 0. 2. For the occupied foreground voxels V f o , we set y f = 1. 3. For the empty voxels inside a bounding box V f e , we set their foreground label y f = 1 and assign a weighting factor (a) Without expansion (b) With expansion Comparisons between generated semantic points (red) with and without "Semantic Area Expansion". ?, where ? &lt; 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>have focused on the performance in a single domain, where training and test data are captured in similar conditions. It is still an open question how to generalize a 3D detector to different domains, where the environment arXiv:2108.06709v1 [cs.CV] 15 Aug 2021 The statistics of OD and Kirk. Each frame contains at most 163.8K points. Kirk Dry is formed by frames with dry weather in Kirk training set.</figDesc><table><row><cell>Dataset</cell><cell>Rainy frames</cell><cell>Avg. number of missing points per frame</cell><cell>Avg. number of points per vehicle</cell><cell>3D L1 AP</cell></row><row><cell cols="2">OD Val 0.5 %</cell><cell>23.0K</cell><cell>306.2</cell><cell>56.54</cell></row><row><cell cols="2">Kirk Dry 0.0 %</cell><cell>25.1K</cell><cell>303.6</cell><cell>55.98</cell></row><row><cell cols="2">Kirk Val 100.0%</cell><cell>42.8K</cell><cell>222.3</cell><cell>34.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on the Waymo Open Dataset 1.0 and the Kirkland Dataset. Results for PointPillars are based on our own implementation following [25]. We use the PV-RCNN source code and obtain training settings for the Waymo Open Dataset [54] via direct communication with the author.metric on the Kirk validation set, during UDA. Three strate- gies are implemented: 1. RndDrop, where we randomly drop 17% of the points in the source domain during train- ing. This dropout ratio is chosen for the number of points in the source and target domain to match (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>CVPR 2020 85.99 77.40 70.53 77.97 Voxel R-CNN[13] AAAI 2021 90.90 81.62 77.06 83.19 PV-RCNN[49] CVPR 2020 90.25 81.43 76.82 82.83 SPG+PV-RCNN -90.50 82.13 78.90 83.84</figDesc><table><row><cell>Car -3D AP</cell></row></table><note>Comparisons of different strategies targeting at the deteriorating point cloud quality. The models are trained on OD and evaluated on Kirk. The metric is LEVEL 1 Vehicle 3D AP. We use PointPillars[25] as the baseline.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Car detection Results on the KITTI test set. See the full list of comparisons in the supplemental.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Hard Easy Mod. Hard Easy Mod. Hard PointPillars 87.75 78.39 75.18 92.03 88.05 86.66 57.30 51.41 46.87 61.59 56.01 52.04 SPG + PointPillars 89.77 81.36 78.85 94.38 89.92 87.97 59.65 53.55 49.24 65.38 59.48 55.32 Improvement +2.02 +2.97 +3.67 +2.35 +1.87 +1.31 +2.35 +2.14 +2.47 +3.79 +3.47 +3.28 PV-RCNN 92.10 84.36 82.48 93.02 90.33 88.53 64.26 56.67 51.91 67.97 60.52 55.80 SPG + PV-RCNN 92.53 85.31 82.82 94.99 91.11 88.86 69.66 61.80 56.39 71.79 64.50 59.51 Improvement +0.43 +0.95 +0.34 +1.97 +0.78 +0.33 +5.40 +5.13 +4.48 +3.82 +3.98 +3.71</figDesc><table><row><cell></cell><cell>Car -3D AP</cell><cell>Car -BEV AP</cell><cell>Pedestrian -3D AP</cell><cell>Pedestrian -BEV AP</cell></row><row><cell>Method</cell><cell>Easy Mod. Hard</cell><cell>Easy Mod.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparisons on the KITTI validation set. Average Precision (AP) is computed over 40 recall positions. The baseline results<ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b55">55]</ref> are obtained based on publically released models. See more results (including Cyclist) in the supplemental.</figDesc><table><row><cell>of pedestrian detection by 5.40%, 5.13%, 4.48% on easy,</cell></row><row><cell>moderate and hard levels, respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>) 23.56 36.67 139.96 156.85 16.82 Parameters 4.83M 5.22M 13.12M 13.51M 0.39M</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">PV-RCNN</cell><cell>-</cell></row><row><cell>With SPG</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Latency (ms</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Latency and model parameters. "M" stands for million. The last column shows the results of standalone SPG. The evaluation is based on a 1080Ti GPU with a batch size of 1. The latency is averaged over the KITTI val split.</figDesc><table><row><cell cols="2">4.4. Ablation Studies</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Hide &amp; Foreground 3D</cell><cell></cell></row><row><cell>Model</cell><cell cols="5">Expansion Predict Confidence AP Improve</cell></row><row><cell>Baseline</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>34.65</cell><cell>?</cell></row><row><cell>SPG</cell><cell>?</cell><cell>?</cell><cell></cell><cell cols="2">35.89 +1.24</cell></row><row><cell>SPG</cell><cell>?</cell><cell>25%</cell><cell></cell><cell cols="2">38.09 +3.44</cell></row><row><cell>SPG</cell><cell cols="2">(?=0.0) 25%</cell><cell></cell><cell cols="2">38.96 +4.31</cell></row><row><cell>SPG</cell><cell cols="2">(?=1.0) 25%</cell><cell></cell><cell cols="2">38.42 +3.77</cell></row><row><cell>SPG</cell><cell>(?=0.5)</cell><cell>?</cell><cell></cell><cell cols="2">39.22 +4.57</cell></row><row><cell>SPG</cell><cell cols="2">(?=0.5) 25%</cell><cell>?</cell><cell cols="2">37.96 +3.31</cell></row><row><cell cols="3">SPG(ours) (?=0.5) 25%</cell><cell></cell><cell cols="2">41.56 +6.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Ablation studies on the probability threshold P thresh (only keep the semantic point ifP f &gt; P thresh ). Our best SPG model uses P thresh = 0.5. The metric is LEVEL 1 Vehicle 3D AP on the Kirk validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>lists the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>The unsupervised domain adaptation vehicle detection results on both Waymo Open Dataset (OD) and Kirkland Dataset (Kirk). We show the vehicle 3D AP results in this table. The AP distance breakdowns are provided by the official evaluation tool.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Target Domain -Kirk</cell><cell></cell><cell></cell><cell cols="2">Source Domain -OD</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Vehicle BEV AP (IoU = 0.7)</cell><cell cols="4">Vehicle BEV AP (IoU = 0.7)</cell></row><row><cell>Difficulty</cell><cell>Method</cell><cell cols="3">Overall 0-30m 30-50m</cell><cell>50-Inf</cell><cell cols="3">Overall 0-30m 30-50m</cell><cell>50-Inf</cell></row><row><cell></cell><cell>PointPillars</cell><cell>51.88</cell><cell>75.56</cell><cell>46.04</cell><cell>25.55</cell><cell>72.26</cell><cell>92.23</cell><cell>71.35</cell><cell>51.11</cell></row><row><cell>LEVEL 1</cell><cell>SPG + PointPillars</cell><cell>60.44</cell><cell>80.89</cell><cell>53.73</cell><cell>38.24</cell><cell>77.63</cell><cell>93.39</cell><cell>75.96</cell><cell>61.16</cell></row><row><cell></cell><cell>Improvement</cell><cell>+8.56</cell><cell>+5.33</cell><cell>+7.69</cell><cell>+12.69</cell><cell>+5.37</cell><cell>+1.16</cell><cell>+4.61</cell><cell>+10.05</cell></row><row><cell></cell><cell>PointPillars</cell><cell>47.93</cell><cell>71.18</cell><cell>42.41</cell><cell>23.47</cell><cell>69.09</cell><cell>91.83</cell><cell>68.87</cell><cell>45.53</cell></row><row><cell>LEVEL 2</cell><cell>SPG + PointPillars</cell><cell>56.94</cell><cell>77.13</cell><cell>49.99</cell><cell>35.04</cell><cell>74.90</cell><cell>93.06</cell><cell>73.96</cell><cell>54.51</cell></row><row><cell></cell><cell>Improvement</cell><cell>+9.01</cell><cell>+5.95</cell><cell>+7.58</cell><cell>+11.57</cell><cell>+5.81</cell><cell>+1.23</cell><cell>+5.09</cell><cell>+8.98</cell></row><row><cell></cell><cell>PV-RCNN</cell><cell>70.38</cell><cell>84.27</cell><cell>65.31</cell><cell>52.98</cell><cell>85.13</cell><cell>95.99</cell><cell>84.02</cell><cell>72.19</cell></row><row><cell>LEVEL 1</cell><cell>SPG + PV-RCNN</cell><cell>72.56</cell><cell>84.43</cell><cell>68.79</cell><cell>58.49</cell><cell>87.38</cell><cell>97.54</cell><cell>86.63</cell><cell>74.59</cell></row><row><cell></cell><cell>Improvement</cell><cell>+2.18</cell><cell>+0.16</cell><cell>+3.48</cell><cell>+5.51</cell><cell>+2.25</cell><cell>+1.55</cell><cell>+2.61</cell><cell>+2.40</cell></row><row><cell></cell><cell>PV-RCNN</cell><cell>60.13</cell><cell>78.10</cell><cell>54.36</cell><cell>40.67</cell><cell>76.84</cell><cell>93.29</cell><cell>76.64</cell><cell>58.29</cell></row><row><cell>LEVEL 2</cell><cell>SPG + PV-RCNN</cell><cell>62.03</cell><cell>78.86</cell><cell>56.47</cell><cell>44.94</cell><cell>78.05</cell><cell>94.45</cell><cell>80.25</cell><cell>59.56</cell></row><row><cell></cell><cell>Improvement</cell><cell>+1.90</cell><cell>+0.76</cell><cell>+2.11</cell><cell>+4.27</cell><cell>+1.21</cell><cell>+1.16</cell><cell>+3.61</cell><cell>+1.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>The unsupervised domain adaptation vehicle detection results on both Waymo Open Dataset (OD) and Kirkland Dataset (Kirk). We show the vehicle BEV AP results in this table. The AP distance breakdowns are provided by the official evaluation tool.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Target Domain -Kirk</cell><cell></cell><cell></cell><cell cols="2">Source Domain -OD</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Pedestrian 3D AP (IoU = 0.5)</cell><cell cols="4">Pedestrian 3D AP (IoU = 0.5)</cell></row><row><cell>Difficulty</cell><cell>Method</cell><cell cols="8">Overall 0-30m 30-50m 50-Inf Overall 0-30m 30-50m 50-Inf</cell></row><row><cell></cell><cell>PointPillars</cell><cell>20.65</cell><cell>43.98</cell><cell>9.27</cell><cell>3.24</cell><cell>55.20</cell><cell>69.24</cell><cell>52.04</cell><cell>32.72</cell></row><row><cell>LEVEL 1</cell><cell>SPG + PointPillars</cell><cell>23.72</cell><cell>50.19</cell><cell>9.11</cell><cell>5.57</cell><cell>56.06</cell><cell>69.32</cell><cell>53.12</cell><cell>34.73</cell></row><row><cell></cell><cell>Improvement</cell><cell>+3.07</cell><cell>+6.21</cell><cell>-0.16</cell><cell>+2.33</cell><cell>+0.86</cell><cell>+0.08</cell><cell>+1.08</cell><cell>+2.01</cell></row><row><cell></cell><cell>PointPillars</cell><cell>17.66</cell><cell>40.67</cell><cell>7.40</cell><cell>2.32</cell><cell>51.33</cell><cell>65.85</cell><cell>49.32</cell><cell>29.29</cell></row><row><cell>LEVEL 2</cell><cell>SPG + PointPillars</cell><cell>19.57</cell><cell>46.42</cell><cell>7.44</cell><cell>3.99</cell><cell>52.33</cell><cell>65.63</cell><cell>50.10</cell><cell>31.25</cell></row><row><cell></cell><cell>Improvement</cell><cell>+1.91</cell><cell>+5.75</cell><cell>+0.04</cell><cell>+1.67</cell><cell>+1.00</cell><cell>-0.22</cell><cell>+0.78</cell><cell>+1.96</cell></row><row><cell></cell><cell>PV-RCNN</cell><cell>24.47</cell><cell>39.69</cell><cell>14.24</cell><cell>8.05</cell><cell>65.34</cell><cell>72.23</cell><cell>64.89</cell><cell>50.04</cell></row><row><cell>LEVEL 1</cell><cell>SPG + PV-RCNN</cell><cell>30.82</cell><cell>48.04</cell><cell>18.80</cell><cell>13.39</cell><cell>66.93</cell><cell>73.55</cell><cell>66.60</cell><cell>50.82</cell></row><row><cell></cell><cell>Improvement</cell><cell>+6.35</cell><cell>+8.35</cell><cell>+4.56</cell><cell>+5.34</cell><cell>+1.59</cell><cell>+1.32</cell><cell>+1.71</cell><cell>+0.78</cell></row><row><cell></cell><cell>PV-RCNN</cell><cell>17.16</cell><cell>36.39</cell><cell>9.64</cell><cell>3.51</cell><cell>56.03</cell><cell>66.88</cell><cell>56.58</cell><cell>35.76</cell></row><row><cell>LEVEL 2</cell><cell>SPG + PV-RCNN</cell><cell>22.05</cell><cell>44.07</cell><cell>12.91</cell><cell>5.77</cell><cell>57.68</cell><cell>68.28</cell><cell>58.29</cell><cell>37.64</cell></row><row><cell></cell><cell>Improvement</cell><cell>+4.89</cell><cell>+7.68</cell><cell>+3.27</cell><cell>+2.26</cell><cell>+1.65</cell><cell>+1.40</cell><cell>+1.71</cell><cell>+1.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>The unsupervised domain adaptation pedestrian detection results on both Waymo Open Dataset (OD) and Kirkland Dataset (Kirk). We show the pedestrian 3D AP results in this table. The AP distance breakdowns are provided by the official evaluation tool.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Target Domain -Kirk</cell><cell></cell><cell></cell><cell cols="2">Source Domain -OD</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Pedestrian BEV AP (IoU = 0.5)</cell><cell cols="4">Pedestrian BEV AP (IoU = 0.5)</cell></row><row><cell>Difficulty</cell><cell>Method</cell><cell cols="8">Overall 0-30m 30-50m 50-Inf Overall 0-30m 30-50m 50-Inf</cell></row><row><cell></cell><cell>PointPillars</cell><cell>22.33</cell><cell>45.00</cell><cell>10.50</cell><cell>3.49</cell><cell>63.82</cell><cell>76.33</cell><cell>61.90</cell><cell>42.81</cell></row><row><cell>LEVEL 1</cell><cell>SPG + PointPillars</cell><cell>24.83</cell><cell>51.44</cell><cell>10.80</cell><cell>5.71</cell><cell>64.66</cell><cell>76.11</cell><cell>62.69</cell><cell>44.98</cell></row><row><cell></cell><cell>Improvement</cell><cell>+2.50</cell><cell>+6.44</cell><cell>+0.30</cell><cell>+2.22</cell><cell>+0.84</cell><cell>-0.22</cell><cell>+0.79</cell><cell>+2.17</cell></row><row><cell></cell><cell>PointPillars</cell><cell>18.40</cell><cell>41.63</cell><cell>8.58</cell><cell>2.49</cell><cell>60.13</cell><cell>73.34</cell><cell>58.77</cell><cell>38.83</cell></row><row><cell>LEVEL 2</cell><cell>SPG + PointPillars</cell><cell>20.67</cell><cell>47.56</cell><cell>8.98</cell><cell>4.11</cell><cell>60.93</cell><cell>72.94</cell><cell>59.54</cell><cell>41.11</cell></row><row><cell></cell><cell>Improvement</cell><cell>+2.27</cell><cell>+5.93</cell><cell>+0.40</cell><cell>+1.62</cell><cell>+0.80</cell><cell>-0.40</cell><cell>+0.77</cell><cell>+2.28</cell></row><row><cell></cell><cell>PV-RCNN</cell><cell>25.39</cell><cell>40.23</cell><cell>14.72</cell><cell>9.76</cell><cell>70.35</cell><cell>76.22</cell><cell>70.49</cell><cell>56.77</cell></row><row><cell>LEVEL 1</cell><cell>SPG + PV-RCNN</cell><cell>31.92</cell><cell>49.06</cell><cell>19.87</cell><cell>14.87</cell><cell>70.37</cell><cell>75.86</cell><cell>72.29</cell><cell>57.47</cell></row><row><cell></cell><cell>Improvement</cell><cell>+6.53</cell><cell>+8.83</cell><cell>+5.15</cell><cell>+5.11</cell><cell>+0.02</cell><cell>-0.36</cell><cell>+1.80</cell><cell>+0.70</cell></row><row><cell></cell><cell>PV-RCNN</cell><cell>17.88</cell><cell>36.89</cell><cell>9.97</cell><cell>4.23</cell><cell>60.81</cell><cell>69.22</cell><cell>61.86</cell><cell>41.32</cell></row><row><cell>LEVEL 2</cell><cell>SPG + PV-RCNN</cell><cell>22.65</cell><cell>44.57</cell><cell>13.48</cell><cell>6.38</cell><cell>60.88</cell><cell>70.62</cell><cell>63.65</cell><cell>43.27</cell></row><row><cell></cell><cell>Improvement</cell><cell>+4.77</cell><cell>+7.68</cell><cell>+3.51</cell><cell>+2.15</cell><cell>+0.07</cell><cell>+1.40</cell><cell>+1.79</cell><cell>+1.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>The unsupervised domain adaptation pedestrian detection results on both Waymo Open Dataset (OD) and Kirkland Dataset (Kirk). We show the pedestrian BEV AP results in this table. The AP distance breakdowns are provided by the official evaluation tool.</figDesc><table><row><cell></cell><cell cols="3">Car -3D AP (R11)</cell><cell cols="3">Pedestrian -3D AP (R11)</cell><cell cols="3">Cyclist -3D AP (R11)</cell></row><row><cell>Method</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>PointPillars[25]</cell><cell>86.46</cell><cell>77.28</cell><cell>74.65</cell><cell>57.75</cell><cell>52.29</cell><cell>47.90</cell><cell>80.05</cell><cell>62.68</cell><cell>59.70</cell></row><row><cell>SPG + PointPillars</cell><cell>87.98</cell><cell>78.54</cell><cell>77.32</cell><cell>59.91</cell><cell>54.58</cell><cell>50.34</cell><cell>81.58</cell><cell>65.70</cell><cell>62.28</cell></row><row><cell>Improvement</cell><cell>+1.52</cell><cell>+1.26</cell><cell>+2.67</cell><cell>+2.16</cell><cell>+2.29</cell><cell>+2.44</cell><cell>+1.53</cell><cell>+3.02</cell><cell>+2.58</cell></row><row><cell>PVRCNN[49]</cell><cell>89.35</cell><cell>83.69</cell><cell>78.70</cell><cell>64.60</cell><cell>57.90</cell><cell>53.23</cell><cell>85.22</cell><cell>70.47</cell><cell>65.75</cell></row><row><cell>SPG + PVRCNN</cell><cell>89.81</cell><cell>84.45</cell><cell>79.14</cell><cell>69.04</cell><cell>62.18</cell><cell>56.77</cell><cell>86.82</cell><cell>73.35</cell><cell>69.30</cell></row><row><cell>Improvement</cell><cell>+0.46</cell><cell>+0.76</cell><cell>+0.44</cell><cell>+4.44</cell><cell>+4.28</cell><cell>+3.54</cell><cell>+1.60</cell><cell>+2.88</cell><cell>+3.55</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc>Result comparisons on the KITTI validation set. The results are evaluated by the Average Precision with 11 recall positions. The baseline detectors, PointPillars and PV-RCNN, are directly evaluated by using the checkpoints released by<ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b55">55]</ref>.78.39   </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>75.18</cell><cell>57.30</cell><cell>51.41</cell><cell>46.87</cell><cell>81.57</cell><cell>62.94</cell><cell>58.98</cell></row><row><cell>SPG+PointPillars</cell><cell>89.77</cell><cell>81.36</cell><cell>78.85</cell><cell>59.65</cell><cell>53.55</cell><cell>49.24</cell><cell>83.27</cell><cell>66.11</cell><cell>61.99</cell></row><row><cell>Improvement</cell><cell>+2.02</cell><cell>+2.97</cell><cell>+3.67</cell><cell>+2.35</cell><cell>+2.14</cell><cell>+2.37</cell><cell>+1.70</cell><cell>+3.17</cell><cell>+3.01</cell></row><row><cell>PVRCNN[49]</cell><cell>92.10</cell><cell>84.36</cell><cell>82.48</cell><cell>64.26</cell><cell>56.67</cell><cell>51.91</cell><cell>88.88</cell><cell>71.95</cell><cell>66.78</cell></row><row><cell>SPG+PVRCNN</cell><cell>92.53</cell><cell>85.31</cell><cell>82.82</cell><cell>69.66</cell><cell>61.80</cell><cell>56.39</cell><cell>91.75</cell><cell>74.35</cell><cell>69.49</cell></row><row><cell>Improvement</cell><cell>+0.43</cell><cell>+0.95</cell><cell>+0.34</cell><cell>+5.40</cell><cell>+5.13</cell><cell>+4.48</cell><cell>+2.87</cell><cell>+2.40</cell><cell>+2.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 :</head><label>16</label><figDesc>Result comparisons on the KITTI validation set. The results are evaluated by the Average Precision with 40 recall positions. The baseline detectors, PointPillars and PV-RCNN, are directly evaluated by using the checkpoints released by<ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b55">55]</ref>.</figDesc><table><row><cell></cell><cell cols="3">Car -BEV AP (R40)</cell><cell cols="3">Pedestrian -BEV AP (R40)</cell><cell cols="3">Cyclist -BEV AP (R40)</cell></row><row><cell>Method</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell><cell>Easy</cell><cell>Mod.</cell><cell>Hard</cell></row><row><cell>PointPillars[25]</cell><cell>92.03</cell><cell>88.05</cell><cell>86.66</cell><cell>61.59</cell><cell>56.01</cell><cell>52.04</cell><cell>85.27</cell><cell>66.34</cell><cell>62.36</cell></row><row><cell>SPG + PointPillars</cell><cell>94.38</cell><cell>89.92</cell><cell>87.97</cell><cell>65.38</cell><cell>59.48</cell><cell>55.32</cell><cell>90.29</cell><cell>71.43</cell><cell>66.96</cell></row><row><cell>Improvement</cell><cell>+2.35</cell><cell>+1.87</cell><cell>+1.31</cell><cell>+3.79</cell><cell>+3.47</cell><cell>+3.28</cell><cell>+5.02</cell><cell>+5.09</cell><cell>+4.60</cell></row><row><cell>PVRCNN[49]</cell><cell>93.02</cell><cell>90.33</cell><cell>88.53</cell><cell>67.97</cell><cell>60.52</cell><cell>55.80</cell><cell>91.02</cell><cell>74.54</cell><cell>69.92</cell></row><row><cell>SPG + PVRCNN</cell><cell>94.99</cell><cell>91.11</cell><cell>88.86</cell><cell>71.79</cell><cell>64.50</cell><cell>59.51</cell><cell>93.62</cell><cell>76.45</cell><cell>71.64</cell></row><row><cell>Improvement</cell><cell>+1.97</cell><cell>+0.78</cell><cell>+0.33</cell><cell>+3.82</cell><cell>+3.98</cell><cell>+3.71</cell><cell>+2.60</cell><cell>+1.91</cell><cell>+1.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 :</head><label>17</label><figDesc>Result comparisons on the KITTI validation set. The results are evaluated by the Average Precision with 40 recall positions. The baseline detectors, PointPillars and PV-RCNN, are directly evaluated by using the checkpoints released by<ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b55">55]</ref>.</figDesc><table><row><cell>Car -3D AP</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>We would like to thank Boqing Gong for the helpful discussions. We also thank Jingwei Ji for the careful proofreading.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Range conditioned dilated convolutions for scale invariant 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drago</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katashi</forename><surname>Nagao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05807</idno>
		<title level="m">Neural style transfer for point clouds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Psnet: A style transfer network for point cloud stylization on geometry and color</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katashi</forename><surname>Nagao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3337" to="3345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object as hotspots: An anchor-free 3d object detection approach via firing of hotspots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unpaired point cloud completion on real scans using adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy J</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6526" to="6534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptive faster r-cnn for object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12536" to="12545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selfensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taekyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Voxel r-cnn: Towards high performance voxel-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15712</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic-transferable weakly-supervised endoscopic lesions segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Associate-3ddet</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual-to-conceptual association for 3d point cloud object detection</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A general pipeline for 3d detection of vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marcelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sertac</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3194" to="3200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset. The International</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structure aware single-stage 3d object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-adversarial faster-rcnn for unrestricted object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Progressive domain adaptation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Kai</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Han</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Epnet: Enhancing point features with image semantics for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Realtime 3d object detection for automated driving using stereo vision and semantic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>K?nigshof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><forename type="middle">Ole</forename><surname>Salscheider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1405" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pu-gan: a point cloud upsampling adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR, 2015. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised scene adaptation for semantic segmentation of urban mobile laser scanning point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kourosh</forename><surname>Khoshelham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongcheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="253" to="267" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lasernet: An efficient probabilistic 3d object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">K</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Minimal-entropy correlation alignment for unsupervised deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacopo</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10288</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Clocs: Cameralidar object candidates fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayder</forename><surname>Radha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00784</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15651</idno>
		<title level="m">Contrastive learning for unpaired image-to-image translation</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object-centric stereo matching for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8383" to="8389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generatively inferential co-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pointdan: A multi-scale 3d domain adaption network for point cloud representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-C Jay</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Domain adaptation for vehicle detection from bird&apos;s eye view lidar point cloud data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abobakr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Iskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hossny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Nahvandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pixel and feature level based domain adaptation for object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhu</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><forename type="middle">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chee Meng</forename><surname>Chew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xerxes</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijaysai</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Caine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Openpcdet: An opensource toolbox for 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openpcdet</forename><surname>Development Team</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenPCDet,2020.8" />
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Train in germany, test in the usa: Making 3d object detectors generalize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11713" to="11723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4376" to="4382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cia-ssd: Confident iou-aware single-stage object detector from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijin Chen Li Jiang Chi-Wing Fu Wu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2021</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Grnet: Gridding residual network for dense point cloud completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiageng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03761</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Grid-gcn for fast and scalable point cloud learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Ying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panqu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5661" to="5670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Zoomnet: Part-aware adaptive zooming neural network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liusheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12557" to="12564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Pixor: Realtime 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Sarpnet: Shape attention regional proposal network for lidar-based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Segvoxelnet: Exploring semantic context and depth-aware features for 3d vehicle detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Patch-based progressive 3d point set upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Hyeok</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yecheol</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><forename type="middle">Song</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Won</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12636</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Pu-net: Point cloud upsampling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pcn: Point completion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Iou loss for 2d/3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenye</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Joint 3d instance segmentation and object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xibin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for 3d keypoint estimation via view consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Karpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Jiquan Ngiam, and Vijay Vasudevan. End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
