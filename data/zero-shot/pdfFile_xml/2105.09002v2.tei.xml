<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QuatDE: Dynamic Quaternion Embedding for Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haipeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yang</surname></persName>
							<email>yangkun@std.uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxue</forename><surname>Yang</surname></persName>
							<email>yuxueyang@std.uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Rufai</surname></persName>
							<email>rufai6@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><forename type="middle">Wilson</forename><surname>Zakari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Owusu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">QuatDE: Dynamic Quaternion Embedding for Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph embedding has been an active research topic for knowledge base completion (KGC), with progressive improvement from the initial TransE, TransH, RotatE et al to the current state-of-the-art QuatE. However, QuatE ignores the multi-faceted nature of the entity and the complexity of the relation, only using rigorous operation on quaternion space to capture the interaction between entitiy pair and relation, leaving opportunities for better knowledge representation which will finally help KGC. In this paper, we propose a novel model, QuatDE, with a dynamic mapping strategy to explicitly capture the variety of relational patterns and separate different semantic information of the entity, using transition vectors to adjust the point position of the entity embedding vectors in the quaternion space via Hamilton product, enhancing the feature interaction capability between elements of the triplet. Experiment results show QuatDE achieves state-of-the-art performance on three wellestablished knowledge graph completion benchmarks. In particular, the MR evaluation has relatively increased by 26% on WN18 and 15% on WN18RR, which proves the generalization of QuatDE. * Corresponding author.</p><p>ship with Seven Spielberg, we can infer that Seven Spielberg is a friend of Bill Clinton equally, i.e. (Seven Spielberg, Friendship, Bill Clinton).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Billions of facts in the world can be stored in the Knowledge Graphs (KGs) with triples succinctly, and each triple (h, r, t) consist of two nodes and a directed edge between them. KGs such as Freebase <ref type="bibr">(Bollacker et al. 2008)</ref>, <ref type="bibr">YAGO(Suchanek, Kasneci, and Weikum 2007)</ref> and DBpedia <ref type="bibr" target="#b7">(Lehmann et al. 2015)</ref> are useful in many AI applications, such as question answering (QA) <ref type="bibr" target="#b1">(Cui et al. 2019)</ref>, recommended system , relation extraction(Wang et al. 2020), etc. Even though the KGs have been studied for many years, they still suffer from incompleteness, which makes their downstream assignments more challenging. As a result, researchers have put more focus on knowledge graph completion (KGC) task, which dedicates to predict missing links between nodes. In other words, KGC infers the implicit triples based on the true triplets that exist in the KGs. For example, if the triple (Bill Clinton, Friendship, Seven Spielberg) is correct, i.e. Bill Clinton has a friend- Conventional approaches for KGC have achieved substantial improvement via embedding entities and relations into low-dimensional continuous space, such as TransE <ref type="bibr">(Bordes et al. 2013)</ref>, <ref type="bibr">TransH(Wang et al. 2014)</ref>, TransD <ref type="bibr" target="#b5">(Ji et al. 2015)</ref>, TransR <ref type="bibr" target="#b8">(Lin et al. 2015)</ref>, etc. Instead of using a realvalued space, ComplEx <ref type="bibr" target="#b10">(Trouillon et al. 2016)</ref>, RotatE ) project entities and relations into a complex space preforming the strong representation ability. Meanwhile, QuatE ), Rotate3D  show the rich feature interaction capacity between triple with Hamilton product in quaternion space, obtaining stateof-the-art (SOTA) link prediction results. Now, although most models surrounding the KGC task are powerful enough, handling complex relation patterns are still a major challenge. QuatE utilizes quaternion embeddings to represent entities and relations. Relations are modelled as rotations enjoying the highly expressive ability. However, QuatE has two main problems: 1) failing to dig deep information, capturing the ability of representation and feature interaction between entities and relations are relatively weak, because it only relies on the rigorous rotation calculation of three embedding vectors; 2) Various relation patterns including one-to-many, many-to-one, and many-tomany are not to be considered. <ref type="figure" target="#fig_1">Figure 2 (a)</ref> shows the posi- and directed links (r n ) are represented as colored circles and black arrows, respectively. The red curves (S mn ) symbolize the dynamic mapping strategy which are determined by the elements in different triples. We draw similar entities into one dotted circle.</p><p>tions of entities and relations in quaternion space determined by QuatE are absolute rather than continuously changing, resulting the distances of similar entities (such as 1941 and Schindler's List, directed by Steven Spielberg) in the space are close. It's detrimental to predict the fact who played in 1941 and Schindler's list respectively.</p><p>To address this issue, we propose a new model QuatDE. The basic idea is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> (b), we consider the deep and multifaceted meaning of the entity. The same or similar entities in different triplets are represented by distinct vector representations, dynamically determined by specific relations and positions (head or tail). So, based on QuatE, we introduce a dynamic mapping strategy (S mn ) consisting of three additional quaternion vectors (defined as subject transfer vector, object transfer vector and relation transfer vector). It used to adaptively adjust the position and separates different semantic information of the entity. Meanwhile, the model gains full expressive ability.</p><p>Our Contributions In summary, our proposed model has the following contributions:</p><p>? We propose a novel knowledge graph embedding method in the quaternion space, using a dynamic mapping method (strongly related to entities and relations) to explicitly enhance the interaction among triplets, modeling the diversity of relations and multiple semantics of the entities.</p><p>? Our model has multiple-level improvements in embedding dimensions and has strong generalization capabilities (MR). To be precise, MR and Hit@10 of our model are still better than the QuatE model when the embedding dimension is 50 in WN18 (one-sixth compared with QuatE).</p><p>? QuatDE is an excellent result of combining QuatE and quaternion-valued neural network and we expound on the superiority of QuatDE from the perspective of QNN. ? Our method is extended to standard benchmark datasets:</p><p>FB15K-237, WN18 and WN18RR. Experiment results prove that our method is superior to the previous methods, and our code can be available on GitHub: https: //github.com/hopkin-ghp/QuatDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">State of the art</head><p>In this section, we will roughly describe the related work in two parts: translational distance models and semantic matching models. Note that semantic matching models exploit similarity-based scoring functions including bilinear models and neural network based models. Then we will discuss the connection between our approach and others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>translational distance models</head><p>The translational distance models utilize distance-based cost function and are efficient with low computational cost. We will describe this type of models with a unified formula, the scoring function is designed as follows:</p><formula xml:id="formula_0">f r (h, t) = ||S r (h) + r ? S r (t) l1/l2</formula><p>(1) in which function f r (h, t) represents the score of triple (h, r, t), and S r (?) donates as the linear function, projecting entity embedding vectors in the relation-specific vector space. Generally, such models measure distance from source entities to target entities with l1-norm or l2-norm.</p><p>TransE <ref type="bibr" target="#b0">(Abboud et al. 2020</ref>) is the most primitive but prominent model: S r (h) = h, S r (t) = t. However, TransE does not do well in dealing with 1-to-N, N-to-1, N-to-N relations. To solve this problem, the variants of above model, TransH, TransD, TransR, etc, consider unequal mapping strategy to project relation embedding in vector or matrix space, or capturing relational interactions . <ref type="bibr">TransH(Wang et al. 2014</ref>) interprets a relation as a hyperplane with a translation operation, thus, original entity embedding vectors are projected into corresponding relation hyperplane. TransH donates the normal vector in the hyperplane as w r : S r (e) = e ? w r ew r . TransR <ref type="bibr" target="#b8">(Lin et al. 2015)</ref> proposes to use a relation-specific projection matrix M r rather than hyperplane to project the entities embedding vectors into the space: S r (e) = M r e. It can be seen that large-scale parameters will be designed, so training the model is demanding which requires a lot of storage space. TransD <ref type="bibr" target="#b5">(Ji et al. 2015</ref>) models the relational mapping matrix M re in a more flexible way: S r (e) = M re e, where M re = w r w e + I. TranSparse <ref type="bibr" target="#b6">(Ji et al. 2016</ref>) leverages a numerical space to deal with the heterogeneity and imbalance issues of KGs. TransM <ref type="bibr" target="#b2">(Fan et al. 2014</ref>) focus on the structure of the knowledge graph via pre-calculating the distinct weight for each training triplet according to its relational mapping property: f r (h, t) = ?? r h + r ? t l1/l2 . TransAP <ref type="bibr" target="#b13">(Zhang, Sun, and Zhang 2020)</ref> notes that scoring function based on translation can't deal with the circle structure and hierarchical structure, so it introduces positionaware entity embeddings and attention mechanism to capture different semantic of triples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>semantic matching models</head><p>To be precise, translation models only obtain shallow linear characteristics through simple subtraction or multiplication operations. The scoring functions of semantic matching models reflect the confidence of the semantic information of the triples. RESCAL(Nickel, Tresp, and Kriegel 2011) represents each relation as a full rank matrix, optimizing a scoring function that computes a bilinear product between head and tail entity embedding vectors and relation matrix. Due to the large number of parameters, the model has overfitting problem. To alleviate the issue, DistMult <ref type="bibr" target="#b11">(Yang et al. 2014</ref>) uses a diagonal matrix for each relation which reduces parameters to a certain extent. Subsequently, Com-plEx <ref type="bibr" target="#b10">(Trouillon et al. 2016</ref>) extends DistMult to the complexvalued space, and use a trick that head and tail entity embeddings of the same entity are complex conjugates. Sim-plE(Kazemi and Poole 2018) and TuckER(Bala?evi?, Allen, and Hospedales 2019b) build on canonical polyadic (CP) and Tucker decomposition, respectively. TuckER shows that several linear models, RESCAL, DistMult, ComplEx, Sim-plE, are special cases of TuckER.</p><p>To explore deep information, ConvE(Dettmers et al. 2018) propose a simple multi-layer convolutional architecture for link prediction. ConvE splices and reshapes the subject and relation embedding vectors, then performs a 2D convolution operation, vector flattening, fully connected layer, finally, it matches with all candidate object embeddings. To obtain deeper features, the 3-column matrix of the triple embedding vector is used in <ref type="bibr">ConvKB(Nguyen et al. 2017)</ref> and <ref type="bibr">CapsE(Vu et al. 2019)</ref>, ConvKB changes the form of input data of ConvE while CapsE is based on the capsule network. InteractE <ref type="bibr" target="#b10">(Vashishth et al. 2020</ref>) is improved in the convolution step of ConvE, which captures entity and relation feature interactions through three ideas: Feature Permutation, Checkered Reshaping, and Circular Convolution. HypER(Bala?evi?, Allen, and Hospedales 2019a) propose a hypernetwork architecture that generates simplified relationspecific convolutional filters.</p><p>Recently, the approaches in geometric rotation with complex-valued and quaternion-valued embeddings which link prediction have proposed. RotatE  introduce relation-based rotation from subject entity to object entity in complex space, which can leverage the advantages of ComplEx <ref type="bibr" target="#b10">(Trouillon et al. 2016)</ref> and DistMult <ref type="bibr" target="#b11">(Yang et al. 2014</ref>) and infer multiple relation models. QuatE ) extends this idea to the quaternion space, and prove that by making rotations on two planes rather than on a single plane (RotatE), which has a high degree of freedom. Currently, Rotate3D ) models the non-commutative composition pattern in three-dimensional space with quaternion representation. BoxE <ref type="bibr" target="#b0">(Abboud et al. 2020</ref>) embeds entities as points, and relations as a set of hyper-rectangles (or boxes), which spatially characterize basic logical properties. HittER(Chen et al. 2020), a Hierarchical Transformer model consists of two Transformer blocks, joined to learn entity-relation composition and relational contextualization based on information of entity's neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture design</head><p>KGs are usually expressed as form: (E, R, O), in which E is the set of entities, R is the set of relations and O is the set of fact represented as triplets (h,r,t). Link prediction task aims to utilize observed triples to predict hidden triples. We use lowercase letters h, r, t to denote subject entities, relations, and object entities, and the corresponding bold letters h, r, t denote column embedding vectors. Real-valued space and quaternion space are defined as R and H, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>Quaternion algebra <ref type="bibr">(Hamilton 1844)</ref> is an expansion of the complex algebra, belongs to the hypercomplex number system. Usually, a quaternion q = a1 + bi + cj + dk ? H is composed of one real part and three imaginary parts, where a, b, c, d ? R, and 1, i, j, k are the quaternion unit basis and i 2 = j 2 = k 2 = ijk = ?1. Some basic definitions of quaternion are defined as follows (declaring two quaternions:</p><formula xml:id="formula_1">q 1 = a 1 + b 1 i + c 1 j + d 1 k and q 2 = a 2 + b 2 i + c 2 j + d 2 k):</formula><p>? Quaternion ordered pairs:</p><formula xml:id="formula_2">q = [a, v] = [a, bi + cj + dk] v ? R 3 (2) i.e. q 1 = [a 1 , v 1 ], q 2 = [a 2 , v 2 ].</formula><p>In this representation, we see the similarity between quaternion and complex number.</p><p>? Product of i, j, k:</p><formula xml:id="formula_3">ij = k jk = i ki = j ji = ?k kj = ?i ik = ?j<label>(3)</label></formula><p>? Quaternion Addition and Subtraction:</p><formula xml:id="formula_4">q 1 ? q 2 = [a 1 ? a 2 , v 1 ? v 2 ]<label>(4)</label></formula><p>? Inner Product:</p><formula xml:id="formula_5">q 1 ?q 2 = [a 1 ? a 2 , v 1 ? v 2 ] = a 1 ?a 2 +b 1 ?b 2 +c 1 ?c 2 +d 1 ?d 2<label>(5)</label></formula><p>? Conjugate q * of q:</p><formula xml:id="formula_6">q * = [a, ?v] = [a, ?bi ? cj ? dk]<label>(6)</label></formula><p>? Quaternion Normalization q of q:</p><formula xml:id="formula_7">q = q ? a 2 + b 2 + c 2 + d 2<label>(7)</label></formula><p>? Hamilton Product (Quaternion Multiplication):</p><formula xml:id="formula_8">q 1 ? q 2 = (a 1 a 2 ? b 1 b 2 ? c 1 c 2 ? d 1 d 2 ) + (a 1 b 2 + b 1 a 2 + c 1 d 2 ? d 1 c 2 ) i + (a 1 c 2 ? b 1 d 2 + c 1 a 2 + d 1 b 2 ) j + (a 1 d 2 + b 1 c 2 ? c 1 b 2 + d 1 a 2 ) k (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QuatDE</head><p>Specifically, we represent the entity embedding matrix Q ? H |E|?k and the relation embedding matrix W ? H |R|?k in the quaternion space, where | ? | represents the number of elements in set, and k represents the embedding dimension. We use the following formula to calculate score of a triple (h, r, t) with our model:</p><formula xml:id="formula_9">f r (h, t) = S r (Q h ) ? W r ? S r (Q t )<label>(9)</label></formula><p>in the formula, Q h , Q t and W r donated as:</p><formula xml:id="formula_10">Q h = a h + b h i + c h j + d h k Q t = a t + b t i + c t j + d t k W r = ar+bri+crj+drk ? a 2 r +b 2 r +c 2 r +d 2 r (10) Where Q h , Q t and unit quaternion W r ? H k . Correspond- ingly, the coefficient a h , b h , c h , d h , a t , b t , c t , d t , a r , b r , c r and d r ? R k .</formula><p>Here, S r (e) is a dynamic mapping function driven by entity ontology e and relation r. Symbol ? defines the Hamilton product operation and symbol ? defines the inner product operation, respectively. For our model, dynamic mapping function lies on entity transition matrix P ? H |E|?k and relation transition matrix V ? H |E|?k . We link h, t, r to vector P h , P t and V r ? H k , and S r (Q h ) and S r (Q t ) are represented as follows:</p><formula xml:id="formula_11">S r (Q h ) = Q h ? P h ? V r S r (Q t ) = Q t ? P t ? V r<label>(11)</label></formula><p>where P h = a ph + b ph i + c ph j + d ph k, P t = a pt + b pt i + c pt j + d pt k are normalized entity transfer vectors, and V r = a vr + b vr i + c vr j + d vr k is normalized relation transfer vectors, in which a ph , b ph , c ph , d ph , a pt , b pt , c pt , d pt , a vr , b vr , c vr , d vr ? R k . The entity transfer vector P e can adjust the spatial position of the same entity e when facing different triples, and the relation transfer vector V r projects same entity to different relation-special representation spaces. The dynamic mapping function S r (e) combines the embedding vector of the entity, the entity transfer vector and the relation transfer vector via Hamilton product. In this way, QuatDE captures more detailed information, and can fit in all triples in the overall knowledge representation of the knowledge graph. Formally, the score function of QuatDE can be represented as follows:</p><formula xml:id="formula_12">f r (h, t) = [Q h ? P h ? V r ] ? W r ? [Q t ? P t ? V r ] (12)</formula><p>Loss Function : QuatDE were trained using Adagrad optimizer, by minimizing the negative log-likelihood of the logistic model with L 2 regularization on the parameters w of our model: Our work is also inspired by the widespread success of Quaternion number across countless fields, such as heterogeneous image processing(Parcollet, Morchid, and Linar?s 2019), theme identification of telephone conversations(Parcollet, Morchid, and Linar?s 2017), automatic speech recognition <ref type="bibr" target="#b8">(Parcollet et al. 2018)</ref>. As far as we know, we are the first to use the idea of QNN to connect and explain the knowledge graph embedding models. In this section, we will  <ref type="figure" target="#fig_3">(Figure 4</ref>) from the perspective of QNN, which is not mentioned in QuatE. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the four components of a quaternion are represented by different colors, the weights are also represented with quaternions and transformation is performed with Hamilton product or inner product. In the QuatE model, the input is the quaternion embedding of head; the weights of the first layer are unit quaternion of relation; we can get the intermediate vector via Hamilton product; the intermediate vector is filled into the second layer, and carries out inner product with tail quaternion embedding; Finally, the resulting output vector can be viewed as real number, so we can calculate the triplet score by addition.</p><formula xml:id="formula_13">L = (h,r,t)?{O?O ? } log 1 + exp ?l (h,r,t) f r (h, t) +?||w 2 in which, l (h,r,t) = 1 for (h, r, t) ? O ?1 for (h, r, t) ? O ?<label>(</label></formula><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, on the basis of QuatE, we add four quaternion feed-forward layers, two layers are used to construct the dynamic strategy function of the head, and the other two are used for that of the tail. The weights of four new layers are closely related to elements of the triplet, i.e. subject transfer vector P h , object transfer vector P t and relation transfer vector V r , rather than random parameters like traditional neural networks. Meanwhile, increased layers enable more complicated interactions and is less likely to cause over-fitting.</p><p>Ability to handle complex relations We take relation director (1-to-N) as an example to describe our solution strategy, and choose the triples (Steven Spielberg, director, 1941) and (Steven Spielberg, director, Schindler's List), whose labels are true. So according to the score function of QuatDE, S director (Q 1941 ) ? S director (Q Schindler's List ), but the similarity between Q 1941 and Q Schindler's List are still depend on P 1941 and P Schindler's List which combine the feature extracted from the triples which include 1941 or Schindler's List. QuatDE makes a good trade-off between model efficiency and parameter complexity. Although introducing additional transfer vectors, QuatDE shows strong capabilities in some indicators by setting a smaller embedding dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connection to QuatE</head><p>We applied our motivation to QuatE, and the result proves the feasibility of QuatDE. We believe that through the Hamilton product, the diversity problem of other quaternion models in the knowledge graph completion task can be solved. The transfer vector is removed, QuatDE degenerates to QuatE.</p><p>Connection to TransH, TransR, TransD Although our model belongs to semantic matching models, it also incorporates the advantages of translational distance models. TransH, TransR, and TransD alleviate the problem that TransE does not do well in dealing with 1-to-N, N-to-1, Nto-N relations at different perspectives. TransH and TransR dynamically model the connection structure characteristics between different triples facing different relations. However, TransD do not only consider the diversity of relation-ships, but also pays attention to entities, which is in line with our paper. The biggest motivation of QuatDE comes from the problem of entities and relations diversity in quaternion space, and we extend this idea to quaternion space via Hamilton product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment and Analysis Datasets</head><p>We evaluate the performance of our model on three general data sets: WN18, WN18RR and FB15K-237. Statistics about the data set are shown in the table 2. FB15K <ref type="bibr">(Bordes et al. 2013</ref>) is a subset of Freebase, which is a large dataset contains the facts about sports, actors, movies and others. FB15K-237(Toutanova and Chen 2015) was extracted from FB15K and removed inverse relations, which prevent the leakage issue of test triples. WN18 <ref type="bibr">(Bordes et al. 2013</ref>) is the subset of Wordnet, and it is full of lexical relations between words. WN18 also has many inverse relations, hence, WN18RR(Dettmers et al. 2018) is removed inverse relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation protocol</head><p>We evaluate related methods on two tasks: link prediction and triplet classification. Link prediction task aims to infer the answer of the query (h, r, ?) or (?, t, r) where ? means the missing element. So, for each test triple, we calculate the score of all possible triples which can be obtained by substituting subject and object, and sort all scores in descending order.</p><p>Mean Rank (MR) and Hit at n are standard evaluation measures for these datasets which are applied in our models. MR measures the average rank of each triplet to predict the correct answer. MRR is defined as the average value of the reciprocated rank, and Hit@n calculates the probability of including the correct entity in the top n ranks. Note that we use the filtered metrics following bordes <ref type="bibr">(Bordes et al. 2013)</ref>. The metrics remove all golden triples that appear in either training, validation or test set from the ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training details</head><p>Our code is based on the OpenKE framework and QuatE, and was implemented with PyTorch. We set 100 batches for all datasets. We train our model for 3000 epochs and valid the performance every 300 epochs on three datasets. The dimensionality of embeddings k ? {50, 100, 200, 300, 400}, the number of negative triples for each triple ? {1, 5, 10}, learning rate is selected in {0.05, 0.1}, and L 2 regularization parameters ? ? {0.05, 0.1}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results</head><p>Link prediction <ref type="table">Table 1 shows</ref>   ). Most of the experimental data are quoted from the original papers. <ref type="table">Table 1</ref> shows that QuatDE achieved competitive scores than others.</p><p>More deeply, we can observe that: 1) QuatDE obtains best scores for MR, MRR, Hit@10 and Hit@3 on WN18RR, and MRR, Hit@10, Hit@3 and Hit@1 on FB15K-237. 2) On WN18, Rotate3D, QuatE and QuatDE are modeled in quaternion space and achieve comparable scores, but our model performs best on MR than the other two. 3) QuatDE is fruitful on FB15K-237, and gains a 0.017 higher MRR, 1.3% higher Hit@10, 1.8% higher Hit@3 and 2.0% higher Hit@1 than baseline QuatE. 4) The progressive of MR is most obvious. Notably, QuatDE obtains significant improvement of 2314 ? 1977 = 337 in MR (which is about 15% relative improvement) on WN18RR, and 162 ? 120 = 42 (which is about 26% relative improvement) on WN18.</p><p>Triplets Classification This task aims to judge whether a given triple (h, r, t) is correct or not. <ref type="table">Table 3 presents</ref>   <ref type="bibr" target="#b7">(Jia, Cheng, and Su 2020)</ref>) are taken from PCon-vKB <ref type="bibr" target="#b7">(Jia, Cheng, and Su 2020)</ref>. Overall, our model QuatDE obtained the best results on three data sets. Especially on FB15K-237 where QuatDE gains considerable improvements of 83.0 ? 81.8 = 1.2 compared with QuatE, and 83.0 ? 82.1 = 0.9 improvement with PConvKB.</p><p>Multi-relation analysis We analyzed the experimental results of complex relations on FB15K-237 and WN18RR. There are 224 relation types in the FB15K-237 test triplets, and QuatDE has achieved an equal or higher score than QuatE on 186 relations (which accounts for 84%) when taking Hit@10 as a measure, proving the ability of QuatDE to mitigate the multi-relations. As shown in <ref type="table">Table 4</ref>, we extract a few examples in each relational pattern <ref type="bibr">(1-to-1, 1-to-N, N-to-1, N-to-N)</ref>. It's obviously observed that QuatDE can also increase the prediction accuracy of the 1-to-1 relations, for example, the prediction accuracy of relation campuses and educational institution is 100% in Hit@10, the reason is attributed to intimate feature interaction between the elements of triplet via Hamilton product. And we confirm that QuatDE obtains better MR and Hit@10 than QuatE. <ref type="table" target="#tab_4">Table 5</ref> shows the MRR for each relation on WN18RR, confirming how to effectively model complex information in a space (real number, complex number or quaternion) is a key challenge. A superior-minded model usually performs better representation capabilities than based-model that only rely on three pure embeddings of the triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimension analysis</head><p>We make dimensional analysis on WN18 and FB15K-237, and compared each result when different embedding dimension size K was selected, which is shown in <ref type="figure">Figure 5</ref> and <ref type="figure">Figure 6</ref>.   <ref type="table">Table 3</ref>: Triplets classification result. <ref type="figure">Figure 5</ref> shows the impact of dimension on the QuatDE performance with the changing of dimension size K. We can observe that when the dimension is set to 10, Hit@10 has achieved notable scores higher than 0.94 and MRR is close to 0.90. It proves that QuatDE can generate fewer parameters to speed up the model, which allows QuatDE to be extended to a large knowledge graph. Hit@10 is used as the criterion for selecting the best model, and we fix the embedding dimension as 50, the results of the QuatDE experiment are better than the optimal score (0.959) of QuatE whose dimension is selected to 300 (six times than QuatDE), and QuatDE achieves the optimal model when the dimension is 100. When the dimension size is selected large than 100, the curve starts to fluctuate up and down gradually due to the introduction of excessive parameters. <ref type="figure">Figure 6</ref> depicts the dimensional-related experimental results of FB15K-237. On the whole, the resulting curve of FB15K-237 is smoother and more stable than that of WN18, which once again verifies the negative influence of the reverse relations on the experimental results, in detail, WN18 contains a large proportion of reverse relationships, while FB15K-237 excludes that relations. In addition, Hit@10  <ref type="table">Table 4</ref>: Hit@10 and MR of QuatE and QuatDE on some one-to-one, one-to-many, many-to-one, many-tomany relations in FB15K-237.</p><p>and MRR are both improved and slowly saturated around k = 300.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel embedding model QuatDE for link prediction task and triplet classification task in quaternion space. QuatDE utilizes a dynamic mapping method to enhance the character interaction with Hamiltonian product between the triple explicitly, and the model has a higher degree of freedom when training and fitting. The experimental results show that QuatDE outperforms other state-of-the-art models on three benchmark datasets WN18, WN18RR, and FB15K-237. Recently, QDN(quaternion deep network) has been proposed, but the network design of architecture isn't mature and lacking the interpretability, future work will focus on details and theory of QDN, furtherly explore the application of the quaternion deep network to the knowledge graph completion task, and expand the method to open domain knowledge graph tasks.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>subgraph of knowledge graph in FB15K-237. Entities are represented as golden blocks and directed links are represented as black arrows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Simple illustration of QuatDE, Entities (e m )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>13) in our model, the parameters w for L 2 norm include the embedding vectors and transfer vectors with rate ?. O is the set of golden triples, and O ? donates the set of negative triplets. Following opinion of Wang(Wang et al. 2014), adopting a Bernoulli distribution to generate negative triplets. architecture of QuatE Connection to quaternion-valued neural network (QNN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>architecture of QuatDE visualize the model architecture of QuatE (Figure 3) and QuatDE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>link prediction performance of various models: including translational distance models: TransE(Bordes et al. 2013), semantic matching models: ComplEx(Trouillon et al. 2016), ConvE(Dettmers et al. 2018), rotatE(Sun et al. 2019), QuatE(Zhang et al. 2019), and recent well-performing models: Inter-actE(Vashishth et al. 2020), ATTH(Chami et al. 2020), CompGCN(Vashishth et al. 2019), Rotate3D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>triple classification accuracy of different models on WN18RR, FB15K-237 and WN18. We reproduced the code of QuatE and recorded the result of the triple classification task in a table, while others(TransE(Bordes et al. 2013), TransH(Wang et al. 2014), HolE(Nickel, Rosasco, and Poggio 2016), ConvE(Dettmers et al. 2018), ConvKB(Nguyen et al. 2017), PConvKB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>MRR and Hit@10 with different dimensions of WN18. model for knowledge base completion. arXiv preprint arXiv:2007.06267. [Bala?evi?, Allen, and Hospedales 2019a] Bala?evi?, I.; Allen, C.; and Hospedales, T. M. 2019a. Hypernetwork knowledge graph embeddings. In International Conference on Artificial Neural Networks, 553-565. Springer. [Bala?evi?, Allen, and Hospedales 2019b] Bala?evi?, I.; Allen, C.; and Hospedales, T. M. 2019b. Tucker: Tensor factorization for knowledge graph completion. arXiv preprint arXiv:1901.09590. [Bollacker et al. 2008] Bollacker, K.; Evans, C.; Paritosh, P.; Sturge, T.; and Taylor, J. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 1247-1250. [Bordes et al. 2013] Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In Neural Information Processing Systems (NIPS), 1-9. [Chami et al. 2020] Chami, I.; Wolf, A.; Juan, D.-C.; Sala, F.; Ravi, S.; and R?, C. 2020. Low-dimensional hyperbolic knowledge graph embeddings. arXiv preprint arXiv:2005.00545. MRR and Hit@10 with different dimensions of FB15K-237.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>statistics about the experimental datasets.</figDesc><table><row><cell>Model</cell><cell cols="3">WN18RR FB15K-237 WN18</cell></row><row><cell>TransE</cell><cell>74.0</cell><cell>75.6</cell><cell>87.6</cell></row><row><cell>TransH</cell><cell>77.0</cell><cell>77.0</cell><cell>96.5</cell></row><row><cell>HoLE</cell><cell>71.4</cell><cell>70.3</cell><cell>88.1</cell></row><row><cell>ConvE</cell><cell>78.3</cell><cell>78.2</cell><cell>95.4</cell></row><row><cell>ConvKB</cell><cell>79.1</cell><cell>80.1</cell><cell>96.4</cell></row><row><cell>PConvKB</cell><cell>80.3</cell><cell>82.1</cell><cell>97.6</cell></row><row><cell>QuatE</cell><cell>86.7</cell><cell>81.8</cell><cell>97.9</cell></row><row><cell>QuatDE</cell><cell>87.6</cell><cell>83.0</cell><cell>98.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>MRR of QuatE vs QuatDE</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Hit@10</cell><cell>MRR</cell><cell></cell><cell></cell></row><row><cell>value</cell><cell>0.97 0.96 0.95 0.94 0.92 0.93</cell><cell>0.945 0.96</cell><cell>0.962 0.961 0.924 0.943</cell><cell>0.961 0.95</cell><cell>0.958 0.948</cell><cell>0.959 0.95</cell><cell>0.952 0.947</cell></row><row><cell></cell><cell>0.91</cell><cell></cell><cell>0.91</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell cols="2">0.897</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.89</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell><cell>500</cell><cell>600</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">dimension size</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Boxe: A box embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salvatori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12813</idno>
	</analytic>
	<monogr>
		<title level="m">Hitter: Hierarchical transformers for knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Chen et al. 2020</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<idno type="arXiv">arXiv:1903.02419</idno>
	</analytic>
	<monogr>
		<title level="m">Kbqa: learning question answering over qa corpora and knowledge bases</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transition-based knowledge graph embedding with relational mapping properties</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Pacific Asia conference on language, information and computing</title>
		<meeting>the 28th Pacific Asia conference on language, information and computing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rotate3d: Representing relations as rotations in three-dimensional space for knowledge graph embedding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">To the editors of the philosophical magazine and journal. The London</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1844. Lxxviii. on quaternions; or on a new system of imaginaries in algebra</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="489" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing</title>
		<meeting>the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge graph completion with adaptive sparse transfer matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00388</idno>
	</analytic>
	<monogr>
		<title level="m">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving knowledge graph embedding using locally and globally attentive relation paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04868</idno>
		<idno>Lehmann et al. 2015</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="167" to="195" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Dbpediaa large-scale. multilingual knowledge base extracted from wikipedia. Semantic web</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02121</idno>
		<idno>arXiv:1806.07789</idno>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 16th international conference on World Wide Web</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<idno type="arXiv">arXiv:1902.10197</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd workshop on continuous vector space models and their compositionality</title>
		<meeting>the 3rd workshop on continuous vector space models and their compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Observed versus latent features for knowledge base and text inference</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interacte: Improving convolution-based knowledge graph embeddings by increasing feature interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trouillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03082</idno>
		<idno>Vu et al. 2019</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">105928</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 2018 world wide web conference. Wang et al. 2020. and Liu, G. 2020. Direction-sensitive relation extraction using bi-sdp attention model. Knowledge-Based Systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<idno type="arXiv">arXiv:1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<idno type="arXiv">arXiv:1904.10281</idno>
		<title level="m">Quaternion knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improve the translational distance models for knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang ; Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="445" to="467" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
