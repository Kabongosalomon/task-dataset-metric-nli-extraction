<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Three Ways to Improve Semantic Segmentation with Self-Supervised Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Hoyer</surname></persName>
							<email>lhoyer@student.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
							<email>dai@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
							<email>yuhua.chen@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>K?ring</surname></persName>
							<email>adrian.koering@uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
							<email>suman.saha@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Luc</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Zurich</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ku Leuven</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Three Ways to Improve Semantic Segmentation with Self-Supervised Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training deep networks for semantic segmentation requires large amounts of labeled training data, which presents a major challenge in practice, as labeling segmentation masks is a highly labor-intensive process. To address this issue, we present a framework for semisupervised semantic segmentation, which is enhanced by self-supervised monocular depth estimation from unlabeled image sequences. In particular, we propose three key contributions: (1) We transfer knowledge from features learned during self-supervised depth estimation to semantic segmentation, (2) we implement a strong data augmentation by blending images and labels using the geometry of the scene, and (3) we utilize the depth feature diversity as well as the level of difficulty of learning depth in a studentteacher framework to select the most useful samples to be annotated for semantic segmentation. We validate the proposed model on the Cityscapes dataset, where all three modules demonstrate significant performance gains, and we achieve state-of-the-art results for semi-supervised semantic segmentation. The implementation is available at https://github.com/lhoyer/improving_segmentation_ with_selfsupervised_depth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b34">[35]</ref> have achieved state-of-the-art results for various computer vision tasks including semantic segmentation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b4">5]</ref>. However, training CNNs typically requires large-scale annotated datasets, due to millions of learnable parameters involved. Collecting such training data relies primarily on manual annotation. For semantic segmentation, the process can be particularly costly, due to the required dense annotations.</p><p>For example, annotating a single image in the Cityscapes dataset took on average 1.5 hours <ref type="bibr" target="#b8">[9]</ref>.</p><p>Recently, self-supervised learning has shown to be a promising replacement for manually labeled data. It aims to learn representations from the structure of unlabeled data, instead of relying on a supervised loss, which involves manual labels. The principle has been successfully applied in depth estimation for stereo pairs <ref type="bibr" target="#b15">[16]</ref> or image sequences <ref type="bibr" target="#b72">[73]</ref>. Additionally, semantic segmentation is known to be tightly coupled with depth. Several works have reported that jointly learning segmentation and supervised depth estimation can benefit the performance of both tasks <ref type="bibr" target="#b60">[61]</ref>. Motivated by these observations, we investigate the question: How can we leverage self-supervised depth estimation to improve semantic segmentation?</p><p>In this work, we propose a threefold approach to utilize self-supervised monocular depth estimation (SDE) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b16">17]</ref> to improve the performance of semantic segmentation and to reduce the amount of annotation needed. Our contributions span across the holistic learning process from data selection, over data augmentation, up to cross-task representation learning, while being unified by the use of SDE.</p><p>First, we employ SDE as an auxiliary task for semantic image segmentation under a transfer learning and multitask learning framework and show that it noticeably improves the performance of semantic segmentation, especially when supervision is limited. Previous works only cover full supervision <ref type="bibr" target="#b31">[32]</ref>, pretraining <ref type="bibr" target="#b25">[26]</ref>, or improving SDE instead of segmentation <ref type="bibr" target="#b19">[20]</ref>. Second, we propose a strong data augmentation strategy, DepthMix, which blends images as well as their labels according to the geometry of the scenes obtained from SDE. In comparison to previous methods <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b46">47]</ref>, DepthMix explicitly respects the geometric structure of the scenes and generates fewer artifacts (see <ref type="figure" target="#fig_0">Fig. 1</ref>). And third, we propose an Automatic Data Selection for Annotation, which selects the most useful samples to be annotated in order to maximize the gain. The selection is iteratively driven by two criteria: diversity and uncertainty. Both of them are conducted by a novel use of SDE as proxy task in this context. While our method follows the active learning cycle (model training ? query selection ? annotation ? model training) <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b65">66]</ref>, it does not require a human in the loop to provide semantic segmentation labels as the human is replaced by a proxy-task SDE oracle. This greatly improves flexibility, scalability, and efficiency, especially considering crowdsourcing platforms for annotation.</p><p>The main advantage of our method is that we can learn from a large base of easily accessible unlabeled image sequences and utilize the learned knowledge to improve semantic segmentation performance in various ways. In our experimental evaluation on Cityscapes <ref type="bibr" target="#b8">[9]</ref>, we demonstrate significant performance gains of all three components and improve the previous state-of-the-art for semi-supervised segmentation by a considerable margin. Specifically, our method achieves 92% of the full annotation baseline performance with only 1/30 available labels and even slightly outperforms it with only 1/8 labels. Our contributions summarize as follows:</p><p>(1) To the best of our knowledge, we are the first to utilize SDE as an auxiliary task to exploit unlabeled image sequences and significantly improve the performance of semi-supervised semantic segmentation.</p><p>(2) We propose DepthMix, a strong data augmentation strategy, which respects the geometry of the scene and achieves, in combination with (1), state-of-the-art results for semi-supervised semantic segmentation.</p><p>(3) We propose a novel Automatic Data Selection for Annotation based on SDE to improve the flexibility of active learning. It replaces the human annotator with an SDE oracle and lifts the requirement of having a human in the loop of data selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">(Semi-Supervised) Semantic Segmentation</head><p>Since Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b34">[35]</ref> were first used by Long et al. <ref type="bibr" target="#b39">[40]</ref> for semantic segmentation, they have become the state-of-the-art method for this problem. Most architectures are based on an encoder decoder design such as <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b5">6]</ref>. Skip connections <ref type="bibr" target="#b50">[51]</ref> and dilated convolutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b66">67]</ref> preserve details in the segmentation and spatial pyramid pooling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b4">5]</ref> aggregates different scales to exploit spatial context information.</p><p>Semi-supervised semantic segmentation makes use of additional unlabeled data during training. For that purpose, Souly et al. <ref type="bibr" target="#b58">[59]</ref> and Hung et al. <ref type="bibr" target="#b22">[23]</ref> utilize generative adversarial networks <ref type="bibr" target="#b17">[18]</ref>. Souly et al. <ref type="bibr" target="#b58">[59]</ref> use that concept to generate additional training samples, while Hung et al. <ref type="bibr" target="#b22">[23]</ref> train the discriminator based on the semantic segmentation probability maps. s4GAN <ref type="bibr" target="#b42">[43]</ref> extends this idea by adding a multi-label classification mean teacher <ref type="bibr" target="#b59">[60]</ref>. Another line of work <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">47]</ref> is based on consistency training, where perturbations are applied to unlabeled images or their intermediate features and a loss term enforces consistency of the segmentation. While Ouali et al. <ref type="bibr" target="#b47">[48]</ref> study perturbation of encoder features, CutMix <ref type="bibr" target="#b11">[12]</ref> mixes crops from the input images and their pseudo-labels to generate additional training data, and ClassMix <ref type="bibr" target="#b46">[47]</ref> uses pseudo-label <ref type="bibr" target="#b35">[36]</ref> class segments to build the mix mask. Our proposed DepthMix module is inspired by these methods but, in contrast, it also respects the structure of the scene when mixing samples. Commonly, several approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b10">11]</ref> include selftraining with pseudo-labels <ref type="bibr" target="#b35">[36]</ref> and a mean teacher framework <ref type="bibr" target="#b59">[60]</ref>, which is extended by Feng et al. <ref type="bibr" target="#b10">[11]</ref> with a class-balanced curriculum. Another related line of work is learning useful representations for semantic segmentation from self-supervised tasks such as tracking <ref type="bibr" target="#b62">[63]</ref>, context inpainting <ref type="bibr" target="#b48">[49]</ref>, colorization <ref type="bibr" target="#b33">[34]</ref>, depth estimation <ref type="bibr" target="#b25">[26]</ref> (see Section 2.3), or optical flow prediction <ref type="bibr" target="#b36">[37]</ref>. However, all of these approaches are outperformed by ImageNet pretraining and are, therefore, not relevant for semi-supervised semantic segmentation in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Active Learning</head><p>Another approach to reduce the number of required annotations is active learning. It iteratively requests the most informative samples to be labeled by a human. On the one side, uncertainty-based approaches select samples with a high uncertainty estimated based on, e.g., entropy <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b53">54]</ref> or ensemble disagreement <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b41">42]</ref>. On the other side, diversity-based approaches select samples, which most increase the diversity of the labeled set <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58]</ref>. For segmentation, active learning is typically based on uncertainty measures such as MC dropout <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b40">41]</ref>, entropy <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b63">64]</ref>, or multi-view consistency <ref type="bibr" target="#b56">[57]</ref>. In addition to methods selecting whole images <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b63">64]</ref>, several approaches apply a more fine-grained label request at region level <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b56">57]</ref> and also include a label cost estimate <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>In contrast to these works, we perform automatic data selection for annotation by replacing the human with SDE as oracle. Therefore, we do not require human-in-theloop annotation during the active learning cycle. Previous works performing unsupervised data selection are restricted to shallow models <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b38">39]</ref>, classification with low-dimensional inputs <ref type="bibr" target="#b37">[38]</ref>, or do not perform an iterative data selection <ref type="bibr" target="#b71">[72]</ref> to dynamically adapt to the uncertainty of the model trained on the currently labeled set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Improving Segmentation with SDE</head><p>Self-supervised depth estimation (SDE) aims to learn depth estimation from the geometric relations of stereo im-age pairs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> or monocular videos <ref type="bibr" target="#b72">[73]</ref>. Due to the better availability of videos, we use the latter approach, where a neural network estimates depth and camera motion of two subsequent images and a photometric loss is computed after a differentiable warping. The approach has been improved by several follow-up works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b73">74]</ref>.</p><p>The combination of semantic segmentation and SDE was studied in previous works with the goal of improving depth estimation. While <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref> learn both tasks jointly, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref> distill knowledge from a teacher semantic segmentation network to guide SDE. To further utilize coherence between semantic segmentation and SDE, <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b6">7]</ref> proposed additional loss terms that encourage spatial proximity between depth discontinuities and segmentation contours.</p><p>In contrast to these works, we do not aim to improve SDE but rather semi-supervised semantic segmentation. The closest to our approach are <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b45">[46]</ref>, and <ref type="bibr" target="#b31">[32]</ref>. Jiang et al. <ref type="bibr" target="#b25">[26]</ref> utilizes relative depth computed from optical flow to replace ImageNet pretraining for semantic segmentation.</p><p>In contrast, we additionally study multi-task learning of SDE and semantic segmentation and show that combining SDE with ImageNet features can even further boost performance. Novosel et al. <ref type="bibr" target="#b45">[46]</ref> and Klingner et al. <ref type="bibr" target="#b31">[32]</ref> improve the semantic segmentation performance by jointly learning SDE. However, they focus on the fully-supervised setting, while our work explicitly addresses the challenges of semisupervised semantic segmentation by using the depth estimates to generate additional training data and an automatic data selection mechanism based on SDE. Another work supporting the usefulness of SDE for semantic segmentation from another viewpoint is <ref type="bibr" target="#b30">[31]</ref> demonstrating an improved noise and attack robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In this section, we present our three ways to improve the performance of semantic segmentation with self-supervised depth estimation (SDE). They focus on three different aspects of semantic segmentation, covering data selection for annotation, data augmentation, and multi-task learning. Given N images and K image sequences from the same domain, our first method, Automatic Data Selection for Annotation, uses SDE learned on the K (unlabeled) sequences to select N A images out of the N images for human annotation (see Alg. 1). Our second approach, termed DepthMix, leverages the learned SDE to create geometrically-sound 'virtual' training samples from pairs of labeled images and their annotations (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Our third method learns semantic segmentation with SDE as an auxiliary task under a multi-tasking framework (see <ref type="figure">Fig. 2</ref>). The learning is reinforced by a multi-task pretraining process combining SDE with image classification.</p><p>For SDE, we follow the method of Godard et al. <ref type="bibr" target="#b16">[17]</ref>, which we briefly introduce in the following. We first train a depth estimation network f D to predict the depth of a target image and a pose estimation network f T to estimate the camera motion from the target image and the source image. Depth and pose are used to produce a differentiable warping to transform the source image into the target image. The photometric error between the target image and multiple warped source frames is combined by a pixel-wise minimum. Besides, stationary pixels are masked out and an edge-aware depth smoothness term is applied resulting in the final self-supervised depth loss L D . We refer the reader to the original paper <ref type="bibr" target="#b16">[17]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Automatic Data Selection for Annotation</head><p>We use SDE as proxy task for selecting N A samples out of a set of N unlabeled samples for a human to create semantic segmentation labels. The selection is conducted progressively in multiple steps, similar to the standard active learning cycle (model training ? query selection ? annotation ? model training). However, our data selection is fully automatic and does not require a human in the loop as the annotation is done by a proxy-task SDE oracle.</p><p>Let's denote by G, G A , and G U , the whole image set, the selected sub-set for annotation, and the un-selected sub-set. Initially, we have G A = ? and G U = G. The selection is driven by two criteria: diversity and uncertainty. Diversity sampling encourages that selected images are diverse and cover different scenes. Uncertainty sampling favors adding unlabeled images that are near a decision boundary (with high uncertainties) of the model trained on the current G A . For uncertainty sampling, we need to train and update the model with G A . It is inefficient to repeat this every time a new image is added. For the sake of efficiency, we divide the selection into T steps and only train the model T times. In each step t, n t images are selected and moved from G U to G A , so we have T t=1 n t = N A . After each step t, a model is trained on G A and evaluated on G U to get updated uncertainties for step t + 1. Diversity Sampling: To ensure that the chosen annotated samples are diverse enough to represent the entire dataset well, we use an iterative farthest point sampling based on the L2 distance over features ? SDE computed by an intermediate layer of the SDE network. At step t, for each of the n t samples, we choose the one in G U with the largest distance to the current annotation set G A . The set of selected samples G A is iteratively extended by moving one image at a time from G U to G A until the n t images are collected:</p><formula xml:id="formula_0">G U = G U \ {I i } and G A = G A ? {I i },<label>(1)</label></formula><formula xml:id="formula_1">i = arg max Ii?G U min Ij ?G A ||? SDE i ? ? SDE j || 2 .<label>(2)</label></formula><p>Uncertainty Sampling: While Diversity Sampling is able to select diverse new samples, it is unaware of the uncertainties of a semantic segmentation model over these samples. Uncertainty Sampling aims to select difficult samples,</p><formula xml:id="formula_2">Algorithm 1: Automatic Data Selection 1: t = 1 2: i ? uniform(1, N ) 3: G A = {I i } and G U = G U \ {I i } 4: for k = 2 to N A do 5: if k == t t =1 n t then 6: Train depth student ? SIDE on the current G A 7: Calculate E(i) ?I i ? G U 8: t = t + 1 9: end if 10:</formula><p>if t == 1 then 11:</p><p>Obtain index i according to Eq. 2 <ref type="bibr">12:</ref> else <ref type="bibr">13:</ref> Obtain index i according to Eq. 4 <ref type="bibr">14:</ref> end if <ref type="bibr" target="#b14">15</ref>:</p><formula xml:id="formula_3">G A = G A ? {I i } and G U = G U \ {I i } 16: end for</formula><p>i.e., samples in G U that the model trained on the current G A cannot handle well. In order to train this model, active learning typically uses a human-in-the-loop strategy to add annotations for selected samples. In this work, we use a proxy task based on self-supervised annotations, which can run automatically, to make the method more flexible and efficient. Since our target task is single-image semantic segmentation, we choose to use single-image depth estimation (SIDE) as the proxy task. Importantly, due to our SDE framework, depth pseudo-labels are available for G. Using these pseudo-labels, we train a SIDE method on G A and measure the uncertainty of its depth predictions on G U . Due to the high correlation of single-image semantic segmentation and SIDE, the generated uncertainties are informative and can be used to guide our sampling procedure. As the depth student model is trained only on G A , it can specifically approximate the difficulty of candidate samples with respect to the already selected samples in G A . The student is trained from scratch in each step t, instead of being finetuned from t ? 1, to avoid getting stuck in the previous local minimum. Note that the SDE method is trained on a much larger unlabeled dataset, i.e., the K image sequences, and can provide good guidance for the SIDE method.</p><p>In particular, the uncertainty is signaled by the disparity error between the student network f SIDE and the teacher network f SDE in the log-scale space under L1 distance:</p><formula xml:id="formula_4">E(i) = || log(1 + f SDE (I i )) ? log(1 + f SIDE (I i ))|| 1 . (3)</formula><p>As the disparity difference of far-away objects is small, the log-scale is used to avoid the loss being dominated by closerange objects. This criterion can be added into Eq. 2 to also select samples with higher uncertainties for the dataset update in Eq. 1:</p><formula xml:id="formula_5">i = arg max Ii?G U min Ij ?G A ||? SDE i ? ? SDE j || 2 + ? E E(i),<label>(4)</label></formula><p>where ? E is a parameter to balance the contribution of the two terms. For diversity sampling, we still use SDE features instead of SIDE student features as SDE is trained on the entire dataset, which provides better features for diversity estimation. When n t images have been selected according to Eq. 1 and Eq. 4 at step t, a new SIDE model will be trained on the current G A in order to continue further. As presented previously, our selection proceeds progressively in T steps until we collect all N A images. The algorithm of this selection is summarized in Alg. 1, where t t =1 n t describes the desired size of G A at the end of step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">DepthMix Data Augmentation</head><p>Inspired by the recent success of data augmentation approaches that mixup pairs of images and their (pseudo) labels to generate more training samples for semantic segmentation <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">47]</ref>, we propose an algorithm, termed DepthMix, to utilize self-supervised depth estimates to maintain the integrity of the scene structure during mixing.</p><p>Given two images I i and I j of the same size, we would like to copy some regions from I i and paste them directly into I j to get a virtual sample I . The copied regions are indicated by a mask M , which is a binary image of the same size as the two images. The image creation is done as</p><formula xml:id="formula_6">I = M I i + (1 ? M ) I j ,<label>(5)</label></formula><p>where denotes the element-wise product. The label maps of the two images S i and S j are mixed up with the same mask M to generate S . The mixing can be applied to labeled data and unlabeled data using human ground truths or pseudo-labels, respectively. Existing methods generate this mask M in different ways, e.g., randomly sampled rectangular regions <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b11">12]</ref> or randomly selected object segments <ref type="bibr" target="#b46">[47]</ref>. In those methods, the structure of the scene is not considered and foreground and background are not distinguished. We find images synthesized by these methods often violate the geometric relationships between objects. For instance, a distant object can be copied onto a closerange object or only unoccluded parts of mid-range objects are copied onto the other image. Imagine how strange it is to see a pedestrian standing on top of a car or to see sky through a hole in a building (just as shown in <ref type="figure" target="#fig_0">Fig. 1 left)</ref>.</p><p>Our DepthMix is designed to mitigate this issue. It uses the estimated depthDi andDj of the two images to generate the mix mask M that respects the notion of geometry. It is implemented by selecting only pixels from I i whose depth values are smaller than the depth values of the pixels at the same locations in I j :</p><formula xml:id="formula_7">M (a, b) = 1 ifD i (a, b) &lt;D j (a, b) + 0 otherwise<label>(6)</label></formula><p>where a and b are pixel indices, and is a small value to avoid conflicts of objects that are naturally at the same depth plane such as road or sky. By using this M , DepthMix respects the depth of objects in both images, such that only closer objects can occlude further-away objects. We illustrate this advantage of DepthMix with an example in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semi-Supervised Semantic Segmentation</head><p>In this section, we train a semantic segmentation model utilizing the labeled image dataset G A , the unlabeled image dataset G U , and K unlabeled image sequences. We first discuss how to exploit SDE on the image sequences to improve our semantic segmentation. We then show how to use G U to further improve the performance.</p><p>Learning with Auxiliary Tasks: For learning semantic segmentation and SDE jointly, we use a network with shared encoder f E ? and a separate depth f D ? and segmentation decoder f S ? (see <ref type="figure">Fig. 2</ref>). The depth branch is trained using the SDE loss L D and the segmentation branch g S ? = f S ? ? f E ? is trained using the pixel-wise cross-entropy L ce . In order to initialize the pose estimation network and the depth decoder properly, the architecture is first trained on K unlabeled image sequences for SDE. As a common practice, we initialize the encoder with ImageNet weights as they provide useful semantic features learned during image classification. To avoid forgetting semantic features during the SDE pretraining, we utilize a feature distance loss between the current bottleneck features f E ? and the bottleneck features of the encoder with ImageNet weights f E I :</p><formula xml:id="formula_8">L F = ||f E ? ? f E I || 2 .<label>(7)</label></formula><p>The loss for the depth pretraining is the weighted sum of the SDE loss and the ImageNet feature distance loss:</p><formula xml:id="formula_9">L P = L D + ? F L F .<label>(8)</label></formula><p>To additionally incorporate transfer learning from depth estimation to semantic segmentation, the weights of f D ? are used to initialize f S ? . For effective multi-task learning, we use an attention-guided distillation module <ref type="bibr" target="#b64">[65]</ref> to exchange useful intermediate features between both decoders. Learning with Unlabeled Images: In order to further utilize the unlabeled dataset G U , we generate pseudo-labels using the mean teacher algorithm <ref type="bibr" target="#b59">[60]</ref>, which is commonly used in semi-supervised learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b46">47]</ref>. For that purpose, an exponential moving average is applied to the weights of the semantic segmentation model g S ? to obtain the weights of the mean teacher ? T :</p><formula xml:id="formula_10">? T = ?? T + (1 ? ?)?.<label>(9)</label></formula><p>To generate the pseudo-labels, an argmax over the classes C is applied to the prediction of the mean teacher.</p><formula xml:id="formula_11">S U = arg max c?C (g S ? T (I U )).<label>(10)</label></formula><p>The mean teacher can be considered as a temporal ensemble, resulting in stable predictions for the pseudo-labels, while the argmax ensures confident predictions <ref type="bibr" target="#b46">[47]</ref>. For the semi-supervised setting, the segmentation network is trained with labeled samples (I A , S A ) and pseudolabeled samples (I U , S U ):</p><formula xml:id="formula_12">L SSL = L ce (g S ? (I A ), S A ) + ? P (S U )L ce (g S ? (I U ), S U ))<label>(11)</label></formula><p>? P (S U ) is chosen to reflect the quality of the pseudo-label represented by the fraction of pixels exceeding a threshold ? for the predicted probability of the most confident class max c?C (g S ? T (I U )), as suggested in <ref type="bibr" target="#b46">[47]</ref>. We incorporate DepthMix samples (I , S ), which are obtained from the combined labeled and pseudo-labeled data pool I i , I j ? G A ? G U (see Eq. 5), into Eq. 11 to replace the unlabeled samples (S U , L U ). Our semi-supervised learning is now changed to:</p><formula xml:id="formula_13">L SSL = L ce (g S ? (I A ), S A ) + ? P (S )L ce (g S ? (I ), S )).<label>(12)</label></formula><p>4. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Dataset: We evaluate our method on the Cityscapes dataset <ref type="bibr" target="#b8">[9]</ref>, which consists of 2975 training and 500 validation images with semantic segmentation labels from European street scenes. We downsample the images to 1024 ? 512 pixels. Besides, random cropping to a size of 512?512 and random horizontal flipping are used in the training. Importantly, Cityscapes provides 20 unlabeled frames before and 10 after the labeled image, which are used for SDE training. During the semi-supervised segmentation, only the originally 2975 labeled training images are used. They are randomly split into a labeled and an unlabeled subset. Network Architecture: Our network consists of a shared ResNet101 <ref type="bibr" target="#b20">[21]</ref> encoder with output stride 16 and a separate decoder for segmentation and SDE. The decoder consists of an ASPP <ref type="bibr" target="#b4">[5]</ref> block to aggregate features from multiple scales and another four upsampling blocks with skip connections <ref type="bibr" target="#b50">[51]</ref>. For SDE, the upsampling blocks have a disparity side output at the respective scale. For effective multi-task learning, we additionally follow PAD-Net <ref type="bibr" target="#b64">[65]</ref> and deploy an attention-guided distillation module after the third decoder block. It serves the purpose of exchanging useful features between segmentation and depth estimation. Training: For the SDE pretraining, the depth and pose network are trained using Adam <ref type="bibr" target="#b29">[30]</ref>, a batch size of 4, and an initial learning rate of 1 ? 10 ?4 , which is divided by 10 after 160k iterations. The SDE loss is calculated on four scales with three subsequent images. During the first 300k iterations, only the depth decoder and the pose network are trained. Afterwards, the depth encoder is fine-tuned with an ImageNet feature distance ? F = 1 ? 10 ?2 for another 50k iterations. The encoder is initialized with ImageNet weights, either before depth pretraining or before semantic segmentation if depth pretraining is ablated. For the multi-task setting, we train the network using SGD with a learning rate of 1 ? 10 ?3 for the encoder and depth decoder, 1 ? 10 ?2 for the segmentation decoder, and 1 ? 10 ?6 for the pose network. The learning rate is reduced by 10 after 30k iterations and trained for another 10k iterations. A momentum of 0.9, a weight decay of 5 ? 10 ?4 , and a gradient norm clipping to 10 are used. The loss for segmentation and SDE are weighted equally. The mean teacher has ? = 0.99 and within an iteration, the network is trained on a clean labeled and an augmented mixed batch with size 2, respectively. The latter uses DepthMix with = 0.03, color jitter, and Gaussian blur. Data Selection for Annotation: In the data selection experiment, we use a slimmed network architecture with a ResNet50 encoder and fewer decoder channels for f SIDE . It is trained using Adam with 1 ? 10 ?4 learning rate and polynomial decay with exponent 0.9 for faster convergence. For calculating the depth feature diversity, we use the output of the second depth decoder block after SDE pretraining. It is downsampled by average pooling to a size of 8x4 pixels and the feature channels are normalized to zero-mean unit-variance over the dataset. The student depth error is weighted by ? E = 1000. The number of the selected samples ( t t =1 n t ) is iteratively increased to 25, 50, 100, 200, 372, and 744. For each subset, a student depth network is trained from scratch for 4k, 8k, 12k, 16k, and 20k iterations, respectively, to calculate the student depth error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semi-Supervised Semantic Segmentation</head><p>First, we compare our approach with several state-of-theart semi-supervised learning approaches. We summarize the results in Tab. 1. The performance (mIoU in %) of the semi-supervised methods and their baselines (only trained on the labeled dataset) are shown for a different number of labeled samples. As the performance of the baselines differs, there are columns showing the absolute improvement for better comparability. As our baseline utilizes a more capable network architecture due to the U-Net decoder with ASPP as opposed to a DeepLabv2 decoder used by most previous works, we also reimplemented the state-of-the-art method, ClassMix <ref type="bibr" target="#b46">[47]</ref> with our network architecture and training parameters to ensure a direct comparison.</p><p>As shown in Tab. 1, our method (without data selection) outperforms all other approaches on each labeled subset size for both the absolute performance as well as the improvement to the baseline. The only exception is the absolute improvement of the original results of ClassMix for 100 labeled samples. However, if we consider ClassMix trained in our setting, our method outperforms it also in this case. This can be explained by the considerably higher baseline performance in our setting, which increases the difficulty to achieve an high improvement. Adding data selection even further increases the performance by a significant margin, so that our method, trained with only 1/8 of the labels, even slightly outperforms the fully-supervised baseline.</p><p>To identify whether the improvement originates from access to more unlabeled data or from the effectiveness of our approach, we compare to another baseline "ClassMix (+Video)". More specifically, we also provide all unlabeled image sequences to ClassMix and see how much it can benefit from this additional amount of unlabeled data. Experimental results show no significant difference. This is probably due to the high correlation of the Cityscapes image dataset and the video dataset (the images are the 20th frames of the video clips).</p><p>The adequacy of our approach is also reflected in the example predictions in <ref type="figure" target="#fig_1">Fig. 3</ref>. We can observe that the contours of classes are more precise. Moreover, difficult objects such as bus, train, rider, or truck can be better distinguished. This observation is also quantitatively confirmed by the class-wise IoU improvement shown in <ref type="figure" target="#fig_2">Fig. 4.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Next, we analyze the individual contribution of each component of the proposed method. For this purpose, we <ref type="bibr" target="#b0">1</ref> Results of the reimplementation in our experiment setting.  test several ablated versions of our model for both the cases of 372 and 2975 labeled samples. We summarize the results in Tab. 2. It can be seen that each contribution adds a significant performance improvement over the baseline. For 372 (2975) annotated samples, transfer and multi-task learning improve the performance by +2.10 (+1.99), Depth-Mix with pseudo-labels by +5.00 (+2.06), and automatic data selection by +5.11 (-) mIoU percentage points. As our components are orthogonal, combining them even further increases performance. SDE Multi-Tasking and DepthMix achieve +7.52 (+3.40) and all three components +8.87 (-) mIoU percentage points improvement. Note that the high variance for few labeled samples is mostly due to the high influence of the randomly selected labeled subset. The chosen subset affects all configurations equally and the reported improvements are consistent for each subset. Furthermore, we compare DepthMix with ClassMix as a standalone. For a fair comparison, we additionally include mixing labeled samples with their ground truth to ClassMix. It can be seen that DepthMix outperforms the ClassMix by 0.98 (0.23) percentage points for 372 (2975) annotated samples, which shows the effect of the geometry aware augmentation. <ref type="figure" target="#fig_3">Fig. 5</ref> shows DepthMix examples demonstrating that SDE allows to correctly model occlusions and to produce synthetic samples with a realistic appearance.</p><p>For more insights into possible reasons for these improvements, we visualize the improvement of the architecture components over the baseline for each class separately in <ref type="figure" target="#fig_2">Fig. 4</ref>. It can be seen that depth multi-task learning (DM) improves mostly the classes fence, traffic light, traffic sign, rider, truck, and motorcycle, which is possibly due to their characteristic depth profile learned during SDE. For example, a good depth estimation performance requires correctly segmenting poles or traffic signs as missing them can cause large depth errors. This can also be seen in <ref type="figure" target="#fig_1">Fig. 3</ref>. Depth-Mix (XD) further improves the performance of wall, truck, bus, and train. This might be caused by the fact the Depth-Mix presents those rather difficult objects in another context, which might help the network to generalize better.</p><p>In the suppl. materials, we further show that our method is still applicable if SDE is trained on a different dataset than semantic segmentation within a similar visual domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Automatic Data Selection for Annotation</head><p>Finally, we evaluate the proposed automatic data selection. Tab. 3 shows a comparison of our method with a baseline and a competing method. The baseline selects the la- beled samples randomly, while the second, strong competitor uses active learning and iteratively chooses the samples with the highest segmentation entropy. In contrast to our method, this requires a human in the loop to create the semantic labels for iteratively selected images. It can be seen that our method with the combined Diversity Sampling and Uncertainty Sampling (DS+US) outperforms both comparison methods, demonstrating the effectiveness of ensuring diversity and exploiting difficult samples based on depth. It also supports the assumption that depth estimation and semantic segmentation are correlated in terms of sample difficulty. The class-wise analysis (see the last row of <ref type="figure" target="#fig_2">Fig. 4)</ref> shows that data selection significantly improves the performance of truck, bus, and train, which are usually difficult to distinguish in a semi-supervised setting. We would like to note that our automatic data selection method can be applied to any semantic segmentation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have studied how self-supervised depth estimation (SDE) can be utilized to improve semantic segmentation in both the semi-supervised and the fullysupervised setting. We introduced three effective strategies capable of leveraging the knowledge learned from SDE. First, we show that the SDE feature representation can be transferred to semantic segmentation, by means of SDE pretraining and joint learning of segmentation and depth. Second, we demonstrate that the proposed DepthMix strategy outperforms related mixing strategies by avoiding inconsistent geometry of the generated images. Third, we present an automatic data selection for annotation algorithm based on SDE, which does not require human-in-the-loop annotations. We validate the benefits of the three components by extensive experiments on Cityscapes, where we demonstrate significant gains over the baselines and competing methods. By using SDE, our approach achieves state-ofthe-art performance, suggesting that SDE can be a valuable self-supervision for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Further Implementation Details</head><p>In the following paragraphs, a more detailed description of the network architecture and the training is provided. The reference implementation is available at https://github.com/lhoyer/improving_ segmentation_with_selfsupervised_depth.</p><p>Network Architecture The neural network combines a DeepLabv3 <ref type="bibr" target="#b4">[5]</ref> with a U-Net <ref type="bibr" target="#b50">[51]</ref> decoder for depth and segmentation prediction each. As encoder, a ResNet101 with dilated (instead of strided) convolutions in the last block is used, following <ref type="bibr" target="#b4">[5]</ref>. Features from multiple scales are aggregated by an ASPP <ref type="bibr" target="#b4">[5]</ref> block with dilation rates of 6, 12, and 18. Similar to U-Net <ref type="bibr" target="#b50">[51]</ref>, the decoder has five upsampling blocks with skip connections. Each upsampling block consists of a 3x3 convolution layer (except the first block, which is the ASPP), a bilinear upsampling operation, a concatenation with the encoder features of the corresponding size (skip connection), and another 3x3 convolution layer. Both convolutional layers are followed by an ELU non-linearity. The number of output channels for the blocks are 256, 256, 128, 128, and 64. The last four blocks also have another 3x3 convolutional layer followed by a sigmoid activation attached to their output for the purpose of predicting the disparity at the respective scale. For effective multi-task learning, we additionally follow PAD-Net <ref type="bibr" target="#b64">[65]</ref> and deploy an attention-guided multi-modal distillation module with additional side output for semantic segmentation after the third decoder block. In experiments without multi-task learning, only the semantic segmentation decoder is used. For pose estimation, we use a lightweight ResNet18 encoder followed by four convolutions to produce the translation and the rotation in angle-axis representation as suggested in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Runtime To give an impression of the computational complexity of our architecture, we provide the training time per iteration and the inference time per image on an Nvidia Tesla P100 in Tab. S4. The values are averaged over 100 iterations or 500 images, respectively. Please note that these timings include the computational overhead of the training framework such as logging and validation metric calculation.</p><p>Data Selection In the data selection experiment, we use a slimmed network architecture for f SIDE with a ResNet50 backbone, 256, 128, 128, 64, and 64 decoder channels, and BatchNorm <ref type="bibr" target="#b24">[25]</ref> in the decoder for efficiency and faster convergence. The depth student network is trained using a berHu loss <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b32">33]</ref>. The quality of the selected subset with annotations G A is evaluated for semantic segmentation using our default architecture and training hyperparameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross-Dataset Transfer Learning</head><p>In this section, we show that the unlabeled image sequences and the labeled segmentations can also originate from different datasets within similar visual domains. For that purpose, we train the SDE on Cityscapes sequences and learn the semi-supervised semantic segmentation on the CamVid dataset <ref type="bibr" target="#b1">[2]</ref>, which contains 367 train, 101 validation, and 233 test images with dense semantic segmentation labels for 11 classes from street scenes in Cambridge. To ensure a similar feature resolution, we upsample the CamVid images from 480 ? 360 to 672 ? 512 pixels and randomly crop to a size of 512 ? 512. <ref type="table" target="#tab_5">Table S5</ref> shows that the results on CamVid are similar to our main results on Cityscapes. For 50 labeled training samples, SDE pretraining improves the mIoU by 3.6 percentage points, pseudo-labels and DepthMix by another 4.07 percentage points, and data selection by another 1.41 percentage points. In the end, our proposed method significantly outperforms ClassMix by 2.34 percentage points for 50 labeled samples and 2.14 percentage points for 100 labeled samples. Also for the fully labeled dataset, our method can improve the performance by 3.29 percentage points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further Example Predictions</head><p>Further examples for semantic segmentation and SDE are shown in <ref type="figure">Fig. S6</ref>. In general, the same observations as in the main paper can be made. Our method provides clearer segmentation contours for objects that are bordered by pronounced depth discontinuities such as pole, traffic sign, or traffic light. We also show improved differentiation between similar classes such as truck, bus, and train. On the downside, SDE sometimes fails for cars driving directly in front of the camera (see 7th row in <ref type="figure">Fig. S6</ref>) and violating the reconstruction assumptions. Those cars are observed at the exact same location across the image sequence and can not be correctly reconstructed during SDE training, even with correct depth and pose estimates. However, this differentiation between moving and non-moving cars does not hinder the transfer of SDE-learned features to semantic segmentation but can cause problems with DepthMix (see Section D).  <ref type="figure">Figure S6</ref>. Further example predictions for 100 annotated training samples including the self-supervised disparity estimate of the multi-task learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. DepthMix Real-World Examples</head><p>In <ref type="figure" target="#fig_4">Fig. S7, we</ref> show examples of DepthMix applied to Cityscapes crops. Generally, it can be seen that DepthMix works well in most cases. The self-supervised depth estimates allow to correctly model occlusions and the produced synthetic samples have a realistic appearance.</p><p>In <ref type="figure">Fig. S8</ref>, we show a selection of typical failure cases of DepthMix. First, the SDE can be inaccurate for dynamic objects (see Sec. C), which can cause an inaccurate structure within the mixed image ( <ref type="figure">Fig. S8 a, b, and c)</ref>. However, this type of failure case is common in ClassMix and its frequency is greatly reduced with DepthMix. A remedy might be SDE extensions that incorporate the motion of dynamic objects <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32]</ref>. Second, in some cases, the SDE can be imprecise and the depth discontinuities do not appear at the same location as the class border. This can cause artifacts in the mixed image ( <ref type="figure">Fig. S8 d and e</ref>) but also in the mixed segmentation ( <ref type="figure">Fig. S8</ref> e: sky within the building). Note that the same can happen for ClassMix when using pseudo-labels for creating the mix mask.  <ref type="figure">Figure S8</ref>. DepthMix failure cases. From left to right, the source images with their SDE estimate, the mixed image I overlaid with border of the mix mask M in blue/orange depending on the adjacent source image (i -orange, j -blue), the mixed image without visual guidance I , the mixed depth D , and the mixed segmentation S are shown. For simplicity, the source segmentations for the mixed segmentation S originate from the ground truth labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Concept of the proposed DepthMix augmentation (refer to Sec. 3.2) and its baseline ClassMix<ref type="bibr" target="#b46">[47]</ref>. By utilizing SDE, DepthMix mitigates geometric artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Example semantic segmentations of our method for 100 labeled samples in comparison with ClassMix<ref type="bibr" target="#b46">[47]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Improvement of the class-wise IoU over the baseline performance for 372 labeled samples (DM: SDE Multi-Task Learning, XD: DepthMix with Pseudo-Labels, S: Data Selection).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>DepthMix applied to Cityscapes crops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure S7 .</head><label>S7</label><figDesc>DepthMix applied to Cityscapes crops. From left to right, the source images with their SDE estimate, the mixed image I overlaid with border of the mix mask M in blue/orange depending on the adjacent source image (i -orange, j -blue), the mixed image without visual guidance I , the mixed depth D , and the mixed segmentation S are shown. For simplicity, the source segmentations for the mixed segmentation S originate from the ground truth labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>ce Camera Motion T t,t+1 SDE Loss L D Pose CNN Shared Encoder f E Depth Decoder f D Semantic Decoder f S Image I t Image I t+1 Depth D t Segmentation S t Ground Truth S tF igure</head><label></label><figDesc>2. Architecture for learning semantic segmentation with SDE as auxiliary task according to Sec. 3.3. The dashed paths are only used during training and only if image sequences and/or segmentation ground truth are available for a training sample.</figDesc><table><row><cell>Feature Distance Loss L F</cell><cell></cell></row><row><cell>ImageNet Encoder f I</cell><cell>Seg. Loss L</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance on the Cityscapes validation set (mIoU in %, standard deviation over 3 random seeds). ?1.11 55.25 ?0.66 60.57 ?1.13 67.53 ?0.35 CutMix [12] 51.20 ?2.29 +6.79 60.34 ?1.24 +5.09 63.87 ?0.71 +3.30 67.68 ?0.37 +0.15 ?1.61 +10.23 61.35 ?0.62 +6.51 63.63 ?0.33 +3.55 -Baseline 48.75 ?1.61 59.14 ?1.02 63.46 ?0.38 67.77 ?0.13 ClassMix [47] 1 56.82 ?1.65 +8.07 63.86 ?0.41 +4.72 65.57 ?0.71 +2.11 -ClassMix [47] (+Video) 56.79 ?1.98 +8.04 63.22 ?0.84 +4.08 65.72 ?0.18 +2.26 68.23 ?0.70 +0.46 Ours 58.40 ?1.36 +9.65 66.66 ?1.05 +7.52 68.43 ?0.06 +4.98 71.16 ?0.16 +3.40 Ours (+Data Selection) 62.09 ?0.39 +13.34 68.01 ?0.83 +8.87 69.38 ?0.33 +5.92 -</figDesc><table><row><cell>Labeled Samples</cell><cell>1/30 (100)</cell><cell>1/8 (372)</cell><cell>1/4 (744)</cell><cell>Full (2975)</cell><cell></cell></row><row><cell>Baseline [23]</cell><cell>-</cell><cell>55.50</cell><cell>59.90</cell><cell>66.40</cell><cell></cell></row><row><cell>Adversarial [23]</cell><cell>-</cell><cell>58.80</cell><cell>+3.30 62.30</cell><cell>+2.40 -</cell><cell></cell></row><row><cell>Baseline [43]</cell><cell>-</cell><cell>56.20</cell><cell>60.20</cell><cell>66.00</cell><cell></cell></row><row><cell>s4GAN [43]</cell><cell>-</cell><cell>59.30</cell><cell>+3.10 61.90</cell><cell>+1.70 65.80</cell><cell>-0.20</cell></row><row><cell cols="2">Baseline [12] 44.41 Baseline [11] 45.50</cell><cell>56.70</cell><cell>61.10</cell><cell>66.90</cell><cell></cell></row><row><cell>DST-CBC [11]</cell><cell>48.70</cell><cell>+3.20 60.50</cell><cell>+3.80 64.40</cell><cell>+3.30 -</cell><cell></cell></row><row><cell>Baseline [47]</cell><cell>43.84 ?0.71</cell><cell>54.84 ?1.14</cell><cell>60.08 ?0.62</cell><cell>66.19 ?0.11</cell><cell></cell></row><row><cell>ClassMix [47]</cell><cell>54.07</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>?0.64 +1.31 69.00 ?0.70 +1.23 T 60.80 ?0.69 +1.66 69.47 ?0.38 +1.71 M 61.25 ?0.55 +2.10 69.76 ?0.39 +1.99 62.39 ?0.86 +3.24 -C 63.16 ?0.89 +4.02 69.60 ?0.32 +1.83 D 64.14 ?1.34 +5.00 69.83 ?0.36 +2.06 M D 66.66 ?1.05 +7.52 71.16 ?0.16 +3.40 64.25 ? 0.18 +5.</figDesc><table><row><cell></cell><cell cols="3">Ablation of the architecture components (D-T: SDE</cell></row><row><cell cols="4">Transfer Learning, D-M SDE Transfer and Multi-Task Learning,</cell></row><row><cell cols="4">F: ImageNet Feature Distance Loss, P: Pseudo-Labeling, X-C:</cell></row><row><cell cols="4">Mix Class, X-D: Mix Depth, S -Data Selection). mIoU in %,</cell></row><row><cell cols="3">standard deviation over 3 seeds.</cell></row><row><cell cols="3">D F P X S 372 Samples</cell><cell>2975 Samples</cell></row><row><cell></cell><cell></cell><cell>59.14 ?1.02</cell><cell>67.77 ?0.13</cell></row><row><cell>T</cell><cell></cell><cell cols="2">60.46 11 -</cell></row><row><cell>M</cell><cell>D</cell><cell cols="2">68.01 ?0.83 +8.87 -</cell></row><row><cell>Baseline DM DM+XD+S DM+XD XD</cell><cell cols="3">Road S.walk Building Wall Fence Pole Tr. Light Tr. Sign Veget. Terrain Sky Person Rider Car Truck Bus Train M.cycle Bicycle</cell><cell>0.0 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of data selection methods (DS: Diversity Sampling based on depth features, US: Uncertainty Sampling based on depth student error). mIoU in %, std. dev. over 3 seeds. ?1.61 59.14 ?1.02 63.46 ?0.38 Entropy 53.63 ?0.77 63.51 ?0.68 66.18 ?0.50 Ours (US) 51.75 ?1.12 62.77 ?0.46 66.76 ?0.45 Ours (DS) 53.00 ?0.51 63.23 ?0.69 66.37 ?0.20 Ours (DS+US) 54.37 ?0.36 64.25 ?0.18 66.94 ?0.59</figDesc><table><row><cell># Labeled</cell><cell>1/30 (100) 1/8 (372)</cell><cell>1/4 (744)</cell></row><row><cell>Random</cell><cell>48.75</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table S4 .</head><label>S4</label><figDesc>Training and inference time on an Nvidia Tesla P100 averaged over 100 iterations or 500 images, respectively. D-T: SDE Transfer Learning, D-M SDE Transfer and Multi-Task Learning, P: Pseudo-Labelling, X-D: Mix Depth</figDesc><table><row><cell cols="3">D P X Training Time Inference Time</cell></row><row><cell>T</cell><cell>188 ms/it</cell><cell>66 ms/img</cell></row><row><cell>T</cell><cell>466 ms/it</cell><cell>67 ms/img</cell></row><row><cell>T</cell><cell>D 476 ms/it</cell><cell>66 ms/img</cell></row><row><cell>M</cell><cell>D 1215 ms/it</cell><cell>160 ms/img</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table S5 .</head><label>S5</label><figDesc>Performance on the CamVid test set (mIoU in %, standard deviation over 3 random seeds). The SDE is trained on Cityscapes sequences. DT: SDE Transfer Learning, XD -DepthMix, S: Data Selection. ?1.79 63.05 ?0.59 68.18 ?0.13 Ours (DT) 62.75 ?2.32 +3.60 66.19 ?0.96 +3.15 70.45 ?0.35 +2.27 ClassMix [47] 65.89 ?0.33 +6.73 67.48 ?1.02 +4.43 -Ours (DT+XD) 66.82 ?1.16 +7.66 68.91 ?0.62 +5.86 71.46 ?0.22 +3.29 Ours (DT+XD+S) 68.23 ?0.39 +9.07 69.62 ?0.64 +6.57 -</figDesc><table><row><cell># Labeled</cell><cell>50</cell><cell>100</cell><cell>367 (Full)</cell></row><row><cell>Baseline</cell><cell>59.16</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work is funded by Toyota Motor Europe via the research project TRACE-Zurich and by a research project from armasuisse.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5049" to="5059" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conf</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards scene understanding: Unsupervised monocular depth estimation with semantic-aware representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2624" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7063" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised object motion and depth estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishakh</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1004" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation via dynamic self-training and classbalanced curriculum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08514</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Finlayson</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cost-effective active learning for melanoma segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>G?rriz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><forename type="middle">Gir?</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Carlier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Faure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst. Workshop ML4H: Machine Learning for Health</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantically-guided representation learning for self-supervised monocular depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rares Ambrus, and Adrien Gaidon</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Int. Conf. Learn. Represent</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Active learning via neighborhood reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Joint Conf. Artif. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1415" to="1421" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarial learning for semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><forename type="middle">Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Ting Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf</title>
		<editor>Brit. Mach. Vis</editor>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sample selection for statistical parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational linguistics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="253" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Michael Maire Greg Shakhnarovich, and Erik Learned-Miller. Selfsupervised relative depth learning for urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sense: A shared encoder network for scene-flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3195" to="3204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Look deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Region-based active learning for efficient labeling in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejaswi</forename><surname>Kasarla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gattigorla</forename><surname>Nagendar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guruprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineeth</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. Appl. of Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1109" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved noise and attack robustness for semantic segmentation by using multi-task training with self-supervised depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog. Workshops</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="320" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised monocular depth estimation: Solving the dynamic object problem by semantic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Aike</forename><surname>Term?hlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Fingscheidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Vasileios Belagiannis, Federico Tombari, and Nassir Navab</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Stephen Lin, and In So Kweon. Visuomotor understanding for representation learning of driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongseop</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
		<editor>Brit. Mach. Vis. Conf.</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On deep unsupervised active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Handong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoren</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Joint Conf. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint active learning with feature selection via cur matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weishan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1382" to="1396" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cereals-cost-effective region-based active learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radek</forename><surname>Mackowiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omair</forename><surname>Ghori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Diego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Brit. Mach. Vis. Conf</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Employing em and pool-based active learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kachites</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mccallumzy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and lowlevel consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Active learning using pre-clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">79</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Early active learning via robust representation and structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Boosting semantic segmentation with multi-task selfsupervised learning for autonomous driving applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Novosel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Arsenali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilhelm</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Comput. Vis</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semisupervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?line</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myriam</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12674" to="12684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Geometry meets semantics for semi-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pierluigi Zama Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Mattoccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="298" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Medical Image Computing and Computerassisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Conf. Learn. Represent</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison Department of Computer Sciences</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An analysis of active learning strategies for sequence labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Empirical Methods Natural Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Query by committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>H Sebastian Seung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haim</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Workshop Computational Learning Theory</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Diversifying convex transductive experimental design for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Viewal: Active learning with viewpoint entropy for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawar</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9433" to="9443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Variational adversarial active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5972" to="5981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concetto</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5688" to="5696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multi-task learning for dense prediction tasks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Int. Joint Conf. Artif. Intell</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3635" to="3641" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deal: Difficulty-aware active learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zunlei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Suggestive annotation: A deep active learning framework for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="399" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Active learning via transductive experimental design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Active learning based on locally linear reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2026" to="2038" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Biomedical image segmentation via representative annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peixian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><forename type="middle">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5901" to="5908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Df-net: Unsupervised joint learning of depth and flow using cross-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="36" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Zwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Lambert-Lacroix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.6868</idno>
		<title level="m">The berhu penalty and the grouped effect</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
