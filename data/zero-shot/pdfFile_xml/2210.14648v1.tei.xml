<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MASKED MODELING DUO: LEARNING REPRESENTATIONS BY ENCOURAGING BOTH NETWORKS TO MODEL THE INPUT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Niizumi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daiki</forename><surname>Takeuchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasunori</forename><surname>Ohishi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noboru</forename><surname>Harada</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunio</forename><surname>Kashino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NTT Corporation</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MASKED MODELING DUO: LEARNING REPRESENTATIONS BY ENCOURAGING BOTH NETWORKS TO MODEL THE INPUT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Self-supervised learning</term>
					<term>Masked Autoen- coders</term>
					<term>Masked Image Modeling</term>
					<term>Masked Spectrogram Modeling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Masked Autoencoders is a simple yet powerful self-supervised learning method. However, it learns representations indirectly by reconstructing masked input patches. Several methods learn representations directly by predicting representations of masked patches; however, we think using all patches to encode training signal representations is suboptimal. We propose a new method, Masked Modeling Duo (M2D), that learns representations directly while obtaining training signals using only masked patches. In the M2D, the online network encodes visible patches and predicts masked patch representations, and the target network, a momentum encoder, encodes masked patches. To better predict target representations, the online network should model the input well, while the target network should also model it well to agree with online predictions. Then the learned representations should better model the input. We validated the M2D by learning general-purpose audio representations, and M2D set new state-of-the-art performance on tasks such as UrbanSound8K, VoxCeleb1, AudioSet20K, GTZAN, and SpeechCommandsV2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recently, self-supervised learning (SSL) methods using masked image modeling (MIM) have progressed and yielded promising results in the image domain. Among them, Masked Autoencoders <ref type="bibr" target="#b0">[1]</ref> (MAE) have inspired numerous subsequent studies and influenced not only the image domain <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> but also the audio domain <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</p><p>An MAE effectively learns a representation by reconstructing a large number (i.e., 75%) of masked input patches using a small number of visible patches, encouraging the learned representation to model the input. However, it learns representations indirectly by minimizing the loss between the original input and the reconstructed result, which may not be optimal for learning a representation.</p><p>In contrast, several previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10]</ref> achieve direct learning of representations, typically by using a momentum encoder in a Siamese architecture <ref type="bibr" target="#b10">[11]</ref> to obtain the masked patch representations as a training signal. In this case, all input patches are used to encode these representations.</p><p>We hypothesize that the learned representation would become more useful when pre-trained with a training signal encoded using only masked patches instead of using all patches. While an MAE effectively encourages modeling the input signal by limiting the number of visible patches fed to the encoder, we think feeding all the input patches for obtaining a training signal does not benefit this inductive bias.</p><p>In this paper, we propose a new method, Masked modeling duo (M2D), that learns representations directly by predicting the representations of masked patches from visible patches only. As illus-  <ref type="figure">Fig. 1</ref>. M2D pre-training scenario. The online network encodes visible patches and predicts masked patch representations, while the target network encodes masked patches. The M2D maximizes the agreement between these two outputs to learn representations. We provide only the masked patches to the target illustrated as (a), unlike conventional methods (e.g., data2vec <ref type="bibr" target="#b9">[10]</ref>) depicted as (b), encouraging representations to model the input from both the online and target networks. trated in <ref type="figure">Fig.1</ref>, the target representations are encoded from only the masked patches, not from all the input patches as they are in the previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10]</ref>. Although our method adds a target momentum encoder to the MAE, the entire framework remains simple.</p><p>The M2D encourages modeling of the input in the two networks, namely online and target networks. For example, to reduce the prediction error for a heartbeat audio input consisting of two sounds (S1 and S2), the online network should encode the given visible patches around S2 into representations modeled as part of the whole heartbeat in order to predict the representations of masked patches around S1. Conversely, the masked patch representations around S1 encoded by the target network are more likely to agree with the prediction if it is encoded as part of the whole heartbeat. Therefore, our method encourages modeling the input from both sides.</p><p>In our experiments, we validated our method by learning a general-purpose audio representation using an audio spectrogram as input and confirmed the effectiveness of learning the representation directly and providing only masked patches to the target. In addition, M2D set new state-of-the-art (SOTA) performance on several audio tasks. Our code is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>This study was inspired by MAE <ref type="bibr" target="#b0">[1]</ref> for an MIM and Bootstrap Your Own Latent <ref type="bibr" target="#b11">[12]</ref> (BYOL) as a framework for directly learning la-tent representations using a target network. An MAE learns to reconstruct the input data, whereas our M2D learns to predict masked latent representations. BYOL differs from ours in that it is a framework for learning representations invariant to data augmentation. SIM <ref type="bibr" target="#b1">[2]</ref>, MSN <ref type="bibr" target="#b2">[3]</ref>, and data2vec <ref type="bibr" target="#b9">[10]</ref> learn to predict masked patch representations using a target network, but, unlike ours, all input patches are fed to the target. CAE <ref type="bibr" target="#b3">[4]</ref> and SplitMask <ref type="bibr" target="#b4">[5]</ref> encode target representation using only masked patches, which is similar to ours but without the use of a target network. While SIM, MSN, CAE, and SplitMask learn image representations, data2vec also learns audio representations.</p><p>In this work, we experimented through learning general-purpose audio representations. To learn speech and audio, various methods learn representations using masked input, such as Mockingjay <ref type="bibr" target="#b12">[13]</ref>, wav2vec2 <ref type="bibr" target="#b13">[14]</ref>, HuBERT <ref type="bibr" target="#b14">[15]</ref>, and BigSSL <ref type="bibr" target="#b15">[16]</ref> for speech, and SSAST <ref type="bibr" target="#b16">[17]</ref> for audio. Methods more closely related to ours are MAE-AST <ref type="bibr" target="#b6">[7]</ref>, MaskSpec <ref type="bibr" target="#b7">[8]</ref>, MSM-MAE <ref type="bibr" target="#b8">[9]</ref>, and Audio-MAE <ref type="bibr" target="#b5">[6]</ref>, which adapt MAE to learn audio representations. However, they differ from our method in that they do not use a target network.</p><p>Other SSL methods for learning audio representations include Wang et al. <ref type="bibr" target="#b17">[18]</ref> and DeLoRes <ref type="bibr" target="#b18">[19]</ref>, and especially BYOL-A <ref type="bibr" target="#b19">[20]</ref>, BYOL-S <ref type="bibr" target="#b20">[21]</ref>, and ATST <ref type="bibr" target="#b21">[22]</ref>, which use BYOL as the learning framework; they do not mask the input. For supervised learning, AST <ref type="bibr" target="#b22">[23]</ref>, EAT <ref type="bibr" target="#b23">[24]</ref>, PaSST <ref type="bibr" target="#b24">[25]</ref>, and HTS-AT <ref type="bibr" target="#b25">[26]</ref> have shown SOTA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MASKED MODELING DUO</head><p>Our method learns representations by using only visible patches to predict the masked patch representations. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, it consists of two networks, referred to as the online and target networks.  Processing input The framework partitions the input data x (audio spectrogram, image, etc.) into a grid of patches, adds positional encoding, and randomly selects a number of patches according to a masking ratio as masked patches xm and the rest as visible patches xv. While we use the same positional encoding as MAE <ref type="bibr" target="#b0">[1]</ref>, we tested various masking ratios as discussed in Section 4.4.</p><p>Online and target networks The online network, defined by a set of weights ?, encodes the visible patches xv using the online encoder f ? into the representation zv = f ? (xv). It concatenates the masked patches m to zv, adds the position encoding p, and predict? z, representations of entire input patches, using the predictor g ? .</p><formula xml:id="formula_0">z = g ? (concat(zv, m) + p)<label>(1)</label></formula><p>It then filters the prediction result? to output?m = {?[i] | i ? IM }, containing only masked patch representations, where IM is the set of indices of the masked patches. The target network is defined by parameter ? and consists only of momentum encoder f ? , which is identical to the online encoder except for the parameter. The network encodes masked patches xm using f ? to output the representation zm = f ? (xm). We then standardize zm tozm = (zm ? mean(zm))/ var(zm), for stabilizing the training, which we empirically confirmed in preliminary experiments, rather than for performance gain as in MAE. Calculating loss The loss is calculated using the standardized target outputzm as a training signal against the online prediction output?m. Inspired by BYOL <ref type="bibr" target="#b11">[12]</ref>, we calculate the loss L by the mean square error (MSE) of l2-normalized?m andzm.</p><formula xml:id="formula_1">L ||l2(?m) ? l2(zm)|| 2 2 = 2 ? 2 ? ?m,zm ||?m||2 ? ||zm||2 ,<label>(2)</label></formula><p>where ?, ? denotes the inner product.</p><p>Updating network parameters Our framework updates parameters ? and ? after each training step. It updates ? only by minimizing the loss L as depicted by the stop-gradient in <ref type="figure" target="#fig_2">Fig. 2</ref>, whereas updating ? is based on a slowly moving exponential average of ? with a decay rate ? :</p><formula xml:id="formula_2">? ? ? ? + (1 ? ? )?,<label>(3)</label></formula><p>It has been empirically shown that stop-gradient operation can avoid collapsing to an uninformative solution, and the movingaverage behavior may lead to learning effective representations <ref type="bibr" target="#b10">[11]</ref>. After the training, we transfer only the f ? as a pre-trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We validated the M2D step-by-step experiments that examined the effectiveness of learning representations directly by comparing our M2D with MAE (Section 4.2), the effectiveness of feeding only masked patches to the target (Section 4.3), the impact of various masking ratios (Section 4.4), and comparing ours with SOTA (Section 4.5).</p><p>In all experiments, we applied our M2D to masked spectrogram modeling (MSM) <ref type="bibr" target="#b8">[9]</ref>, with an audio spectrogram as input to learn general-purpose audio representations. We evaluated the performance of pre-trained models in both a linear evaluation and finetuning on a variety of audio downstream tasks spanning environmental sounds, speech, and music.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We mainly focused on comparing M2D with an MAE, then adapted MAE implementations and settings with as few changes as possible. We implemented an additional target network on top of the MAE code and adopted the MAE decoder as our predictor g ? without changes. We used vanilla ViT-Base <ref type="bibr" target="#b26">[27]</ref> with a 768-d output feature as our encoders (f ? and f ? ) and fixed the patch size to 16 ? 16 for all experiments. We tested with masking ratios of 0.6 and 0.7, which showed good performance in preliminary experiments.</p><p>We used the MSM-MAE <ref type="bibr" target="#b8">[9]</ref> as an MAE for comparison, an MAE variant optimized for MSM by making the decoder smaller  We preprocessed audio samples to a log-scaled mel spectrogram with a sampling frequency of 16,000 Hz, window size of 25 ms, hop size of 10 ms, and mel-spaced frequency bins F = 80 in the range of 50 to 8,000 Hz and normalized them with a dataset statistics. All downstream task audios were cropped to the dataset's average duration or added with zero padding at the end. Long audio samples were split into the input length of the model without overlapping, encoded to z each, concatenated along time, and averaged over time to an audio sample level feature z . Pre-training details The input audio duration of the ViT used in the experiments was set to 6s, the same as ATST, for comparison with the SOTA methods. The input spectrogram has a size of 80 ? 608 (Freq. bins ? Time frames), making NF = 80/16 = 5 and NT = 608/16 = 38 with a patch size of 16 ? 16.</p><p>We set the number of epochs as 300, warm-up epochs as 20, batch size as 2048, and the base learning rate as 3e-4. All other settings were the same as in the MAE, including the learning rate scheduling and optimizer. The EMA decay rate ? for the target network update was linearly interpolated from 0.99995 at the start of training to 0.99999 at the end. We used AudioSet <ref type="bibr" target="#b27">[28]</ref> as a pretraining dataset with 2,005,132 samples (5,569 h) of 10s audio from the balanced and unbalanced train segments. We randomly cropped 6s audio from a 10s sample. All these settings were common in the M2D and MSM-MAE pre-training, except for the EMA decay rate. Linear evaluation details In the linear evaluation, we trained a linear classifier on top of frozen pre-trained models, and tested the performance on a variety of downstream tasks.</p><p>All evaluation details and downstream tasks are the same as in our previous study <ref type="bibr" target="#b19">[20]</ref>. Tasks include environmental sound classification ESC-50 <ref type="bibr" target="#b28">[29]</ref> and UrbanSound8K <ref type="bibr" target="#b29">[30]</ref> (US8K), speechcommand classification Speech Commands V2 <ref type="bibr" target="#b30">[31]</ref> (SPCV2), speaker identification VoxCeleb1 <ref type="bibr" target="#b31">[32]</ref> (VC1), language identification VoxForge <ref type="bibr" target="#b32">[33]</ref> (VF), speech emotion recognition CREMA-D <ref type="bibr" target="#b33">[34]</ref> (CRM-D), music genre recognition GTZAN <ref type="bibr" target="#b34">[35]</ref>, musical instrument classification NSynth <ref type="bibr" target="#b35">[36]</ref>, and a pitch audio classification Pitch Audio Dataset (Surge synthesizer) <ref type="bibr" target="#b36">[37]</ref>. All the tasks are classification problems, and all the results are accuracies. Fine-tuning details We used the tasks commonly used in previous studies: ESC-50, SPCV2, and VC1, the same as in the linear evaluation, plus AudioSet20K (AS20K), which learns only the bal-anced train segments of AudioSet <ref type="bibr" target="#b27">[28]</ref> and results in a mean average pooling (mAP) of multi-label classification with 527 classes.</p><p>The fine-tuning pipeline follows ATST. A linear classifier was added on top of the pre-trained model to train the entire network. All evaluations were trained for 200 epochs, and the learning rate was optimized for each task and scheduled with cosine annealing <ref type="bibr" target="#b37">[38]</ref> after five epochs of warm-up. We used SGD and AdamW for the optimizer, Mixup <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref>, Random Resize Crop (RRC) <ref type="bibr" target="#b19">[20]</ref> for data augmentation, and Structured Patchout (SPO) proposed in PaSST <ref type="bibr" target="#b24">[25]</ref> that masks patches during training. When using the SPO, we used 768-d features calculated by averaging ViT outputs over time because the MSM-MAE feature calculation is not applicable to the masked patches. <ref type="table" target="#tab_1">Table 2</ref> summarizes the settings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Validation of Learning Representations Directly</head><p>We validated the effectiveness of learning representations directly instead of through the reconstruction task by comparing the M2D with MAE. Note that the pre-trained models shared exactly the same ViT, enabling us to compare the difference of pre-training schemes. <ref type="table" target="#tab_0">Table 1</ref> shows the comparison results with the MAE. Both (a) fine-tuning and (b) linear evaluation results show that the M2D improves the performance of the conventional MAE in most tasks. However, the performance degrades on Surge (pitch classification) in the linear evaluation and on VoxCeleb1 (speaker classification) in fine-tuning. For these tasks, pitch information is considered important, while for the other tasks, pitch is considered less important. This suggests that the M2D representations are less sensitive to pitch than the MAE ones, and thus performance is degraded in Surge and VoxCeleb1, while it is improved in the other tasks that typically discriminate events regardless of pitch.</p><p>These results also suggest that learning by reconstructing data, as in the MAE, may facilitate detailed and local representations, while learning latent representations directly may facilitate more abstract representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Validation of Input to the Target</head><p>We validate the effectiveness of feeding masked patches only to the target by comparing our proposal with providing all patches to the target, which is employed in methods such as data2vec <ref type="bibr" target="#b9">[10]</ref>. We compare the average results of the linear evaluation. The results for both masking ratios in <ref type="table" target="#tab_2">Table 3</ref> shows that giving the target only the masked patch improves the result more than giving it all the patches, confirming our hypothesis.  -----76.9 --- ? Underlined results were obtained in this study and <ref type="bibr" target="#b19">[20]</ref> using publicly available pre-trained models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Masking Ratio Ablations</head><p>We evaluate the impact of various masking ratios using linear evaluation. <ref type="figure" target="#fig_4">Fig. 3</ref> shows the results for the three task groups and the overall average, showing the best average result (Avg.) at 0.6. However, the three groups show different trends, indicating that the optimal masking ratio depends on the task. The environmental sound tasks show the best result at 0.8, while the speech tasks have a peak of 0.6. The music task shows the best result at 0.7. Thus, it is difficult to set a single common optimal value. We suspect that these optimal masking ratios are due to differences in the density of the target information, as discussed in the MAE paper. The sounds in the speech tasks consist of successive phonemes, making many masks difficult to predict, while the environmental and music tasks include long continuous sounds (e.g., the sound of an air conditioner or instrumental sounds with long notes), making them easier to predict even at higher masking ratios. The results may indicate that the usefulness of the learned representations for various sounds can be varied and even controlled by masking ratios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with SOTA</head><p>We compare the M2D with SOTA methods in this section. Linear evaluation results shown in <ref type="table" target="#tab_3">Table 4</ref> confirm that our method outperforms existing SSL methods in four tasks: UrbanSound8K, Vox-Celeb1, CREMA-D, and GTZAN. Among them, M2D outperforms all previous methods with 87.6% on UrbanSound8K, 73.1% on Vox-Celeb1, and 83.9% on GTZAN.</p><p>The fine-tuning results in <ref type="table" target="#tab_4">Table 5</ref> show that our method outperforms existing SSL methods on ESC-50 and Speech commands V2 tasks and gives the same SOTA result as ATST <ref type="bibr" target="#b21">[22]</ref> on Au-dioSet20K. In particular, the result of 98.5% on Speech commands V2 outperforms all previous methods, including supervised learning. MSM-MAE <ref type="bibr" target="#b8">[9]</ref> shows a new SOTA VoxCeleb1 result, and M2D with a masking ratio of 0.6 shows a comparable result.</p><p>While the M2D showed SOTA performance on many tasks, it underperforms supervised learning methods (EAT <ref type="bibr" target="#b23">[24]</ref>, PaSST <ref type="bibr" target="#b24">[25]</ref>, and HTS-AT <ref type="bibr" target="#b25">[26]</ref>) on ESC-50 and speech models (Wav2Vec2 <ref type="bibr" target="#b13">[14]</ref>, BigSSL <ref type="bibr" target="#b15">[16]</ref>, and BYOL-S <ref type="bibr" target="#b20">[21]</ref>) on the speech tasks, suggesting a future research direction. In summary, the experiments demonstrate the effectiveness of M2D among SOTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this study, we proposed a new method, Masked Modeling Duo (M2D), to learn representations directly by predicting masked patch representations using two networks. Unlike previous methods, we encode training signal representations using only masked patches rather than all input patches. We made the M2D so that it encourages both online and target networks to model the entire input in visible and masked patch representations, respectively, achieving an effective representation. To evaluate our method, we applied it to masked spectrogram modeling to learn general-purpose audio representations with an audio spectrogram as input. We evaluated the performance of our method on a variety of downstream tasks. Experiments validated the effectiveness of our method, and M2D showed state-of-the-art results on UrbanSound8K, VoxCeleb1, CREMA-D, and GTZAN in linear evaluation, and on AudioSet20K, ESC-50, and Speech commands V2 in fine-tuning. Our code is available online.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the M2D framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>tions to optimize M2D representations for MSM as in MSM-MAE. The MSM-MAE outputs z ? R B?N T ?N F D by concatenating the features of frequency bins for each time frame, instead of simply averaging the z ? R B?N F N T ?D output from ViT, where B is batch size, NF is the number of patches along frequency, NT is the number of patches along time, and D is a patch feature dimension. Then, we summarized an audio sample level feature z = 1/NT N T t=1 z [t], averaging z over time. The z becomes a 3,840-d feature, where D = 768 and NF = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Masking ratio ablations: linear evaluation results (%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Fine-tuning and linear evaluation results. AS20K result is an mAP, and all others are accuracies (%) with 95% CI. MAE [9] 36.7 ? 0.5 94.0 ? 0.2 98.4 ? 0.1 95.3 ? 0.1 88.6 ? 1.5 86.3 ? 0.3 94.5 ? 0.2 72.2 ? 0.2 97.5 ? 0.1 70.2 ? 1.1 78.4 ? 2.8 75.9 ? 0.2 42.5 ? 0.7 78.5 ? 0.8 M2D ratio=0.6 36.8 ? 0.1 94.7 ? 0.3 98.5 ? 0.0 94.8 ? 0.1 89.7 ? 0.2 87.6 ? 0.2 95.4 ? 0.1 73.1 ? 0.1 97.9 ? 0.1 71.7 ? 0.3 83.3 ? 1.0 75.3 ? 0.1 41.0 ? 0.2 79.5 ? 0.3 M2D ratio=0.7 37.4 ? 0.1 95.0 ? 0.2 98.5 ? 0.1 94.4 ? 1.3 89.8 ? 0.3 87.1 ? 0.3 94.5 ? 0.1 71.3 ? 0.4 97.7 ? 0.1 71.6 ? 0.3 83.9 ? 1.4 76.9 ? 1.3 41.8 ? 0.4 79.4 ? 0.5 with four layers, six heads, and a width (embedding dimension) of 384-d. Other parameters, including a default masking ratio of 0.75, were the same as in the original MAE. Preliminary experiments verified that the MSM-MAE outperforms a vanilla MAE with an eightlayer decoder. We employed the MSM-MAE feature calculation in evalua-</figDesc><table><row><cell></cell><cell></cell><cell cols="2">(a) Fine-tuning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Linear evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Env. sound tasks</cell><cell></cell><cell cols="2">Speech tasks</cell><cell></cell><cell></cell><cell>Music tasks</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>AS20K</cell><cell>ESC-50</cell><cell>SPCV2</cell><cell>VC1</cell><cell>ESC-50</cell><cell>US8K</cell><cell>SPCV2</cell><cell>VC1</cell><cell>VF</cell><cell>CRM-D</cell><cell>GTZAN</cell><cell>NSynth</cell><cell>Surge</cell><cell>Avg.</cell></row><row><cell>MSM-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Fine-tuning settings</figDesc><table><row><cell cols="5">Parameter AS20K ESC-50 SPCV2 VC1</cell></row><row><cell>LR</cell><cell>1.0</cell><cell>0.5</cell><cell>0.5</cell><cell>0.001</cell></row><row><cell cols="2">Optimizer SGD</cell><cell>SGD</cell><cell>SGD</cell><cell>AdamW</cell></row><row><cell>Mixup</cell><cell>0.3</cell><cell>0.0</cell><cell>0.3</cell><cell>0.0</cell></row><row><cell>RRC</cell><cell></cell><cell></cell><cell></cell><cell>-</cell></row><row><cell cols="2">SPO ratio 0.5</cell><cell>0.5</cell><cell>0.5</cell><cell>0.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Linear evaluation average results (%) for target inputs.</figDesc><table><row><cell></cell><cell cols="2">Input ratio</cell><cell cols="2">Masking ratio (r)</cell></row><row><cell>Target input</cell><cell>Online</cell><cell>Target</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell cols="2">Masked patches only (ours) 1.0 ? r</cell><cell>r</cell><cell cols="2">79.5 ? 0.3 79.4 ? 0.5</cell></row><row><cell>All patches (conventional)</cell><cell>1.0 ? r</cell><cell>1.0</cell><cell cols="2">79.1 ? 0.3 79.3 ? 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Linear evaluation comparison with SOTA models (%). Supervised learning methods and non-standard linear evaluation results are grayed out as a reference. ? 0.8 66.9 ? 0.4 96.6 ? 0.0 40.9 ? 0.6 99.2 ? 0.1 65.5 ? 1.7 57.8 ? 1.3 56.6 ? 0.6 15.2 ? 0.9 ? 1.5 86.3 ? 0.3 94.5 ? 0.2 72.2 ? 0.2 97.5 ? 0.1 70.2 ? 1.1 78.4 ? 2.8 75.9 ? 0.2 42.5 ? 0.7 M2D ratio=0.6 89.7 ? 0.2 87.6 ? 0.2 95.4 ? 0.1 73.1 ? 0.1 97.9 ? 0.1 71.7 ? 0.3 83.3 ? 1.0 75.3 ? 0.1 41.0 ? 0.2 M2D ratio=0.7 89.8 ? 0.3 87.1 ? 0.3 94.5 ? 0.1 71.3 ? 0.4 97.7 ? 0.1 71.6 ? 0.3 83.9 ? 1.4 76.9 ? 1.3 41.8 ? 0.4</figDesc><table><row><cell></cell><cell cols="2">Env. sound tasks</cell><cell></cell><cell cols="2">Speech tasks</cell><cell></cell><cell></cell><cell>Music tasks</cell><cell></cell></row><row><cell>Representation</cell><cell>ESC-50</cell><cell>US8K</cell><cell>SPCV2</cell><cell>VC1</cell><cell>VF</cell><cell>CRM-D</cell><cell>GTZAN</cell><cell>NSynth</cell><cell>Surge</cell></row><row><cell cols="2">Wav2Vec2 [14]  ? 57.6 DeLoRes-M [19] -</cell><cell>82.7</cell><cell>89.7</cell><cell>45.3</cell><cell>88.0</cell><cell>-</cell><cell>-</cell><cell>75.0</cell><cell>-</cell></row><row><cell>SF NFNet-F0 [18]</cell><cell>91.1</cell><cell>-</cell><cell>93.0</cell><cell>64.9</cell><cell>90.4</cell><cell>-</cell><cell>-</cell><cell>78.2</cell><cell>-</cell></row><row><cell>BYOL-A [20]</cell><cell cols="9">83.2 ? 0.6 79.7 ? 0.5 93.1 ? 0.4 57.6 ? 0.2 93.3 ? 0.3 63.8 ? 1.0 70.1 ? 3.6 73.1 ? 0.8 37.6 ? 0.3</cell></row><row><cell>ATST Base [22]  ?</cell><cell cols="2">92.9 ? 0.3 84.1</cell><cell>95.1</cell><cell>72.0</cell><cell cols="4">97.4 ? 0.2 68.6 ? 0.2 76.4 ? 1.8 75.6</cell><cell>37.7 ? 0.2</cell></row><row><cell cols="2">MSM-MAE [9] 88.6 ConformerXL-P Non-RA [16] -</cell><cell>-</cell><cell>97.5</cell><cell>50.3</cell><cell>99.7</cell><cell>88.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AST-Fusion#5#12 [40]</cell><cell>94.2</cell><cell>85.5</cell><cell>80.4</cell><cell>24.9</cell><cell>87.6</cell><cell>60.7</cell><cell>82.9</cell><cell>77.6</cell><cell>34.6</cell></row><row><cell>BYOL-S [21]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Fine-tuning comparison with SOTA models. Supervised learning method results are grayed out as a reference. ? 0.5 94.0 ? 0.2 98.4 ? 0.1 95.3 ? 0.1 M2D ratio=0.6 36.8 ? 0.1 94.7 ? 0.3 98.5 ? 0.0 94.8 ? 0.1 M2D ratio=0.7 37.4 ? 0.1 95.0 ? 0.2 98.5 ? 0.1 94.4 ? 1.3</figDesc><table><row><cell>Model</cell><cell>AS20K</cell><cell>ESC-50</cell><cell>SPCV2</cell><cell>VC1</cell></row><row><cell></cell><cell>mAP</cell><cell>acc(%)</cell><cell>acc(%)</cell><cell>acc(%)</cell></row><row><cell>DeLoRes-M [19]</cell><cell>-</cell><cell>-</cell><cell>96.0</cell><cell>62.0</cell></row><row><cell>MAE-AST Patch/Frame [7]</cell><cell>30.6</cell><cell>90.0</cell><cell>98.0</cell><cell>63.3</cell></row><row><cell cols="2">MaskSpec/MaskSpec-small [8] 32.3</cell><cell>90.7</cell><cell>97.7</cell><cell>-</cell></row><row><cell>SSAST 250/400 [17]</cell><cell>31.0</cell><cell>88.8</cell><cell>98.2</cell><cell>66.6</cell></row><row><cell>data2vec [10]</cell><cell>34.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Audio-MAE (local) [6]</cell><cell>37.1</cell><cell>94.1</cell><cell>98.3</cell><cell>94.8</cell></row><row><cell>ATST Base [22]</cell><cell>37.4</cell><cell>-</cell><cell>98.0</cell><cell>94.3</cell></row><row><cell cols="2">MSM-MAE [9] 36.7 AST (Single), AST-P/S [23] 34.7</cell><cell>95.6</cell><cell>98.11</cell><cell>-</cell></row><row><cell>EAT-S/M [24]</cell><cell>-</cell><cell>96.3</cell><cell>98.15</cell><cell>-</cell></row><row><cell>PaSST [25]</cell><cell>-</cell><cell>96.8</cell><cell>-</cell><cell>-</cell></row><row><cell>HTS-AT [26]</cell><cell>-</cell><cell>97.0</cell><cell>98.0</cell><cell>-</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To reviewers, we plan to upload our code after the notification and will add a URL to this footnote.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Siamese image modeling for self-supervised vision representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01204</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Masked siamese networks for label-efficient learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07141</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Context autoencoder for self-supervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03026</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Are large-scale datasets necessary for self-supervised pre-training?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10740</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Masked autoencoders that listen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.06405</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">MAE-AST: Masked Autoencoding Audio Spectrogram Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="2438" to="2442" />
		</imprint>
	</monogr>
	<note>Interspeech, 2022</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Masked spectrogram prediction for self-supervised audio pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.12768</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Masked spectrogram modeling using masked autoencoders for learning general-purpose audio representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.12260</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="1298" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6419" to="6423" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HuBERT: Self-supervised speech representation learning by masked prediction of hidden units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bolte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lakhotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Language Process</title>
		<imprint>
			<biblScope unit="page" from="3451" to="3460" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13226</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SSAST: Self-supervised audio spectrogram transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2022</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10699" to="10709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards learning universal audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4593" to="4597" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Decorrelating feature spaces for learning general-purpose audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Umesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">BYOL for Audio: Exploring pre-trained general-purpose audio representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07402</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SERAB: A multi-lingual benchmark for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Scheidwasser-Clow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cernak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7697" to="7701" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">ATST: Audio Representation Learning with Teacher-Student Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="4172" to="4176" />
		</imprint>
	</monogr>
	<note>Interspeech, 2022</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">AST: Audio spectrogram transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="571" to="575" />
		</imprint>
	</monogr>
	<note>Interspeech, 2021</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Endto-end audio strikes back: Boosting augmentations towards an efficient audio classification network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gazneli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zimerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.11479</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient training of audio transformers with patchout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interspeech</title>
		<imprint>
			<biblScope unit="page" from="2753" to="2757" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="646" to="650" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-MM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-MM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno>arXiv::1804.03209</idno>
		<title level="m">Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Voxceleb: A large-scale speaker identification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2616" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Voxforge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maclean</surname></persName>
		</author>
		<ptr target="http://www.voxforge.org/home" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CREMA-D: Crowd-sourced emotional multimodal actors dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Keutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Musical genre classification of audio signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural audio synthesis of musical notes with WaveNet autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">One billion audio sounds from GPU-enabled modular synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Composing general audio representation by fusing multilayer features of a pre-trained model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niizumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EUSIPCO, 2022</title>
		<imprint>
			<biblScope unit="page" from="200" to="204" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
