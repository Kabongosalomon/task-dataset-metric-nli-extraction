<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON MULTIMEDIA 1 Multi-level Second-order Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON MULTIMEDIA 1 Multi-level Second-order Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TMM.2022.3142955</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Few-shot Learning</term>
					<term>Second-order Statistics</term>
					<term>Im- age Classification</term>
					<term>Action Recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a Multi-level Second-order (MlSo) fewshot learning network for supervised or unsupervised few-shot image classification and few-shot action recognition. We leverage so-called power-normalized second-order base learner streams combined with features that express multiple levels of visual abstraction, and we use self-supervised discriminating mechanisms. As Second-order Pooling (SoP) is popular in image recognition, we employ its basic element-wise variant in our pipeline. The goal of multi-level feature design is to extract feature representations at different layer-wise levels of CNN, realizing several levels of visual abstraction to achieve robust few-shot learning. As SoP can handle convolutional feature maps of varying spatial sizes, we also introduce image inputs at multiple spatial scales into MlSo. To exploit the discriminative information from multi-level and multiscale features, we develop a Feature Matching (FM) module that reweights their respective branches. We also introduce a selfsupervised step, which is a discriminator of the spatial level and the scale of abstraction. Our pipeline is trained in an end-to-end manner. With a simple architecture, we demonstrate respectable results on standard datasets such as Omniglot, mini-ImageNet, tiered-ImageNet, Open MIC, fine-grained datasets such as CUB Birds, Stanford Dogs and Cars, and action recognition datasets such as HMDB51, UCF101, and mini-MIT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Convolutional Neural Networks (CNNs) have advanced a variety of models e.g., object category recognition, scene classification and fine-grained image recognition. However, CNNs rely on large numbers of training labeled images and cannot be easily adapted to new tasks given very few samples. In contrast, the ability of humans to learn new visual concepts from very few examples highlights the superiority of biological vision. Thus, researchers study the so-called few-shot learning paradigm for which networks are trained or adapted to new concepts with few training samples. For example, recent fewshot learning approaches <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b65">66]</ref> build on the notion of similarity learning <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b18">19]</ref>. In this paper, we study the one-and few-shot learning problems, and we focus on a simple design capturing robust statistics for the purpose of similarity learning.</p><p>In what follows, we employ second-order statistics of datapoints, which have advanced the performance of numerous methods, including object recognition, texture categorization, action representation, and tracking <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">38]</ref>. For example, in the popular region covariance descriptors <ref type="bibr" target="#b82">[83]</ref>, a H. Zhang* is with Systems Engineering Institute, AMS. H. <ref type="bibr">Li</ref>  covariance matrix computed over multimodal features from image regions is used as an object representation for recognition and tracking. Covariance descriptors have been extended to many other applications <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b16">17]</ref> including end-toend training of CNNs, leading to state-of-art results on action recognition, texture classification, scene and fine-grained recognition <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>. As second-order representations capture correlation patterns of features, they are a powerful tool used in several recognition pipelines <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b100">101]</ref>.</p><p>A typical few-shot learning network consists of a backbone that generates image features, and a base learner that learns to classify the so-called query images (c.f . class labels). In this paper, we use a multi-level network to obtain multiple levels of feature abstraction based on second-order features. We leverage intermediate outputs from the backbone, which helps the pipeline capture relations between the query and support images at multiple levels of abstraction. We note that a cascaded network was used before by GoogLeNet <ref type="bibr" target="#b78">[79]</ref> with the goal of image classification rather than the similarity learning in the few-shot regime, which is a novel learning scenario not explored before.</p><p>By analyzing the class-wise activation maps, we ascertain that the features extracted from different levels of the backbone generally describe objects with respect to their different visual properties. Thus, such complementary to each other activation maps improve modeling of object relations across different levels of abstraction. To this end, we leverage second-order statistics formed from features of multi-level network streams. Firstly, we form and pass such second-order representations via the so-called Power Normalization (PN) to prevent the so-called burstiness effect <ref type="bibr" target="#b41">[42]</ref>, a statistical uncertainty of feature counts. As second-order pooling can effectively process feature maps of different spatial resolutions, we also employ inputs at multiple spatial scales to improve the quality of matching between objects at various scales. Secondly, we apply a so-called Feature Matching module which determines the importance of each level of abstraction and scale per querysupport pair. Subsequently, we form relationship descriptors and pass them to a so-called base learner which learns the similarity by comparing relationship descriptors (representing query-support pairs) via the Mean Square Error (MSE) loss. Finally, we refine the multi-level second-order matrices, each corresponding to some level of abstraction in the multi-level network, by applying a self-supervised pretext task <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b8">9]</ref> with the level and scale indexes used as auxiliary labels. Such a self-supervised step helps the multi-level network learn more distinctive and complementary abstraction responses.</p><p>We apply our network to the few-shot image and action recognition tasks. In contrast to the large-scale object classification, few-shot learning requires an investigation into the ef-arXiv:2201.05916v1 [cs.CV] 15 Jan 2022 fective use of multiple levels of feature abstraction, combined with second-order relationship descriptors, to determine the best performing architecture. As second-order statistics require appropriate pooling for such a new problem, we employ PN which is known to act as detector of visual features. PN discards the so-called nuisance variability (burstiness), the uncertainty of the frequency of specific visual features which vary unpredictably from image to image of the same class <ref type="bibr" target="#b41">[42]</ref>. We speculate that, as we capture relationships between multiple images in a so-called episode, such a nuisance variability would be multiplicative w.r.t. the number of images per episode. PN limits such a harmful effect.</p><p>Our contributions are summarized below:</p><p>i. We propose to generate scale-wise second-order representations at multiple levels of abstraction via a multi-level network, and we introduce the Feature Matching (FM) module to reweight the importance of each abstraction level and scale. FM selects the most discriminative pairs for relation learning. We show the importance of reweighting and matching across multiple spatial scales at multiple levels of feature abstraction.</p><p>ii. We investigate how to build second-order relational descriptors from feature maps to capture the similarity between query-support pairs for few-shot learning.</p><p>iii. We develop a self-supervised discriminator acting on scalewise second-order representations of multiple levels of abstraction whose role is to predict the index of abstraction and scale, thus improving the complementarity and distinctiveness between different abstraction levels and scales.</p><p>To the best of our knowledge, we are the first to investigate multiple levels of feature abstraction, multiple input scales, and self-supervision for one-and few-shot learning. In this work, we build upon the SoSN model <ref type="bibr" target="#b96">[97]</ref>, the first few-shot learning model successfully leveraging second-order pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Below, we describe popular one-and few-shot learning models, and discuss other related topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning From Few Samples</head><p>The ability of 'learning quickly from only a few examples is definitely the desired characteristic to emulate in any brainlike system' <ref type="bibr" target="#b59">[60]</ref>. This desired principle poses a challenge to CNNs which typically leverage large-scale datasets <ref type="bibr" target="#b63">[64]</ref>. Current trends in computer vision highlight the need for the 'ability of a system to recognize and apply knowledge and skills learned in previous tasks to novel tasks or new domains, which share some commonality'. For one-and fewshot learning, a robust 'transfer of particle', introduced in 1901 by Woodworth <ref type="bibr" target="#b90">[91]</ref>, is also a desired mechanism because generalizing based on one or few datapoints to account for intra-class variability of thousands images is formidable. One-and Few-shot Learning (FSL) have been studied in computer vision in both shallow <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b44">45]</ref> and deep learning scenarios <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b106">107,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b93">94]</ref>. For brevity, we review only the deep learning techniques. Siamese Network <ref type="bibr" target="#b31">[32]</ref>, a CNN based on twostreams, generates image descriptors and learns the similarity between them. Matching Network <ref type="bibr" target="#b83">[84]</ref> introduces the concept of the support set, and the L-way Z-shot learning protocols. Matching Network captures the similarity between a query and several support images, and generalizes well to previously unseen test classes. Prototypical Networks <ref type="bibr" target="#b72">[73]</ref> learn a model that computes distances between a datapoint and prototype representations of each class. Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b12">[13]</ref> and Task-Agnostic Meta-Learning (TAML) perform a rapid adaptation to new tasks via meta-learning. Moreover, a large family of meta-learning approaches apply some form of the gradient correction e.g., Meta-SGD <ref type="bibr" target="#b46">[47]</ref>, MAML++ <ref type="bibr" target="#b0">[1]</ref>, Reptile <ref type="bibr" target="#b52">[53]</ref>, CAVIA <ref type="bibr" target="#b107">[108]</ref> LEO <ref type="bibr" target="#b64">[65]</ref> and ModGrad <ref type="bibr" target="#b71">[72]</ref> adapt the step-size of the gradient updates. Relation Net <ref type="bibr" target="#b77">[78]</ref> learns the relationship between query and support images by leveraging a similarity learning network wired with the backbone. Note that relationship learning in few-shot learning is closely related to metric learning <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b32">33]</ref> rather than relationship learning in graphs <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b87">88]</ref>. SalNet <ref type="bibr" target="#b98">[99]</ref> is an efficient saliency-guided end-to-end meta-hallucination approach. AFL <ref type="bibr" target="#b106">[107]</ref> proposes a novel attribute-guided twolayer learning framework to improve the generalized performance of image representations. LRPABN <ref type="bibr" target="#b23">[24]</ref> uses an effective low-rank pairwise bilinear pooling operation to capture the nuanced differences between images. FAML <ref type="bibr" target="#b56">[57]</ref> proposes a novel GAN-based few-shot image generation approach, which is capable of generating new realistic images for unseen target classes in the low-sample regime. Zhu et al. <ref type="bibr" target="#b103">[104]</ref> propose a novel global grouping metric to incorporate the global context, resulting in a per-channel modulation of local relation features. Moreover, a cross-modal retrieval can be performed by a modified meta-learning framework <ref type="bibr" target="#b73">[74]</ref>. Finally, Graph Neural Networks (GNN) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b102">103]</ref> have also been used in few-shot learning <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b85">86]</ref>, and achieved competitive results. Self-supervision has also been studied in few-shot learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b97">98]</ref> where transformation-based auxiliary self-supervised classifiers are employed to improve the robustness of few-shot learning models. For instance, FLAT <ref type="bibr" target="#b93">[94]</ref> uses a self-supervision strategy to pre-train the auto-encoder via the reconstruction of transformations applied to images, followed by training with the supervised loss.</p><p>Our pipeline is somewhat similar to Relation Net <ref type="bibr" target="#b77">[78]</ref> and Prototypical Networks <ref type="bibr" target="#b72">[73]</ref> in that we use the two basic building blocks underlying such approaches, that is, the feature encoder (or backbone) and the similarity learning network. However, Relation Net and Prototypical Net are first-order models which do not use multiple levels of feature abstraction. In contrast, we investigate second-order representations with PN to capture correlations of features. We also use multiple intermediate feature outputs to obtain different levels of feature abstraction. Our work builds on our SoSN model <ref type="bibr" target="#b96">[97]</ref>, which we extend in this work by adding multiple levels of feature abstraction, multi-scale inputs, a feature matching mechanism, and a self-supervision step. Finally, we also include an extension of our pipeline to the problem of few-shot action recognition.</p><p>Few-shot Action Recognition (FSAR) has been studied by the limited number of recent works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b85">86]</ref>. Though action recognition <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b80">81]</ref> has been studied for a long time, it remains a challenging problem under the fewshot setting. Mishra et al. <ref type="bibr" target="#b50">[51]</ref> propose a generative framework for zero-and few-shot action recognition, by modeling each action class by a probability distribution. Guo et al. <ref type="bibr" target="#b17">[18]</ref> leverage neural graph matching to learn to recognize previously unseen 3D action classes. Xu et al. <ref type="bibr" target="#b92">[93]</ref> propose a dilated network to simultaneously capture local and longterm spatial temporal information. Zhu et al. <ref type="bibr" target="#b104">[105]</ref> propose a novel compound memory network. Hu et al. <ref type="bibr" target="#b22">[23]</ref> learn a dual-pooling GNN to improve the discriminative ability for selecting the representative video content and refine video relations. Finally, noteworthy are few-shot pipelines such as VideoPuzzle <ref type="bibr" target="#b7">[8]</ref> which generates aesthetically enhanced longshot videos from short video clips, and JEANIE <ref type="bibr" target="#b85">[86]</ref> which performs FSAR on datasets of articulated human 3D body joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Second-order Statistics/Power Normalization</head><p>Below we discuss several shallow and CNN-based methods which use second-order statistics. We conclude with details of so-called pooling and Power Normalization. Second-order statistics have been used for texture recognition <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b62">63]</ref> by so-called Region Covariance Descriptors (RCD) and further applied to tracking <ref type="bibr" target="#b57">[58]</ref>, semantic segmentation <ref type="bibr" target="#b6">[7]</ref> and object category recognition <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>. Cooccurrence patterns have also been used in the CNN setting. A recent approach <ref type="bibr" target="#b67">[68]</ref> extracts feature vectors at two separate locations in feature maps to perform an outer product in a CNN co-occurrence layer. Higher-order statistics have also been used for action recognition from the body skeleton sequences <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37]</ref> and for domain adaptation <ref type="bibr" target="#b34">[35]</ref>. In this work, we perform end-to-end training for one-and few-shot learning by the use of second-order relation descriptors (a novel proposition) that capture relations between the query and support images (or videos) before passing them to the similarity learning network. Second-order statistics have to deal with the so-called burstiness, 'the property that a given visual element appears more times in an image than a statistically independent model would predict' <ref type="bibr" target="#b25">[26]</ref>. This is achieved by Power Normalization <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38]</ref> which is known to suppress this burstiness. PN has been extensively studied and evaluated in the context of Bag-of-Words <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> and category recognition with deep learning <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>A theoretical relation between Average and Max-pooling was studied in <ref type="bibr" target="#b4">[5]</ref>, which highlighted the underlying statistical reasons for the superior performance of max-pooling. A survey <ref type="bibr" target="#b39">[40]</ref> showed that so-called MaxExp pooling in <ref type="bibr" target="#b3">[4]</ref> acts as a detector of 'at least one particular visual word being present in an image', and thus it can be approximated with a simple min n (1, ?? kn ) for ? &gt; 0, whose variant we use in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BACKGROUND</head><p>Below we detail our notations, and explain how to compute second-order statistics with PN.</p><p>Notations. Let x ? R d be a d -dimensional feature vector. I N stands for the index set {1, 2, ..., N}. We also define 1 1 1 = [1, ..., 1] T . Operators ; 1 , ; 2 and ; 3 denote concatenation of tensors along the first, second and third mode, respectively. Operator (:) denotes vectorization of a matrix or tensor. Typically, capitalized bold symbols such as ? ? ? denote matrices, lowercase bold symbols such as ? ? ? denote vectors, and regular symbols such as n or Z denote scalars. Also,</p><formula xml:id="formula_0">? i j is the (i, j)-th coefficient of ? ? ?.</formula><p>Autocorrelation matrices. The linearization of sum of Polynomial kernels results in two autocorrelation matrices.</p><formula xml:id="formula_1">Proposition 1. Let ? ? ? A ? {? ? ? n } n?N A , ? ? ? B ? {? ? ? *</formula><p>n } n?N B be datapoints from two images ? A and ? B , where N = |N A | and N * =|N B | are the numbers of data vectors e.g., obtained from the last convolutional feature map of CNN for images ? A and ? B . Autocorrelation matrices emerge from the linearization of the sum of Polynomial kernels of degree 2:</p><formula xml:id="formula_2">K(? ? ? A , ? ? ? B ) = F F F(? ? ? A ), F F F(? ? ? B ) = 1 NN * ? n?N A ? n ?N B ? ? ? n , ? ? ? * n 2 , where F F F(? ? ?) = 1 N ? n?N ? ? ? n ? ? ? T n .<label>(1)</label></formula><p>Proof. See <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> for a derivation of this expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Power Normalization.</head><p>In what follows, we use autocorrelation matrices with a pooling operator related to the following two propositions.</p><p>Proposition 2. Assume two event vectors ? ? ? , ? ? ? ?{0, 1} N which store the N trials each, performed according to the Bernoulli distribution under the i.i.d. assumption. Let p be the probability of an event (? n ?? n = 1) denoting a co-occurrence, and 1? p of (? n ?? n = 0) denoting the lack of co-occurrence. Let p = avg n ? n ? n denote an expected value. Then the probability of at least one co-occurrence event (? n ?? n = 1) in ? n and ? n simultaneously in N trials becomes ? =1?(1?p) ? for ? =N.</p><p>Proof. See <ref type="bibr" target="#b41">[42]</ref> for a proof.</p><p>Proposition 3. The first-order Taylor expansion of 1?(1?p) ? around p = 0 and p = 1 equals ? p and 1, respectively.</p><formula xml:id="formula_3">Thus, we have 1 ? (1 ? p) ? ? min(? p, 1) on p ? [0; 1]. If we treat coefficients (i, j) of matrix M M M = ? ? ?? ? ? T /Tr(? ? ?? ? ? T )</formula><p>as approximately proportional to the co-occurrence probability of ? in and ? jn , ?n ? I N , then for ? ? ? ? 0, we obtain PN maps ? ? ? (M M M; ?) = min(?M M M, 1), where ? ? N is an adjustable parameter accounting for the fact that we do not operate on the actual variable p drawn according to the Bernoulli distribution.</p><p>We adopt the above PN operator as it works well in practice according to survey <ref type="bibr" target="#b39">[40]</ref>. Proposition 3 is a novel proposition.</p><p>IV. PIPELINE Below, we describe our network followed by our relationship descriptors whose role is to capture co-occurrences in the image and video representations. We also detail the Feature ENcoder (FEN) with multiple levels of abstraction outputs, the Feature Matching (FM) module and the self-supervised Visual Abstraction Level and Scale Discriminator. </p><formula xml:id="formula_4">SoP Prediction SoP SoP Conv. Block Conv. Block Conv. Block Base Learner Scale-wise Inputs d-1 s-2 d-1 s-1 d-1 s-3 d-2 s-2 d-2 s-1 d-2 s-3 d-3 s-2 d-3 s-1 d-3 s-3 VALSD Scale &amp; Level Index Matching Matching Matching Optimal Transport Gate Module GM g( ? )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scale-wise Scores</head><p>Scale-wise Scores @level 3</p><formula xml:id="formula_5">T C3D Conv. Block Action Recognition (AR) t=0 t=1 t=T ? ? ? ? ? ? ? Fig. 2:</formula><p>For Few-shot Action Recognition, we use the C3D convolutional blocks and the second-order pooling which aggregates over the temporal mode (in addition to spatial locations of feature maps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-level Second-order Few-shot Learning</head><p>Our Multi-level Second-order (MlSo) few-shot learning pipeline, shown in <ref type="figure" target="#fig_0">Figure 1</ref>, consists of two major parts which are (i) the backbone (feature encoder, FEN) with multiple branches of features and (ii) the base learner i.e., Similarity Network (SN), Logistic Regression (LR) or Nearest Neighbor (NN) classifier. The role of the backbone is to generate convolutional feature vectors which are then used as image descriptors. The base learner infers relations between querysupport pairs. Our work differs from the Relation Net <ref type="bibr" target="#b77">[78]</ref> in that we use multiple (i.e., three) levels of feature abstraction passed to second-order representations with PN, which form query-support relation descriptors. We feed such descriptors into SN for comparison of second-rather than the firstorder statistics. For clarity, we first describe the Second-order Similarity Network which uses a single FEN output from the last layer. Subsequently, we explain the details of MlSo.</p><p>SoSN. Let FEN be defined as an operator f : R W?H ?R |F | R K?N , where W and H denote the width and height of an input image, K is the length of feature vectors (number of filters), N = N W ?N H is the total number of spatial locations in the last convolutional feature map. For simplicity, we denote an image descriptor by ? ? ? ? R K?N , where ? ? ? = f (X; F ) for an image X ? R W?H , and F are learnable parameters of the encoding network. The role of SN, denoted by r : R K ?R |R| R, is to compare two datapoints encoded as some K dimensional vectorized second-order representations. Typically, we write s(? ? ?; R), where ? ? ? ? R K , whereas R are learnable parameters of the similarity network. We define a relationship descriptor ? : R K?N?Z ? R K?N K which captures some relationship between descriptors built from the Z-shot support images and a query image. This relationship is encoded via computing second-order autocorrelation matrices with PN for query and support embeddings, and forming some relation between query-support features e.g., by concatenation, inner-product, subtraction, etc., as explained later.</p><p>For the L-way Z-shot problem, assume that we have some support images {X n } n?W l from some set W l and their corresponding image descriptors {? ? ? n } n?W l form Z-shot relation descriptors. Moreover, assume that we have one query image X * with its image descriptor ? ? ? * (the asterisk usually denotes query-related variables). Both the Z-shot and the   query embeddings belong to one of L classes in the subset C ? ?{c 1 , ..., c L }?I C ?C . Specifically, for an L-way problem, one obtains L relation descriptors, where one of L descriptors contains query-support pair of the same class, whereas the remaining L ? 1 relation descriptors contain non-matching query-support classes. The L-way Z-shot learning step can be defined as learning the similarity w.r.t. relation descriptors:</p><formula xml:id="formula_6">? lq = r ? [? ? ? n ] n?W l , ? ? ? * q , R ,<label>(2)</label></formula><p>where </p><formula xml:id="formula_7">? ? ? n = f (X n ; F ) and ? ? ? * q = f (X * q ; F ),</formula><formula xml:id="formula_8">F ,R E (W 1 ,...,W L , q)?E ? l?I L ? lq ? ? (c l ?c * q ) 2 ,<label>(3)</label></formula><p>where c l are labels of support subsets and the query label c * q is always set as c 1 (so that the query image has the same label as the first support subset), ? (c l ?c * q ) equals 1 if c l = c * q , 0 otherwise.</p><p>For FSAR, we propose a C3D Second-order Similarity Network (C3D SoSN) that is equipped with FEN with C3D Operator (?+F) Full single autocorrelation per support-query concatenated feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(?+R)</head><p>Rank differing autocorrelations, one per support, one per query, followed by the concatenation.</p><p>(?) Auto-correlations, one per support, averaged over shots? ? ?, one per query, followed by the concatenation.</p><formula xml:id="formula_9">? (?) = few-shot support samples Avg few-shot support samples ? ? ? ? ? ?? ? ? T / Tr(? ? ?? ? ? T ) (:) where? ? ? = ? ? ?n ; 1 ? ? ? * n?W l<label>(10)</label></formula><p>few-shot support samples  <ref type="formula" target="#formula_2">(10)</ref>, <ref type="formula" target="#formula_2">(11)</ref> and <ref type="formula" target="#formula_2">(12)</ref>.</p><formula xml:id="formula_10">? ? ? ? ? ? ?? ? ? T /Tr(? ? ?? ? ? T ) ; 3 ? ? ? ? ? ? * ? ? ? * T /Tr(? ? ? * ? ? ? * T ) (:) where? ? ? = ? ? ?n n?W l (11) ( a ) few-shot support samples ? few-shot support samples ? ? ? ? ? ?? ? ? T /Tr(? ? ?? ? ? T ) ; 3 ? ? ? ? ? ? * ? ? ? * T /Tr(? ? ? * ? ? ? * T ) (:) where? ? ? = ? n?W l ? ? ?n<label>(12)</label></formula><p>convolutional blocks, as in <ref type="figure">Figure 2</ref>. Firstly, we obtain C3Dbased FEN embeddings over a video and then we compute the autocorrelation matrix with PN to obtain relation descriptors for query-support video pairs. We consider C3D SoSN as a baseline FSAR model. Thus, we do not investigate here any elaborate aggregation strategies, and we do not use the optical flow. However, we believe that further improvements over the C3D SoSN baseline can be easily achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Relationship Descriptor ?</head><p>Table I offers some choices for the operator ? whose role is to capture and summarize the information stored in querysupport embeddings of image or video pairs. Such a summary is then passed to SN for learning to compare query-support pairs. Operator (?+F) concatenates the support and query feature vectors prior to the outer-product step. Operator (?+R) performs the outer-product on the support and query feature vectors separately prior to concatenation step along the third mode. Operator (?) averages over embeddings of Z support images from a given W l , followed by the outer-product step on the mean support and query vectors, and the concatenation step along the third mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-level Relation Learning</head><p>It is generally known that different layers of CNN are able to capture different kinds of visual abstraction e.g., early layers may respond to edges, blobs and texture patterns, whereas deeper layers may respond to parts of objects under some small deformations. Therefore, we propose the Multi-level Secondorder Similarity Network whose goal is to learn the similarity between query-support image or video pairs. To this end, we leverage the multiple levels of visual abstraction to learn object relations based on fine-to-coarse embeddings of images (or videos).</p><p>The architecture of MlSo is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In contrast to SoSN which uses the final output of FEN as embeddings, MlSo extracts feature vectors across fine-to-coarse levels that correspond to consecutive convolutional blocks, and forms autocorrelation matrices with PN and GM to produce the corresponding feature representations at multiple levels. Subsequently, an autocorrelation matrix per level is passed to SN with the goal of computing the relation scores which are fused to make the final prediction. <ref type="figure">Figure 5</ref> visualizes class activation maps at three different levels of FEN (ResNet-12 backbone) to demonstrate the complementary nature of activated regions. The figure shows that each level of visual abstraction in MlSo responds differently to the same visual stimulus.</p><p>Subsequently, we employ SN learners with parameters R d , d ? I D , to learn the relations between query-support pairs for each level of abstraction. The L-way Z-shot learning step over D levels of feature abstraction can be redefined as learning the similarity w.r.t. relation descriptors:</p><formula xml:id="formula_11">? (d) lq = r ? [? ? ? (d) n ] n?W l , ? ? ? * (d) q , R d ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_12">? ? ? (d) n = f (d) (X n ; F ) and ? ? ? * (d) q = f (d) (X * q ; F ), with the following MSE loss: arg min F ,R 1 ,...,R D L R ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_13">L R = E (W 1 ,...,W L , q)?E ? l?I L ? d?I D ? (d) lq ? ? (c l ?c * q ) 2</formula><p>, and the label-related symbols are defined as for Eq. (3). During the inference step, we average the individual classifier votes obtained from multiple levels of feature abstraction. We determine the final class c * q for a given testing episode (W 1 , ..., W L , q) ? E test as follows:</p><formula xml:id="formula_14">c * q = c l where l = arg min l?I L ? d?I D S (d) lq ? 1 2 .<label>(6)</label></formula><p>Scale-wise Boosting (SB). Below we demonstrate how the inputs at multiple spatial scales are utilized by our multi-level network. The use of scale-wise inputs is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Given a pair of support and query images, X n and X * q , we first downsample them to obtain the multi-scale inputs, X </p><formula xml:id="formula_15">= r(? (? ? ? s n , ? ? ? * s q ), R),<label>(7)</label></formula><p>where s, s ? I S are the scale indexes ranging from 1 to 3, referring to inputs at 1, 1/2 and 1/4 of the original resolution.</p><p>The number of scales we use is S = 3. As ? (ss ) lq is the similarity between X n and X * q at scales s and s , the remaining part of the pipeline may deal with the scale-wise matching of objects in various ways. Note that the scale-wise matching deals with the scale variations of objects, whereas the feature abstraction relates to the level of semantic composition. Thus, both strategies may be combined to improve the few-shot learning step.</p><p>Below is a simple extension of our pipeline to a variant with multiple levels of abstraction and multiple spatial scales:</p><formula xml:id="formula_16">? (d,ss ) lq = r ? [? ? ? (d,s) n ] n?W l , ? ? ? * (d,s ) q , R d ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_17">? ? ? (d,s) n = f (d) (X s n ; F ) and ? ? ? * (d,s ) q = f (d) (X * s q ; F ),</formula><p>with the following MSE loss:</p><formula xml:id="formula_18">arg min F ,R 1 ,...,R D L R , where L R = (9) E (W 1 ,...,W L ,q)?E ? l?I L ? d?I D ? s?S ? s ?S 1 ss ? (d,ss ) lq ? ? (c l ?c * q ) 2 .</formula><p>Such a basic formulation outperforms our earlier formulations. However, the relation network in Eq. (8) and the loss in Eq. (9) can be modified to form a better strategy utilizing the abstraction level and scale matching steps, as detailed next. Feature Matching (FM). Although MlSo can extract feature vectors at various spatial scales and levels of visual abstraction, not all scales and visual abstraction levels have the same importance. <ref type="figure">Figure 5</ref> shows that the features across different levels may contain foreground, context or even background information. In order to control the respective contributions of each scale and level of feature abstraction, we introduce a so-called Feature Matching. The FM step acts as an adaptive switch or weight for every branch of the relation learner.</p><p>Moreover, FM can be applied at the intra-or inter-level. <ref type="figure" target="#fig_2">Figure 4 (a)</ref> shows that FM can be applied within the single-level scale-wise features. Such a strategy is called intralevel matching as exclusive base learners are trained to make predictions at each respective level. Alternatively, one can choose to apply FM over multi-level representations, as shown in <ref type="figure" target="#fig_2">Figure 4 (b)</ref>. Such a strategy employs a shared base learner to make predictions over all available pairs.</p><p>For intra-and inter-matching, we investigate four types of matching: Cosine Matching (CM), Gate Module (GM), Optimal Transport (OT) and GRaph matching (GR). Let us take the intra-level matching scheme as example.</p><p>Let ? ? ? ). For CM and GM, the updated relation score is defined as follows:</p><formula xml:id="formula_19">? (d) nq = ? (d,ss ) nq r(? ? ? (d,s) n , ? ? ? * (d,s ) q ; R d ).<label>(13)</label></formula><p>Subsequently, we form the following MSE loss:</p><formula xml:id="formula_20">arg min F ,R 1 ,...,R D L R =<label>(14)</label></formula><p>where</p><formula xml:id="formula_21">L R = E (W 1 ,...,W L , q)?E ? l?I L ? n?W l ? d?I D ? (d) nq ?? (c l ?c * q ) 2 ,</formula><p>where the label-related symbols are defined as for Eq. (3). Optimal Transport (OT). DeepEMD <ref type="bibr" target="#b95">[96]</ref> transports the support location-wise representations to match the query locationwise representations (location-wise matching). In contrast, we propose a strategy complementary to design of DeepEMD. We solve a linear program to transport the support intra-level scale-wise representations into the query intra-level scale-wise representations. We obtain the dominant matching pattern of spatial scales at the given abstraction levels by: . Subsequently, we apply the loss in Eq. <ref type="bibr" target="#b14">(15)</ref>. GRaph matching (GR) strategy is based on a Graph Neural Network (GNN), which learns correlations between support and query samples at different levels of abstraction and scales. To implement GR, we form an adjacency matrix ? ? ? (d) nq ? R 2S?2S representing a weighted undirected graph capturing scales of the support-query pair for the abstraction level d. Following the standard GNN notation, we have H</p><formula xml:id="formula_22">? (d) nq = 1 ? min ? ? ? (d) nq ?0 ? s,s ? (d,ss ) nq ? (d,ss ) nq ,<label>(15)</label></formula><formula xml:id="formula_23">(d,l +1) nq = ? (? ? ? (d) nq H (d,l )</formula><p>nq W (l ) ), where ? (?) is a non-linearity and l refers to the GNN layer index. The node information matrix is given by H</p><formula xml:id="formula_24">(d,1) nq ? [? ? ? (d,s) n ] T s?I S ; 1 [? ? ? * (d,s ) q</formula><p>] T s ?I S , and W (l ) are filters of GNN. The adjacency matrix ? ? ? is defined as:</p><formula xml:id="formula_25">? ? ? (d) nq = ? ? ? ? ? ? ? ? ? (d,s) n ,? ? ? (d,s ) n ?e ?(s?s ) 2 2? 2 s?I S s ?I S ? ? ? (d,s) n ,? ? ? * (d,s ) q ?e ?(s?s ) 2 2? 2 s?I S s ?I S ? ? ? * (d,s) q ,? ? ? (d,s ) n ?e ?(s?s ) 2 2? 2 s?I S s ?I S ? ? ? * (d,s) q ,? ? ? * (d,s ) q ?e ?(s?s ) 2 2? 2 s?I S s ?I S ? ? ? ? ? ? ,<label>(16)</label></formula><p>where the dot product ?, ? captures the visual similarity between representations at scales s, s ?I S , and e ?(s?s ) 2 2? 2 is the Radial Basis Function (RBF) similarity prior w.r.t. scales s and s . Moreover, ? = 1 3 (S?1) ensures that for the maximum difference of scales equal S?1, the RBF kernel decays by 90% of its maximum value of one.</p><p>Subsequently, we apply a fully-connected layer to H (d,L ) , where the last layer L = 3:</p><formula xml:id="formula_26">? (d) nq = MLP H (d,L ) nq ,<label>(17)</label></formula><p>and then ? (d) nq can be substituted into Eq. (15). Visual Abstraction Level and Scale Discriminator. Selfsupervised learning has become a mainstream tool in devising robust representations. Below we propose to employ a selfsupervised Visual Abstraction Level and Scale Discriminator, called VALSD, whose role is to learn to predict the level of abstraction and the spatial scale indexes by observing the autocorrelation matrices of a given branch. Such a design is consistent with the idea of employing a simple pretext task. Self-supervised learning encourages the network to preserve additional information about images, leading to highly discriminative representations.</p><p>The VALSD unit is indicated in <ref type="figure" target="#fig_0">Figure 1</ref>. We proceed by simply taking second-order representations ? ? ? with the goal of predicting the joint abstraction-scale label d ?S+s, which ranges from 1 to 9 for S = 3 and three spatial scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Unsupervised Pipeline</head><p>For completeness, below we introduce an unsupervised FSL pipeline which does not rely on any class-wise training annotations. Inspired by self-supervised learning, we apply extensive augmentations to available training images (or videos) to form the so-called positive set, and we label these instances as similar. The above step promotes FEN to learn invariance to augmentations in the embedding space.</p><p>Specifically, given two image inputs X and Y, we first apply random augmentations on these images e.g., rotation, flip, resized crop and the color jitter via operator Aug(?) which samples these transformations according to a uniform distribution. We obtain a set of M augmented images:</p><formula xml:id="formula_27">X n ? Aug(X), Y q ? Aug(Y), n, q ? I M .<label>(18)</label></formula><p>Subsequently, we pass the augmented images to the feature encoder f and obtain their embeddings. Equations below assume D levels of feature abstraction but using a single level may be achieved by dropping the superscript (d) in what follows. For the augmented samples of X, we form relation descriptors which represent positive pairs (similar instances).</p><p>For the augmented samples of Y, we repeat the above step to also form positive pairs. We then form exhaustively the relation descriptors between the augmented samples of X and Y which represent negative pairs (dissimilar instances). Finally, we obtain relation predictions ? ? ? (d) , ? ? ? * (d) ? R M?M from the relation network r(?) for the augmented samples of X and Y, respectively. We also obtain the relation predictions ? ? ? (d) ? R M?M evaluated between the augmented samples of X and Y. The above steps are realized by:</p><formula xml:id="formula_28">? ? ? (d) n = f (d) ( X n ; F ), ? ? ? * (d) q = f (d) ( Y q ; F ), n, q, n , q ? I M , d ? I D , ? (d) nn = r ? ? ? ? (d) n , ? ? ? (d) n ; R d , ? (d) nq = r ? ? ? ? (d) n , ? ? ? * (d) q ; R d , ? * (d) qq = r ? ? ? ? * (d) q , ? ? ? * (d) q ; R d .<label>(19)</label></formula><p>Finally, we minimize the contrastive loss L uns w.r.t. F and R in order to push closer the positive embedding pairs of the augmented samples generated from the same image (X followed by Y) and push apart the negative pairs of embeddings of the augmented samples generated from X and Y (two different images):</p><p>arg min</p><formula xml:id="formula_29">F ,R 1 ,...,R D E (X,Y) E ( X n , Y q ) n,q?I M ? d?I D ? ? ? (d) ?1 2 F + ? ? ? * (d) ?1 2 F + ? ? ? (d) 2 F , where (X, Y) ? E uns and ( X n , Y q ) ? Aug(X)?Aug(Y).<label>(20)</label></formula><p>During the inference step, we simply apply Eq. (6). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>Below we demonstrate the usefulness of our approach by evaluating it on the Omniglot <ref type="bibr" target="#b44">[45]</ref>, mini-ImageNet <ref type="bibr" target="#b83">[84]</ref> and tiered-ImageNet <ref type="bibr" target="#b61">[62]</ref> datasets, a recently proposed Open MIC dataset <ref type="bibr" target="#b35">[36]</ref>, fine-grained Flower102 <ref type="bibr" target="#b53">[54]</ref>, CUB-200 <ref type="bibr" target="#b84">[85]</ref> and Food-101 <ref type="bibr" target="#b2">[3]</ref> datasets, and action recognition datasets such as HMDB51 <ref type="bibr" target="#b43">[44]</ref>, UCF101 <ref type="bibr" target="#b74">[75]</ref> and mini-MIT <ref type="bibr" target="#b74">[75]</ref>. We evaluate the performance of SoSN and MlSo in both supervised and unsupervised settings to demonstrate their superior performance compared to first-order representations in fewshot learning. We train our network with the Adam solver. The layer configurations of our SoSN model are shown in <ref type="figure">Figure 3</ref>. The results are compared against several state-ofthe-art methods in one-and few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Below we describe our setup, as well as the category recognition, fine-grained and action recognition datasets. Omniglot <ref type="bibr" target="#b44">[45]</ref> consists of 1623 characters from 50 alphabets. Samples in each class are drawn by 20 different people. The dataset is split into 1200 classes for training and 423 classes for testing. All images are resized to 28?28 pixels. mini-ImageNet <ref type="bibr" target="#b83">[84]</ref> consists of 60000 RGB images from 100 classes, each class containing 600 samples. We follow the standard protocol <ref type="bibr" target="#b83">[84]</ref> and use 80 classes for training (16 classes selected for validation) and remaining 20 classes for testing. We use images of 84?84 pixels. tiered-ImageNet <ref type="bibr" target="#b61">[62]</ref> consists of 608 classes from ImageNet. We follow the protocol with 351 base classes, 96 validation classes and 160 novel test classes. Open MIC stands for the Open Museum Identification Challenge (Open MIC) <ref type="bibr" target="#b35">[36]</ref>, a recent dataset with photos of various exhibits e.g., paintings, timepieces, sculptures, glassware, relics, science exhibits, natural history pieces, ceramics, pottery, tools and indigenous crafts, captured within 10 museum exhibition spaces according to which this dataset is divided into 10 sub-problems. In total, Open MIC has 866 diverse classes and 1-20 images per class. The within-class images undergo various geometric and photometric distortions as the data was captured with wearable cameras. This makes Open MIC a perfect candidate for testing one-shot learning algorithms. We combine (shn+hon+clv), (clk+gls+scl), (sci+nat) and (shx+rlc) into sub-problems p1, ..., p4. We form 12 where 70 classes are used for training, 20 classes for validation and the remaining 30 classes for testing, which is consistent with the protocol in <ref type="bibr" target="#b23">[24]</ref>. Stanford Cars <ref type="bibr" target="#b27">[28]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup</head><p>For the Omniglot dataset, we follow the setup in <ref type="bibr" target="#b77">[78]</ref>. For mini-ImageNet, we use the 5-way 1-shot and 5-way 5-shot protocols. For every training and testing episodes, we randomly select 5 and 3 query samples per class, respectively. We compute an average over 600 episodes to obtain results. We use the initial learning rate of 1e?3 and train the model with 200000 episodes. For tiered-ImageNet, we follow the settings used for the mini-ImageNet dataset. For Open MIC, we meancenter images per sub-problem. We use the initial learning rate of 1e?4 and train the network with 15000 episodes. For the fine-grained classification datasets, we evaluate our models on the 5-way 1-shot and 5-way 5-shot protocols. The numbers of support and query samples in each episode are the same as in the mini-ImageNet setting. We use the initial learning rate of 1e?3 and train the model with 200000 episodes. For the action recognition datasets, we randomly sample 50 frames per video along the temporal mode. We resize video frames to 84?84 pixels, which results in a lightweight model. We evaluate our algorithm on the 5-way 1-shot and 5-way 5shot protocols on the three datasets detailed earlier. We adopt the hyper-parameter configuration used on the mini-ImageNet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Results</head><p>Below we evaluate SoSN and its multi-level extension MlSo, and we compare them with state-of-the-art methods on datasets introduced above. Omniglot. <ref type="table" target="#tab_3">Table II</ref> shows rather saturated results. We consider experiments on the Omniglot dataset as a sanity check to validate the performance of SoSN in the best default setting, that is, using the relationship descriptor from Eq. (12) and III: Evaluations on the mini-ImageNet dataset (5-way accuracy). Asterisk '*' highlights that we converted the supervised Prototypical Net and the supervised Relation Net into the unsupervised contrastive pipelines, U-Prototypical Net and U-Relation Net, using the same mechanism (described in Section IV-D) as the one applied for U-SoSN and U-MlSo.  Power Normalization from Proposition 3. We note that the basic multi-level multi-scale variant of MlSo based on Eq. (9) outperforms SoSN.</p><p>mini-ImageNet. <ref type="table" target="#tab_3">Table III</ref> demonstrates that our method outperforms other approaches on both 1-and 5-shot evaluation protocols. Firstly, we note that comparisons between various relation descriptors with/without PN are included and discussed as ablation studies in Section V-D.</p><p>For the image size of 84?84 and the 5-way 1-shot experiment, our best singe-level SoSN model achieves ?2.5% higher accuracy than Relation Net <ref type="bibr" target="#b77">[78]</ref>. Our best singe-level SoSN also outperforms Prototypical Net by ?3.5% accuracy on the 5-way 1-shot protocol. Not shown in the table are results for SoSN trained with images of 224?224 pixels, in which case the accuracy scores on both protocols increase by 5.45% and 4.33%, respectively. Such an accuracy gain demonstrates that SoSN benefits from large image sizes as second-order matrices are of higher rank for higher image resolutions due to the higher spatial resolution of feature maps compared with the low-resolution counterparts. Our similarity learning network works with variable resolutions of input images because SoSN operates on matrices whose size depends only on the number of output channels of encoding network.   Furthermore, our variant of MlSo with the ResNet-12 backbone, based on the inter-level matching strategy of the multilevel multi-scale feature representations by the Gate Module, achieved a significant gain between 6% and 9% accuracy over SoSN. For unsupervised FSL, <ref type="table" target="#tab_3">Table III</ref> shows that our U-SoSN and U-MlSo significantly outperform other unsupervised baselines ('U-' stands for the unsupervised FSL setting) e.g., U-Relation Net and U-Prototypical Net. Compared to the U-SoSN model, the U-MlSo model improves the top-1 accuracy by another 2% and 3% on the 1-and 5-shot protocols, respectively.</p><p>We discuss the ablations on MlSo in Section V-D. Firstly, we discuss the results of the single-level SoSN model on more datasets. In the following tables, we drop '(?)+PN' from our notations but this particular relation descriptor with PN is the best performing variant, thus it is used across the remaining experiments.</p><p>tiered-ImageNet. <ref type="table" target="#tab_3">Table IV</ref> shows the performance of our proposed methods on tiered-ImageNet. Our SoSN achieves ? 4.1% and 3.9% improvement in accuracy, compared with Relation Net for 1-and 5-shot protocols, respectively. More-over, MlSo with the Gate Module achieves ? 7.0% and 3.9% improvement in accuracy, compared with Relation Net for 1and 5-shot protocols, respectively. The unsupervised variant, U-MlSo, yields ? 6% and 8% gain in accuracy over U-Relation Net for 1-and 5-shot protocols, respectively. Our supervised and unsupervised FSL models outperform all other FSL approaches based on the Conv-4 backbone.</p><p>Open MIC.   MlSo outperformed supervised SoSN on 4 splits. The above experiment shows that the unsupervised framework is especially effective in the scenario if (i) the number of training samples from the base classes is limited, and (ii) the problem is closely related to the image retrieval rather than the pure object category recognition.</p><p>Fine-grained FSL. For the fine-grained datasets, our proposed models are evaluated on the CUB Birds <ref type="bibr" target="#b84">[85]</ref>, Stanford Dogs <ref type="bibr" target="#b27">[28]</ref> and Cars <ref type="bibr" target="#b42">[43]</ref> datasets given the 5-way 1-shot and 5way 5-shot protocols. We follow the same training, validation and testing splits as provided in <ref type="bibr" target="#b23">[24]</ref>. <ref type="table" target="#tab_9">Table V</ref>  Few-shot Action Recognition (FSAR). We conclude our evaluations on FSL Action Recognition, denoted as FSAR for short. To this end, results are obtained on the HMDB51 <ref type="bibr" target="#b43">[44]</ref>, UCF101 <ref type="bibr" target="#b74">[75]</ref> and mini-MIT <ref type="bibr" target="#b51">[52]</ref> datasets. We follow the evaluation protocols in <ref type="bibr" target="#b99">[100]</ref>. As every action clip may contains hundreds of frames, we resize all frames to 84 ? 84 pixels and downsample clips along the temporal mode to reduce the usage of the GPU memory and limit the computational footprint.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Studies</head><p>Below we conduct ablations studies and provide discussions regarding several components of our pipeline. Relationship Descriptors and Power Normalization. Firstly, <ref type="table" target="#tab_3">Table III</ref> shows that the use of PN brings ? 0.8% gain in accuracy over not using it. Not included in the tables, similar were our observations on the Open MIC dataset. Given the simplicity of PN, we have included it in our evaluations unless stated otherwise.</p><p>Relationship descriptors from <ref type="table" target="#tab_3">Table I</ref> are evaluated in Table III according to which the single-level SoSN(?) model outperforms the single-level SoSN( ?+F) and SoSN(?+R) models. We expect that averaging over Z support descriptors from Z support images, as in SoSN(?), removes the uncertainty in few-shot statistics, whereas the outer-products of support/query datapoints still enjoy the benefit of spatiallywise large convolutional feature maps (which helps form the robust second-order statistics). Importance of Second-order Pooling. The gain in accuracy can be attributed to the fact that SoSN is equipped with second-order pooling. <ref type="table" target="#tab_3">Table III</ref> shows that on mini-ImageNet, Relation Net scores 50.44% and 65.32% accuracy on the 1-and 5-shot protocols, whereas SoSN scores 52.96% and 68.63% accuracy, respectively.</p><p>The similar trend can be observed on mini-ImageNet when aggregating over feature maps of images of 256?256 pixels. In such a setting (not included in the tables as 256?256 pixels resolution is not a part of standard FSL protocol), Relation Net yields 54.01% and 68.56% accuracy on the 1-and 5shot protocols. In contrast, SoSN yields 57.74% and 71.08% accuracy, respectively.</p><p>SoP represents features of each image as second-order statistics which are invariant to the spatial order of features in feature maps, and the spatial size of these feature maps. Second-order statistics are also richer than first-order statistics, as indicated by gains in accuracy attained by SoSN in comparison with Relation Net. SoP is the most beneficial when feature representations provide many feature vectors for aggregation e.g., N ? d. In such a case, the autocorrelation matrices may be of full rank (rank-d), thus capturing more statistical information in comparison to the first-order prototypes (equivalent of the rank-1 statistic).</p><p>MlSo, SB, FM and VALSD. <ref type="table" target="#tab_3">Table III</ref> demonstrates on mini-ImageNet that our MlSo outperforms SoSN and a larger number of prior works. The performance on the 5-way 1-shot and 5-way 5-shot protocols achieves the peak gain of ? 6.0% in accuracy given 4 encoding levels and the Optimal Transport matching step, compared to the best single-level SoSN model. <ref type="table" target="#tab_3">Table VIII</ref> shows that adding our Scale-wise Boosting (SB), the Feature Matching (FM) and the Visual Abstraction Level and Scale Discriminator (VALSD) yields around 2% gain in accuracy over the baseline model. For the SB+FM case, we mean that the advanced abstraction level and spatial scale matching strategy is used, thus the loss in Eq. (15) is used for SB+FM, whereas the loss in Eq. (9) is used for SB alone. <ref type="table" target="#tab_3">Table IX</ref> shows that using four level of visual abstraction and three spatial scales is the best, which is consistent with our claim that the levels of visual abstraction and spatial scales need to be taken into account in few-shot learning. <ref type="table" target="#tab_17">Table X</ref> shows that the Intra-level Matching strategy is overall better than the Inter-level Matching strategy. This is consistent with our expectations as the levels of visual abstraction and spatial scales are quite complementary. Therefore, matching spatial scales irrespective of abstraction levels is a meaningful strategy. Finally, the Gate Module and the Optimal Transport are two best performing strategies, followed by the GRaph matching (GR) strategy and the Cosine Matching (CM) strategy. We suspect that GR was somewhat suboptimal due to the small-size dense adjacency matrix in our problem rather than the large scale sparse adjacency matrix which would normally capture a complex topology of some large graph (node classification, etc.). OT performed robust matching as it is designed to find an optimal transportation plan between different levels of abstraction and spatial scales of support-query pairs. Nonetheless, GM appears to provide the best matching trade-off, that is, GM is almost as good as OT in terms of accuracy, and it is faster than OT in terms of computational complexity (i.e., there is no need to solve any linear programs). Finally, <ref type="figure">Figure 5</ref> shows the visualization of multiple levels of feature abstraction in our FEN. Across all visualizations and their Gate Module scores (GM), the coarse-to-fine levels of feature abstraction appear to be complementary with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>We have presented the end-to-end trainable SoSN and MlSo models for supervised and unsupervised few-shot learning. With the use of multiple levels of feature abstractions, multiple spatial scales, the Gate Module and the Visual Abstraction Level and Scale Discriminator, we have shown how to learn efficiently the similarity between the support-query pairs captured by the relation descriptors. We have investigated how to capture relations between the query and support matrices, and how to produce multiple complementary levels of feature information. We have also investigated several strategies for the level of abstraction and spatial scale matching to showcase the importance of decomposing the support-query pairs into multiple spatial scales at multiple levels of feature abstraction. MlSo demonstrates consistent large gains in accuracy across all benchmarks for both supervised and unsupervised learning. Given the simplicity of our approach, we believe that the SoSN and MlSo models are interesting propositions that can serve as a starting point in designing more elaborate FSL approaches.</p><p>Hongguang Zhang Hongguang Zhang is currently an assistant professor in Systems Engineering Institute, AMS. Before this, he received his PhD degree in computer vision and machine learning at the Australian National University and Data61/CSIRO, Canberra, Australia in 2020. He received the BSc degree in electrical engineering and automation from Shanghai Jiao Tong University, Shanghai, China in 2014. He received his MSc degree in electronic science and technology from National University of Defense Technology, Changsha, China in 2016. His interests include fine-grained image classification, zero-shot learning, few-shot learning and deep learning methods.</p><p>Hongdong Li was with NICTA Canberra Labs, prior to 2010, where he involved in the "Australia Bionic Eyes" Project. He is currently a Professor with the Computer Vision Group, The Australian National University. He is also a Chief Investigator of the Australia ARC Centre of Excellence for Robotic Vision. His research interests include 3D vision reconstruction, structure from motion, multi-view geometry, as well as applications of optimization methods in computer vision. He was a recipient of the CVPR Best Paper Award in 2012 and the Marr Prize Honorable Mention in 2017. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Piotr Koniusz</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The pipeline of our Multi-level Second-order (MlSo) few-shot learning. Convolutional feature maps corresponding to inputs at multiple spatial scales are extracted at multiple levels of the CNN Feature Encoder to perform advanced matching. At each level, we apply second-order pooling with the Matching Module before passing these scale-wise representations to the base learner, with the MSE loss per level of visual abstraction applied, and the Visual Abstraction Level and Scale Discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>SoF</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Our matching strategies for multiple spatial scales and multiple levels of feature abstraction. Firstly, we downsample the original support and query images (or videos) to 1/2 or 1/4 of the original resolution. Subsequently, we feed them into the Feature Encoder and construct scale-wise support-query pairs for relation learning. Cosine Matching, Gate Module, Optimal Transport and Graph Matching strategies are detailed in Section IV-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>.</head><label></label><figDesc>Then we feed them into the feature encoder to generate the convolutional features ? ? ? (s) and ? ? ? * (s ) , where s and s are scale-wise indexes. Subsequently, we form the supportquery relation descriptors at spatial scales (s, s ), pass them via function r, and obtain the scale-wise relation similarity scores ? (ss ) lq : ? (ss ) lq</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 Fig. 5 :</head><label>35</label><figDesc>Visualization of features at several levels of abstraction. We indicate the Gate Module scores (GM) in the right bottom corner per image. We use ResNet-12 FEN of MlSo to obtain features with the CAM<ref type="bibr" target="#b101">[102]</ref> visualization model on images of mini-ImageNet. The class-wise activation maps vary at different levels of abstraction. Given animals in rows 1-4, shallower features are localized at boundaries between the object and background. More abstract features appear on salient fine-grained parts of animals. For images of aerospace airplanes and radars, shallower features come from objects, whereas more abstract features capture salient parts of objects or the context around them. Therefore, multiple levels of abstraction provide complementary descriptions of various visual concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?</head><label></label><figDesc>(? ? ? (d,s) n ) (:) and ? ? ? * (d,s ) q ? (? ? ? * (d,s ) q ) (:) be pooled (as in Eq. (12) but without the concatenation step) support and query descriptors, vectorized by the operator (:). Cosine Matching (CM) simply reweights scale-wise feature pairs via the cosine similarity ? (d,ss ) nq = ? ? ? (d,s) n , ? ? ? * (d,s ) q . Gate Module (GM) in Fig. 3 learns to assign an attention score for each pair based on a convolutional network g(?) so that ? (d,ss ) nq = g(? ? ? (d,s) n )?g(? ? ? * (d,s ) q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>S s =1 ? ? ? * (d,s ) q ) and ? * (d,s ) q = max(0, ? ? ? * (d,s ) q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(d,s) n and ? ? ? * (d,s) q for each level of abstraction d ? I D and a spatial scale s ? I S , where (d, s) indexes serve as labels for the pretext task. We employ the softmax cross-entropy classifier on ? ? ? (d,s) n and ? ? ? * (d,s) q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>A Senior Research Scientist in Machine Learning Research Group at Data61/CSIRO, formerly known as NICTA, and a Senior Honorary Lecturer at Australian National University (ANU). Previously, he worked as a postdoctoral researcher in the team LEAR, INRIA, France. He received his BSc degree in Telecommunications and Software Engineering in 2004 from the Warsaw University of Technology, Poland, and completed his PhD degree in Computer Vision in 2013 at CVSSP, University of Surrey, UK. His interests include visual categorization, spectral learning on graphs and tensors, kernel methods and deep learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and P. Koniusz are with the College of Engineering and Computer Science, Australian National University. P. Koniusz is also with Data61/CSIRO. E-mail: zhang. hongguang@outlook.com, hongdong.li@anu.edu.au, piotr.koniusz@data61. csiro.au. Code: https://github.com/HongguangZhang/mlso-tmm-master. This work is supported by National Natural Science Foundation of China (Grant No. 62106282), and Equipment Development Research Fund (Grant No. ZXD2020C2316). Part of this work wass done during H. Zhang's stay at the ANU. Manuscript received May 12, 2021, accepted Jan. 9, 2022. DOI: https: //doi.org/10.1109/TMM.2022.3142955</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>where operator [?] stacks matrices along the third mode, support subsets and query are (W 1 , ..., W L , q) ? E , where E is a set of episodes. Each W 1 shares the label with q, whereas W 2 , ..., W L do not share the label with q. We use the Mean Square Error (MSE) for the objective of our end-to-end SoSN:arg min</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Proposed relationship descriptors ? used in relation learning. Note differences between equations</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Evaluations on the Omniglot dataset.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="5">Fine 5-way accuracy 20-way accuracy</cell></row><row><cell></cell><cell></cell><cell cols="5">Tune 1-shot 5-shot 1-shot 5-shot</cell></row><row><cell cols="2">Conv. Siamese Nets [32] Conv. Siamese Nets [32]</cell><cell>N Y</cell><cell>96.7 97.3</cell><cell>98.4 98.4</cell><cell>88.0 88.1</cell><cell>96.5 97.0</cell></row><row><cell>Matching Net</cell><cell>[84]</cell><cell>N</cell><cell>98.1</cell><cell>98.9</cell><cell>93.8</cell><cell>98.5</cell></row><row><cell>Prototypical Net</cell><cell>[73]</cell><cell>N</cell><cell>99.8</cell><cell>99.7</cell><cell>96.0</cell><cell>98.9</cell></row><row><cell>MAML</cell><cell>[13]</cell><cell>Y</cell><cell>98.7</cell><cell>99.9</cell><cell>95.8</cell><cell>98.9</cell></row><row><cell>Relation Net SoSN+PN MlSo+PN</cell><cell>[78]</cell><cell>N N N</cell><cell>99.6 99.8 99.9</cell><cell>99.8 99.9 99.9</cell><cell>97.6 98.3 98.7</cell><cell>99.1 99.4 99.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV :</head><label>IV</label><figDesc>Evaluations</figDesc><table><row><cell></cell><cell cols="3">on the tiered-ImageNet dataset (5-way</cell></row><row><cell cols="3">accuracy) with the Conv-4 backbone.</cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>Incremental</cell><cell>[61]</cell><cell>51.12 ? 0.45</cell><cell>66.40 ? 0.36</cell></row><row><cell>Soft k-means</cell><cell>[62]</cell><cell>52.39 ? 0.44</cell><cell>69.88 ? 0.20</cell></row><row><cell>MAML</cell><cell>[13]</cell><cell>51.67 ? 1.81</cell><cell>70.30 ? 0.08</cell></row><row><cell>Reptile</cell><cell>[53]</cell><cell>48.97 ? 0.21</cell><cell>66.47 ? 0.21</cell></row><row><cell>Prototypical Net</cell><cell>[73]</cell><cell>53.31 ? 0.89</cell><cell>72.69 ? 0.74</cell></row><row><cell>Relation Net</cell><cell>[78]</cell><cell>54.48 ? 0.93</cell><cell>71.32 ? 0.78</cell></row><row><cell>TPN</cell><cell>[50]</cell><cell>57.41 ? 0.94</cell><cell>71.55 ? 0.74</cell></row><row><cell>SoSN</cell><cell></cell><cell>58.62 ? 0.92</cell><cell>75.19 ? 0.79</cell></row><row><cell>MlSo</cell><cell></cell><cell>61.97 ? 0.91</cell><cell>78.83 ? 0.77</cell></row><row><cell></cell><cell cols="2">Unsupervised Pipelines</cell><cell></cell></row><row><cell cols="2">U-Prototypical Net [73]</cell><cell>37.52 ? 0.93</cell><cell>51.03 ? 0.84</cell></row><row><cell>U-Relation Net</cell><cell>[78]</cell><cell>37.23 ? 0.94</cell><cell>49.54 ? 0.83</cell></row><row><cell>U-SoSN</cell><cell></cell><cell>41.59 ? 0.92</cell><cell>55.81 ? 0.76</cell></row><row><cell>U-MlSo</cell><cell></cell><cell>43.01 ? 0.91</cell><cell>57.53 ? 0.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V :</head><label>V</label><figDesc>Evaluations on the fine-grained classification datasets(5-way accuracy) with the Conv-4 backbone. ? 1.03 59.50 ? 1.01 35.80 ? 0.99 47.50 ? 1.03 34.80 ? 0.98 44.70 ? 1.03 MAML [13] 58.13 ? 0.36 71.51 ? 0.30 44.84 ? 0.31 58.61 ? 0.30 47.25 ? 0.30 61.11 ? 0.29 Proto. Net [73] 37.36 ? 1.00 45.28 ? 1.03 37.59 ? 1.00 48.19 ? 1.03 40.90 ? 1.01 52.93 ? 1.03 Relation Net [78] 58.99 ? 0.52 71.20 ? 0.40 43.29 ? 0.46 55.15 ? 0.39 47.79 ? 0.49 60.60 ? 0.41 LRPABN [24] 63.63 ? 0.77 76.06 ? 0.58 45.72 ? 0.75 60.94 ? 0.66 60.28 ? 0.76 73.29 ? 0.63 MattML [106] 66.29 ? 0.56 80.34 ? 0.30 54.84 ? 0.53 71.34 ? 0.38 66.11 ? 0.54 82.80 ? 0.28 SoSN [97] 64.56 ? 0.91 77.82 ? 0.57 48.21 ? 0.72 63.15 ? 0.67 62.88 ? 0.72 76.10 ? 0.58 MlSo 68.21 ? 0.78 82.18 ? 0.47 55.62 ? 0.58 71.98 ? 0.71 67.83 ? 0.63 84.98 ? 0.48 ? 0.53 56.32 ? 0.41 32.05 ? 0.49 43.96 ? 0.48 33.87 ? 0.57 48.20 ? 0.46 U-Relation Net [78] 35.42 ? 0.55 57.96 ? 0.43 32.75 ? 0.49 44.37 ? 0.46 34.43 ? 0.54 48.71 ? 0.45 U-SoSN [97] 43.14 ? 0.51 65.02 ? 0.43 41.56 ? 0.49 53.62 ? 0.47 40.31 ? 0.55 57.98 ? 0.43 U-MlSo 46.31 ? 0.53 68.67 ? 0.46 45.02 ? 0.48 56.89 ? 0.46 43.81 ? 0.56 61.13 ? 0.41</figDesc><table><row><cell></cell><cell cols="2">CUB Birds</cell><cell cols="2">Stanford Dogs</cell><cell cols="2">Stanford Cars</cell></row><row><cell>Model</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell cols="2">Matching Net 45.30 U-Proto. Net [84] [78] 34.51</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI :</head><label>VI</label><figDesc>Evaluations on the action recognition benchmarks (5-way accuracy). To fairly compare our method with the prior works, we follow the same evaluation splits with<ref type="bibr" target="#b99">[100]</ref>. For our baselines, we re-implement the Prototypical Net and the Relation Net both with the use of 3D convolutions.</figDesc><table><row><cell></cell><cell></cell><cell>HMDB51</cell><cell></cell><cell>UCF101</cell><cell></cell><cell cols="2">mini-MIT</cell><cell cols="2">Kinetics</cell></row><row><cell>Model</cell><cell></cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell><cell cols="2">1-shot 5-shot</cell></row><row><cell>C3D PN</cell><cell>[73]</cell><cell cols="5">38.05 ? 0.97 53.15 ? 0.90 57.05 ? 1.02 78.25 ? 0.73 33.65 ? 1.01</cell><cell>45.1 ? 0.90</cell><cell>57.11</cell><cell>77.92</cell></row><row><cell>C3D RN</cell><cell>[78]</cell><cell cols="6">38.23 ? 0.97 53.17 ? 0.86 58.21 ? 1.02 78.35 ? 0.72 35.71 ? 1.02 47.32 ? 0.91</cell><cell>56.98</cell><cell>77.83</cell></row><row><cell>C3D SoSN</cell><cell>[97]</cell><cell cols="6">40.83 ? 0.96 55.18 ? 0.86 62.57 ? 1.05 81.51 ? 0.75 40.83 ? 0.99 52.16 ? 0.95</cell><cell>58.77</cell><cell>79.02</cell></row><row><cell>CMN</cell><cell>[105]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>60.50</cell><cell>78.90</cell></row><row><cell>ARN</cell><cell cols="7">[100] 45.52 ? 0.96 58.96 ? 0.87 66.32 ? 0.99 83.12 ? 0.70 43.05 ? 0.97 56.71 ? 0.87</cell><cell>63.70</cell><cell>82.40</cell></row><row><cell>C3D MlSo</cell><cell></cell><cell cols="6">46.69 ? 0.93 60.31 ? 0.83 68.19 ? 0.95 87.11 ? 0.71 44.67 ? 0.95 58.68 ? 0.86</cell><cell>66.32</cell><cell>85.21</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII :</head><label>VII</label><figDesc>Evaluations on the Open MIC dataset (Protocol I) (5-way 1-shot accuracy). Notation x?y means training on exhibition x and testing on y.</figDesc><table><row><cell>Model</cell><cell cols="12">p1?p2 p1?p3 p1?p4 p2?p1 p2?p3 p2?p4 p3?p1 p3?p2 p3?p4 p4?p1 p4?p2 p4?p3</cell></row><row><cell cols="2">Matching Net [84] 68.7</cell><cell>52.3</cell><cell>60.9</cell><cell>45.6</cell><cell>48.1</cell><cell>69.1</cell><cell>48.0</cell><cell>46.9</cell><cell>65.7</cell><cell>42.1</cell><cell>68.9</cell><cell>50.5</cell></row><row><cell>MAML</cell><cell>[13] 69.1</cell><cell>52.8</cell><cell>61.8</cell><cell>46.4</cell><cell>48.2</cell><cell>69.1</cell><cell>48.4</cell><cell>48.3</cell><cell>65.8</cell><cell>43.2</cell><cell>70.2</cell><cell>50.2</cell></row><row><cell>Reptile</cell><cell>[53] 69.2</cell><cell>52.5</cell><cell>62.2</cell><cell>47.1</cell><cell>48.8</cell><cell>69.3</cell><cell>48.5</cell><cell>48.7</cell><cell>66.1</cell><cell>43.6</cell><cell>70.1</cell><cell>50.3</cell></row><row><cell>Proto. Net</cell><cell>[73] 70.0</cell><cell>53.9</cell><cell>62.1</cell><cell>46.5</cell><cell>49.7</cell><cell>69.9</cell><cell>49.1</cell><cell>48.2</cell><cell>67.1</cell><cell>43.9</cell><cell>70.5</cell><cell>51.1</cell></row><row><cell>Relation Net</cell><cell>[78] 71.1</cell><cell>53.6</cell><cell>63.5</cell><cell>47.2</cell><cell>50.6</cell><cell>68.5</cell><cell>48.5</cell><cell>49.7</cell><cell>68.4</cell><cell>45.5</cell><cell>70.3</cell><cell>50.8</cell></row><row><cell>SoSN</cell><cell>[97] 81.4</cell><cell>65.2</cell><cell>75.1</cell><cell>60.3</cell><cell>62.1</cell><cell>77.7</cell><cell>61.5</cell><cell>82.0</cell><cell>78.0</cell><cell>59.0</cell><cell>80.8</cell><cell>62.5</cell></row><row><cell>MlSo</cell><cell>81.6</cell><cell>66.3</cell><cell>77.2</cell><cell>62.4</cell><cell>63.1</cell><cell>78.2</cell><cell>64.2</cell><cell>81.4</cell><cell>78.0</cell><cell>60.1</cell><cell>81.6</cell><cell>63.6</cell></row><row><cell cols="2">U-Relation Net [78] 67.1</cell><cell>48.1</cell><cell>62.5</cell><cell>41.2</cell><cell>45.3</cell><cell>58.0</cell><cell>50.1</cell><cell>57.5</cell><cell>53.8</cell><cell>46.9</cell><cell>66.1</cell><cell>43.3</cell></row><row><cell>U-SoSN</cell><cell>[97] 78.6</cell><cell>58.8</cell><cell>74.3</cell><cell>61.1</cell><cell>57.9</cell><cell>72.4</cell><cell>62.3</cell><cell>75.6</cell><cell>73.7</cell><cell>58.5</cell><cell>76.5</cell><cell>54.6</cell></row><row><cell>U-MlSo</cell><cell>80.9</cell><cell>61.5</cell><cell>76.3</cell><cell>62.0</cell><cell>60.3</cell><cell>75.1</cell><cell>64.1</cell><cell>77.9</cell><cell>76.2</cell><cell>59.9</cell><cell>79.1</cell><cell>58.2</cell></row><row><cell cols="6">p1: shn+hon+clv, p2: clk+gls+scl, p3: sci+nat, p4: shx+rlc.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table VII demonstrates that our single-level SoSN model outperforms the Relation Net [78] for all train/test subproblems of Protocol I. For the 5-and 20-way protocols, Relation Net scores 55.45% and 31.58% accuracy, respectively. In contrast, our single-level SoSN scores 70.46% and 49.05% accuracy, respectively.For the unsupervised pipelines, the U-SoSN and U-MlSo models achieve very promising results on Open MIC considering no class-wise annotations were used during the training step. To demonstrate this point further, the performance of U-SoSN is better than that of the supervised Relation Net. The U-MlSo model with the Gate Module further outperforms U-SoSN by up to ?3% accuracy, which is close to the performance of the supervised SoSN model. Specifically, U-</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VIII :</head><label>VIII</label><figDesc>Ablation study on mini-ImageNet w.r.t. different modules of our pipeline (5-way accuracy, the 'Conv-4-64' backbone, 4 stages and 3 scales, and OT matching were used.)</figDesc><table><row><cell>Baseline SB FM VALSD</cell><cell cols="2">1-shot 5-shot</cell></row><row><cell></cell><cell>55.93</cell><cell>71.05</cell></row><row><cell></cell><cell>57.28</cell><cell>72.01</cell></row><row><cell></cell><cell>57.79</cell><cell>72.65</cell></row><row><cell></cell><cell>58.03</cell><cell>73.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE IX :</head><label>IX</label><figDesc>Ablation study on mini-ImageNet w.r.t. the number of abstraction levels and spatial scales (1-shot/5-shot accuracy, FM and VALSD were not used).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Levels</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell></cell><cell>1</cell><cell cols="2">55.24/70.85 55.66/70.88 55.93/71.05</cell></row><row><cell>Scales</cell><cell>2</cell><cell cols="2">55.95/70.77 56.19/71.13 56.63/71.72</cell></row><row><cell></cell><cell>3</cell><cell cols="2">56.67/71.81 57.03/71.98</cell><cell>57.28/72.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>demonstrates that our models outperform the baseline models such as Relation Net<ref type="bibr" target="#b77">[78]</ref>, LRPABN<ref type="bibr" target="#b23">[24]</ref> and MattML<ref type="bibr" target="#b105">[106]</ref>. For CUB Birds, our best MlSo with FM (OT) achieves ? 9.2% and ? 11.0% improvements in accuracy (the 1-and 5-shot protocols, respectively) compared to the 1-st order Relation Net. For the Stanford Dogs and Cars datasets, the overall improvements of our MlSo are more significant. To illustrate this point, MlSo outperforms Relation Net by up to ? 12.4% and ? 16.8% accuracy on Stanford Dogs, and ? 20.0% and ? 24.8% on Stanford Cars (the 1-and 5-shot protocols, respectively). Furthermore, our unsupervised FSL variants, U-SoSN and U-MlSo, attain even higher gains in accuracy which are often in the range of between 10% and 25% compared to U-Relation Net.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE X :</head><label>X</label><figDesc>Ablation study on mini-ImageNet w.r.t. the choice of matching algorithm and mode on the mini-Imagenet dataset (1-and 5-shot accuracy). We used the 'Conv-4-64' backbone.<ref type="bibr" target="#b56">57</ref>.83 58.03 57.79 56.87 57.72 57.91 56.91 5 72.21 72.58 73.06 72.49 71.59 72.43 72.36 71.77</figDesc><table><row><cell></cell><cell cols="3">Intra-level Matching</cell><cell></cell><cell></cell><cell cols="2">Inter-level Matching</cell><cell></cell></row><row><cell>shot</cell><cell>CM</cell><cell>GM</cell><cell>OT</cell><cell>GR</cell><cell>CM</cell><cell>GM</cell><cell>OT</cell><cell>GR</cell></row><row><cell>1</cell><cell>57.49</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>Table IV (tiered-ImageNet) shows that Relation Net achieves 54.48% and 71.32 % accuracy on the 1-and 5-shot protocols, respectively. Relation Net can be considered identical with the SoSN model but it uses first-order pooling. In contrast, SoSN achieves 58.62 and 75.19 % accuracy (1-and 5-shot protocols, respectively).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">possible pairs in which sub-problem x is used for training and sub-problem y is used for testing (x?y). Caltech-UCSD-Birds 200-2011 (CUB Birds)<ref type="bibr" target="#b84">[85]</ref> has 11788 images of 200 bird species. We follow the splits from<ref type="bibr" target="#b23">[24]</ref>, that is, 130 classes are selected for training, 20 classes for validation and the remaining 50 categories for testing. Stanford Dogs<ref type="bibr" target="#b42">[43]</ref> has 17150 instances of 120 dogs classes</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to train your maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-generalization: Learning novel classes from a single example by feature replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="672" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Food-101 -mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine learning (ICML&apos;10)</title>
		<meeting>International Conference on Machine learning (ICML&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning mid-level features for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Videopuzzle: Descriptive one-shot video composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="534" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object classification from a single example utilizing class relevance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="449" to="456" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action recognition from video using feature covariance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakash</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janusz</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2479" to="2494" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural graph matching networks for fewshot 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De-An</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="653" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint dimensionality reduction and metric learning: A geometric take</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<idno>PMLR, 2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1404" to="1413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attribute-enhanced face recognition with neural tensor fusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Neil M Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3744" to="3753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning dualpooling graph neural networks for few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Low-rank pairwise alignment bilinear network for few-shot fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingsong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Task agnostic meta-learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the burstiness of visual elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised meta-learning for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislau</forename><surname>Boloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nityananda</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangpeng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gunnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tensor representations via kernel linearization for action recognition from 3d skeletons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Domain adaptation by mixture of alignments of second-or higher-order scatter tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Museum exhibit identification challenge for the supervised domain adaptation and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="788" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Tensor representations for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Higherorder Occurrence Pooling on Mid-and Low-level Features: Visual Concept Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Higher-order occurrence pooling for bags-ofwords: Visual concept detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Comparison of mid-level feature coding approaches and pooling strategies in visual concept detection. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="479" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Power normalizations in fine-grained image, few-shot image and graph classification. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A deeper look at power normalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the annual meeting of the cognitive science society</title>
		<meeting>the annual meeting of the cognitive science society</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Metasgd: Learning to learn quickly for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Secondorder democratic aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A generative approach to zero-shot and few-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinay Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiva Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Arulkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="372" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Moments in time dataset: one million videos for event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Pau Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="721" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Few-shot image recognition with knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fast adaptive meta-learning for few-shot image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniwat</forename><surname>Phaphuangwittayakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<title level="m">Covariance tracker. CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Redro: Efficiently learning large-sized spd visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Neural Information Processing: Research and Development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipo</forename><surname>Jagath Chandana Rajapakse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer-Verlag Berlin and Heidelberg GmbH &amp; Co. KG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Incremental few-shot learning with attention attractor networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.07218</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Meta-learning for semi-supervised few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 6th International Conference on Learning Representations ICLR</title>
		<meeting>6th International Conference on Learning Representations ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Enhanced local binary covariance matrices (elbcm) for texture analysis and object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mich?le</forename><surname>Gouiff?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Lacassagne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th international conference on computer vision/computer graphics collaboration techniques and applications</title>
		<meeting>the 6th international conference on computer vision/computer graphics collaboration techniques and applications</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garcia</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Bruna</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deep co-occurrence feature learning for visual object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Fang</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang-Ming</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Weakly-shared deep transfer networks for heterogeneousdomain knowledge propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Hierarchical long short-term concurrent memory for human interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Host-parasite: Graph lstm-in-lstm for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="663" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">On modulating the gradient for meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Real-world cross-modal retrieval via sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1708" to="1721" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">When does self-supervision improve few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Fisher-bures adversary graph convolutional networks. UAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="465" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Generalized deep transfer networks for knowledge propagation in heterogeneous domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Coherence constrained graph lstm for group activity recognition. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Region covariance: A fast descriptor for detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">3d skeleton-based fewshot action recognition with jeanie is not so na?ve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Tracking by thirdorder tensor representation. Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenli</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="385" to="396" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Relation embedding for personalised translation-based poi recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flora</forename><forename type="middle">D</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongli</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearestneighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">The influence of improvement in one mental function upon the efficiency of other functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Thorndike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review (I)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="247" to="261" />
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Holanda De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Dense dilated network for few shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baohan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Hao Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Luwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Flat: Few-shot learning via autoencoding transformation regularizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12674</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Variational few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Jia Yu Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1685" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Power normalizing second-order similarity network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Rethinking class relations: Absoluterelative supervised and unsupervised few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlei</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Fewshot learning via saliency-guided hallucination of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Few-shot action recognition with permutation-invariant attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part V 16</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Beyond covariance: SICE and kernel based visual feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="320" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Simple spectral graph convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Oneshot texture retrieval using global grouping metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Compound memory networks for few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Multiattention meta learning for few-shot fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Attributeguided feature learning for few-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqing</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Fast context adaptation via metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyriacos</forename><surname>Shiarli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7693" to="7702" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
