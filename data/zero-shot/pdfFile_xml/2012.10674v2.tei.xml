<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Camera-aware Proxies for Unsupervised Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baisheng</forename><surname>Lai</surname></persName>
							<email>baisheng.lbs@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
							<email>jianqiang.hjq@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Gong</surname></persName>
							<email>gongxj@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
							<email>huaxiansheng@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Camera-aware Proxies for Unsupervised Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper tackles the purely unsupervised person reidentification (Re-ID) problem that requires no annotations. Some previous methods adopt clustering techniques to generate pseudo labels and use the produced labels to train Re-ID models progressively. These methods are relatively simple but effective. However, most clustering-based methods take each cluster as a pseudo identity class, neglecting the large intra-ID variance caused mainly by the change of camera views. To address this issue, we propose to split each single cluster into multiple proxies and each proxy represents the instances coming from the same camera. These camera-aware proxies enable us to deal with large intra-ID variance and generate more reliable pseudo labels for learning. Based on the camera-aware proxies, we design both intra-and inter-camera contrastive learning components for our Re-ID model to effectively learn the ID discrimination ability within and across cameras. Meanwhile, a proxy-balanced sampling strategy is also designed, which facilitates our learning further. Extensive experiments on three large-scale Re-ID datasets show that our proposed approach outperforms most unsupervised methods by a significant margin. Especially, on the challenging MSMT17 dataset, we gain 14.3% Rank-1 and 10.2% mAP improvements when compared to the second place. Code is available at: https://github.com/Terminator8758/CAP-master.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Backbone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global clusters</head><p>Camera-aware proxies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LInter LIntra</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model updating step</head><p>Cam-1 Cam-2 Cam-C</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Person re-identification (Re-ID) is the task of identifying the same person in non-overlapping cameras. This task has attracted extensive research interest due to its significance in surveillance and public security. State-of-the-art Re-ID performance is achieved mainly by fully supervised methods <ref type="bibr" target="#b25">(Sun et al. 2018;</ref><ref type="bibr" target="#b0">Chen et al. 2019)</ref>. These methods need sufficient annotations that are expensive and timeconsuming to attain, making them impractical in real-world deployments. Therefore, more and more recent studies focus on unsupervised settings, aiming to learn Re-ID models via unsupervised domain adaptation (UDA) <ref type="bibr" target="#b30">(Wei et al. 2018a;</ref><ref type="bibr" target="#b20">Qi et al. 2019b;</ref><ref type="bibr" target="#b42">Zhong et al. 2019)</ref> or purely unsupervised <ref type="bibr" target="#b14">(Lin et al. 2019;</ref><ref type="bibr" target="#b13">Li, Zhu, and Gong 2018;</ref> The corresponding author. This work was supported by Major Scientific Research Project of Zhejiang Lab (No. 2019DB0ZX01). Copyright ? 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 2019) techniques. Although considerable progress has been made in the unsupervised Re-ID task, there is still a large gap in performance compared to the supervised counterpart. This work addresses the purely unsupervised Re-ID task, which does not require any labeled data and therefore is more challenging than the UDA-based problem. Previous methods mainly resort to pseudo labels for learning, adopting Clustering <ref type="bibr" target="#b14">(Lin et al. 2019;</ref><ref type="bibr" target="#b37">Zeng et al. 2020</ref>), k-nearest neighbors (k-NN) <ref type="bibr" target="#b13">(Li, Zhu, and Gong 2018;</ref><ref type="bibr" target="#b2">Chen, Zhu, and Gong 2018)</ref>, or graph <ref type="bibr" target="#b36">(Ye et al. 2017;</ref><ref type="bibr" target="#b32">Wu et al. 2019)</ref> based association techniques to generate pseudo labels. The clustering-based methods learn Re-ID models by iteratively conducting a clustering step and a model updating step. These methods have a relatively simple routine but achieve arXiv:2012.10674v2 [cs.CV] 5 Feb 2021 promising results. Therefore, we follow this research line and propose a more effective approach.</p><p>Previous clustering-based methods <ref type="bibr" target="#b14">(Lin et al. 2019;</ref><ref type="bibr" target="#b37">Zeng et al. 2020;</ref><ref type="bibr" target="#b6">Fan et al. 2018;</ref><ref type="bibr" target="#b38">Zhai et al. 2020</ref>) treat each cluster as a pseudo identity class, neglecting the intra-ID variance caused by the change of pose, illumination, and camera views. When observing the distribution of features extracted by an ImageNet <ref type="bibr" target="#b12">(Krizhevsky, Sutskever, and Hinton 2012</ref>)pretrained model from Market-1501 <ref type="bibr" target="#b39">(Zheng et al. 2015)</ref>, we notice that, among the images belonging to a same ID, those within cameras are prone to gather closer than the ones from different cameras. That is, one ID may present multiple subclusters, as demonstrated in <ref type="figure" target="#fig_0">Figure 1(b)</ref> and (c).</p><p>The above-mentioned phenomenon inspires us to propose a camera-aware proxy assisted learning method. Specifically, we split each single cluster, which is obtained by a camera-agnostic clustering method, into multiple cameraaware proxies. Each proxy represents the instances coming from the same camera. These camera-aware proxies can better capture local structures within IDs. More important, when treating each proxy as an intra-camera pseudo identity class, the variance and noise within a class are greatly reduced. Taking advantage of the proxy-based labels, we design an intra-camera contrastive learning  component to jointly tackle multiple camera-specific Re-ID tasks. When compared to the global Re-ID task, each camera-specific task deals with less number of IDs and smaller variance while using more reliable pseudo labels, and therefore is easier to learn. The intra-camera learning enables our Re-ID model to effectively learn discrimination ability within cameras. Besides, we also design an intercamera contrastive learning component, which exploits both positive and hard negative proxies across cameras to learn global discrimination ability. A proxy-balanced sampling strategy is also adopted to select appropriate samples within each mini-batch, facilitating the model learning further.</p><p>In contrast to previous clustering-based methods, the proposed approach distinguishes itself in the following aspects:</p><p>? Instead of using camera-agnostic clusters, we produce camera-aware proxies which can better capture local structure within IDs. They also enable us to deal with large intra-ID variance caused by different cameras, and generate more reliable pseudo labels for learning.</p><p>? With the assistance of the camera-aware proxies, we design both intra-and inter-camera contrastive learning components which effectively learn ID discrimination ability within and across cameras. We also propose a proxy-balanced sampling strategy to facilitate the model learning further.</p><p>? Extensive experiments on three large-scale datasets, including Market-1501 <ref type="bibr" target="#b39">(Zheng et al. 2015)</ref>, DukeMTMC-reID <ref type="bibr" target="#b40">(Zheng, Zheng, and Yang 2017)</ref>, and MSMT17 <ref type="bibr" target="#b31">(Wei et al. 2018b)</ref>, show that the proposed approach outperforms both purely unsupervised and UDA-based methods. Especially, on the challenging MSMT17 dataset, we gain 14.3% Rank-1 and 10.2% mAP improvements when compared to the second place.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised Person Re-ID</head><p>According to whether using external labeled datasets or not, unsupervised Re-ID methods can be grouped into purely unsupervised or UDA-based categories.</p><p>Purely unsupervised person Re-ID does not require any annotations and thus is more challenging. Existing methods mainly resort to pseudo labels for learning. Clustering <ref type="bibr" target="#b14">(Lin et al. 2019;</ref><ref type="bibr" target="#b37">Zeng et al. 2020</ref>), k-NN <ref type="bibr" target="#b13">(Li, Zhu, and Gong 2018;</ref><ref type="bibr" target="#b2">Chen, Zhu, and Gong 2018)</ref>, or graph <ref type="bibr" target="#b36">(Ye et al. 2017;</ref><ref type="bibr" target="#b32">Wu et al. 2019</ref>) based association techniques have been developed to generate pseudo labels. Most clustering-based methods like BUC <ref type="bibr" target="#b14">(Lin et al. 2019)</ref> and HCT <ref type="bibr" target="#b37">(Zeng et al. 2020</ref>) perform in a camera-agnostic way, which can maintain the similarity within IDs but may neglect the intra-ID variance caused by the change of camera views. Conversely, TAUDL <ref type="bibr" target="#b13">(Li, Zhu, and Gong 2018)</ref>, DAL <ref type="bibr" target="#b2">(Chen, Zhu, and Gong 2018)</ref>, and UGA  divide the Re-ID task into intra-and inter-camera learning stages, by which the discrimination ability learned from intra-camera can facilitate ID association across cameras. These methods generate intra-camera pseudo labels via a sparse sampling strategy, and they need a proper way for inter-camera ID association. In contrast to them, our cross-camera association is straightforward. Moreover, we propose distinct learning strategies in both intra-and inter-camera learning parts.</p><p>Unsupervised domain adaptation (UDA) based person Re-ID requires some source datasets that are fully annotated, but leaves the target dataset unlabeled. Most existing methods address this task by either transferring image styles <ref type="bibr" target="#b30">(Wei et al. 2018a;</ref><ref type="bibr" target="#b3">Deng et al. 2018a;</ref><ref type="bibr" target="#b16">Liu et al. 2019)</ref> or reducing distribution discrepancy <ref type="bibr" target="#b20">(Qi et al. 2019b;</ref><ref type="bibr" target="#b32">Wu, Zheng, and Lai 2019)</ref> across domains. These methods focus more on transferring knowledge from source to target domain, leaving the unlabeled target datasets underexploited. To sufficiently exploit unlabeled data, clustering <ref type="bibr" target="#b6">(Fan et al. 2018;</ref><ref type="bibr" target="#b38">Zhai et al. 2020;</ref> or k-NN <ref type="bibr" target="#b42">(Zhong et al. 2019)</ref> based methods have also been developed, analogous to those introduced in the purely unsupervised task. Differently, these methods either take into account both original and transferred data <ref type="bibr" target="#b6">(Fan et al. 2018;</ref><ref type="bibr" target="#b42">Zhong et al. 2019;</ref>, or integrate a clustering procedure together with an adversarial learning step <ref type="bibr" target="#b38">(Zhai et al. 2020</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intra-Camera Supervised Person Re-ID</head><p>Intra-camera supervision (ICS) <ref type="bibr" target="#b44">(Zhu et al. 2019;</ref><ref type="bibr" target="#b19">Qi et al. 2020</ref>) is a new setting proposed in recent years. It assumes that IDs are independently labeled within each camera view and no inter-camera ID association is annotated. Therefore, how to effectively perform the supervised intra-camera learning and the unsupervised inter-camera learning are two key problems. To address these problems, various methods such as PCSL <ref type="bibr" target="#b19">(Qi et al. 2020)</ref>, ACAN <ref type="bibr" target="#b18">(Qi et al. 2019a</ref>), MTML <ref type="bibr" target="#b44">(Zhu et al. 2019)</ref>, MATE , and Precise-ICS <ref type="bibr" target="#b28">(Wang et al. 2021</ref>) have been developed. Most of these methods pay much attention to the association of IDs across cameras. When taking camera-aware proxies as pseudo labels, our work shares a similar scenario in the intra-</p><formula xml:id="formula_0">? ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proxy-level memory bank</head><p>Clustering step <ref type="figure">Figure 2</ref>: An overview framework of the proposed method. It iteratively alternates between a clustering step and a model updating step. In the clustering step, a global clustering is first performed and then each cluster is split into multiple cameraaware proxies to generate pseudo labels. In the model updating step, intra-and inter-camera losses are designed based on a proxy-level memory bank to perform contrastive learning. camera learning with these ICS methods. Differently, our inter-camera association is straightforward due to the proxy generation scheme. We therefore focus more on the way to generate reliable proxies and conduct effective learning. Besides, the unsupervised Re-ID task tackled in our work is more challenging than the ICS problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Metric Learning with Proxies</head><p>Metric learning plays an important role in person Re-ID and other fine-grained recognition tasks. An extensively utilized loss for metric learning is the triplet loss <ref type="bibr" target="#b11">(Hermans, Beyer, and Leibe 2017)</ref>, which considers the distances of an anchor to a positive instance and a negative instance. Proxy-NCA (Movshovitz-Attias et al. 2017) proposes to use proxies for the measurement of similarity and dissimilarity. A proxy, which represents a set of instances, can capture more contextual information. Meanwhile, the use of proxies instead of data instances greatly reduces the triplet number. Both advantages help metric learning to gain better performance. Further, with the awareness of intra-class variances, Magnet <ref type="bibr" target="#b23">(Rippel et al. 2016)</ref>, MaPML <ref type="bibr" target="#b22">(Qian et al. 2018)</ref>, SoftTriple <ref type="bibr" target="#b21">(Qian et al. 2019)</ref> and and GEORGE <ref type="bibr" target="#b25">(Sohoni et al. 2020</ref>) adopt multiple proxies to represent a single cluster, by which local structures are better represented. Our work is inspired by these studies. However, in contrast to set a fixed number of proxies for each class or design a complex adaptive strategy, we split a cluster into a variant number of proxies simply according to the involved camera views, making our proxies more suitable for the Re-ID task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Clustering-based Re-ID Baseline</head><p>We first set up a baseline model for the unsupervised Re-ID task. As the common practice in the clustering-based methods <ref type="bibr" target="#b6">(Fan et al. 2018;</ref><ref type="bibr" target="#b14">Lin et al. 2019;</ref><ref type="bibr" target="#b37">Zeng et al. 2020)</ref>, our baseline learns a Re-ID model iteratively and, at each iteration, it alternates between a clustering step and a model up-dating step. In contrast to these existing methods <ref type="bibr" target="#b6">(Fan et al. 2018;</ref><ref type="bibr" target="#b14">Lin et al. 2019;</ref><ref type="bibr" target="#b37">Zeng et al. 2020)</ref>, we adopt a different strategy in the model updating step, making our baseline model more effective. The details are introduced as follows.</p><p>Given</p><formula xml:id="formula_1">an unlabeled dataset D = {x i } N i=1</formula><p>, where x i is the i-th image and N is the image number. We build our Re-ID model upon a deep neural network f ? with parameters ?. The parameters are initialized by an ImageNet (Krizhevsky, Sutskever, and Hinton 2012)-pretrained model. When image x is input, the network performs feature extraction and outputs feature f ? (x). Then, at each iteration, we adopt DB-SCAN <ref type="bibr" target="#b5">(Ester et al. 1996)</ref> to cluster the features of all images, and further select reliable clusters by leaving out isolated points. All images within each cluster are assigned with a same pseudo identity label. By this means, we get a labeled</p><formula xml:id="formula_2">dataset D = {(x i ,? i )} N i=1 , in which? i ? {1, ? ? ? , Y } is a generated pseudo label.</formula><p>N is the number of images contained in the selected clusters and Y is the cluster number.</p><p>Once pseudo labels are generated, we adopt a nonparametric classifier <ref type="bibr" target="#b35">(Wu et al. 2018</ref>) for model updating. It is implemented via an external memory bank and a nonparametric Softmax loss. More specifically, we construct a memory bank K ? R d?Y , where d is the feature dimension. During back-propagation when the model parameters are updated by gradient descent, the memory bank is updated by</p><formula xml:id="formula_3">K[j] ? ?K[j] + (1 ? ?)f ? (x i ), (1) where K[j]</formula><p>is the j-th entry of the memory, storing the updated feature centroid of class j. Moreover, x i is an image belonging to class j and ? ? [0, 1] is an updating rate.</p><p>Then, the non-parametric Softmax loss is defined by</p><formula xml:id="formula_4">L Base = ? N i=1 log exp(K[? i ] T f ? (x i )/? ) Y j=1 exp(K[j] T f ? (x i )/? ) ,<label>(2)</label></formula><p>where ? is a temperature factor. This loss achieves classification via pulling an instance close to the centroid of its class while pushing away from the centroids of all other classes. This non-parametric loss plays a key role in recent contrastive learning techniques <ref type="bibr" target="#b35">(Wu et al. 2018;</ref><ref type="bibr" target="#b42">Zhong et al. 2019;</ref><ref type="bibr" target="#b1">Chen et al. 2020;</ref><ref type="bibr" target="#b9">He et al. 2019)</ref>, demonstrating a powerful ability in unsupervised feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Camera-aware Proxy Assisted Method</head><p>Like previous clustering-based methods <ref type="bibr" target="#b6">(Fan et al. 2018;</ref><ref type="bibr" target="#b14">Lin et al. 2019;</ref><ref type="bibr" target="#b37">Zeng et al. 2020;</ref><ref type="bibr" target="#b38">Zhai et al. 2020)</ref>, the abovementioned baseline model conducts clustering in a cameraagnostic way. This clustering way may maintain the similarity within each identity class, but neglect the intra-ID variance. Considering that most severe intra-ID variance is caused by the change of camera views, we split each single class into multiple camera-specific proxies. Each proxy represents the instances coming from the same camera. The obtained camera-aware proxies not only capture the variance within classes, but also enable us to divide the model updating step into intra-and inter-camera learning parts. Such a divide-and-conquer strategy facilitates our model updating.</p><p>The entire framework is illustrated in <ref type="figure">Figure 2</ref>, in which the modified clustering step and the improved model updating step are alternatively iterated.</p><p>More specifically, at each iteration, we split the cameraagnostic clustering results into camera-aware proxies, and generate a new set of pseudo labels that are assigned in a per-camera manner. That is, the proxies within each camera view are independently labeled. It also means that two proxies split from the same cluster may be assigned with two different labels. We denote the newly labeled dataset of the c-th camera by</p><formula xml:id="formula_5">D c = {(x i ,? i ,z i , c i )} Nc i=1 .</formula><p>Here, image x i , which previously is annotated with a global pseudo label? i , is additionally annotated with an intra-camera pseudo label z i ? {1, ? ? ? , Z c } and a camera label c i = c ? {1, ? ? ? , C}. N c and Z c are, respectively, the number of images and proxies in camera c, and C is the number of cameras. Then, the entire labeled dataset is D = C c=1 D c . Consequently, we construct a proxy-level memory bank K ? R d?Z , where Z = C c=1 Z c is the total number of proxies in all cameras. Each entry of the memory stores a proxy, which is updated by the same strategy as introduced in Eq. (1) but considers only the images belonging to the proxy. Based on the memory bank, we design an intracamera contrastive learning loss L Intra that jointly learns per-camera non-parametric classifiers to gain discrimination ability within cameras. Meanwhile, we also design an intercamera contrastive learning loss L Inter , which considers both positive and hard negative proxies across cameras to boost the discrimination ability further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Intra-camera Contrastive Learning</head><p>With the per-camera pseudo labels, we can learn a classifier for each camera and jointly learn all the classifiers. This strategy has the following two advantages. First, the pseudo labels generated from the camera-aware proxies are more reliable than the global pseudo labels. It means that the model learning can suffer less from label noise and gain better intra-camera discrimination ability. Second, the feature extraction network shared in the joint learning is optimized to be discriminative in different cameras concurrently, which implicitly helps the Re-ID model to gain cross-camera discrimination ability. Therefore, we learn one non-parametric classifier for each camera and jointly learn classifiers for all cameras. To this end, we define the intra-camera contrastive learning loss as follows.</p><formula xml:id="formula_6">LIntra = ? C c=1</formula><p>1 Nc</p><formula xml:id="formula_7">x i ?Dc log exp(K [j] T f (xi)/? ) A+Zc i k=A+1 exp(K [k] T f (xi)/? ) .<label>(3)</label></formula><p>Here, given image x i , together with its per-camera pseudo labelz i and camera label c i , we set A = ci?1 c=1 Z c to be the total proxy number accumulated from the first to the c i ?1-th camera, and j = A +z i to be the index of the corresponding entry in the memory. 1 Nc is to balance the various number of images in different cameras.</p><p>This loss performs contrastive learning within cameras. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>(a), this loss pulls an instance close to the proxy to which it belongs and pushes it away from all other proxies in the same camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Inter-camera Contrastive Learning</head><p>Although the intra-camera learning introduced above provides our model with considerable discrimination ability, the model is still weak at cross-camera discrimination. Therefore, we propose an inter-camera contrastive learning loss, which explicitly exploits correlations across cameras to boost the discrimination ability. Specifically, given image x i , we retrieve all positive proxies from different cameras, which share the same global pseudo label? i . Besides, the K-nearest negative proxies in all cameras are taken as the hard negative proxies, which are crucial to deal with the similarity across identity classes. The inter-camera contrastive learning loss aims to pull an image close to all positive proxies while push away from the mined hard negative proxies, as demonstrated in <ref type="figure" target="#fig_1">Figure 3</ref>(b). To this end, we define the loss as follows.</p><formula xml:id="formula_8">LInter = ? N i=1 1 |P| p?P log S(p, xi) u?P S(u, xi) + q?Q S(q, xi) ,<label>(4)</label></formula><p>where P and Q denote the index sets of the positive and hard negative proxies, respectively. |P| is the cardinality of P. Moreover, S(p, x i ) = exp(K [p] T f (x i )/? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A Summary of the Algorithm</head><p>The proposed approach iteratively alternates between the camera-aware proxy clustering step and the intra-and intercamera learning step. The entire loss for model learning is</p><formula xml:id="formula_9">L = L Intra + ?L Inter ,<label>(5)</label></formula><p>where ? is a parameter to balance two terms. We summarize the whole procedure in Algorithm 1.</p><p>Algorithm 1 Camera-aware Proxy Assisted Learning Input: An unlabeled training set D, a DNN model f ? , the iteration number num iters, the training batches num batches, momentum ?, and temperature ? ; Output: Trained model f ? ; 1: for iter = 1 to num iters do A proxy-balanced sampling strategy. A mini-batch in Algorithm 1 involves an update to the Re-ID model using a small set of samples. It is not trivial to choose appropriate samples in each batch. Traditional random sampling strategy may be overwhelmed by identities having more images than the others. Class-balanced sampling, that randomly chooses P classes and K samples per class as in <ref type="bibr" target="#b11">(Hermans, Beyer, and Leibe 2017)</ref>, tends to sample an identity more frequently from image-rich cameras, causing ineffective learning for image-deficient cameras. To make samples more effective, we propose a proxy-balanced sampling strategy. In each mini-batch, we choose P proxies and K samples per proxy. This sampling strategy performs balanced optimization of all camera-aware proxies and enhances the learning of rare proxies, thus promoting the learning efficacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setting</head><p>Datasets and metrics. We evaluate the proposed method on three large-scale datasets: Market-1501 <ref type="bibr" target="#b39">(Zheng et al. 2015)</ref>, DukeMTMC-reID <ref type="bibr" target="#b40">(Zheng, Zheng, and Yang 2017)</ref>, and MSMT17 <ref type="bibr" target="#b31">(Wei et al. 2018b)</ref>.</p><p>Market-1501 <ref type="bibr" target="#b39">(Zheng et al. 2015)</ref> contains 32,668 images of 1,501 identities captured by 6 disjoint cameras. It is split into three sets. The training set has 12,936 images of 751 identities, the query set has 3,368 images of 750 identities, and the gallery set contains 19,732 images of 750 identities.</p><p>DukeMTMC-reID <ref type="bibr" target="#b40">(Zheng, Zheng, and Yang 2017</ref>) is a subset of DukeMTMC <ref type="bibr" target="#b24">(Ristani et al. 2016)</ref>. It contains 36,411 images of 1,812 identities captured by 8 cameras. Among them, 702 identities are used for training and the rest identities are for testing.</p><p>MSMT17 <ref type="bibr" target="#b31">(Wei et al. 2018b</ref>) is the largest and most challenging dataset. It has 126,411 images of 4,101 identities captured in 15 camera views, containing both indoor and outdoor scenarios. 32,621 images of 1041 identities are for training, the rest including 82,621 gallery images and 11,659 query images are for testing.</p><p>Performance is evaluated by the Cumulative Matching Characteristic (CMC) and mean Average Precision (mAP), as the common practice. For the CMC measurement, we report Rank-1, Rank-5, and Rank-10. Note that no postprocessing techniques like re-ranking <ref type="bibr" target="#b41">(Zhong, Zheng, and Li 2017)</ref> are used in our evaluation. Implementation details. We adopt an ImageNetpretrained ResNet-50 <ref type="bibr" target="#b10">(He et al. 2016)</ref> as the network backbone. Based upon it, we remove the fully-connected classification layer, and add a Batch Normalization (BN) layer after the Global Average Pooling (GAP) layer. The L 2 normalized feature is used for the updating of proxies in the memory during training, and also for the distance ranking during inference. The memory updating rate ? is empirically set to be 0.2, the temperature factor ? is 0.07, the number of hard negative proxies is 50, and the balancing factor ? in Eq. (5) is 0.5. At the beginning of each epoch (i.e. iteration), we compute Jaccard distance with k-reciprocal nearest neighbors <ref type="bibr" target="#b41">(Zhong, Zheng, and Li 2017)</ref> and use DBSCAN <ref type="bibr" target="#b5">(Ester et al. 1996)</ref> with a threshold of 0.5 for the camera-agnostic global clustering. During training, only the intra-camera loss is used in the first 5 epochs. In the remaining epochs, both the intra-and inter-camera losses work together.</p><p>We use ADAM as the optimizer. The initial learning rate is 0.00035 with a warmup scheme in the first 10 epochs, and is divided by 10 after each 20 epochs. The total epoch number is 50. Each training batch consists of 32 images randomly sampled from 8 proxies with 4 images per proxy. Random flipping, cropping and erasing are applied as data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies</head><p>In this subsection, we investigate the effectiveness of the proposed method by examining the intra-and inter-camera learning components, together with the proxy-balanced sampling strategy. For the purpose of reference, we first present the results of the baseline model introduced in section 3, as shown in <ref type="table" target="#tab_0">Table 1</ref>. Then, we examine six variants of the proposed camera-aware proxy (CAP) assisted model, which are referred to as CAP1-6.</p><p>Compared with the baseline model, the proposed full model (CAP6) significantly boosts the performance on all three datasets. The full model gains 11.7% Rank-1 and 16.3% mAP improvements on Market-1501, and 6.8% Camera-guided local prototype learning for UDA re-ID  Rank-1 and 9.8% mAP improvements on DukeMTMC-ReID. Moreover, it dramatically boosts the performance on MSMT17, achieving 33.4% Rank-1 and 23.2% mAP improvements over the baseline. The MSMT17 dataset is a lot more challenging than the other two datasets, containing complex scenarios and appearance variations. The superior performance on MSMT17 shows that our full model gains an outstanding ability to deal with severe intra-ID variance.</p><formula xml:id="formula_10">? #0 #1 ID #4 ID #5 ID #6 ID #7</formula><p>In the followings, we take a close look at each component. Effectiveness of the intra-camera learning. Compared with the baseline model, the intra-camera learning benefits from two aspects. 1) Each intra-camera Re-ID task is easier than the global counterpart because it deals with less number of IDs and smaller intra-ID variance. 2) Intra-camera learning suffers less from label noise since the per-camera pseudo labels are more reliable. These advantages enable the intra-camera learning to gain promising performance. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the CAP1 model which only employs the intra-camera loss, performs comparable to the baseline. When adopting the proxy-based sampling strategy, the CAP2 model outperforms the baseline on all datasets. In addition, we can also observe that the performance drops when removing the intra-camera loss from the full model (CAP4 vs. CAP6), validating the necessity of this component.</p><p>Effectiveness of the inter-camera learning. Complementary to the above-mentioned intra-camera learning, the inter-camera learning improves the Re-ID model by explicitly exploiting the correlations across cameras. It not only can deal with the intra-ID variance via pulling positive proxies together, but also can tackle the inter-ID similarity problem via pushing hard negative proxies away. With this component, both CAP5 and CAP6 significantly boost the performance over CAP1 and CAP2 respectively. In addition, we find out that the inter-camera loss alone (CAP3) is able to produce decent performance, and adding the intra-camera loss or sampling strategy boosts performance further.</p><p>Effectiveness of the proxy-balanced sampling strategy. The proxy-balanced sampling strategy is proposed to balance the various number of images contained in different proxies. To show that the proxy-balanced sampling strategy is indeed helpful, we compare it with the extensively used class-balanced strategy which ignores camera information. <ref type="table" target="#tab_0">Table 1</ref> shows that the models (CAP2, CAP4, and CAP6) using our sampling strategy are superior to the counterparts, validating the effectiveness of this strategy.</p><p>Visualization of learned feature representations. In order to investigate how each learning component behaves, we utilize t-SNE (van der Maaten and Hinton 2008) to visualize the feature representations learned by the baseline model, the intra-camera learned model CAP2, and the full model CAP6. <ref type="figure" target="#fig_3">Figure 4</ref> presents the image features of 10 IDs taken from MSMT17. From the figure we observe that the baseline <ref type="table">Table 2</ref>: Comparison with state-of-the-art methods. Both purely unsupervised and UDA-based methods are included. We also provide several fully supervised methods for reference. The first and second best results among all unsupervised methods are, respectively, marked in red and blue. ? indicates an UDA-based method working under the purely unsupervised setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with State-of-the-Arts</head><p>In this section, we compare the proposed method (named as CAP) with state-of-the-art methods. The comparison results are summarized in <ref type="table">Table 2</ref>. Comparison with purely unsupervised methods. Five most recent purely unsupervised methods are included for comparison, which are BUC <ref type="bibr" target="#b14">(Lin et al. 2019)</ref>, UGA , SSL <ref type="bibr" target="#b15">(Lin et al. 2020)</ref>, HCT <ref type="bibr" target="#b37">(Zeng et al. 2020)</ref>, and CycAs <ref type="bibr" target="#b29">(Wang et al. 2020b</ref>). Both BUC and HCT are clustering-based, sharing the same technique with ours. Additionally, we also compare with MMCL ? (Wang and Zhang 2020) and SpCL ? , two UDA-based methods working under the purely unsupervised setting. From the table, we observe that our proposed method outperforms all state-of-the-art counterparts by a great margin. For instance, compared with the second place method, our approach obtains 3.3% Rank-1 and 6.1% mAP gain on Market, 3.2% Rank-1 and 7.2% mAP gain on Duke, and 17.3% Rank-1 and 10.2% mAP gain on MSMT17.</p><p>Comparison with UDA-based methods. Recent unsupervised works focus more on UDA techniques that exploit external labeled data to boost the performance. <ref type="table">Table 2</ref> presents eight UDA methods. Surprisingly, without using any labeled information, our approach outperforms seven of them on both Market and Duke, and is on par with SpCL. On the challenging MSMT17 dataset, our approach surpasses all methods by a great margin, achieving 14.3% Rank-1 and 10.4% mAP gain when compared to SpCL.</p><p>Comparison with fully supervised methods. Finally, we provide two fully supervised method for reference, including one well-known method PCB (Sun et al. 2018) and one state-of-the-art method ABD-Net . We also report the performance of our network backbone trained with ground-truth labels, which indicates the upper bound of our approach. We observe that our unsupervised model (CAP) greatly mitigates the gap with PCB on all three datasets. Besides, there is still room for improvement if we could improve our backbone via integrating recent attentionbased techniques like ABD-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have presented a novel camera-aware proxy assisted learning method for the purely unsupervised person Re-ID task. Our method is able to deal with the large intra-ID variance resulted from the change of camera views, which is crucial for a Re-ID model to improve performance. With the assistance of camera-aware proxies, our proposed intra-and inter-camera learning components effectively improve ID-discrimination within and across cameras, as validated by the experiments on three large-scale datasets. Comparisons with both purely unsupervised and UDA-based methods demonstrate the superiority of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) T-SNE (van der Maaten and Hinton 2008) visualization of the feature distribution on Market-1501. The features are extracted by an ImageNet-pretrained model for images of 20 randomly selected IDs. The images from one camera are marked with the same colored bounding boxes. (b) and (c) display two sub-regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of intra-and inter-camera losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>camera-aware proxies, and generate per-camera pseudo labeled dataset D ;4:Construct a proxy-level memory bank K ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>T-SNE visualization of features extracted by the models of Baseline, CAP2, and CAP6, respectively shown from left to right in the upper row. Typical examples of IDs #4-7 are shown at bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the proposed method and its variants. L Intra refers to the intra-camera learning, L Inter is the intercamera learning, and PBsampling is the proxy-balanced sampling strategy. When PBsampling is not selected, the model uses the class-balanced sampling strategy.</figDesc><table><row><cell>Models</cell><cell>Components L Intra L Inter PBsampling R1</cell><cell>Market-1501 R5 R10 mAP R1</cell><cell>DukeMTMC-ReID R5 R10 mAP R1</cell><cell>MSMT17 R5 R10 mAP</cell></row><row><cell>Baseline</cell><cell cols="4">79.7 88.3 91.2 62.9 74.3 82.7 86.0 57.5 34.0 43.7 49.0 13.7</cell></row><row><cell>CAP1</cell><cell cols="4">78.7 89.3 92.9 58.9 74.0 83.7 86.6 57.0 48.6 61.7 67.1 23.0</cell></row><row><cell>CAP2</cell><cell cols="4">82.3 91.7 94.1 64.6 76.5 86.4 89.8 60.9 51.3 64.0 69.4 24.8</cell></row><row><cell>CAP3</cell><cell cols="4">89.8 95.4 97.1 75.1 76.7 84.8 86.8 59.9 66.3 76.5 80.0 34.0</cell></row><row><cell>CAP4</cell><cell cols="4">91.1 96.3 97.4 79.9 78.0 85.6 87.9 61.6 66.9 77.4 80.7 35.3</cell></row><row><cell>CAP5</cell><cell cols="4">89.5 94.9 96.4 75.9 79.1 87.8 89.9 64.5 66.7 76.9 80.5 35.1</cell></row><row><cell>CAP6</cell><cell cols="4">91.4 96.3 97.7 79.2 81.1 89.3 91.8 67.3 67.4 78.0 81.4 36.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>#0 and #1, #4 and #5, #6 and #7. In contrast, the CAP2 model, which conducts the intracamera learning only, separates #4 and #5, #8 and #9 better. With the additional inter-camera learning component, the full model can distinguish most of the IDs, by greatly improving the intra-ID compactness and inter-ID separability. But it may still fail in some tough cases such as #6 and #7.</figDesc><table><row><cell>Methods</cell><cell>Reference</cell><cell>R1</cell><cell cols="4">Market-1501 R5 R10 mAP R1</cell><cell cols="5">DukeMTMC-ReID R5 R10 mAP R1</cell><cell>MSMT17 R5 R10 mAP</cell></row><row><cell>Purely Unsupervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BUC (Lin et al. 2019)</cell><cell>AAAI19</cell><cell cols="9">66.2 79.6 84.5 38.3 47.4 62.6 68.4 27.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UGA (Wu et al. 2019)</cell><cell>ICCV19</cell><cell>87.2</cell><cell>-</cell><cell>-</cell><cell cols="3">70.3 75.0</cell><cell>-</cell><cell>-</cell><cell cols="2">53.3 49.5</cell><cell>-</cell><cell>-</cell><cell>21.7</cell></row><row><cell>SSL (Lin et al. 2020)</cell><cell>CVPR20</cell><cell cols="9">71.7 83.8 87.4 37.8 52.5 63.5 68.9 28.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMCL  ? (Wang and Zhang 2020)</cell><cell>CVPR20</cell><cell cols="11">80.3 89.4 92.3 45.5 65.2 75.9 80.0 40.2 35.4 44.8 49.8 11.2</cell></row><row><cell>HCT (Zeng et al. 2020)</cell><cell>CVPR20</cell><cell cols="9">80.0 91.6 95.2 56.4 69.6 83.4 87.4 50.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CycAs (Wang et al. 2020b)</cell><cell>ECCV20</cell><cell>84.8</cell><cell>-</cell><cell>-</cell><cell cols="3">64.8 77.9</cell><cell>-</cell><cell>-</cell><cell cols="2">60.1 50.1</cell><cell>-</cell><cell>-</cell><cell>26.7</cell></row><row><cell>SpCL  ? (Ge et al. 2020)</cell><cell cols="5">NeurIPS20 88.1 95.1 97.0 73.1</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">42.3 55.6 61.2 19.1</cell></row><row><cell>CAP</cell><cell cols="12">This paper 91.4 96.3 97.7 79.2 81.1 89.3 91.8 67.3 67.4 78.0 81.4 36.9</cell></row><row><cell>Unsupervised Domain Adaptation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PUL (Fan et al. 2018)</cell><cell cols="10">TOMM18 45.5 60.7 66.7 20.5 30.0 43.4 48.5 16.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SPGAN (Deng et al. 2018b)</cell><cell>CVPR18</cell><cell cols="9">51.5 70.1 76.8 22.8 41.1 56.6 63.0 22.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ECN (Zhong et al. 2019)</cell><cell>CVPR19</cell><cell cols="11">75.1 87.6 91.6 43.0 63.3 75.8 80.4 40.4 30.2 41.5 46.8 10.2</cell></row><row><cell>pMR (Wang et al. 2020a)</cell><cell>CVPR20</cell><cell cols="9">83.0 91.8 94.1 59.8 74.5 85.3 88.7 55.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMCL (Wang and Zhang 2020)</cell><cell>CVPR20</cell><cell cols="11">84.4 92.8 95.0 60.4 72.4 82.9 85.0 51.4 43.6 54.3 58.9 16.2</cell></row><row><cell>AD-Cluster (Zhai et al. 2020)</cell><cell>CVPR20</cell><cell cols="9">86.7 94.4 96.5 68.3 72.6 82.5 85.5 54.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMT (Ge, Chen, and Li 2020)</cell><cell>ICLR20</cell><cell cols="11">87.7 94.9 96.9 71.2 78.0 88.8 92.5 65.1 50.1 63.9 69.8 23.3</cell></row><row><cell>SpCL (Ge et al. 2020)</cell><cell cols="12">NeurIPS20 90.3 96.2 97.7 76.7 82.9 90.1 92.5 68.8 53.1 65.8 70.5 26.5</cell></row><row><cell>Fully Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCB (Sun et al. 2018)</cell><cell>ECCV18</cell><cell>93.8</cell><cell>-</cell><cell>-</cell><cell cols="3">81.6 83.3</cell><cell>-</cell><cell>-</cell><cell cols="2">69.2 68.2</cell><cell>-</cell><cell>-</cell><cell>40.4</cell></row><row><cell>ABD-Net (Chen et al. 2019)</cell><cell>ICCV19</cell><cell>95.6</cell><cell>-</cell><cell>-</cell><cell cols="3">88.3 89.0</cell><cell>-</cell><cell>-</cell><cell cols="3">78.6 82.3 90.6</cell><cell>-</cell><cell>60.8</cell></row><row><cell>CAP's Upper Bound</cell><cell cols="12">This paper 93.3 97.5 98.4 85.1 87.7 93.7 95.4 76.0 77.1 87.4 90.8 53.7</cell></row><row><cell>model fails to distinguish</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ABD-Net: Attentive but Diverse Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep association learning for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved selfsimilarity and domain-dissimilarity for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Kdd</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised person re-identification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACM TOMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Selfpaced Contrastive Learning with Hybrid Memory for Domain Adaptive Object Re-ID</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<title level="m">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Bottom-Up Clustering Approach to Unsupervised Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification via softened similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive Transfer Network for Cross-Domain Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">No Fuss Distance Metric Learning using Proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adversarial Camera Alignment Network for Unsupervised Cross-camera Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Progressive Cross-camera Soft-label Learning for Semi-supervised Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE TCSVT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Largescale Distance Metric Learning with Uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Metric Learning with Adaptive Density Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multitarget, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sohoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neurips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV. van der Maaten</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>No Subclass Left Behind: Fine-Grained Robustness in Coarse-Grained Classification Problems. and Hinton, G. 2008. Visualizing Data using t-SNE. JMLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised Person Reidentification via Multi-label Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Smoothing Adversarial Domain Attack and P-Memory Reconsolidation for Cross-Domain Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards Precise Intra-camera Supervised Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CycAs: Self-supervised Cycle Association for Learning Re-identifiable Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by camera-aware similarity consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised Graph Association for Person Reidentification</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dynamic Label Graph Matching for Unsupervised Video Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical Clustering With Hard-Batch Triplet Loss for Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable Person Re-identification: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Re-ranking Person Re-identification with k-Reciprocal Encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Intra-Camera Supervised Person Re-Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05046</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Intra-Camera Supervised Person Re-Identification: A New Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
