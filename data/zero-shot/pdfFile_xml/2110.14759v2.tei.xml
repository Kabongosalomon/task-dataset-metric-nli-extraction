<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regularized Frank-Wolfe for Dense CRFs: Generalizing Mean Field and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09">Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Khu?</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes, Inria</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>LJK 38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?-Huu</forename><surname>Karteek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes, Inria</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>LJK 38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alahari</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes, Inria</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Grenoble INP</orgName>
								<address>
									<postCode>LJK 38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Regularized Frank-Wolfe for Dense CRFs: Generalizing Mean Field and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09">Sep 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce regularized Frank-Wolfe, a general and effective algorithm for inference and learning of dense conditional random fields (CRFs). The algorithm optimizes a nonconvex continuous relaxation of the CRF inference problem using vanilla Frank-Wolfe with approximate updates, which are equivalent to minimizing a regularized energy function. Our proposed method is a generalization of existing algorithms such as mean field or concave-convex procedure. This perspective not only offers a unified analysis of these algorithms, but also allows an easy way of exploring different variants that potentially yield better performance. We illustrate this in our empirical results on standard semantic segmentation datasets, where several instantiations of our regularized Frank-Wolfe outperform mean field inference, both as a standalone component and as an end-to-end trainable layer in a neural network. We also show that dense CRFs, coupled with our new algorithms, produce significant improvements over strong CNN baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fully-connected or dense conditional random fields (CRFs) <ref type="bibr" target="#b33">[34]</ref>-combined with strong pixel-level classifiers such as a convolutional neural network (CNN) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b42">42]</ref>-have been a highly-successful paradigm for semantic segmentation. Top-performing systems on the PASCAL VOC benchmark <ref type="bibr" target="#b21">[22]</ref> used to include a CRF as either a post-processing step <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b45">45]</ref> or a trainable component <ref type="bibr">[4,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr" target="#b73">73,</ref><ref type="bibr" target="#b76">76]</ref>. However, as CNNs got stronger, the improvements that CRFs brought decreased over time, and as a result they fell out of favor since 2017 <ref type="bibr" target="#b45">[45]</ref>.</p><p>In this paper, we revisit dense CRFs with two contributions. First, on the theoretical side, we propose regularized Frank-Wolfe, a new class of algorithms for inference and learning of CRFs that perform better than the popular mean field <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b58">58]</ref>-the method of choice in the aforementioned works. Regularized Frank-Wolfe optimizes a nonconvex continuous relaxation of the CRF inference problem ( ?2) by performing approximate conditional-gradient updates ( ?3.1), which is equivalent to minimizing a regularized energy using the generalized Frank-Wolfe method <ref type="bibr" target="#b53">[53]</ref> ( ?3.2). Several of its instantiations lead to new algorithms that have not been studied before in the MAP inference literature ( ?3.3). Moreover, we show that it also includes several existing methods, including mean field and the concave-convex procedure <ref type="bibr" target="#b75">[75]</ref>, as special cases ( ?3.4). This generalized perspective allows a unified analysis of all these old and new algorithms in a single framework ( ?4). In particular, we show that they achieve a sublinear rate of convergence O(1/ ? k) for suitable stepsize schemes, and in certain cases (such as strongly-convex regularizer or concave energy) this can be improved to O(1/k) ( ?4.1). Furthermore, we provide a tightness analysis for the resulting nonconvex relaxation of the regularized energy, recovering some existing tightness results <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b61">61]</ref> as special cases ( ?4.2). The proposed algorithms are easy to implement, converge quickly in practice, and have (sub)differentiable iterates. Such properties are important for successful gradient-based learning via backpropagation <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b62">62]</ref>.</p><p>Our second contribution lies on the practical side. In addition to mean field and regularized Frank-Wolfe variants, we re-implement several existing first-order inference methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b48">48]</ref>-those that are amenable to gradient-based learning-for comparison. Remarkably, we find that dense CRFs can still achieve important improvements over the strong DeepLabv3+ <ref type="bibr" target="#b16">[17]</ref> CNN model for all these solvers ( ?5). In particular, our best variant of regularized Frank-Wolfe achieves a mean intersectionover-union (mIoU) score of 88.0 on the PASCAL VOC test set ( ?5.3), improving over DeepLabv3+. We hope that these encouraging results could attract interest from the community in considering dense CRFs (again) for tasks such as semantic segmentation. Our source code is made publicly available under the GNU general public license for this purpose. <ref type="bibr" target="#b0">1</ref> 2 Background</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inference in CRFs</head><p>Let s ? S 1 ? ? ? ? ? S n denote an assignment to n discrete random variables S 1 , . . . , S n , where each variable S i takes values in a finite set of states (or labels) S i . Let G = (V, E) be a graph of n nodes (V = {1, 2, . . . , n}). A Markov random field (MRF) defined by G encodes a family of joint distributions that can be factorized as follows, where ? i : S i ? R + and ? ij : S i ? S j ? R + are the so-called unary and pairwise (respectively) potential functions, and Z is a normalization factor:</p><formula xml:id="formula_0">p(s) = 1 Z i?V ? i (s i ) ij?E ? ij (s i , s j ).<label>(1)</label></formula><p>Note that <ref type="bibr" target="#b0">(1)</ref> can also include conditional distributions, i.e., p(s | o) with observed variables o. In this case the potentials may also depend on o, e.g., ? i (s i ; o), and this model is referred to as conditional random field (CRF) <ref type="bibr" target="#b36">[37]</ref>. We will present later ( ?5.1) such a model for image segmentation. In the following, we use MRF and CRF interchangeably. We assume further that all nodes have the same set of labels: S i = S ?i, with cardinality d = |S|. It is convenient to express p(s) as 1 Z exp(?e(s)), where the so-called energy e(s) is defined as e(s) = i?V ? i (s i ) + ij?E ? ij (s i , s j ), with ? i (s i ) = ? log ? i (s i ) (idem for ? ij ).</p><p>(</p><p>The task of maximum a posteriori (MAP) inference consists in finding the most probable joint assignment, also known as energy minimization (which is NP-Hard in general <ref type="bibr" target="#b65">[65]</ref>):</p><p>s * = argmax s?S n p(s) = argmin s?S n e(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Continuous relaxation of MAP inference</head><p>Let x is be a binary variable such that x is = 1 iff label s is assigned to node i. Then, x i = (x is ) s?S ? {0, 1} d denotes the one-hot vector for node i. Let x ? {0, 1} nd be the concatenation of all x i , and let ? ? ? i = (? i (s)) s?S ? R d , ? ? ? ij = (? ij (s, t)) s?S t?S ? R d?d . The energy <ref type="bibr">(2)</ref> is then</p><formula xml:id="formula_3">E(x; ? ? ?) = i?V ? ? ? ? i x i + ij?E x ? i ? ? ? ij x j ,<label>(4)</label></formula><p>where ? ? ? is a parameter vector composed of all ? ? ? i and ? ? ? ij . The MAP inference problem (3) transforms to minimizing E(x; ? ? ?) over the new variables x. A natural approach is to relax the binary constraint and solve the continuous relaxation min x?X E(x; ? ? ?) with</p><formula xml:id="formula_4">X = x ? R nd : x ? 0, 1 ? x i = 1 ?i ? V .<label>(5)</label></formula><p>This continuous relaxation is known to be tight <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b61">61]</ref>. For convenience, we represent the unary potentials as a vector u(? ? ?) = (? ? ? i ) i?V ? R nd , and the pairwise potentials as a symmetric n ? n block matrix P(? ? ?) ? R nd?nd , where the (i, j) block is ? ? ? ij . The problem is reduced to min x?X E(x; ? ? ?)</p><formula xml:id="formula_5">1 2 x ? P(? ? ?)x + u(? ? ?) ? x.<label>(6)</label></formula><p>The reason we have made ? ? ? explicit in <ref type="bibr">(4)</ref> and <ref type="formula" target="#formula_5">(6)</ref> is to provide more clarity when discussing the differentiability of their solutions for the learning task. Note that an optimal solution x * to <ref type="bibr">(4)</ref> and <ref type="formula" target="#formula_5">(6)</ref> is a function of ? ? ?, and thus should be written as x * (? ? ?). Therefore, when we say a solution is differentiable, it is understood that it is so with respect to ? ? ?. When there is no ambiguity, we omit ? ? ? and write simply E(x), P, and u. We will be interested in problem <ref type="bibr" target="#b5">(6)</ref> in the rest of the paper, though it should be noted that our method also applies to the so-called linear programming (LP) relaxation, which takes the form min x?XLP E LP (x; ? ? ?), where E LP (x; ? ? ?) = ? ? ? ? x and X LP is the so-called local polytope <ref type="bibr" target="#b72">[72]</ref>. We refer to Appendix A for the details.</p><p>Consider the following problem, where f : R m ? R?{+?} is differentiable but possibly nonconvex, and g : R m ? R ? {+?} is proper, closed, and convex but possibly non-differentiable:</p><formula xml:id="formula_6">min x F (x) f (x) + g(x).<label>(9)</label></formula><p>Generalized Frank-Wolfe solves (9) by iterating p k ? argmin p ?f (x k ), p + g(p) ,</p><formula xml:id="formula_7">x k+1 = x k + ? k (p k ? x k ).<label>(10)</label></formula><p>If g is the indicator function ? X of X (i.e., ? X (x) = 0 if x ? X and ? X (x) = +? otherwise), then the algorithm clearly reduces to vanilla Frank-Wolfe <ref type="bibr" target="#b6">(7)</ref> for min x?X f (x). Therefore, generalized Frank-Wolfe applied to <ref type="bibr" target="#b5">(6)</ref> with f = E and g = ? X will yield exactly the same updates <ref type="bibr" target="#b6">(7)</ref>. Now let us apply this algorithm to an approximate objective E r (x) = E(x) + r(x) for some function r. Choosing f = E and g = r + ? X , it is straightforward that (10) reduces to <ref type="bibr" target="#b7">(8)</ref>. Therefore, we have recovered the same algorithm as in ?3.1, but this time through different machinery.</p><p>This framework offers a great flexibility as one can choose f and g in many different ways to obtain new algorithms. The only conditions are f being differentiable and g being convex, so that the subproblem in <ref type="formula" target="#formula_0">(10)</ref> is well-defined and globally solvable. <ref type="bibr">3</ref> For example, instead of choosing f = E and g = r+? X as above, one can choose f (x) = 1 2 x ? Px and g(x) = u ? x + r(x) + ? X (x). We will recover later in ?3.4 some existing algorithms (as special cases) through this kind of decomposition. Finally, we present Algorithm 1 for (approximately) solving MAP inference <ref type="bibr" target="#b5">(6)</ref>. 1: Choose a regularizer r such that there exist f (differentiable) and g (convex) satisfying f + g = E + r + ? X . Typically (but not necessarily) r is convex on X and is constant on</p><formula xml:id="formula_8">X ? {0, 1} nd . 2: Initialization: k ? 0, x 0 ? X , number of iterations N . 3: Compute p k ? argmin p ?f (x k ), p + g(p) and compute the stepsize ? k . 4: Update x k+1 = x k + ? k (p k ? x k )</formula><p>. Let k ? k + 1 and go to Step 3 until k = N . 5: Rounding: convert x to a discrete solution and return.</p><p>While the choice of (r, f, g) can be highly flexible, it would make little sense to optimize a function that has nothing to do with the original objective (i.e., the discrete energy). Let X = X ? {0, 1} nd denote the discrete domain of our problem. If we choose r such that it is constant on X (as suggested in Step 1 above), then minimizing E on X is equivalent to minimizing E + r on X , and thus Algorithm 1 actually solves the continuous relaxation of a (different) discrete problem that is equivalent to MAP inference. Further discussion on this matter, as well as on the rounding Step 5, are deferred until ?4.2.</p><p>Finally, we should note that adding a strongly-convex regularizer is not new in the MAP inference literature <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b69">69]</ref>. In particular, some previous work even applied (vanilla) Frank-Wolfe to optimizing such regularized energy <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b67">67,</ref><ref type="bibr" target="#b69">69]</ref>. All these algorithms, however, suffer from the aforementioned zero-gradient issue, as already explained in the beginning of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Particular instantiations</head><p>The previous section presents regularized Frank-Wolfe as a general algorithm for inference. We now discuss concrete examples of its instantiations. To the best of our knowledge, all the algorithms presented in this section are new and have not been studied previously in the MAP inference literature. In particular, despite some similarities with proximal gradient <ref type="bibr" target="#b48">[48]</ref> and mirror descent <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b55">55]</ref>, our following euclidean and entropic variants are actually different from these methods. <ref type="bibr">4</ref> Euclidean Frank-Wolfe Perhaps the most natural choice is ? 2 regularization. In Algorithm 1, let us choose f (x) = E(x) and r(x) = ? 2 x 2 2 , where ? &gt; 0 is a regularization weight. Let ? X (v) be the projection of a vector v onto X . It can be shown ( ?D.1) that Step 3 in Algorithm 1 becomes</p><formula xml:id="formula_9">p k = argmin p?X Px k + u, p + ? 2 p 2 2 = ? X ? 1 ? (Px k + u) ?k ? 0.<label>(11)</label></formula><p>Entropic Frank-Wolfe In Algorithm 1, let us choose f (x) = E(x) and r(x) = ??H(x), where ? &gt; 0 is a regularization weight and H(x) = ? i?V s?S x is log x is is the entropy of x over X . It can be shown ( ?D.2) that Step 3 in Algorithm 1 becomes</p><formula xml:id="formula_10">p k = argmin p?X Px k + u, p ? ?H(p) = softmax ? 1 ? (Px k + u) ?k ? 0,<label>(12)</label></formula><p>where</p><formula xml:id="formula_11">v = softmax(x) with x ? R nd means v ? R nd and v is = exp(xis) t?S exp(xit) ?i ? V, ?s ? S.</formula><p>The resulting algorithm has a tight connection with (parallel) mean field <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> (discussed in ?3.4).</p><p>Other variants One can consider more sophisticated regularizers, e.g., a weighted combination of ? 2 norm and entropy. Other options include the many different regularizers that have been used in diverse machine learning applications, such as ? p norm <ref type="bibr" target="#b57">[57]</ref>, lasso variants <ref type="bibr" target="#b57">[57]</ref>, or binary entropy <ref type="bibr">[3]</ref>. Although these variants also lead to new MAP inference algorithms, their implementations are more sophisticated since their subproblems (10) require numerical solutions as no closed form ones exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Recovering existing algorithms as special cases</head><p>In addition to the above new algorithms, regularized Frank-Wolfe also includes several existing ones as special cases. We present some of them below and refer to Appendix A for further details.</p><p>Mean field This is a special case of the above Entropic Frank-Wolfe. Indeed, if we choose ? = 1 in (12) and a constant stepsize ? k = 1 ?k ? 0 in Algorithm 1, then it is straightforward that this algorithm is reduced to the following update step, where N i is the set of neighbors of node i:</p><formula xml:id="formula_12">x k+1 = softmax(?Px k ? u) ?? x k+1 is = 1 Zi exp ? ? i (s) ? j?Ni t?S ? ij (s, t)x k jt ?i ? V, s ? S.</formula><p>This is precisely a (parallel) mean field update <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. To conclude, parallel mean field is an instance of Entropic Frank-Wolfe with unit regularization weight and unit stepsize. Interestingly, the update <ref type="formula" target="#formula_0">(12)</ref> is the well-known softmax function with temperature in the deep learning literature <ref type="bibr" target="#b27">[28]</ref>. One could have easily come up with such a simple extension of mean field by adding a temperature to softmax (yet surprisingly this has not been tried before), but here we have provided a principled way to achieve that. As shown later in the experiments, with suitable ?, this extension yields much better results than vanilla mean field. Finally, we should note that the tight connection between mean field and first-order methods has been noticed before. Kr?henb?hl and Koltun <ref type="bibr" target="#b34">[35]</ref> proposed several mean-fieldtype variants based on the concave-convex procedure <ref type="bibr" target="#b75">[75]</ref>, while closely similar variants can also be obtained through proximal gradient <ref type="bibr">[2,</ref><ref type="bibr" target="#b5">6]</ref>, but unlike our generalized algorithm, these algorithms cannot recover exactly the original mean field of Kr?henb?hl and Koltun <ref type="bibr" target="#b33">[34]</ref>.</p><p>Concave-convex procedure CCCP <ref type="bibr" target="#b75">[75]</ref> solves <ref type="bibr" target="#b8">(9)</ref>, assuming f is concave and g is convex, by updating x k+1 as a solution to ??f (x k ) ? ?g(x k+1 ), 5 which is precisely (10) with stepsize ? k = 1.</p><p>We conclude that CCCP is a special case of generalized Frank-Wolfe with f concave and unit stepsize. As a result, many existing CCCP-based inference algorithms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref> can be seen as special cases of regularized Frank-Wolfe. For example, the ones presented by Desmaison et al. <ref type="bibr" target="#b20">[21]</ref> are instantiations of the proposed algorithm with either</p><formula xml:id="formula_13">f (x) = ?x ? diag(c)x and r(x) = E(x) + x ? diag(c)x (where c ? R nd is large enough so that r(x) is convex), or f (x) = x ? (P ? C)x and r(x) = u ? x + x ? Cx (where C</formula><p>is some matrix such that f is concave and r is convex). Note that in these instantiations,</p><p>Step 3 in Algorithm 1 requires an iterative (numerical) solution. Finally, all the algorithms presented by Kr?henb?hl and Koltun <ref type="bibr" target="#b34">[35]</ref> are also instantiations of the proposed method because they are based on CCCP. We refer to Appendix A for further details.</p><p>Vanilla Frank-Wolfe This is trivially a special case of regularized Frank-Wolfe and we briefly discuss it for completeness. Choosing f (x) = E(x) and r(x) = 0 we obtain the algorithm by L?-Huu and Paragios <ref type="bibr" target="#b41">[41]</ref>. Likewise, the one by Desmaison et al. <ref type="bibr" target="#b20">[21]</ref> corresponds to f (x) = E(x) ? c ? x + x ? diag(c)x and r(x) = 0, where c ? R nd is large enough for f to be convex. In addition, we can also recover existing LP-based algorithms by choosing X = X LP , r(x) = 0, and f (x) = E LP (x) + R(x) with suitable R(x). Indeed, the one by Meshi et al. <ref type="bibr" target="#b52">[52]</ref> takes R(x) as the squared ? 2 -norm of linear constraints, while the ones by Sontag and Jaakkola <ref type="bibr" target="#b67">[67]</ref> and Tang et al. <ref type="bibr" target="#b69">[69]</ref> correspond to R(x) being an entropy approximation and its generalization, respectively (see ?A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical analysis 4.1 Convergence</head><p>We provide a convergence analysis for the generalized Frank-Wolfe algorithm, and the results for CRF inference special cases will then follow as a consequence. Convergence of vanilla Frank-Wolfe has been well studied in the literature <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>. For generalized Frank-Wolfe, different analyses exist for the case where both f and g in (9) are convex <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b74">74]</ref>. We are particularly interested in the general case where f is nonconvex, <ref type="bibr" target="#b5">6</ref> as the CRF energy is often highly so in practice. Mine and Fukushima <ref type="bibr" target="#b53">[53]</ref> (and subsequently Bredies et al. <ref type="bibr" target="#b11">[12]</ref>) proved the global convergence of the algorithm under mild conditions, though no rate of convergence was given. Recently, Beck <ref type="bibr" target="#b6">[7]</ref> obtained an O(1/ ? k) rate of convergence for convex g under adaptive or line-search stepsizes. We extend their analysis with several contributions. First, we include the case where g is strongly convex, which is important as our main variants for inference (e.g., mean field or ? 2 -Frank-Wolfe) use stronglyconvex regularizers. Second, to also include CCCP <ref type="bibr" target="#b75">[75]</ref> as a special case, we relax their Lipschitz smoothness assumption on f to semi-concavity (which is weaker, as any L-smooth function is also Lconcave). Third, we also consider much weaker stepsize schemes such as constant or non-summable ones. We show that for either concave f or strongly-convex g, a better O(1/k) rate of convergence can be achieved, even under the (weak) constant stepsize. It should be noted that our results are new.</p><p>All the results in this section are stated under the following assumptions, where L f and ? g are nonnegative constants and ? denotes the ? 2 norm. Their proofs are given in Appendix B.</p><formula xml:id="formula_14">Assumption 1. f is differentiable and L f -semi-concave (i.e., f (x) ? L f 2 x</formula><p>2 is concave) on dom f , which is assumed to be open and convex. When L f = 0, f is concave.</p><p>Assumption 2. g is proper, closed, and ? g -strongly-convex (i.e., g(x) ? ?g 2 x 2 is convex), and dom g ? dom f is compact. When ? g &gt; 0, g is strongly convex.</p><p>Let p x denote a solution of min p { ?f (x), p + g(p)} and let p k = p x k . The following quantity, called the conditional gradient norm <ref type="bibr" target="#b6">[7]</ref>, will serve as an optimality measure:</p><formula xml:id="formula_15">S(x) = ?f (x), x ? p x + g(x) ? g(p x ).<label>(13)</label></formula><formula xml:id="formula_16">Lemma 1. S(x) ? ?g 2 x ? p x 2</formula><p>?x ? dom f , and S(x) = 0 iff x is a stationary point of (9).</p><p>The following theorem contains our convergence results for the most common stepsize schemes, including the following adaptive and line-search stepsizes, respectively:</p><formula xml:id="formula_17">? k = min 1, 1 L f + ? g S(x k ) p k ? x k 2 + ? g 2 , ? k = argmin ??[0,1] F (x k + ?(p k ? x k )). (14)</formula><p>Theorem 1. Let F * be the minimum value of F , ? be the diameter of dom g, </p><formula xml:id="formula_18">? k = F (x k ) ? F * , ? = ?g L f +?g , ?(?) = ? min 1, 2 ? ? ? , ?(?) = 1 2 [(L f + ? g )? ? ? g ], and ? = 2L f ? 0 . For any k ? 0, we have min 0?i?k S(x i ) ? B k ,</formula><formula xml:id="formula_19">? k = ? &gt; 0 ?k ? k = ? p k ?x k ?k +? k=0 ? k = ? convex g ? 0 ?(k+1) + L f ? 2 ? 2 ? 0 ? ?(k+1) + L f ?? 2 ? 0 + L f ? 2 2 k i=0 ? 2 i k i=0 ? i max 2? 0 k+1 , ?? ? k+1 strongly convex g ? 0 ?(k+1) + ?(?)? 2 ???2? ? 0 ?(?)(k+1) ??&lt;2? ? 0 ? ? 2?g (k+1) + (L f +?g)? 2 ? 2?g 2 ? k(?) k i=k(?) ? i ? 0 ?(k+1) concave f ? 0 ?(k+1) ? 0 ? ?(k+1) ? 0 k i=0 ? i ? 0 k+1</formula><p>In the above, k(?) = min {k : ? i &lt; 2? ?i ? k}, with further assumption that lim k?? ? k = 0 for (jointly) non-concave f and non-summable ? k . For the non-highlighted cases, we have lim k?? S(x k ) = 0 and any limit point of the sequence (x k ) k?0 is a stationary point of <ref type="bibr" target="#b8">(9)</ref>.</p><p>The table in Theorem 1 also provides rates of convergence for the algorithm. Prior to our work, the O(1/ ? k) rate for the adaptive or line-search stepsizes (top-right cell of the table, due to Beck <ref type="bibr" target="#b6">[7]</ref>) was the best for nonconvex objectives. <ref type="bibr" target="#b6">7</ref> We have improved this rate to O(1/k) when f is concave or g is strongly convex, even under weaker stepsize schemes. In particular, convergence is guaranteed for all considered stepsize schemes when f is concave, for which the best bound is obtained when ? k = 1 ?k, which explains the default unit stepsize in CCCP <ref type="bibr" target="#b75">[75]</ref> (see ?3.4). Convergence is also guaranteed for the (diminishing) non-summable scheme (which includes common stepsizes such as ? k = 2/(k + 2) or ? k = 1/ ? k), but the rate depends on the rate of divergence of k i=0 ? i . More detailed results and analyses can be found in Appendix B.</p><p>Convergence for MAP inference For all the instantiations of regularized Frank-Wolfe presented in ?3.3 and ?3.4, it is easy to check that Assumptions 1 and 2 are satisfied. In addition, the regularizers in most of them (euclidean or entropic variants, including mean field) are strongly convex, thus we would expect a rate of convergence of at least O(1/k) in practice for these algorithms under the adaptive, line search, or (suitable) constant stepsizes. Note that the adaptive scheme requires to know L f and ? g , which is possible in our case: a lower bound on ? g is ? for both euclidean and entropic variants, while an upper bound on L f is P 2 for the energy <ref type="bibr" target="#b5">(6)</ref>. In practice, however, these bounds could be too loose to yield good convergence.</p><p>Convergent mean field It is well-known that parallel mean field may diverge <ref type="bibr" target="#b34">[35]</ref>. Our Entropic Frank-Wolfe can be viewed as an improved variant of mean field that is globally convergent for different stepsize schemes, without resorting to a concave approximation as done by Kr?henb?hl and Koltun <ref type="bibr" target="#b34">[35]</ref>. Our above analysis also provides an explanation for a known phenomenon <ref type="bibr" target="#b5">[6]</ref>: damped mean field (corresponding to Entropic Frank-Wolfe with ? = 1 and ? k = ? &lt; 1 ?k) is more likely (than mean field) to guarantee convergence when the energy is not concave.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tightness of the relaxation</head><p>We have seen that regularized Frank-Wolfe (Algorithm 1) minimizes a modified continuous energy. It is thus reasonable to ask whether doing so also minimizes the original discrete energy (which is the main objective). In this section, we partially answer this question by providing some tightness guarantee for this regularized continuous relaxation. Our analysis is quite general and also includes several existing tightness results <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b61">61]</ref> as special cases. All proofs can be found in Appendix C.2.</p><p>The last step in Algorithm 1 consists in converting x to a discrete solution. We consider two such rounding schemes. The simplest one is perhaps nearest rounding, which assigns each node i with the label s i ? argmax t?S x it . Intuitively, this sets x i to the nearest vertex of the simplex</p><formula xml:id="formula_20">x i ? R d + : 1 ? x i = 1 .</formula><p>The second scheme, called BCD rounding <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b61">61]</ref>, consists in minimizing E(x) over x i while keeping all x j (j = i) fixed (i.e., block coordinate descent), which amounts to iteratively assigning each node i with label s i ? argmin s?S ? i (s) + j?Ni t?S ? ij (s, t)x jt . In practice, we only use nearest rounding because BCD rounding is too expensive for dense graphs. However, an important property of the latter is that it does not increase the energy, which is useful for our theoretical analysis. The following theorem provides an additive bound on the energy.</p><formula xml:id="formula_21">Theorem 2. Let x * r be a global minimum of E r (x) = E(x) + r(x) over X ,x * r be the discrete solution rounded from x * r , and E * be the minimum discrete energy. Assume that r(x) is bounded: 8 m ? r(x) ? M ?x ? X . We have E * ? E(x * r ) ? E * + M ? m + C, where C = n 1 ? 1 d ( u 2 + ? n P 2 )</formula><p>for nearest rounding and C = 0 for BCD rounding.</p><p>Let us derive the energy BCD bound for some particular cases (see ?C.2 for details). Obviously with no regularization (r = 0), we have M = m = 0 and thus E(x * r ) ? E * ? E(x * r ), yielding E(x * r ) = E * , i.e., the relaxation is tight. We have thus recovered a previously known result <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b61">61]</ref>.</p><formula xml:id="formula_22">For r(x) = ?c ? x + x ? diag(c)</formula><p>x with c ? 0, we have M = 0 and m = ? 1 4 1 ? c, which recovers exactly the additive bound given by Ravikumar and Lafferty <ref type="bibr" target="#b61">[61]</ref> for the convex QP relaxation. For the ? 2 regularizer r(x) = ? 2 x 2 2 , we have M = ?n 2 and m = ?n 2d , thus we obtain a bound of ?n 2 1 ? 1 d . For the entropy regularizer r(x) = ??H(x), we have M = 0 and m = ??n log d, thus the bound is ?n log d, which is worse than the ? 2 bound for any d ? 5.</p><p>Note that the bound provided by Theorem 2 is achieved from a global minimum of the regularized relaxation. This can be attained in some cases, e.g., when the energy is submodular or when the (convex) regularizer is large enough to make the objective convex. In the general case, however, the algorithm is only guaranteed to reach a stationary point, and the (theoretical) quality of such point remains unknown. It would be interesting to investigate whether the algorithm can provide an approximation guarantee for some classes of energies (e.g., supermodular ones), similar to some existing algorithms <ref type="bibr" target="#b10">[11]</ref>. These open questions are left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We compare regularized Frank-Wolfe with existing methods on the semantic segmentation task, in terms of both inference and learning performance. Two variants, namely Euclidean Frank-Wolfe (? 2 FW) and Entropic Frank-Wolfe (eFW) ( ?3.3), will be compared to the following methods: Mean field (MF) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> (which is our baseline), nonconvex vanilla Frank-Wolfe (FW) <ref type="bibr" target="#b41">[41]</ref> ( ?2.3), projected gradient descent (PGD) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b41">41]</ref>, fast proximal gradient method (PGM) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b48">48]</ref>, and alternating direction method of multipliers (ADMM) <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b41">41]</ref>. Convex vanilla Frank-Wolfe <ref type="bibr" target="#b20">[21]</ref> and (entropic) mirror descent <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b55">55]</ref> were found to perform poorly in our experiments, and thus excluded from the presentation. Other methods based on CCCP <ref type="bibr" target="#b20">[21]</ref> or LP relaxation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref> are also excluded due to their sophisticated implementations. For all methods, we set the initial solution to x 0 = softmax(?u), following previous work <ref type="bibr" target="#b33">[34]</ref>. Further details on implementation, running time, and memory footprint can be found in Appendices D-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>Our segmentation model is a standard combination of a CNN and a CRF <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b76">76]</ref> (Appendix E.1). For the CNN part, we consider two strong architectures: DeepLabv3 with ResNet101 backbone <ref type="bibr" target="#b15">[16]</ref>, and DeepLabv3+ with Xception65 backbone <ref type="bibr" target="#b16">[17]</ref>. The CRF part is a fully-connected one <ref type="bibr" target="#b33">[34]</ref> in which any pair of pixels</p><formula xml:id="formula_23">(i, j) is an edge with potential ? ij (s, t) = ?(s, t)k(f i , f j ) ?s, t ? S,</formula><p>where ? : S ? S ? R is called label compatibility function, and k is a Gaussian kernel over image features based on pixel coordinates and colors. The setup of our models are similar to Zheng et al. <ref type="bibr" target="#b76">[76]</ref>. We use the Potts compatibility function: ?(s, t) = w? [s =t] with w = 1 for the inference experiments in ?5.2, and also for CRF initialization in the learning experiments in ?5.3. For all experiments, a fullytrained CNN is needed. We follow closely the published recipes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> for this task. We first pretrain DeepLabv3 and DeepLabv3+ on the COCO dataset <ref type="bibr" target="#b46">[46]</ref> and then finetune them on PASCAL VOC (trainaug) and Cityscapes (train) to obtain similar results to previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>  <ref type="table" target="#tab_2">(Table 1</ref>, CNN column). Finally, we perform experiments on two popular datasets: (augmented) PASCAL VOC <ref type="bibr" target="#b21">[22]</ref> and Cityscapes <ref type="bibr" target="#b18">[19]</ref>. Further details are given in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Inference performance</head><p>In this section, we compare the performance of regularized Frank-Wolfe against the competing methods in terms of inference. We consider a Potts CRF on top of a CNN, which is the typical setup for using dense CRF in post-processing. <ref type="figure" target="#fig_0">Figure 1a</ref> shows the discrete energy per inference iteration for each method, averaged over the 1449 val images of PASCAL VOC, using DeepLabv3+. One can observe that Frank-Wolfe variants completely outperform the other methods. In addition, regularized Frank-Wolfe outperforms all the other methods for a large range of ?, as shown in <ref type="figure" target="#fig_0">Figure 1b</ref>. <ref type="table" target="#tab_2">Table 1</ref> shows the performance on the validation sets of PASCAL VOC and Cityscapes, for a Potts CRF with both DeepLabv3 and DeepLabv3+ as backbone. In this experiment, we run all the methods for 10 iterations. One can observe that ? 2 FW achieved the best performance, followed by eFW. <ref type="bibr">CNN</ref>   We should note some inconsistency compared to the energy results previously presented in <ref type="figure" target="#fig_0">Figure 1a</ref>. For example, eFW achieved much lower energy than MF, yet the mIoU gap is marginal; also, FW accuracy is slightly worse than MF while the energy is much better (lower). This can be explained by the fact that the Potts  model is not a perfect representation (i.e., lower energy in this model does not necessarily translate to higher accuracy). In the next section, we will see how the methods perform when the CRF parameters are learned from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Learning performance</head><p>In this section, we evaluate the performance of the methods for joint CNN-CRF end-to-end training.</p><p>The CNN is initialized with its fully-trained weights on the corresponding dataset, and the CRF is initialized with the Potts model with random noise added. We train the model for 20 epochs with 5 CRF iterations, <ref type="bibr" target="#b8">9</ref> using the same poly schedule as before. As the CNN has been already fully-trained, we set its learning rate to a small value of 0.0001. For the CRF, we tried 4 different values of initial learning rates ? 0 ? {1.0, 0.1, 0.01, 0.001} and found that 1.0 is too high (training diverges quickly) while 0.001 is too low (slow progress) for all methods. For the remaining candidates {0.1, 0.01}, we perform 4 additional trainings for each method (i.e., a total of 5 runs for each configuration).</p><p>Let us summarize our findings. First, we observe that (vanilla) FW fails to learn. This is illustrated in <ref type="figure" target="#fig_0">Figure 1c</ref>, where we show the validation accuracy per epoch on PASCAL VOC for each method: FW did not make any progress. We tried a different optimizer (Adam <ref type="bibr" target="#b32">[33]</ref>) and obtained similar results. This is expected as the gradient in vanilla FW is zero almost everywhere, as previously discussed in ?3.1 (see also ?C.1). Our second observation is that training is quite unstable for PGD, PGM, ADMM, eFW .3 , and ? 2 FW. In particular, ? 0 = 0.1 is still too high for these methods, and even with ? 0 = 0.01, some of the runs produced bad results. By contrast, MF and eFW .7 are stable for both learning rates, with 0.1 being slightly better. A possible explanation is that PGD, PGM, ADMM, and ? 2 FW all employ a simplex projection step that is not fully differentiable (but only so almost everywhere). For eFW .3 (which is fully differentiable), we hypothesize that the low regularization makes the problem less "smooth", which may also harm gradient-based training. Finally, with the above training scheme, we observe that none of the CRF methods could improve over the CNN (but rather the opposite) on Cityscapes. We have seen that the Potts CRF was able to achieve some marginal improvements <ref type="table" target="#tab_2">(Table 1)</ref>, thus it is reasonable to expect even better performance with end-to-end training.</p><p>In view of the above observations, we present a simple trick to make CRF training more stable. The idea is to replace the CRF output x * with 1 2 (x * + x 0 ), where we recall that the initialization x 0 is the softmax of the CNN logits. Intuitively, this adds a skip connection from the CNN to the CRF output in the computation graph, which makes the gradient of the loss propagate directly to the CNN. We found that this trick also slightly improves eFW .7 , but has a negative effect on MF. Therefore, it is applied to all methods except MF. Finally, as Cityscapes requires a very high number of epochs, we set this value to 100. Also because training on Cityscapes requires a lot more computing resources, we only perform a single run on DeepLabv3+. The results are presented in <ref type="table" target="#tab_5">Table 2</ref>. <ref type="bibr">CNN</ref>    Performance on the test sets We select the best performing method (DeepLabv3+ with ? 2 FW CRF) for evaluation on the test sets. For PASCAL VOC, we further train our model on the union of the train and val subsets for 50 epochs. For Cityscapes, we further train 200 epochs on train and train_extra, using the high-quality annotations provided by Tao et al. <ref type="bibr" target="#b70">[70]</ref> (for train_extra). At the 150 th epoch, we replace train_extra with val. For this fine-tuning step, learning rates were set to 0.001 for CNN and 0.1 for CRF. For prediction, we apply test time augmentation including left-right flipping and multi-scales. For reference, we train DeepLabv3+ alone using the same recipes. <ref type="table" target="#tab_6">Table 3</ref> shows that we were able to closely match the performance reported by Chen et al. <ref type="bibr" target="#b16">[17]</ref>. Adding the ? 2 FW CRF yields an improvement of 0.4 points on PASCAL VOC. Unfortunately we only observe a marginal improvement on Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation studies</head><p>Trainable ? k and ? It is possible to learn ? k and ? from data by simply setting them to be trainable. We carried out such an experiment with ? 2 FW and eFW but did not observe significant improvements, though we should note that a more sophisticated training recipe (e.g., using custom learning rates for these variables) might lead to better results. Details are provided in Appendix F.2.</p><p>Fine-grained analysis We observe that CRF improved over CNN on most of the semantic classes. In particular, on bicycle (known to be the most challenging class of PASCAL VOC <ref type="bibr" target="#b15">[16]</ref>), ? 2 FW and eFW achieved improvements of over 10% absolute in mIoU. See Appendix F.3 for the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion &amp; conclusion</head><p>Why does it work? Theoretically, all the methods in ?5 should reach a stationary point, so how can one be better than another? In fact, <ref type="figure" target="#fig_0">Figure 1a</ref> only shows that Frank-Wolfe variants work better than the other methods in the first few iterations, but not necessarily in a later stage. Indeed, the same conclusion no longer holds after 100 iterations (see ?F.4), but this long regime is not practical because it would lead to vanishing/exploding gradients <ref type="bibr" target="#b76">[76]</ref> and to potentially prohibitive memory consumption. As to why Frank-Wolfe achieves lower energy in the early stage, we hypothesize that this could be due to the discreteness of its iterates <ref type="bibr" target="#b6">(7)</ref>. With small ?, the solution by regularized Frank-Wolfe should be close to the vanilla one, and thus also benefits from this property. It is important to note that the benefit of regularized Frank-Wolfe does not lie in the extra (sometimes small) energy improvement over vanilla Frank-Wolfe, but in its ability to seamlessly solve the zero-gradient issue.</p><p>How to tune ?? We found that similar curves to <ref type="figure" target="#fig_0">Figure 1b</ref> can be obtained using a small random subset (e.g., 10 samples) of the data, which suggests a quick way of tuning ? by random subsampling. In practice, this step takes only a few seconds, which is negligible in most training scenarios.</p><p>Limitations While one variant of regularized Frank-Wolfe (? 2 FW) consistently achieves the best results, the difference compared to the other methods is sometimes small. In addition, the improvement of dense CRFs over CNNs is marginal on the Cityscapes test set. Nevertheless, we hope the encouraging results on PASCAL VOC could attract interest from the community in CRF research, potentially leading to creative ways of overcoming these limitations.</p><p>Societal impact Semantic segmentation models can be used in surveillance systems, which might raise potential privacy concerns. Furthermore, the datasets that our models were trained on are known to present strong built-in bias <ref type="bibr" target="#b71">[71]</ref>, thus they should be used with caution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details on special cases of regularized Frank-Wolfe inference</head><p>We have seen in ?3 multiple instantiations of regularized Frank-Wolfe, leading to new algorithms for MAP inference, as well as recovering many existing ones. In this section we provide further details on this matter.</p><p>Recall the notation n = |V| , m = |E| , d = |S|, where V, E and S are the sets of nodes, edges, and labels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Algorithms based on QP relaxation with vanilla Frank-Wolfe</head><p>Nonconvex vanilla Frank-Wolfe This algorithm, previously studied by L?-Huu and Paragios <ref type="bibr" target="#b41">[41]</ref>, was already presented in ?2.3. It consists in applying vanilla Frank-Wolfe directly to the energy <ref type="bibr" target="#b5">(6)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convex vanilla Frank-Wolfe</head><p>This involves the convex QP relaxation of MAP inference introduced by Ravikumar and Lafferty <ref type="bibr" target="#b61">[61]</ref>. The idea is to add a sufficiently large vector c to the diagonal of P to make it positive semidefinite. If x ? {0, 1} nd then it is easy to check that x ? diag(c)x = c ? x for any c ? R nd . Therefore, the (discrete) energy can be written as</p><formula xml:id="formula_24">E(x) = 1 2 x ? (2 diag(c) + P)x + (u ? c) ? x.<label>(15)</label></formula><p>It can be shown that the above function is convex if c is chosen as follows:</p><formula xml:id="formula_25">c is = 1 2 j?Ni t?S ? ij (s, t) ?i ? V, s ? S,<label>(16)</label></formula><p>where N i denotes the set of neighbors of node i. Applying vanilla Frank-Wolfe to minimizing the above convex energy over X yields the algorithm presented in section 4 of Desmaison et al. <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Algorithms based on LP relaxation with vanilla Frank-Wolfe</head><p>Let us first present the LP relaxation of MAP inference. We use the same notation leading to the energy formulation (4), namely the indicator variables x is ? {0, 1}, the indicator vectors x i ? {0, 1} d , and the potential vectors ? ? ? i ? R d for all nodes i ? V and labels s ? S. In addition, define for all edges ij ? E and pairs of labels (s, t) ? S 2 :</p><formula xml:id="formula_26">? New pairwise indicator variables x ijst = x is x jt ? {0, 1}. ? New pairwise indicator vectors x ij = (x ijst ) s?S,t?S ? {0, 1} d 2 .</formula><p>? New pairwise potential vectors ? ? ? ij = (? ij (s, t)) s?S,t?S ? R d 2 , which can be viewed as the flatten version of the potential matrices ? ? ? ij in (4).</p><p>Then, the energy (4) can be rewritten as a linear function:</p><formula xml:id="formula_27">E LP (x; ? ? ?) = i?V ? ? ? ? i x i + ij?E ? ? ? ? ij x ij ,<label>(17)</label></formula><p>where by slight abuse of notation, we let x and ? ? ? again denote the vectors of all variables and parameters, respectively. Note that x and ? ? ? are now (nd + md 2 )-dimensional vectors and not nd-dimensional as in <ref type="formula" target="#formula_3">(4)</ref>. The LP relaxation consists in minimizing E LP over the following local polytope <ref type="bibr" target="#b72">[72]</ref>:</p><formula xml:id="formula_28">X LP = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? x ? R nd+md 2 x ? 0, 1 ? x i = 1 ?i ? V, t?S x ijst = x is ?ij ? E, ?s ? S, s?S x ijst = x jt ?ij ? E, ?t ? S. ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? .<label>(18)</label></formula><p>The last two constraints in the above (called local consistency) can be written as Ax = 0 for some (2md) ? (nd + md 2 ) matrix A. We can thus rewrite the LP relaxation compactly as: </p><formula xml:id="formula_29">min E LP (x; ? ? ?) ? ? ? ? x, s.t. x ? X LP x ? R nd+md 2 + : 1 ? x i = 1 ?i ? V, Ax = 0 .</formula><p>for some regularizer r. These works differ in the choice of r.</p><p>Local-consistency regularization Choosing r(x) = ? 2 Ax 2 2 we obtain the algorithm presented by Meshi et al. <ref type="bibr" target="#b52">[52]</ref> (which corresponds to the primal algorithm in the top-right cell of their <ref type="table" target="#tab_2">Table 1</ref>).</p><p>Bethe and TRW entropic regularization Sontag and Jaakkola <ref type="bibr" target="#b67">[67]</ref> also apply vanilla Frank-Wolfe to a regularized LP energy (corresponding to Step 3 in their Algorithm 1; note that we consider only the first outer iteration of their algorithm). They consider regularizers of the form</p><formula xml:id="formula_31">r(x) = ?H(x),<label>(21)</label></formula><p>whereH(x) is some approximation to the entropy H(x) of the distribution over x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define the singleton entropy</head><formula xml:id="formula_32">H(x i ) = ? s?S x is log x is ?i ? V,<label>(22)</label></formula><p>and the pairwise mutual information</p><formula xml:id="formula_33">I(x ij ) = s?S t?S x ijst log x ijst x is x jt = ?H(x ij ) + H(x i ) + H(x j ) ?ij ? E.<label>(23)</label></formula><p>The so-called Bethe approximation is defined as:</p><formula xml:id="formula_34">H Bethe (x) = i?V H(x i ) ? ij?E I(x ij ).<label>(24)</label></formula><p>The second approximation considered by <ref type="bibr" target="#b67">[67]</ref> is called tree-reweighted (TRW) approximation. To achieve this, we decompose the the graph into a convex combination of spanning trees according to some distribution (over the trees), and let ? ij be the so-called edge appearance probability, which is computed as the number of spanning trees containing the edge ij in the current decomposition, divided by the total number of all possible spanning trees containing ij (in the entire distribution). The TRW approximation is then given byH</p><formula xml:id="formula_35">TRW (x) = i?V H(x i ) ? ij?E ? ij I(x ij ).<label>(25)</label></formula><p>? ? ?-reweighted entropic regularization Tang et al. <ref type="bibr" target="#b69">[69]</ref> consider a more general term than the previous ones, based on the following approximation to z log z for z ? [0, 1], parameterized by ? ? [0, 1]:</p><formula xml:id="formula_36">H ? (z) = ?z log z if z ? [?, 1], ?? log ? ? (1 + log ?)(z ? ?) ? (z??) 2 2? if z ? [0, ?].<label>(26)</label></formula><p>Define a similar version for vectors:</p><formula xml:id="formula_37">H ? (z) = p i=1 H ? (z i ) ?z ? R p .<label>(27)</label></formula><p>Their ? ? ?-reweighted approximation to the entropy H(x) is given by:</p><formula xml:id="formula_38">H ? ? (x) = i?V H ? (x i ) ? ij?E ? ij [?H ? (x ij ) + H ? (x i ) + H ? (x j )]<label>(28)</label></formula><p>Tang et al. <ref type="bibr" target="#b69">[69]</ref> apply vanilla Frank-Wolfe to E LP + r where r = ?H ? ? . Note that their work consists in learning parameters of graphical models through maximum likelihood estimation. Here we only consider the inference part presented in their Section 3.2, which is used as a subroutine for learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Algorithms based on the concave-convex procedure</head><p>In the dense CRF model proposed by Kr?henb?hl and Koltun <ref type="bibr" target="#b33">[34]</ref>, the pairwise potentials consist of weighted sums of Gaussian kernels:</p><formula xml:id="formula_39">? ij (s, t) = C c=1 ? (c) (s, t)k (c) (f i , f j ) ?i, j ? V, ?s, t ? S,<label>(29)</label></formula><p>where C is the number of components, ? (c) : S ? S ? R are the so-called label compatibility functions, and k (c) are Gaussian kernels over some image features (f i , f j ) ( ?E.1 presents a concrete example implemented for our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define kernel matrices K</head><formula xml:id="formula_40">(c) ? R n?n with elements K (c) ij = k(f i , f j ) and compatibility matrices M (c) ? R d?d with elements M (c) st = ? (c) (s, t). Let M = C c=1 M (c) .<label>(30)</label></formula><p>If we assume that K (c) ? R n?n has unit diagonal: K (c) ii = 1 ?i, ?c, then our pairwise potential matrix P can be written as</p><formula xml:id="formula_41">P = C c=1 K (c) ? I n ? M (c) = ?I n ? M + C c=1 K (c) ? M (c) ,<label>(31)</label></formula><p>where ? denotes the Kronecker product, and I n is the n ? n identity matrix.</p><p>In the following, the concave-convex procedure (CCCP) <ref type="bibr" target="#b75">[75]</ref> is applied to minimizing f (x) + g(x) where f is concave and g is convex. (We integrate the constraint set X into g using its indicator function ? X , for consistency with our presentation of regularized Frank-Wolfe.)</p><p>Convergent mean field Kr?henb?hl and Koltun <ref type="bibr" target="#b34">[35]</ref> proposed (in their section 3.1) to minimizing a regularized energy E(x) + x ? log x (entropic regularizer) by applying CCCP to:</p><formula xml:id="formula_42">f (x) = 1 2 x ? (P + I n ? M)x + u ? x,<label>(32)</label></formula><formula xml:id="formula_43">g(x) = ? 1 2 x ? (I n ? M)x + x ? log x + ? X (x).<label>(33)</label></formula><p>Convergent mean field using concave approximation Kr?henb?hl and Koltun <ref type="bibr" target="#b34">[35]</ref> proposed (in their section 3.2) a more efficient algorithm using:</p><formula xml:id="formula_44">f (x) = 1 2 x ? (P + I n ? M)x + u ? x,<label>(34)</label></formula><formula xml:id="formula_45">g(x) = x ? log x + ? X (x)<label>(35)</label></formula><p>CCCP for QP relaxation 1 Desmaison et al. <ref type="bibr" target="#b20">[21]</ref> proposed (in their section 5.1) the following application of CCCP:</p><formula xml:id="formula_46">f (x) = ?x ? diag(c)x,<label>(36)</label></formula><formula xml:id="formula_47">g(x) = 1 2 x ? (2 diag(c) + P)x + u ? x + ? X (x),<label>(37)</label></formula><p>where c is defined by <ref type="bibr" target="#b15">(16)</ref>.</p><p>CCCP for QP relaxation 2 Inspired by Kr?henb?hl and Koltun <ref type="bibr" target="#b34">[35]</ref>, Desmaison et al. <ref type="bibr" target="#b20">[21]</ref> also proposed (in their section 5.2) another more efficient variant:</p><formula xml:id="formula_48">f (x) = 1 2 x ? (P + I n ? M)x,<label>(38)</label></formula><formula xml:id="formula_49">g(x) = ? 1 2 x ? (I n ? M)x + u ? x + ? X (x).<label>(39)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Summary of special cases</head><p>We provide in <ref type="table" target="#tab_9">Table 4</ref> a summary of special cases discussed in this section as well as in ?3.3 and ?3.4. There we show how these algorithms can be obtained from regularized Frank-Wolfe by suitably choosing f, g and r in Algorithm 1. Recall that f + g = E + r + ? X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Detailed convergence analysis</head><p>In this section, let ? denote the ? 2 norm. The following lemma is useful for the proofs.</p><p>Lemma 2. If f and g satisfy Assumption 1 and 2, then</p><formula xml:id="formula_50">f (y) ? f (x) + ?f (x), y ? x + L f 2 y ? x 2 ?x, y ? dom f,<label>(40)</label></formula><formula xml:id="formula_51">g(y) ? g(x) + d, y ? x + ? g 2 y ? x 2 ?x, y ? dom g, ?d ? ?g(x).<label>(41)</label></formula><p>Proof. For a convex function h, we have</p><formula xml:id="formula_52">h(y) ? h(x) + d, y ? x ?x, y, ?d ? ?h(x).<label>(42)</label></formula><p>Applying the above inequality with, respectively, h(x) = ?f (x) + , we obtain <ref type="bibr" target="#b40">(40)</ref> and <ref type="bibr" target="#b41">(41)</ref>. (Note that for the second case, h is convex and thus ?g(x) = ?h(x) + ? g x.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of Lemma 1</head><p>First we show that</p><formula xml:id="formula_53">S(x) ? ? g 2 x ? p x 2 ?x ? dom f.<label>(43)</label></formula><p>Algorithm</p><formula xml:id="formula_54">f (x) g(x) ? ? X (x)</formula><p>Parallel mean field Kr?henb?hl and Koltun <ref type="bibr" target="#b33">[34]</ref> E(x) x ? log x Convergent mean field 1 ?3.1 in Kr?henb?hl and Koltun <ref type="bibr" target="#b34">[35]</ref> E(x)   </p><formula xml:id="formula_55">+ 1 2 x ? (I n ? M) x ? 1 2 x ? (I n ? M) x + x ? log</formula><formula xml:id="formula_56">E(x) + 1 2 x ? (I n ? M) x ? u ? x ? 1 2 x ? (I n ? M)x + u ? x LP</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notice that</head><formula xml:id="formula_57">p x ? argmin p { ?f (x), p + g(p)} ?? ??f (x) ? ?g(p x ).<label>(44)</label></formula><p>Hence, applying (41) we obtain</p><formula xml:id="formula_58">g(x) ? g(p x ) + ??f (x), x ? p x + ? g 2 x ? p x 2 ,<label>(45)</label></formula><p>which is precisely <ref type="bibr" target="#b43">(43)</ref>.</p><p>To complete the proof, we need to show that S(x * ) = 0 if and only if x * is a stationary point of (9), i.e., ??f (x * ) ? ?g(x * ). The following is due to Beck <ref type="bibr" target="#b6">[7]</ref>. Notice that</p><formula xml:id="formula_59">S(x) = max p { ?f (x), x ? p + g(x) ? g(p)} ,<label>(46)</label></formula><p>we have</p><formula xml:id="formula_60">S(x * ) = 0 ?? S(x * ) ? 0 ?? ?f (x * ), x * ? p + g(x * ) ? g(p) ? 0 ?p (47) ?? g(p) ? g(x * ) + ??f (x * ), p ? x * ?p (48) ?? ??f (x * ) ? ?g(x * ).<label>(49)</label></formula><p>The proof is completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Theorem 1</head><p>We need an additional lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 3.</head><p>For any x ? dom f and any ? ? [0, 1] we have</p><formula xml:id="formula_61">F (x + ?(p x ? x)) ? F (x) ? ??S(x) + K(?) p x ? x 2 ,<label>(50)</label></formula><p>where</p><formula xml:id="formula_62">K(?) = 1 2 (L f + ? g )? 2 ? ? g ? .</formula><p>Proof. On one hand, from <ref type="bibr" target="#b40">(40)</ref> we have</p><formula xml:id="formula_63">f (x + ?(p x ? x)) ? f (x) + ? ?f (x), p x ? x + L f ? 2 2 p x ? x 2 .<label>(51)</label></formula><p>On the other hand, from the ? g -strong-convexity of g:</p><formula xml:id="formula_64">g(x + ?(p x ? x)) ? (1 ? ?)g(x) + ?g(p x ) ? ? g ?(1 ? ?) 2 p x ? x 2 .<label>(52)</label></formula><p>Summing up the above two inequalities, we obtain <ref type="bibr" target="#b50">(50)</ref>.</p><formula xml:id="formula_65">Let S k = S(x k ), r k = p k ? x k 2 , and F k = F (x k ). Applying (50) we have F k ? F k+1 ? ? k S k ? K(? k )r k .<label>(53)</label></formula><p>Therefore,</p><formula xml:id="formula_66">? 0 = F 0 ? F * ? F 0 ? F k+1 = k i=0 (F i ? F i+1 ) ? S k i=0 ? i ? k i=0 r i K(? i ),<label>(54)</label></formula><p>which implies</p><formula xml:id="formula_67">S ? ? 0 + k i=0 r i K(? i ) k i=0 ? i .<label>(55)</label></formula><p>This is an important inequality that will help us to obtain the convergence results for the weak stepsize schemes such as constant and non-summable ones.</p><p>For the adaptive (and line-search) stepsizes, the following observations will be useful. Notice that the RHS of (50) can be written as 1 2 at 2 ? bt where</p><formula xml:id="formula_68">a = (L f + ? g ) p x ? x 2 , b = S(x) + ? g 2 p x ? x 2 .</formula><p>Therefore:</p><p>? If L f = ? g = 0 then the RHS is just ?tS(x).</p><formula xml:id="formula_69">? If L f = 0 then the RHS is ?tS(x) ? ?g 2 t(1 ? t) p x ? x 2 ? ?tS(x)</formula><p>.</p><p>? If L f + ? g &gt; 0 then the minimum of the RHS is</p><formula xml:id="formula_70">? b 2 2a = ? p x ? x 2 2(L f + ? g ) S(x) p x ? x 2 + ? g 2 2 ,</formula><p>achieved at</p><formula xml:id="formula_71">t * = b a = 1 L f + ? g S(x) p x ? x 2 + ? g 2 .</formula><p>The adaptive stepsize <ref type="bibr" target="#b13">(14)</ref> are defined for the case L f + ? g &gt; 0 is thus:</p><formula xml:id="formula_72">? k = min {1, ? * k } , where ? * k = 1 L f + ? g S k r k + ? g 2 .<label>(56)</label></formula><p>Notice that the RHS of (53) is a quadratic function of ? k with critical point ? * k . If ? * k ? 1, or equivalently ?g 2 + L f ? S k r k , then ? k = ? * k and thus <ref type="bibr" target="#b53">(53)</ref> becomes</p><formula xml:id="formula_73">F k ? F k+1 ? ? * k S k ? K(? * k )r k = r k 2(L f + ? g ) S k r k + ? g 2 2 .<label>(57)</label></formula><p>If ? * k &gt; 1, or equivalently ?g 2 + L f &lt; S k r k , then ? k = 1 and thus <ref type="bibr" target="#b53">(53)</ref> becomes</p><formula xml:id="formula_74">F k ? F k+1 ? S k ? K(1)r k = S k ? L f 2 r k .<label>(58)</label></formula><p>For simplicity and clarity, we will consider separately the two cases: g is strongly convex (? g &gt; 0) or simply convex (? g = 0). Recall that ? is the (finite) diameter of dom g, and thus we have r k ? ? 2 ?k, a fact that we will be using repeatedly in the sequel. Let S = min 0?i?k S i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Convex g</head><p>In this section we consider the case where g is convex but not strongly convex, i.e., ? g = 0. Inequality (55) becomes</p><formula xml:id="formula_75">S ? ? 0 + L f 2 k i=0 r i ? 2 i k i=0 ? i .<label>(59)</label></formula><p>Constant stepsize Consider ? k = ? &gt; 0 ?k. From r k ? ? 2 and (59) we obtain</p><formula xml:id="formula_76">S ? ? 0 (k + 1)? + L f ? 2 ? 2 .<label>(60)</label></formula><p>The right-hand side converges to 1 2 L f ? 2 ? as k ? ?, i.e., the lower-bound S on the conditional gradient norm converges to within 1 2 L f ? 2 ?. It is easy to deduce from the last inequality that S ? L f ? 2 ? within k ? 2?0 L f ? 2 ? 2 steps. We conclude that the algorithm converges to an approximate stationary point for the constant stepsize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constant step length For a constant step length</head><formula xml:id="formula_77">: x k+1 ? x k = ?. Recall that x k+1 = x k + ? k (p k ? x k ), the corresponding stepsize is thus ? k = ? p k ?x k = ? ? r k . Inequality (59) becomes S ? ? 0 + L f 2 (k + 1)? 2 ? k i=0 1 ? r k ? ? 0 + L f 2 (k + 1)? 2 ? k+1 ? = ? 0 ? (k + 1)? + L f ?? 2 .<label>(61)</label></formula><p>Therefore, S converges to within L f ?? 2 , and S ? L f ?? within k ? 2?0 L f ? 2 steps. We conclude that the algorithm converges to an approximate stationary point for the stepsizes with constant step length.</p><p>Non-summable but square-summable stepsizes Assume that the stepsizes ? k satisfy</p><formula xml:id="formula_78">+? k=0 ? k = ?, +? k=0 ? 2 k &lt; ?.<label>(62)</label></formula><p>A typical example is ? k = ? k+? , where ? &gt; 0 and ? ? 0. This includes the common Frank-Wolfe stepsize ? k = 2 k+2 . From r k ? ? 2 and (59) we obtain</p><formula xml:id="formula_79">S ? ? 0 + L f ? 2 2 k i=0 ? 2 i k i=0 ? i ,<label>(63)</label></formula><p>which clearly converges to 0 as k ? ?. Therefore, the algorithm is guaranteed to converge to a stationary point in this case.</p><p>Diminishing (and non-summable) stepsizes Assume that the stepsizes ? k satisfy</p><formula xml:id="formula_80">+? k=0 ? k = ?, lim k?? ? k = 0.<label>(64)</label></formula><p>A typical example is ? k = ? ? k , where ? &gt; 0. Notice that for any ? &gt; 0, we have ? 2 i &lt; ?? i with i large enough, it is straightforward to show that k i=0 ? 2 i k i=0 ?i ? 0 as k ? ?, and thus <ref type="bibr" target="#b63">(63)</ref> implies that S ? 0 as well. We conclude that the algorithm is guaranteed to converge to a stationary point.</p><p>Adaptive stepsizes This result was obtained previously by Beck <ref type="bibr" target="#b6">[7]</ref>. These stepsizes are computed according to <ref type="bibr" target="#b56">(56)</ref>, which can be simplified as the following for ? g = 0:</p><formula xml:id="formula_81">? k = min {1, ? * k } , where ? * k = S k L f r k . (65) Then, if ? * k ? 1, (57) yields F k ? F k+1 ? S 2 k 2L f r k ? S 2 k 2L f ? 2 .<label>(66)</label></formula><p>If ? * k &gt; 1 then <ref type="bibr" target="#b58">(58)</ref> and</p><formula xml:id="formula_82">L f r k &lt; S k yield F k ? F k+1 ? S k ? L f 2 r k ? S k 2 .<label>(67)</label></formula><p>Combining the two cases, we obtain</p><formula xml:id="formula_83">F k ? F k+1 ? S k 2 min 1, S k L f ? 2 ? S 2 min 1, S L f ? 2 .<label>(68)</label></formula><p>Therefore,</p><formula xml:id="formula_84">? 0 = F 0 ? F * ? F 0 ? F k+1 = k i=0 (F i ? F i+1 ) ? (k + 1) S 2 min 1, S L f ? 2 ,<label>(69)</label></formula><p>which yields</p><formula xml:id="formula_85">S ? max 2? 0 k + 1 , 2L f ? 2 ? 0 ? k + 1 .<label>(70)</label></formula><p>Therefore, the algorithm is guaranteed to converge to a stationary point, and the rate of convergence convergence is at least O(1/ ? k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.2 Strongly-convex g</head><p>In this section we consider the case where g is strongly convex with parameter ? g &gt; 0.</p><p>Recall from <ref type="bibr" target="#b53">(53)</ref> and Lemma 3 that</p><formula xml:id="formula_86">F k ? F k+1 ? ? k S k ? K(? k )r k ?k ? 0, where K(?) = 1 2 ? [(L f + ? g )? ? ? g ] .<label>(71)</label></formula><p>Thus if ? k ? ?g L f +?g we have K(? k ) ? 0 and <ref type="formula" target="#formula_0">(71)</ref> yields</p><formula xml:id="formula_87">F k ? F k+1 ? ? k S k .<label>(72)</label></formula><p>Consider now the case ? k &gt; ?g L f +?g for which K(? k ) &gt; 0. From (43) we have r k ? 2S k ?g , and thus (71) yields</p><formula xml:id="formula_88">F k ? F k+1 ? ? k S k ? K(? k ) 2S k ? g = ? k ? 2K(? k ) ? g S k = ? k 2 ? L f + ? g ? g ? k S k .<label>(73)</label></formula><p>Combining the two cases, we obtain</p><formula xml:id="formula_89">F k ? F k+1 ? ? k min 1, 2 ? L f + ? g ? g ? k S k .<label>(74)</label></formula><p>If ? k ? 2?g L f +?g then the RHS of (74) is non-positive, thus this inequality is not helpful. In this case, we can obtain another inequality from <ref type="bibr" target="#b71">(71)</ref>, noticing that r k ? ? 2 ?k and K(? k ) &gt; 0:</p><formula xml:id="formula_90">F k ? F k+1 ? ? k S k ? K(? k )? 2<label>(75)</label></formula><p>Constant stepsize Assume that ? k = ? ?k ? 0. If 0 &lt; ? &lt; 2?g L f +?g then (74) yields</p><formula xml:id="formula_91">F k ? F k+1 ? ? min 1, 2 ? L f + ? g ? g ? S ?k ? 0.<label>(76)</label></formula><p>Hence</p><formula xml:id="formula_92">? 0 ? F 0 ? F k+1 = k i=0 (F i ? F i+1 ) ? (k + 1)? min 1, 2 ? L f + ? g ? g ? S.<label>(77)</label></formula><p>We obtain</p><formula xml:id="formula_93">S ? ? 0 ? min 1, 2 ? L f +?g ?g ? (k + 1) ?? &lt; 2? g L f + ? g .<label>(78)</label></formula><p>We conclude that the algorithm is guaranteed to converge to a stationary point with rate of convergence of (at least) O(1/k) for any 0 &lt; ? &lt; 2?g L f +?g .</p><p>For the remaining case ? ? 2?g L f +?g , we will derive an upper bound for S. Applying (75) we obtain</p><formula xml:id="formula_94">? 0 ? k i=0 (F i ? F i+1 ) ? ? k i=0 S i ? (k + 1)K(?)? 2 ? (k + 1)?S ? (k + 1)K(?)? 2 ,<label>(79)</label></formula><p>which yields</p><formula xml:id="formula_95">S ? ? 0 ?(k + 1) + K(?)? 2 ? = ? 0 ?(k + 1) + 1 2 [(L f + ? g )? ? ? g ] ? 2 .<label>(80)</label></formula><p>Therefore, for ? ? 2?g L f +?g the algorithm converges to an approximate stationary point at which the conditional gradient norm is bounded above by <ref type="bibr">1 2</ref> [(L f + ? g )? ? ? g ] ? 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constant step length Consider the stepsize ?</head><formula xml:id="formula_96">k = ? p k ?x k = ? ? r k for which x k+1 ? x k = ?. Inequality (71) becomes F k ? F k+1 ? ? ? r k S k ? K ? ? r k r k (81) = ? ? r k S k ? 1 2 (L f + ? g ) ? 2 r k ? ? g ? ? r k r k (82) = ? ? r k S k + ? g ? ? r k 2 ? 1 2 (L f + ? g )? 2 (83) ? 2 ? ? r k S k ? g ? ? r k 2 ? 1 2 (L f + ? g )? 2 (84) = ? 2? g S k ? 1 2 (L f + ? g )? 2 .<label>(85)</label></formula><p>It follows that</p><formula xml:id="formula_97">? 0 ? (k + 1)? 2? g S ? k + 1 2 (L f + ? g )? 2 (86) =? ? S ? ? 0 ? 2? g (k + 1) + (L f + ? g )? 2 2? g .<label>(87)</label></formula><p>For this stepsize scheme, the algorithm converges to an approximate stationary point, within</p><formula xml:id="formula_98">(L f +?g ) 2 ? 2 8?g</formula><p>. One can observe that, even though the strong convexity of g still cannot guarantee convergence to a stationary point, it helps improve the bound as well as the rate of convergence from <ref type="bibr" target="#b61">(61)</ref>.</p><p>Diminishing (and non-summable) stepsizes Assume that the stepsizes ? k satisfy</p><formula xml:id="formula_99">+? k=0 ? k = ?, lim k?? ? k = 0.<label>(88)</label></formula><p>This scheme also includes the non-summable but square-summable one. Since lim k?? ? k = 0, there exists an integer k(?) such that ? k ? ? = ?g L f +?g ?k ? k(?). Now applying (72)</p><formula xml:id="formula_100">? k(?) ? F k(?) ? F k+1 = k i=k(?) (F i ? F i+1 ) ? k i=k(?) ? i S i ? ? ? k i=k(?) ? i ? ? S,<label>(89)</label></formula><p>which yields</p><formula xml:id="formula_101">S ? ? k(?) k i=k(?) ? i .<label>(90)</label></formula><p>Since (? k ) is non-summable, the algorithm converges to a stationary point. Compared to the nonstrongly-convex case, we observe that the assumption that dom g is compact can be relaxed (its diameter ? is not used in the proof).</p><p>Adaptive stepsizes Recall that the stepsizes in this scheme are given by <ref type="bibr" target="#b56">(56)</ref> as</p><formula xml:id="formula_102">? k = min {1, ? * k } , where ? * k = 1 L f + ? g S k r k + ? g 2 .<label>(91)</label></formula><p>If ? * k ? 1, from (57) and the inequality (a + b) 2 ? 4ab, we obtain</p><formula xml:id="formula_103">F k ? F k+1 ? r k 2(L f + ? g ) 4 S k r k ? g 2 = ? g S k L f + ? g . (92) If ? * k &gt; 1, which is r k &lt; S k ?g 2 +L f , then (58) yields F k ? F k+1 ? S k ? L f 2 S k ?g 2 + L f = ? g + L f ? g + 2L f S k ? ? g ? g + L f S k .<label>(93)</label></formula><p>Therefore, we always have F k ? F k+1 ? ?S k where ? = ?g ?g +L f . It then follows that</p><formula xml:id="formula_104">? 0 ? k i=0 (F i ? F i+1 ) ? k i=0 ?S k ? (k + 1)?S =? S ? ? 0 ?(k + 1)</formula><p>.</p><p>Finally, the line search scheme is guaranteed to achieve the best decrease in the objective, thus the inequality F k ? F k+1 ? ?S k also holds and we obtain the same results for this scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.3 Concave f</head><p>The results for this case can be obtained in a straightforward manner by setting L f = 0 in the "convex g" case. In particular, for the adaptive stepsizes, (65) yields ? k = 1 and thus it becomes the constant stepsize scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.4 Summary of convergence results</head><p>We summarize the results in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.5 Convergence of S(x k )</head><p>To complete the proof of Theorem 1, we need to show that for the non-highlighted cases of its table (page 6), we have lim k?? S(x k ) = 0 and any limit point of the sequence (x k ) k?0 is a stationary point of <ref type="bibr" target="#b8">(9)</ref>. Indeed, for these cases, (F k ) k?0 is a decreasing sequence because F k ? F k+1 is bounded below by a non-negative quantity ? k (according to <ref type="table">Table 5</ref> presented in the previous section). Therefore, F k is convergent as it is bounded below by F * . Consequently F k ? F k+1 ? 0, which implies ? k ? 0 and thus S k ? 0 as well for the considered cases (see <ref type="table">Table 5</ref>). The results follow in a straightforward manner.</p><formula xml:id="formula_106">stepsize decrease lower bound ? k optimality upper bound B k (F k ? F k+1 ? ? k ) (min 0?i?k S i ? B k ) convex g ? k = ? &gt; 0 ?S k ? L f ? 2 ? 2 2 ?0 (k+1)? + L f ? 2 ? 2 ? k = ? p k ?x k ? ? S k ? L f ? 2 2 ?0? (k+1)? + L f ?? 2 +? k=0 ? k = ? lim k?? ? k = 0 ? k S k ? L f ? 2 ? 2 k 2 ?0 k i=0 ?i + L f ? 2 2 k i=0 ? 2 i k i=0 ?i adaptive or line search (14) 1 2 min S k , S 2 k L f ? 2 max 2?0 k+1 , ? 2L f ? 2 ?0 ? k+1 strongly-convex g ? k = ? &lt; 2? ? min 1, 2 ? ? ? S k ?0 ? min(1,2? ? ? )(k+1) ? k = ? ? 2? ?S k ? K(?)? 2 ?0 ?(k+1) + K(?) ? ? 2 ? k = ? p k ?x k ? 2? g S k ? 1 2 (L f + ? g )? 2 ?0 ? ? 2?g (k+1) + (L f +?g )? 2 ? 2?g 2 +? k=0 ? k = ? lim k?? ? k = 0 ? k min 1, 2 ? ? k ? S k ? k(?) k i=k(?) ?i adaptive or line search (14) ?S k ?0 ?(k+1) concave f ? k = ? &gt; 0 ?S k ?0 (k+1)? ? k = ? p k ?x k ? ? S k ?0? (k+1)? +? k=0 ? k = ? ? k S k ?0 k i=0 ?i</formula><p>adaptive or line search <ref type="bibr" target="#b13">(14)</ref> S k ?0 k+1 <ref type="table">Table 5</ref>: Summary of convergence analysis of the generalized Frank-Wolfe algorithm. Recall that ? = ?g L f +?g . Whenever a result does not involve ?, the assumption that dom g being compact can be relaxed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs of other theoretical results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Vanilla Frank-Wolfe fails to learn: the zero-gradient issue</head><p>We claimed in ?3.1 that vanilla Frank-Wolfe <ref type="formula">(7)</ref> is problematic for learning with SGD because its iterates are piecewise-constant and thus their gradients are zero almost everywhere (more precisely the gradient is undefined on the boundaries while being zero everywhere else). In this section, we present a theoretical justification for this claim.</p><p>It suffices to show that p * = argmin p?X c, p is piecewise-constant with respect to c. Let ? d denote the simplex z ? R d | 1 ? z = 1, z ? 0 . Clearly, the set X (defined by (5)) can be written as x ? R nd | x i ? ? d ?i ? V , and thus the above minimization problem can be reduced to solving the following problem for each i ? V independently:</p><formula xml:id="formula_107">p * i ? argmin pi?? d c i , p i .<label>(95)</label></formula><p>For notational convenience, consider the following problem with a constant vector b</p><formula xml:id="formula_108">= (b 1 , b 2 , . . . , b d ) ? R d : z * ? argmin z?? d b, z .<label>(96)</label></formula><p>Let s * be the index of the minimum element of b, i.e., s * = argmin s b s . Let e s ? {0, 1} d denote the one-hot vector where the s th element is one. We have:</p><formula xml:id="formula_109">b, z = d s=1 b s z s ? d s=1 b s * z s = b s * d s=1 z s = b s * = b, e s * ?z ? ? d .<label>(97)</label></formula><p>Therefore, e s * is an optimal solution to (96). It is straightforward that the index of the minimum element of a vector is piecewise constant, thus e s * is also piecewise constant (as a function of b). Therefore, e s * is not continuous (thus non-differentiable) on the boundaries, while in the constant regions, its gradient is zero.</p><p>Remark. We can deduce that the iteration complexity of vanilla Frank-Wolfe is O(nd).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of the relaxation tightness (Theorem 2)</head><p>We give a proof of Theorem 2 in ?4.2. Recall that we have to prove</p><formula xml:id="formula_110">E * ? E(x * r ) ? E * + M ? m + C, where C = n 1 ? 1 d ( u 2 + ? n P 2 ) for nearest rounding 0 for BCD rounding.<label>(98)</label></formula><p>Let x * be such that E(x * ) = E * and consider first the BCD rounding scheme. As this scheme is guaranteed to not increase the energy, we have</p><formula xml:id="formula_111">E(x * r ) ? E(x * r ) = E r (x * r ) ? r(x * r ) ? E r (x * ) ? r(x * r ) = E(x * ) + r(x * ) ? r(x * r ) ? E * + M ? m.</formula><p>It remains to prove the result for the nearest rounding scheme. In this scheme, the discrete energy may increase (or decrease), but it can be shown that the variation is bounded by the given constant:</p><formula xml:id="formula_112">|E(x * r ) ? E(x * r )| ? C.</formula><p>Then, the rest of the proof is similar to the BCD case. This bounding inequality is proved as follows.</p><p>Suppose that we obtain a discrete solution y ? X ? {0, 1} nd from some x ? X using nearest rounding. We will prove that</p><formula xml:id="formula_113">|E(x) ? E(y)| ? C,<label>(99)</label></formula><formula xml:id="formula_114">where C = n 1 ? 1 d u 2 + ? n P 2 .<label>(100)</label></formula><p>Lemma 4. For any z ? ? d (see ?C.1 for notation) and its rounded vector</p><formula xml:id="formula_115">v ? ? d ? {0, 1} d , i.e., v i = 1 if i = argmax 1?j?d v j and v j = 0 ?j = i, we have z ? v 2 2 ? 1 ? 1 d .<label>(101)</label></formula><p>Proof. Without loss of generality, assume that z 1 is the maximum element of z. Then, we have v 1 = 1 and v j = 0 ?j &gt; 1.</p><formula xml:id="formula_116">z ? v 2 2 = d i=1 (z i ? v i ) 2 = (z 1 ? 1) 2 + d i=1 z 2 i = S 2 + z 2 2 + ? ? ? + z 2 d ,<label>(102)</label></formula><p>where S = z 2 + ? ? ? + z d . We will make use of the following trivial inequality:</p><formula xml:id="formula_117">z i + S ? 1 ?i ? 2.<label>(103)</label></formula><p>On one hand, summing the d ? 1 inequalities (103) (for i = 2, . . . , d) we obtain</p><formula xml:id="formula_118">S + (d ? 1)S ? d ? 1 =? S ? 1 ? 1 d .<label>(104)</label></formula><p>On the other hand, multiplying (103) with z i and summing up the obtained d ? 1 inequalities we get</p><formula xml:id="formula_119">d i=2 z 2 i + S 2 ? S<label>(105)</label></formula><p>Finally, from (102), (104), and (105) we get (101).</p><p>Back to (99). Applying (101) we have</p><formula xml:id="formula_120">x ? y 2 2 = n i=1 x i ? y i 2 2 ? n 1 ? 1 d .<label>(106)</label></formula><p>On the other hand</p><formula xml:id="formula_121">x + y 2 2 = n i=1 x i + y i 2 2 ? n i=1 1 ? (x i + y i ) 2 = 4n.<label>(107)</label></formula><p>is given as follows. Sort c in decreasing order to obtain a vector a = (a 1 , a 2 , . . . , a d ) (i.e., a 1 ? a 2 ? ? ? ? ? a d ) and let ? k = 1 k (a 1 + a 2 + ? ? ? + a k ? 1), k = 1, 2, . . . , d.</p><p>Let k * be the largest k such that a k &gt; ? k , then the optimal solution is given by</p><formula xml:id="formula_123">z * = max(c ? ? k * , 0).<label>(113)</label></formula><p>In the above, the "max" and "?" operations are understood to be element-wise.</p><p>Remark. If we use an O(d log d) sorting algorithm, then we see that the per-iteration complexity of Euclidean Frank-Wolfe is O <ref type="figure">(nd log d)</ref>. It should be noted, however, that highly-efficient simplexprojection algorithms exist and have O(d) complexity in practice [18, <ref type="table" target="#tab_2">Table 1</ref>], yielding O(nd) complexity, which is the same as in vanilla Frank-Wolfe (see ?C.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Entropic Frank-Wolfe (eFW)</head><p>We give the details for the main update <ref type="bibr" target="#b11">(12)</ref> of Entropic Frank-Wolfe as presented in ?3.3. We need to show that</p><formula xml:id="formula_124">p k = argmin p?X Px k + u, p ? ?H(p) = softmax ? 1 ? (Px k + u) ?k ? 0,<label>(114)</label></formula><p>where H(x) = ? i?V s?S x is log x is . Again, the above reduces to n independent subproblems over each i ? V to which the solutions are given by the following lemma. Lemma 6. For a given vector c ? R d , the optimal solution z * to</p><formula xml:id="formula_125">min 1 ? z=1,z?0 c, z + d s=1 z s log z s (115) is z * = softmax(?c).</formula><p>Proof. The Lagrangian of the above problem is given by</p><formula xml:id="formula_126">L(z, ? ? ?, ?) = c, z + d s=1 z s log z s + ? ? ? ? (?z) + ?(1 ? z ? 1) (116) = ?? + d s=1 (c s z s + z s log z s ? ? s z s + ?z s ) ,<label>(117)</label></formula><p>where ? ? ? = (? 1 , ? 2 , . . . , ? d ) ? 0 and ? ? R are the Lagrange multipliers.</p><p>Observe that the given problem is convex and the corresponding Slater's constraint qualification holds (i.e., there exists z ? R d such that 1 ? z = 1 and z &gt; 0), it suffices to solve the following Karush-Kuhn-Tucker (KKT) system to obtain the optimal solution:</p><formula xml:id="formula_127">?L(z, ? ? ?, ?) ?z s = c s + log z s + 1 ? ? s + ? = 0 ?1 ? s ? d,<label>(118)</label></formula><formula xml:id="formula_128">1 ? z = 1, (119) z ? 0, (120) ? ? ? ? 0, (121) ? s z s = 0 ?1 ? s ? d.<label>(122)</label></formula><p>The first equation implies z s &gt; 0 ?s, and thus in combination with the last, we obtain ? s = 0 ?s. Therefore, the first equation becomes</p><formula xml:id="formula_129">z s = exp(?1 ? ?) exp(?c s ) ?s.<label>(123)</label></formula><p>Summing up this result for all s, and taking into account the second equation, we obtain</p><formula xml:id="formula_130">exp(?1 ? ?) = 1 d s=1 exp(?c s ) .<label>(124)</label></formula><p>With this choice of ?, it is easy to check that the Bregman divergence (133) becomes the following so-called Kullback-Leibler divergence:</p><formula xml:id="formula_131">B KL (x, y) = n i=1 d s=1 x is log x is y is . (135)</formula><p>The MD update (132) thus becomes</p><formula xml:id="formula_132">x k+1 = argmin x?X ? k ?E(x k ) ? log x k , x + n i=1 d s=1 x is log x is ,<label>(136)</label></formula><p>where the log operation is taken element-wise. According to Lemma 6 ( ?D.2),we obtain</p><formula xml:id="formula_133">x k+1 = softmax log x k ? ? k ?E(x k ) .<label>(137)</label></formula><p>Let g k denote the gradient ?E(x k ), the above reads</p><formula xml:id="formula_134">x k+1 is = x k is exp(?? k g k is ) d t=1 x k it exp(?? k g k it ) ?i ? V, ?s ? S.<label>(138)</label></formula><p>Numerically stable EMD In practice, the above expression of x k+1 may lead to numerical underflow or overflow. We overcome this by using the following modified iterate:</p><formula xml:id="formula_135">x k+1 is = (x k is + ?) exp(?? k g k is + m k i ) d t=1 (x k it + ?) exp(?? k g k it + m k i ) ?i ? V, ?s ? S,<label>(139)</label></formula><p>where ? = 10 ?10 and m k i = ? k min 1?s?d g k is ?i ? V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 Alternating direction method of multipliers (ADMM)</head><p>The nonconvex ADMM for MAP inference <ref type="bibr" target="#b41">[41]</ref> consists in the following updates, where z 0 = softmax(?u) and y 0 = 0:</p><formula xml:id="formula_136">x k+1 = ? X z k ? 1 ? (y k + 1 2 Pz k + u) ,<label>(140)</label></formula><formula xml:id="formula_137">z k+1 = ? X x k+1 ? 1 ? (?y k + 1 2 Px k+1 ) ,<label>(141)</label></formula><formula xml:id="formula_138">y k+1 = y k + ?(x k+1 ? z k+1 ).<label>(142)</label></formula><p>We refer to the original paper <ref type="bibr" target="#b41">[41]</ref> for more details. In our experiments, we set ? = 1 for simplicity. Since the expensive computation Px are done two times in each ADMM iteration (one in (140), another in (141)), this algorithm is roughly two times slower than the others. For a fair comparison, in our implementation we view (140) and (141) as two separate iterations (note that both x k+1 and z k+1 are feasible points).</p><p>Finally, we should note that the adaptive scheme for the penalty parameter ? proposed by L?-Huu and Paragios <ref type="bibr" target="#b41">[41]</ref> is not applicable to our case, as we use only 5 iterations in our experiments (which is equivalent to only 2.5 regular iterations due to our above iteration separation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Detailed experimental setup and environment E.1 CNN-CRF architectures</head><p>CNN-CRF Our segmentation model is a standard combination of a CNN and a CRF <ref type="bibr" target="#b76">[76]</ref>. Given an input image Z ? R H?W ?3 , the CNN produces an output Y ? R H?W ?K (where K is the number of object classes) called the logits, which is then fed into the CRF to produce a final output X ? R H?W ?K :</p><formula xml:id="formula_139">Y = CNN(Z; ? ? ? u ), X = CRF(Y; ? ? ? p ),<label>(143)</label></formula><p>where ? ? ? u and ? ? ? p are (typically trainable) parameters. The prediction is then obtained by taking the argmax along the last dimension of X. For the CNN part, we consider two strong architectures: DeepLabv3 with ResNet101 backbone <ref type="bibr" target="#b15">[16]</ref>, and DeepLabv3+ with Xception65 backbone <ref type="bibr" target="#b16">[17]</ref>. The reader is referred to the corresponding references for further details.</p><p>Dense CRF The CRF is defined over the input image such that each pixel is a node, and its labels are the object classes. Thus, using the notation defined in ?2.1, we have n = HW , d = K, and S = {1, 2, . . . , K}. The CRF produces X in (143) by minimizing the energy <ref type="bibr" target="#b5">(6)</ref> with appropriately constructed potentials, and then simply reshaping the solution x ? R HW K into H ? W ? K. During training we skip the rounding step in CRF inference, so that the returned x is real-valued, which is more suitable for learning with the standard cross-entropy loss function. The unary potentials u is defined by to be the additive inverse of the logits Y, reshaped correctly: u = ? vec(Y), where vec denotes the flattening operator. We use the fully-connected model introduced by Kr?henb?hl and Koltun <ref type="bibr" target="#b33">[34]</ref> in which any pair of pixels (i, j) is an edge with a pairwise potential of the form ? ij (s, t) = ?(s, t)k(f i , f j ) ?s, t ? S, where ? : S ? S ? R is the so-called label compatibility function, and k is a Gaussian kernel over some image features (f i , f j ). For a pixel i, we use its position p i ? N 2 and its color c i ? [0, 255] 3 as features, and define the kernel as</p><formula xml:id="formula_140">k(f i , f j ) = w 1 exp ? p i ? p j 2 2 2? 2 ? c i ? c j 2 2 2? 2 + w 2 exp ? p i ? p j 2 2 2? 2 ?i, j ? V,<label>(144)</label></formula><p>where w 1 , w 2 are learnable kernel weights, and ?, ?, ? are hyperparameters. Following Zheng et al. <ref type="bibr" target="#b76">[76]</ref>, we use class-dependent kernel weights to increase the number of trainable parameters. Unlike Zheng et al. <ref type="bibr" target="#b76">[76]</ref>, for simplicity we use the default values ? = 80, ? = 13, ? = 3 set by Kr?henb?hl and Koltun <ref type="bibr" target="#b33">[34]</ref> in all experiments, instead of doing a cross validation to find the best values. Finally, we use the Potts compatibility function: ?(s, t) = w? [s =t] with w = 1 for the inference experiments in ?5.2, and also for CRF initialization in the learning experiments in ?5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Datasets</head><p>We provide further details on the datasets. PASCAL VOC <ref type="bibr" target="#b21">[22]</ref> contains 4369 images of 21 classes, split into 1464 (train), 1449 (val), and 1456 (test) image subsets. As a standard practice, we augment the dataset with images from Hariharan et al. <ref type="bibr" target="#b26">[27]</ref>, resulting in 10 582 training images (trainaug). Cityscapes <ref type="bibr" target="#b18">[19]</ref> contains 5000 images of 19 classes, split into 2975 (train), 500 (val), and 1525 (test) image subsets. In addition, it also provides 19 998 coarsely annotated images (train_extra). We report the performance in terms of mIoU across the semantic classes (21 for PASCAL VOC and 19 for Cityscapes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 CNN training recipes</head><p>To fully train DeepLabv3 and DeepLabv3+, we follow closely the published recipes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> for this task. Below we present the most important information, and refer to the references for further details.</p><p>We first pretrain DeepLabv3 and DeepLabv3+ on the COCO <ref type="bibr" target="#b46">[46]</ref> dataset (by selecting only the images that contain the classes defined in PASCAL VOC), and then finetune them on PASCAL VOC (trainaug) and Cityscapes (train). During training, we apply data augmentation by (randomly) left-right flipping, scaling the input images (from 0.5 to 2.0), and cropping (with crop size of 513 ? 513 for PASCAL VOC and 769 ? 769 for Cityscapes). We employ a poly learning rate schedule: ? m = ? 0 1 ? m M p , where ? 0 is the initial learning rate, m is the step counter, and M is the total number of training steps. For all trainings, we set p = 0.9 and ? 0 = 0.001 (except ? 0 = 0.01 for pretraining on COCO), and a batch size of 16 images. The value of M is calculated from the number of training epochs, which we set to be 50 for COCO pretraining, 50 for finetuning on PASCAL VOC, and 500 for finetuning on Cityscapes. We should note some differences compared to the original papers <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. In particular, they did not specify the learning rate and number of steps for COCO pretraining. Furthermore, they used ? 0 = 0.0001, which did not yield better results than ? 0 = 0.001 in our implementation. Finally, in terms of the number of epochs, we used similar values to theirs. Indeed, they set 30 000 and 90 000 training steps for the 10 582 and 2975 training images of PASCAL VOC and Cityscapes, respectively. With a batch size of 16, these are equivalent to 45 and 484 epochs. <ref type="table" target="#tab_11">Table 6</ref> shows that our obtained results are similar to previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Training time and memory footprint</head><p>Our experiments are performed on a Linux server of 4 Nvidia V100 GPUs, using PyTorch 1.7. With a batch size of 16 on 4 GPUs (i.e., 4 images per GPU), both DeepLabv3 and DeepLabv3+ take ?7min/epoch on PASCAL VOC (with 513 ? 513 crops). Thanks to our efficient GPU implementation (which will be made publicly available), plugging in the (5-step) CRF only increases that to ?9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Published <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Detailed results on the test sets</head><p>The detailed results on the test sets can be found on the corresponding submission websites whose URLs are given in <ref type="table" target="#tab_13">Table 7</ref>.  We carried out an experiment with ? 2 FW and eFW (? = 0.7) in which we allow the stepsize ? k at each CRF iteration to be learnable (initialized at 0.5). We observe the stepsizes at all the steps behave very similarly (i.e., increasing or decreasing together). In addition, for eFW they tend to increase during training, while for ? 2 FW they tend to decrease. In addition, we also tried setting the regularization weight ? to trainable. We initialized it at 1.0 for ? 2 FW and at 0.7 for eFW. For both solvers, we found that ? increased during training. Regarding accuracy, we did not observe significant differences compared to fixed ? k and fixed ?, although tuning the learning rates specifically for these variables could potentially lead to improved performance. See <ref type="table" target="#tab_14">Table 8</ref> for the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Results for fined-grained analysis</head><p>We randomly picked a trained checkpoint among the different runs for DeepLabv3+ with ? 2 FW (? = 1.0) and DeepLabv3+ with eFW (? = 0.7), and evaluated them using 5, 10, and 25 CRF iterations on the PASCAL VOC validation set. The results are shown in <ref type="table">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularizer ?</head><p>Stepsize mIoU   <ref type="table">Table 9</ref>: Fined-grained results on PASCAL VOC validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 Additional inference results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4.1 Results for longer inference regime</head><p>We show in <ref type="figure" target="#fig_7">Figure 2</ref> a comparison of the discrete energy across the methods on a subset of 10 val images of PASCAL VOC for 100 inference iterations, using DeepLabv3+ and Potts dense CRF. <ref type="bibr" target="#b19">20</ref>    From these results, we observe that:</p><p>1. Vanilla FW and ? 2 FW already converge after around 20 iterations. ? 2 FW does better than vanilla FW only in the early iterations.</p><p>2. PGM surpasses ? 2 FW at after 70 iterations, and surpasses vanilla FW after 100 iterations.</p><p>3. PGD and ADMM are likely to surpass ? 2 FW and vanilla, too, if given sufficient number of iterations, as these do not show any sign of convergence yet.</p><p>The main observation here is that, the relative performance of the methods are different between the early (typically first 10 iterations) and the later stage. In ?6 we gave some hypotheses on why the proposed regularized Frank-Wolfe may work better than the others. Our main argument is that vanilla Frank-Wolfe is already much better than the other methods (in the first few iterations), and what we do is to equip it with the ability of effectively learning with SGD (potential improvements in terms of energy are rather a byproduct and not the main objective, as the improvements are sometimes small). Let us summarize this situation as follows:       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Generic regularized Frank-Wolfe for (approximately) solving MAP inference<ref type="bibr" target="#b5">(6)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Results on PASCAL VOC validation set using DeepLabv3+ and Potts dense CRF. (a) Comparison between CRF solvers (? k ? means ? k = k/(k + 2) ?k) shows that Frank-Wolfe variants clearly outperform the other methods in terms of energy minimization. (b) Performance of regularized Frank-Wolfe can be greatly affected by ?, but it can still achieve lower energies than the other methods for a large range of ?. (c) Vanilla Frank-Wolfe completely fails to learn because of the zero-gradient issue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>AppendicesA</head><label></label><figDesc>Details on special cases of regularized Frank-Wolfe inference 15 A.1 Algorithms based on QP relaxation with vanilla Frank-Wolfe . . . . . . . . . . . . . . . . . . . 15 A.2 Algorithms based on LP relaxation with vanilla Frank-Wolfe . . . . . . . . . . . . . . . . . . . 16 A.3 Algorithms based on the concave-convex procedure . . . . . . . . . . . . . . . . . . . . . . . . 17 A.4 Summary of special cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B Detailed convergence analysis 18 B.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 B.2 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C Proofs of other theoretical results 25 C.1 Vanilla Frank-Wolfe fails to learn: the zero-gradient issue . . . . . . . . . . . . . . . . . . . . 25 C.2 Proof of the relaxation tightness (Theorem 2) . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 D Implementation details of all methods 27 D.1 Euclidean Frank-Wolfe (?2FW) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 D.2 Entropic Frank-Wolfe (eFW) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 D.3 Projected gradient descent (PGD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 D.4 Fast proximal gradient method (PGM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 D.5 Entropic mirror descent (EMD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 D.6 Alternating direction method of multipliers (ADMM) . . . . . . . . . . . . . . . . . . . . . . . 30 E Detailed experimental setup and environment 30 E.1 CNN-CRF architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 E.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 E.3 CNN training recipes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 E.4 Training time and memory footprint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 F Additional results 32 F.1 Detailed results on the test sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 F.2 Results for trainable ? k and ? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 F.3 Results for fined-grained analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 F.4 Additional inference results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 19 )</head><label>19</label><figDesc>As presented in ?3.4, Sontag and Jaakkola, Meshi et al., Tang et al. [67, 52, 69] apply vanilla Frank-Wolfe to minimize a regularized LP energy: min x?XLP E LP (x; ? ? ?) + r(x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>?10 3 (</head><label>3</label><figDesc>b) Zoomed version of the left figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :</head><label>2</label><figDesc>CRF energy averaged over a subset of 10 val images of PASCAL VOC using DeepLabv3+ and Potts dense CRF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figures 3 and 4</head><label>4</label><figDesc>show more results for the inference experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Energy per CRF iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Per ?, at the 5 th CRF iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 :</head><label>3</label><figDesc>CRF energy averaged over 1449 val images of PASCAL VOC using DeepLabv3+ and Potts dense CRF. (a) Comparison between regularized Frank-Wolfe and the other methods for some selected values of the regularization weight ?. (b) Results of regularized Frank-Wolfe for different values of ?. (c) Energy per ? after 5 iterations. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Energy per CRF iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Per ?, at the 5 th CRF iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 4 :</head><label>4</label><figDesc>CRF energy averaged over 500 val images of Cityscapes using DeepLabv3+ and Potts dense CRF. (a) Comparison between regularized Frank-Wolfe and the other methods for some selected values of the regularization weight ?. (b) Results of regularized Frank-Wolfe for different values of ?. (c) Energy per ? after 5 iterations. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where the bound B k is given as follows:</figDesc><table><row><cell>constant stepsize</cell><cell>constant step length</cell><cell>non-summable</cell><cell>adaptive or</cell></row><row><cell></cell><cell></cell><cell></cell><cell>line search (14)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>PGD PGM ADMM MF FW eFW .7 eFW .3 ? 2 FW VOC DL3 81.83 82.23 82.23 82.22 82.21 82.27 82.26 82.29 82.29</figDesc><table /><note>DL3+ 82.89 83.36 83.37 83.38 83.45 83.43 83.45 83.48 83.50 CITY DL3 76.73 76.88 76.86 76.95 76.97 76.86 76.99 76.99 77.03 DL3+ 79.55 79.64 79.63 79.66 79.63 79.64 79.65 79.66 79.66</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Validation mIoU using a Potts CRF on top of the pre- trained CNN models. DL means DeepLab.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>PGD PGM ADMM MF eFW .7 eFW .3 ? 2 FW VOC DL3 81.83 83.69 CITY DL3+ 79.55 79.80 79.62 79.62 79.74 79.70 79.58 79.95</figDesc><table><row><cell>?0.20</cell><cell>83.75 ?0.23</cell><cell>83.68 ?0.06</cell><cell>83.69 ?0.10</cell><cell>83.50 ?0.10</cell><cell>83.25 ?0.20</cell><cell>83.75 ?0.13</cell></row><row><cell>DL3+ 82.89 84.82 ?0.23</cell><cell>84.79 ?0.20</cell><cell>84.83 ?0.06</cell><cell>84.87 ?0.17</cell><cell>84.64 ?0.23</cell><cell>84.50 ?0.16</cell><cell>85.14 ?0.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Validation mIoU under joint training. For PASCAL VOC, we report the mean and standard deviation from 5 runs.Again, ? 2 FW consistently achieves the best results. Interestingly, while eFW .3 achieved similar performance to ? 2 FW in terms of energy minimization(Figure 1a and Table 1), its performance is worse in joint training. Compared toTable 1, we see that joint training produced much larger improvements over the CNNs, up to 2.25% on PASCAL VOC and 0.4% on Cityscapes. DeepLabv3+ with ? 2 FW CRF 88.0 83.6</figDesc><table><row><cell>Model</cell><cell>VOC CITY</cell></row><row><cell>DeepLabv3+ [17]</cell><cell>87.8 82.1</cell></row><row><cell>DeepLabv3+ (this work)</cell><cell>87.6 83.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance on the test sets. Submission URLs are given in Appendix F.1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Summary of special cases of regularized Frank-Wolfe.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Performance of our reproduced DeepLab models compared to the original papers<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. The mIoU scores are obtained on the val sets, without test time augmentation. minutes (1.2-1.3? slower) for all inference methods (here we should note that CRF's running time is dominated by computing Px at each step, which is why the running is similar across the methods). In terms of memory usage, DeepLabv3+ takes ?27.7GB (per GPU for 4 images) while DeepLabv3 takes ?15.6GB. We found that the additional memory usage of the CRF (which has only 1323 trainable parameters) are negligible for the Frank-Wolfe variants as well as for PGD, while PGM and ADMM require an extra amount of ?300MB (probably due to the additional storage of the variable y at each iteration, see ?D).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>DeepLabv3+ with ? 2 FW CRF 88.0 2 83.6 4 1 http://host.robots.ox.ac.uk:8080/anonymous/BUXULK.html 2 http://host.robots.ox.ac.uk:8080/anonymous/YFJJLW.html 3 https://www.cityscapes-dataset.com/anonymous-results/?id=845bd062fddae249ec0f4987d30f2f9be6e6716654513e7e6733d3f56e976532 4 https://www.cityscapes-dataset.com/anonymous-results/?id=84e788da7c55eeeb4840b70407ed665006494c99e5d34e0bd8704d66d9c8b864</figDesc><table><row><cell>Model</cell><cell cols="2">PASCAL VOC Cityscapes</cell></row><row><cell>DeepLabv3+ [17]</cell><cell>87.8</cell><cell>82.1</cell></row><row><cell>DeepLabv3+ (this work)</cell><cell>87.6 1</cell><cell>83.5 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Performance on the test sets.</figDesc><table /><note>F.2 Results for trainable ? k and ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Comparison between trainable and fixed ? and ? k Method Steps mIoU background aeroplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor CNN 82.89 95.79 91.80 44.89 89.92 71.49 83.54 94.68 91.54 95.42 52.36 95.51 70.25 93.63 93.08 88.27 90.20 68.03 92.62 66.95 92.33 78.45 ?2FW 5 85.51 96.66 93.56 60.56 90.47 80.23 83.51 96.94 91.68 95.38 54.92 95.87 76.11 94.01 93.43 89.45 91.46 69.95 93.59 71.16 95.69 81.00 10 85.52 96.67 93.56 60.49 90.47 80.23 83.53 96.92 91.68 95.38 55.11 95.87 76.16 94.00 93.41 89.43 91.45 69.98 93.57 71.39 95.67 80.95 25 85.56 96.67 93.57 60.37 90.47 80.88 83.50 96.90 91.66 95.38 55.32 95.86 76.25 93.98 93.38 89.41 91.44 69.99 93.53 71.55 95.66 80.92 eFW 5 84.55 96.33 94.88 55.66 90.74 75.54 83.63 95.58 89.60 94.71 54.33 95.93 75.78 93.84 93.14 91.21 91.02 69.56 92.96 69.87 90.60 80.65 10 84.60 96.34 94.88 55.66 90.73 76.53 83.63 95.58 89.58 94.70 54.33 95.97 75.81 93.84 93.14 91.22 91.02 69.54 93.02 69.89 90.60 80.66 25 84.55 96.33 94.88 55.66 90.73 75.47 83.63 95.58 89.60 94.70 54.33 95.97 75.81 93.84 93.14 91.22 91.02 69.54 93.02 69.89 90.60 80.66</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Energy per CRF iteration.</figDesc><table><row><cell>?2 ?1.9 ?10 3 ?1.8</cell><cell>eFW?=.25 ?2FW?=1 FW PGD PGM ADMM MF</cell><cell>?2.24 ?2.22</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?2.1</cell><cell></cell><cell>?2.26</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?2.2</cell><cell></cell><cell>?2.28</cell><cell></cell><cell></cell><cell></cell></row><row><cell>?2.3</cell><cell cols="2">40 (a) 20 60 80 100</cell><cell>40</cell><cell>60</cell><cell>80</cell><cell>100</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="35">35th Conference on Neural Information Processing Systems (NeurIPS 2021).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that backpropagation typically requires the operations to be differentiable (at least) almost everywhere, which p k satisfies. Therefore, the issue here does not lie in differentiability, but in the resulting zero gradients. Blackbox differentiation<ref type="bibr" target="#b60">[60]</ref> can deal with this scenario, but it is limited to LP relaxations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Further mild conditions are required for convergence ( ?4.1).4  In proximal gradient and mirror descent, the current iterate is constrained to stay close to the previous one, while this is not the case in our method. See Appendix D for the details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In the original CCCP<ref type="bibr" target="#b75">[75]</ref>, g is differentiable, thus the update becomes ??f (x k ) = ?g(x k+1 ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Note that g is still assumed to be convex, so that the subproblem (10) can be solved to global optimality.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">If both f and g are convex, a better rate of O(1/k) exists<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b74">74]</ref>. In addition, if g is the indicator function of a convex set X (i.e., vanilla Frank-Wolfe) and either f or X is strongly convex, a linear rate can be obtained<ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b59">59]</ref>. Note that we use quite different machinery from all these analyses, due to the nonconvexity.<ref type="bibr" target="#b7">8</ref> A sufficient condition is r being continuous, as X is compact.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">While we use the same number of iterations at test time to simplify the evaluation protocol, it should be noted that using more iterations could be beneficial. See Appendix F.3 for some results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://github.com/MiguelMonteiro/permutohedral_lattice</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>This work was supported in part by the ANR grant AVENUE (ANR-18-CE23-0011), and was partly done when the first author was affiliated with Manifold Perception (mption.com). The experiments were performed using HPC resources from GENCI-IDRIS (Grants 2020-AD011011321 and 2020-AD011011881). The authors thank the anonymous reviewers and meta-reviewer for their constructive feedback that helped improve the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Applying the two above inequalities, together with the triangle and Cauchy-Schwarz inequalities we have:</p><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation details of all methods</head><p>We present the implementation details for all the methods presented in the experiments ( ?5). Recall that our problem of interest is</p><p>and that the same initialization x 0 = softmax(?u) is used for all methods.</p><p>High-dimensional filtering for gradient computation For all methods, we need to compute the energy gradient ?E(x) = Px + u at each iteration, where the evaluation of Px is an expensive operation because the graph is fully-connected (i.e., P is dense). Fortunately, since the pairwise potentials are Gaussian, this multiplication can be performed efficiently (and approximately) in O(nd) time using high-dimensional filtering, which is the key idea behind the original dense CRFs paper <ref type="bibr" target="#b33">[34]</ref>. We refer to this reference for more details. Our code is based on the efficient GPU implementation of Monteiro et al. <ref type="bibr" target="#b54">[54]</ref>. <ref type="bibr" target="#b9">10</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Euclidean Frank-Wolfe (? 2 FW)</head><p>We give the details for the main update (11) of Euclidean Frank-Wolfe as presented in ?3.3. This step follows from</p><p>Recall that ? X (v) denotes the projection of a vector v onto the set X . Recall also from (5) that X = x ? R nd : x ? 0, 1 ? x i = 1 ?i ? V , thus the projection on X clearly reduces to n independent projections onto the probability simplex</p><p>Projection onto the simplex is a rather well studied problem in the literature <ref type="bibr" target="#b17">[18]</ref>, and we present below the solution (that also shows how we implemented this operation).</p><p>Lemma 5. For a given vector c ? R d , the optimal solution z * to</p><p>Combining <ref type="formula">(123)</ref> and <ref type="formula">(124)</ref> we obtain</p><p>In other words, z = softmax(?c).</p><p>Remark. It is clear that the per-iteration complexity of Entropic Frank-Wolfe is O(nd), which is the same as in vanilla Frank-Wolfe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Projected gradient descent (PGD)</head><p>This algorithm consists in the following updates:</p><p>where the stepsize ? k follows one of the schemes presented in ?4.1. It is worth noting that this variant of PGD is eligible to exact line search <ref type="bibr" target="#b13">(14)</ref>. We observe in our experiments that using the line search scheme produces the same results as setting ? k = 1. Thus we used this constant scheme for both training and prediction. The same applies to the Frank-Wolfe variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Fast proximal gradient method (PGM)</head><p>The original PGM <ref type="bibr" target="#b48">[48]</ref> consists in updating</p><p>which can be re-written as</p><p>).</p><p>(128) The above is precisely another variant of PGD (which is not eligible to exact line search <ref type="bibr" target="#b13">(14)</ref>). While this algorithm is also supported by our implementation, the results presented in ?5 are obtained using another variant called the fast PGM, also known as FISTA <ref type="bibr" target="#b8">[9]</ref>. This algorithm consists in the following updates, where y 0 = x 0 = softmax(?u) and t 0 = 1:</p><p>While the optimal value of ? k can be determined using backtracking <ref type="bibr" target="#b6">[7]</ref>, this process is very expensive as it requires evaluating the energy many times. Therefore, in practice, we use the constant scheme ? k = ? ? [0, 1]. Doing a grid search on a random subset of 10 validation images, we found (again) that ? k = 1 is the best, and thus it is used for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Entropic mirror descent (EMD)</head><p>Mirror descent (MD) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b55">55]</ref> is a generalization of PGM to a more general distance function. Each iteration of MD takes the following form:</p><p>where ? : X ? R is a convex and continuously differentiable function on the interior of X , and B ? : X ? X ? R is its associated Bregman divergence, defined by</p><p>Clearly, for ?(x) = 1 2 x 2 2 we recover the PGM update (127). We provide an implementation for the so-called entropic variant of mirror descent <ref type="bibr" target="#b7">[8]</ref>, corresponding to choosing ? to be the negative entropy:</p><p>x is log x is .</p><p>(134)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient linear programming for dense crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudy</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Pawan</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Proximal mean-field for neural network quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4871" to="4880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The limited multi-label projection layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08707</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Higher order conditional random fields in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="524" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Duality between subgradient and conditional gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="129" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Principled parallel mean-field inference for discrete random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baqu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5848" to="5857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">First-order methods in optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mirror descent and nonlinear projected subgradient methods for convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res. Lett</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="167" to="175" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkage-thresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Definition of a consistent labeling as a global extremum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berthod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page" from="339" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal continuous dr-submodular maximization and applications to provable mean field inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Yatao An Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="644" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A generalized conditional gradient method and its connection to an iterative shrinkage method. Computational Optimization and Applications, Advanced online publication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><forename type="middle">A</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Maass</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10589-007-9083-3</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="402" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast projection onto the simplex and the ?1 ball</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Condat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="575" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient continuous relaxations for dense crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudy</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M Pawan</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marguerite</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">New analysis and results for the frank-wolfe method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="199" to="230" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunihiko</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conditional gradient algorithms for normregularized smooth convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadi</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="75" to="112" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sdca-powered inexact dual augmented lagrangian method for fast CRF learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="988" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Revisiting frank-wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th international conference on machine learning</title>
		<meeting>the 30th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="427" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accelerated dual decomposition for map inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Affine invariant analysis of frankwolfe on strongly convex sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kerdreux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewis</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Scieur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5398" to="5408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="513" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the global linear convergence of frank-wolfe optimization variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Julien</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="496" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
		<idno>1-55860-778-1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The complexity of large-scale convex programming under a linear optimization oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.5550</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A projected gradient descent method for crf inference allowing end-to-end training of arbitrary pairwise potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?ns</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="564" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Alternating direction graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khu? L?-Huu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4914" to="4922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Continuous relaxation of map inference: A nonconvex perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khu? L?-Huu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5533" to="5541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Not all pixels are equal: Difficultyaware semantic segmentation via deep layer cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3193" to="3202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
	<note>Anton Van Den Hengel, and Ian Reid</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The representation of the cumulative rounding error of an algorithm as a taylor expansion of the local rounding errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seppo</forename><surname>Linnainmaa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="page" from="6" to="7" />
		</imprint>
		<respStmt>
			<orgName>University of Helsinki</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
	<note>in Finnish</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Splitting algorithms for the sum of two nonlinear operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Louis</forename><surname>Lions</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Mercier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="964" to="979" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A unified optimization view on generalized matching pursuit and frank-wolfe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="860" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Optimization with first-order surrogate functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="783" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Smooth and strong: Map inference with linear convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Meshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="298" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A minimization method for the sum of a convex function and a continuously differentiable function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Mine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="23" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>M?rio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arlindo L</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliveira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07464</idno>
		<title level="m">Conditional random fields as recurrent neural networks for 3d medical imaging segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Problem complexity and method efficiency in optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadij</forename><surname>Semenovi? Nemirovskij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Borisovich Yudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Smooth minimization of non-smooth functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
		<idno>0025-5610</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A regularized framework for sparse and structured neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3338" to="3348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Statistical field theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Parisi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Linearly convergent frank-wolfe without line-search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>N?giar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Askari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">V?t Musil, Georg Martius, and Michal Rolinek. Differentiation of blackbox combinatorial solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Vlastelica Pogancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Quadratic programming relaxations for metric labeling and markov random field map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Semantic segmentation via structured patch prediction, context crf and guidance crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1953" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Finding maps for belief networks is np-hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimony</forename><surname>Solomon Eyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="399" to="410" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML-31st International Conference on Machine Learning</title>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1611" to="1619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">New outer bounds on the marginal polytope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1393" to="1400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Mixed context networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05854</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Bethe learning of graphical models via MAP decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Ruozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS, volume 51 of JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Map estimation via agreement on trees: message-passing and linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3697" to="3717" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.07122" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Generalized conditional gradient for sparse estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5279" to="5324" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The concave-convex procedure (cccp)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vanilla Frank-Wolfe outperforms other first-order methods such as PGD, PGM, and ADMM during the first few iterations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE international conference on computer vision. and may be surpassed at a later stage, as already shown</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">For SGD learning, in which only a small number of iterations (due to the vanishing/exploding gradient problems, as already observed in previous work [76]), this behavior (reaching quickly a very low energy) of vanilla Frank-Wolfe is highly desirable</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Unfortunately, vanilla Frank-Wolfe iterates are piecewise constant and thus the resulting gradients are zero almost everywhere, which makes learning through backpropagation impossible</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Our regularized Frank-Wolfe is designed to precisely solve this zero-gradient issue</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
