<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SENTRY: Selective Entropy Optimization via Committee Consistency for Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viraj</forename><surname>Prabhu</surname></persName>
							<email>virajp@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivam</forename><surname>Khare</surname></persName>
							<email>skhare31@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deeksha</forename><surname>Kartik</surname></persName>
							<email>dkartik3@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SENTRY: Selective Entropy Optimization via Committee Consistency for Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many existing approaches for unsupervised domain adaptation (UDA) focus on adapting under only data distribution shift and offer limited success under additional crossdomain label distribution shift. Recent work based on selftraining using target pseudolabels has shown promise, but on challenging shifts pseudolabels may be highly unreliable and using them for self-training may lead to error accumulation and domain misalignment. We propose Selective Entropy Optimization via Committee Consistency (SENTRY), a UDA algorithm that judges the reliability of a target instance based on its predictive consistency under a committee of random image transformations. Our algorithm then selectively minimizes predictive entropy to increase confidence on highly consistent target instances, while maximizing predictive entropy to reduce confidence on highly inconsistent ones. In combination with pseudolabelbased approximate target class balancing, our approach leads to significant improvements over the state-of-the-art on 27/31 domain shifts from standard UDA benchmarks as well as benchmarks designed to stress-test adaptation under label distribution shift. Our code is available at https://github.com/virajprabhu/SENTRY.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unsupervised domain adaptation (UDA) learns to transfer a predictive model from a labeled source domain to an unlabeled target domain. The particular instantiation of learning under covariate shift has been extensively studied within the computer vision community <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. However, many modern UDA methods, such as distribution matching based techniques, implicitly assume that the task label distribution does not change across domains, i.e P S (y) = P T (y). When such an assumption is violated, distribution matching is not expected to succeed <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>In many real-world adaptation scenarios, one may encounter data distribution (i.e. covariate) shift across domains together with label distribution shift (LDS). For instance, a source dataset can be curated to have a balanced label distribution while a naturally arising target dataset may follow a  <ref type="figure">Figure 1</ref>: Top: Conventional entropy-minimization based approaches for unsupervised domain adaptation (UDA) operate by increasing model confidence on unlabeled target instances. Under strong distribution shifts, some instances may initially be misaligned and entropy minimization can lead to error accumulation. Bottom: We propose Selective Entropy Optimization via Committee Consistency (SENTRY), a UDA algorithm that i) identifies reliable target instances based on their predictive consistency under a set of random image transformations, and ii) selectively optimizes model entropy on these instances to induce domain alignment.</p><p>power law label distribution, as some categories naturally occur more often than others (e.g. DomainNet <ref type="bibr" target="#b32">[33]</ref>, LVIS <ref type="bibr" target="#b16">[17]</ref>, and MSCOCO <ref type="bibr" target="#b23">[24]</ref>). In order to make domain adaptation broadly applicable, it is critical to develop UDA algorithms that can operate under joint data and label distribution shift.</p><p>Recent works have attempted to address the problem of joint data and label distribution shift <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>, but these approaches can be unstable as they rely on self-training using often noisy pseudo-labels or conditional entropy minimization <ref type="bibr" target="#b22">[23]</ref> over potentially miscalibrated predictions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>. Thus, when learning with unconstrained self-training, early mistakes can result in error accumulation <ref type="bibr" target="#b6">[7]</ref> and significant domain misalignment (see <ref type="bibr">Figure 1,</ref><ref type="bibr">top)</ref>.</p><p>To address the problem of error accumulation arising from unconstrained self-training, we propose Selective Entropy Optimization via Committee Consistency (SENTRY), a novel selective self-training algorithm for UDA. First, rather than using model confidence which can be miscalibrated under a domain shift <ref type="bibr" target="#b40">[41]</ref>, we identify reliable target instances for self-training based on their predictive consistency under a committee of random, label-preserving image transformations. Such consistency checks have been found to be a reliable way to detect model errors <ref type="bibr" target="#b1">[2]</ref>. Having identified reliable and unreliable target instances, we then perform selective entropy optimization: we consider a highly consistent target instance as likely correctly aligned, and increase model confidence by minimizing predictive entropy for such an instance. Similarly, we consider an instance with high predictive inconsistency over transformations as likely misaligned, and reduce model confidence by maximizing predictive entropy. See <ref type="figure">Figure 1</ref> (bottom). Contributions. We propose SENTRY, an algorithm for unsupervised adaptation under simultaneous data and label distribution shift. We make the following contributions:</p><p>1. A novel selection criterion that identifies reliable target instances for self-training based on predictive consistency over a committee of random, label-preserving image transformations. 2. A selective entropy optimization objective that minimizes predictive entropy (increasing confidence) on highly consistent target instances, and maximizes it (reducing confidence) on highly inconsistent ones. 3. We propose using class-balanced sampling on the source (using labels) and target (using pseudolabels), and find it to complement adaptation under LDS. 4. SENTRY sets a new state-of-the-art on 27/31 domain shifts belonging to both standard and LDS versions of several DA benchmarks for classification, including DomainNet <ref type="bibr" target="#b32">[33]</ref>, OfficeHome <ref type="bibr" target="#b48">[49]</ref>, and VisDA <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Domain Adaptation (UDA). The task of transferring models from a labeled source to an unlabeled target domain has seen considerable progress <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b47">48]</ref>. Many approaches align feature spaces via directly minimizing domain discrepancy statistics <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b47">48]</ref>. Recently, distribution matching (DM) via domain-adversarial learning has become a prominent UDA paradigm <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b55">56]</ref>. Such DM-based methods however achieve limited success in the presence of additional label distribution shift (LDS). Some prior work has studied the problem of UDA under LDS, proposing class-weighted domain discrepancy measures <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b53">54]</ref>, generative approaches for pair-wise feature matching <ref type="bibr" target="#b43">[44]</ref>, or asymmetrically-relaxed distribution alignment <ref type="bibr" target="#b54">[55]</ref>. Some prior work in UDA under LDS additionally assumes that the conditional input distribution does not change across domains i.e. p S (y) ? = p T (y), p S (x|y) = p T (x|y) (referred to as "label shift" <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b42">43]</ref>). We tackle the problem of UDA under simultaneous covariate and label distribution shift, without making additional assumptions. Self-training for UDA. Recently, training on model predictions or self-training has proved to be a promising approach for UDA under LDS <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45]</ref>. This typically involves supervised training on confidently predicted target pseudolabels <ref type="bibr" target="#b44">[45]</ref>, confidence regularization [57], or conditional entropy minimization <ref type="bibr" target="#b14">[15]</ref> on target instances <ref type="bibr" target="#b22">[23]</ref>. However, unconstrained self-training can lead to error accumulation. To address this, we propose a selective self-training strategy that first identifies reliable instances for self-training and selectively optimizes model entropy on those. Predictive Consistency. Predictive consistency under augmentations has been found to be useful in several capacities -as a regularizer in supervised learning <ref type="bibr" target="#b10">[11]</ref>, selfsupervised representation learning <ref type="bibr" target="#b7">[8]</ref>, semi-supervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53]</ref>, and UDA <ref type="bibr" target="#b22">[23]</ref>. Bahat et al. <ref type="bibr" target="#b1">[2]</ref> find consistency under image transformations to be a reliable indicator of model errors. Unlike prior work which optimizes for invariance across augmentations, we use predictive consistency under a committee of random image transforms to detect reliable instances for alignment, and selectively optimize model entropy on such instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We address the problem of unsupervised domain adaptation (UDA) of a model trained on a labeled source domain to an unlabeled target domain. In addition to covariate shift across domains, we focus on the practical scenario of additional cross-domain label distribution shift (LDS), and present a selective self-training algorithm for UDA that leads to reliable domain alignment in such a setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation</head><p>Let X and Y denote input and ouput spaces, with the goal being to learn a CNN mapping h : X ? Y parameterized by ?. In unsupervised DA we are given access to labeled source instances (x S , y S ) ? P S (X , Y), and unlabeled target instances x T ? P T (X ), where S and T correspond to source and target domains. We consider DA in the context of C-way image classification: the inputs x are images, and labels y are categorical variables y ? {1, 2, .., C}. For an instance x, let p ? (y|x) denote the final probabilistic output from the model. For each target instance x T ? P T (X ), we estimate a pseudolabel? = argmax p ? (y|x T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Preliminaries: UDA via entropy minimization</head><p>Unsupervised domain adaptation typically follows a twostage training pipeline: source training, followed by target adaptation. In the first stage, a model is trained on the labeled source domain in a supervised fashion, minimizing a cross-  <ref type="figure">Figure 2</ref>: We propose Selective Entropy Optimization via Committee Consistency (SENTRY) for unsupervised DA. For each target instance, we generate a committee of random, label-preserving image transformations. A consistency checker then computes the consistency between model predictions for the original and augmented versions. The algorithm then minimizes predictive entropy (increasing model confidence) on highly consistent target instances, and maximizes predictive entropy (reducing model confidence) on highly inconsistent ones. entropy loss with respect to ground truth labels.</p><formula xml:id="formula_0">L CE = E (x S ,y S )?P S [L CE (h(x S ), y S )]<label>(1)</label></formula><p>In the second stage, the trained source model is adapted to the target with the use of unlabeled target and labeled source data. Recently, self-training via conditional entropy minimization (CEM) <ref type="bibr" target="#b14">[15]</ref> has been shown to lead to strong performance for domain adaptation <ref type="bibr" target="#b36">[37]</ref>. This approach optimizes model parameters to minimize conditional entropy on unlabeled target data H ? (y|x). The entropy minimization objective L EN T is given by:</p><formula xml:id="formula_1">L EN T = E x T ?P T [H ? (y|x T )] = E x T ?P T C c=1 ?p ? (y = c|x T ) log p ? (y = c|x T )<label>(2)</label></formula><p>However, in many real-world scenarios, in addition to covariate shift, label distributions across domains might also shift. Further, there might also be significant label imbalance within the target domain. In such cases, naive CEM has been found to potentially encourage trivial solutions of only predicting the majority class <ref type="bibr" target="#b22">[23]</ref>. Li et al. <ref type="bibr" target="#b22">[23]</ref> regularize CEM with an "information-entropy" objective L IE that encourages the model to make diverse predictions over unlabeled target instances. This is achieved by computing a distribution over classes predicted by the model for the last-Q instances, denoted by q(?), and updating parameters to maximize entropy over these predictions. This method is shown to help with domain alignment in the presence of label-distribution shift [23] 1 . L IE is defined as: <ref type="bibr" target="#b0">1</ref> The objective is referred to as "mutual information maximization" in Li et al. <ref type="bibr" target="#b22">[23]</ref> CEM and error accumulation. While conditional entropy minimization has been a part of many successful approaches for semi-supervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, few-shot learning <ref type="bibr" target="#b12">[13]</ref>, and more recently, UDA <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref>, it suffers from a key challenge in the case of domain adaptation. Intuitively, conditional entropy minimization encourages the model to make confident predictions on unlabeled target data. This makes its success highly dependent on its initialization. Under a good initialization, categories may be reasonably aligned across source and target domains after source training, and such self-training works well. However, under strong domain shifts, several categories may initially be misaligned across domains, often systematically so, and entropy minimization will only lead to reinforcing such errors.</p><formula xml:id="formula_2">L IE = E x T ?P T C c=1 p ? (y = c|x T ) log q(? = c)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SENTRY: Selective Entropy Optimization via Committee Consistency</head><p>Predictive consistency-based selection. To address the problem of error accumulation under CEM, we propose selective optimization on well-aligned instances. The question then becomes: how can we identify reliable instances? One possibility is to use top-1 softmax confidence (or alternatively, predictive entropy), and only self-train on highly confident instances, as done in prior work <ref type="bibr" target="#b44">[45]</ref>. However, under a distribution shift, such confidence measures tend to be miscalibrated and are often unreliable <ref type="bibr" target="#b40">[41]</ref>. Instead, we propose using predictive consistency under a committee of label-preserving image transformations as a more robust measure for instance selection. For a target instance x T ? P T , we generate a committee of k transformed versions {a 1 (x T ), a 2 (x T ), ..., a k (x T )}. We make predictions for each of these k transformed versions, and measure consistency between the model's prediction for the original image and for each of its k augmented versions. In practice, we use a simple majority voting scheme: if the model's prediction for a majority of augmented versions matches its prediction on the original image, we consider the instance as "consistent". Similarly, if the prediction for a majority of augmented versions does not match the original prediction, we mark it as "inconsistent". Selective Entropy Optimization. Having identified consistent and inconsistent instances, we perform Selective Entropy Optimization (SENTRY). First, for an instance marked as consistent, we increase model confidence by minimizing predictive entropy <ref type="bibr" target="#b14">[15]</ref> with respect to one of its consistent augmented versions.</p><p>As described previously, some target instances may be misaligned under a domain shift. Entropy minimization on such instances would increase model confidence, reinforcing such errors. Instead, having identified such an instance via predictive inconsistency, we reduce model confidence by maximizing predictive entropy <ref type="bibr" target="#b34">[35]</ref> with respect to one of its inconsistent augmented versions. While the former encourages confident predictions on highly consistent instances, the latter reduces model confidence on highly inconsistent and likely misaligned instances. In Sec. 4.6, we provide further intuition into the behavior of entropy maximization by illustrating its similarity to a binary cross-entropy loss with respect to the ground truth label for an incorrectly classified example in the binary classification case.</p><p>Without loss of generality, we minimize/maximize entropy with respect to the last consistent/inconsistent transformed version in our experiments. Our selective entropy optimization objective L SENTRY is given by:</p><formula xml:id="formula_3">L SENTRY (x T ) = +H ? (y|a i (x T )) if consistent ?H ? (y|a j (x T )) if inconsistent<label>(4)</label></formula><p>Here i and j denote the index of the last consistent and inconsistent transformed version, respectively. Such an approach may raise two concerns: First, that entropy minimization only on consistent instances might lead to the exclusion of a large percentage of target instances. Second, that indefinite entropy maximization on inconsistent target instances might prove detrimental to learning. Both of these concerns are addressed via the augmentation invariance regularizer built into our objective, which leads to an adaptive selection strategy that we now discuss. Adaptive selection via augmentation invariance regularization. For instances marked as consistent, our approach minimizes entropy with respect to its last consistent augmented version rather than with respect to the original image itself. This yields two benefits: First, this builds data augmentation into the entropy minimization objective, which helps reduce overfitting. Second, it encourages invariance to the same set of augmentations that is used for selecting instances. We find that this makes our selection strategy adaptive, wherein an increasing percentage of target instances are selected for entropy minimization over the course of training, and consequently a decreasing percentage of target instances are selected for entropy maximization.</p><formula xml:id="formula_4">Algorithm 1 SENTRY Optimization 1: Input: X S , Y S , X T , Q, ? 2: for all x (i) T ? X T do ? Init target pseudo-labels 3:? (i) T ? argmax p ? (y|x (i) T ) 4: SrcLoader ? ClassBalancedSampler(X S , Y S ) 5: TgtLoader ? ClassBalancedSampler(X T ,? T ) 6: q ? Queue(size=Q) 7: for epoch ? 1 to MAX_EPOCH do 8:</formula><p>for x S , y S in SrcLoader and x T in TgtLoader do 9:? T ? argmax p ? (y|x T ) ? Clean prediction <ref type="bibr" target="#b9">10</ref>:</p><formula xml:id="formula_5">{a 1 (x T ), . . . , a k (x T )} ? RandAugment(x T ) 11: C ? {ai(xT )|?T = argmax p?(y|ai(xT ))} k i=1 12: IC ? {ai(xT )|?T ? = argmax p?(y|ai(xT ))} k i=1 13:</formula><p>if len(C) &gt; len(IC) then ? Consistent 14:</p><formula xml:id="formula_6">L SENTRY = H ? (y|C.last()) 15: else ? Inconsistent 16: L SENTRY = ?H ? (y|IC.last()) 17: Update(? T ,? T ) 18:</formula><p>q.enqueue(? T ) ? Update pseudo-label queue <ref type="bibr" target="#b18">19</ref>:</p><formula xml:id="formula_7">Minimize L SENTRY + L IE (q) + L CE (x S , y S ) 20: TgtLoader ? ClassBalancedSampler(X T ,? T )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Overcoming LDS via pseudo class balancing</head><p>Under LDS, methods often have to adapt in the presence of severe label imbalance. While label imbalance on the source domain often leads to poor performance on tail classes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b50">51]</ref>, adapting to an imbalanced target often results in poor performance on head classes <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b51">52]</ref>. To overcome this, we employ a simple class-balanced sampling strategy. On the source domain, we perform class-balanced sampling using ground truth labels. On the target domain, we approximate the label distribution via pseudolabels, and perform approximate class-balanced sampling <ref type="bibr">[58]</ref>.</p><p>Such balancing also complements the target informationentropy loss L IE [23] (Eq. 3). To recap, L IE encourages a uniform distribution over predictions. Under severe label imbalance, it is possible to sample highly label-imbalanced batches (with most instances belonging to head classes) and so encouraging a uniform distribution over predictions can adversely affect learning. However, our class-balanced sampling strategy reduces the probability of such a scenario, and we find that it consistently improves performance.</p><p>Algorithm 1 details our full approach. The complete objective we optimize is given by:</p><formula xml:id="formula_8">argmin ? E (x S ,y S ) bal ?P S L CE + E x T pbal ? P T ? IE L IE + ? SENTRY L SENTRY<label>(5)</label></formula><p>where the ?'s denote loss weights, and bal ? and pbal ? denote balanced and pseudo class-balanced sampling.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first describe our experimental setup: datasets and metrics (Sec. 4.1), implementation details (Sec. 4.2), and baselines (Sec. 4.3). We then present our results (Sec. 4.4), ablation studies (Sec. 4.5), and analyze our approach (Sec. 4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Metrics</head><p>We report results on a mix of standard UDA benchmarks and specialized benchmarks designed to stress-test UDA methods under label distribution shift. DomainNet. DomainNet <ref type="bibr" target="#b32">[33]</ref> is a large UDA benchmark for image classification, containing 0.6 million images belonging to 6 domains spanning 345 categories. Due to labeling noise prevalent in its full version, we instead use the subset proposed in Tan et al. <ref type="bibr" target="#b44">[45]</ref>, which uses 40-commonly seen classes from 4 domains: Real (R), Clipart (C), Painting (P), and Sketch (S). As seen in <ref type="figure" target="#fig_1">Fig. 3</ref> (left), there exists a natural label distribution shift across domains, which makes it suitable for testing our method without manual subsampling. OfficeHome. OfficeHome <ref type="bibr" target="#b48">[49]</ref> is an image classificationbased benchmark containing 65 categories of objects found in office and home environments, spanning 4 domains: Realworld (Rw), Clipart (Cl), Product (Pr), and Art (Ar). We report performance on two versions: i) standard: the original dataset proposed in Venkateswara et al. <ref type="bibr" target="#b48">[49]</ref>, and ii) RS-UT: The Reverse-unbalanced Source (RS) and Unbalanced-Target (UT) version from Tan et al. <ref type="bibr" target="#b44">[45]</ref>, wherein source and target label distributions are manually long-tailed to be reversed versions of one another (see <ref type="figure" target="#fig_1">Fig. 3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>We use PyTorch <ref type="bibr" target="#b31">[32]</ref> for all experiments. On Domain-Net, OfficeHome, and VisDA2017, we modify the standard ResNet50 <ref type="bibr" target="#b17">[18]</ref> CNN architecture to a few-shot variant used in recent DA work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>: we replace the last linear layer with a C? way (for C classes) fully-connected layer with Xavier-initialized weights and no bias. We then L 2normalize activations flowing into this layer and feed its output to a softmax layer with a temperature T = 0.05. We match optimization details to Tan et al. <ref type="bibr" target="#b44">[45]</ref>. On DIGITS, we make similar modifications to the LeNet architecture and use T = 0.01 <ref type="bibr" target="#b18">[19]</ref>. For augmenting images for consistency checking, we use RandAugment <ref type="bibr" target="#b10">[11]</ref>, which sequentially applies N label-preserving image transformations randomly sampled from a set of 14 transforms. We set N = 3, use transformation severity M = 2.0, and use a committee of k = 3 transforms. We use class-balanced sampling on the source domain and pseudo class-balanced sampling on the target. We set ? IE and ? SENTRY to 0.1 and 1.0, and match InstaPBM to set Q=256 for the information entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baselines</head><p>As our primary baselines we use four state-of-the art UDA methods from prior work specifically designed for DA under LDS: i) COAL <ref type="bibr" target="#b44">[45]</ref>: Co-aligns feature and label distributions, using prototype-based conditional alignment via MME <ref type="bibr" target="#b36">[37]</ref>, and self-training on confidently-predicted pseudo-labels. ii) MDD + Implicit Alignment (I.A) <ref type="bibr" target="#b19">[20]</ref>: Uses target pseudolabels to construct N ?way (# classes per-batch) K?shot (# examples per class) dataloaders that are "aligned" (i.e. sample the same set of classes within a batch for both source and target), in conjunction with Margin Disparity Discrepancy <ref type="bibr" target="#b55">[56]</ref>, a strong UDA method, iii) InstaPBM <ref type="bibr" target="#b22">[23]</ref>: Proposes "predictive-behavior" matching, which entails matching properties of p ? (y|x) between source and target. This is achieved via optimizing a combination of mutual information maximization, contrastive, and mixup losses, and iv) F-DANN <ref type="bibr" target="#b51">[52]</ref>: Proposes an asymmetrically-relaxed distribution matching-based version of DANN <ref type="bibr" target="#b13">[14]</ref> to deal with LDS. COAL, InstaPBM, and  </p><formula xml:id="formula_9">Method R ? C R ? P R ? S C ? R C ? P C ? S P ? R P ? C P ? S S ? R S ? C S ? P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDD+I.</head><p>A. all make use of target pseudolabels, and COAL and InstaPBM are self-training based approaches.</p><p>For completeness, we also include results for additional baselines from Tan et al. <ref type="bibr" target="#b44">[45]</ref>: i) Conventional feature alignment-based UDA methods: DAN <ref type="bibr" target="#b25">[26]</ref>, JAN <ref type="bibr" target="#b28">[29]</ref>, DANN <ref type="bibr" target="#b13">[14]</ref>, MCD <ref type="bibr" target="#b36">[37]</ref>, and MDD <ref type="bibr" target="#b55">[56]</ref>, ii) BBSE <ref type="bibr" target="#b22">[23]</ref> which only aligns label distributions, iii) Methods that assume non-overlapping labeling spaces: PADA <ref type="bibr" target="#b4">[5]</ref>, ETN <ref type="bibr" target="#b5">[6]</ref>, and UAN <ref type="bibr" target="#b54">[55]</ref>. We also report results for FixMatch <ref type="bibr" target="#b41">[42]</ref>, a state-of-the-art self-training method for semi-supervised learning, on two benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>Results on label-shifted DA benchmarks. We present results on 12 shifts from DomainNet <ref type="table" target="#tab_3">(Table 1</ref>) and 6 shifts from OfficeHome RS?UT ( <ref type="table" target="#tab_4">Table 2</ref>). On DomainNet, SENTRY outperforms the next best performing method InstaPBM <ref type="bibr" target="#b22">[23]</ref> on every shift, and by 3.55% mean accuracy averaged across shifts. On OfficeHome RS-UT, SENTRY outperforms the next best performing method MDD+I.A [20] on 5 out of 6 shifts, and on average by 3.58% mean accuracy. Our method also significantly outperforms F-DANN <ref type="bibr" target="#b51">[52]</ref> (by 11.87% and 11.37%) and COAL <ref type="bibr" target="#b44">[45]</ref> (by 5.50% and 6.85%), which are both UDA strategies designed for adaptation under LDS. 57.6 JAN <ref type="bibr" target="#b28">[29]</ref> 58.3 CDAN <ref type="bibr" target="#b26">[27]</ref> 65.8 BSP <ref type="bibr" target="#b9">[10]</ref> 66.3 MDD <ref type="bibr" target="#b55">[56]</ref> 68.1 FixMatch <ref type="bibr" target="#b41">[42]</ref> 59.0 InstaPBM <ref type="bibr" target="#b22">[23]</ref> 69.2 MDD+I.A <ref type="bibr" target="#b19">[20]</ref> 69.5  <ref type="bibr" target="#b38">[39]</ref> 69.8 CDAN <ref type="bibr" target="#b26">[27]</ref> 70.0 FixMatch <ref type="bibr" target="#b41">[42]</ref> 64.9 MDD <ref type="bibr" target="#b55">[56]</ref> 74.6 MDD+I.A <ref type="bibr" target="#b19">[20]</ref> 75.8 InstaPBM <ref type="bibr" target="#b22">[23]</ref> 76.3 Results on standard DA benchmarks.  ing split, and use it as our unlabeled target train set (test set is unchanged). The long-tailing is performed by sampling from a Pareto distribution and subsampling, with class cardinality following the same sorted order as the source label distribution for simplicity. To systematically vary the degree of imbalance, we modulate the parameters of the Pareto distribution so as to generate a desired Imbalance Factor (IF) <ref type="bibr" target="#b11">[12]</ref>, computed as the ratio of the cardinality of the largest and smallest classes. Larger IF's represent a higher degree of imbalance. We thus create 3 splits with IF ? {20, 50, 100}, corresponding to varying label imbalance but with an identical amount of data (=14.5k instances). Further, we create a control version that also has 14.5k instances but possesses a balanced label distribution. <ref type="table" target="#tab_9">Table 4 (right)</ref> shows the resulting label distributions. We report per-class average accuracies in <ref type="table" target="#tab_9">Table 4</ref> (left). As baselines, we include a domain discrepancy based method (MMD <ref type="bibr" target="#b27">[28]</ref>), an adversarial DA method (DANN <ref type="bibr" target="#b13">[14]</ref>), as well as COAL <ref type="bibr" target="#b44">[45]</ref> and InstaPBM <ref type="bibr" target="#b22">[23]</ref>. Across methods, performance at higher imbalance factors is worse, illustrating the difficulty of adapting under severe label imbalance. However, SENTRY significantly outperforms baselines, even at higher imbalance factors, achieving 13.6% higher mean accuracy than the next competing method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SENTRY</head><formula xml:id="formula_10">SENTRY (Ours) 76.7 (b) VisDA2017</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablations</head><p>We now present ablations of SENTRY, our proposed approach, on the Clipart?Sketch from DomainNet and the Real World?Clipart shift from OfficeHome RS-UT. Selective optimization helps significantly (Tab. 5). We first measure the effect of performing entropy minimization on all samples, as done in prior work. We find this to perform significantly worse (by 10.7%, 11.9%) than our method! Clearly, consistency-based selective optimization is crucial. Entropy maximization helps consistently (Tab. 5). Next, we opt to only minimize entropy on consistent target instances, but do not perform entropy maximization. We find this to underperform against our min-max optimization (by 1.8%, 1.5%). Further, as an oracle approach, we use ground   truth target labels to determine whether an instance is correctly or incorrectly classified, and perform two experiments: entropy minimization on correct instances (and no maximization), and min-max entropy optimization on correct and incorrect instances. Selective min-max optimization again outperforms just minimization by 2% and 2.4%, showing that reducing confidence on misaligned instances helps. Ablating consistency checker. In Tables 6a, 6b, we vary the committee size k and number of RandAugment transforms N used by our consistency checker. We do not find our method to be very sensitive to either hyperparameter. In <ref type="table" target="#tab_12">Table 6c</ref>, we vary the voting strategy used to judge committee consistency and inconsistency. We experiment with majority voting (atleast k 2 + 1 votes needed) and unanimous voting (k votes needed), and find the former to generalize better. Gains are not simply due to stronger augmentation. To verify this, we continue using RandAugment (with N=1) for consistency checking, but backpropagate on the original (rather than augmented) target instances, effectively removing data augmentation entirely. On C?S and Rw?Cl, this achieves 73.1% and 52.6%, which is still 0.3% and 2.6% better than the next best baseline on each shift, despite not using any data augmentation for optimization at all. Pseudo class-balanced sampling helps. We find that classbalanced sampling using pseudolabels on the target improves per-class average accuracy over random sampling by 0.91% and 0.52% on C?S and Rw?Cl. In the absence of the target information entropy regularizer L IE , this performance gap grows to 2.9% and 3.7%. As explained in Sec 3.4, both objectives contribute towards overcoming LDS in similar ways, and we find here that using both together works best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Analysis</head><p>% of target instances selected over time. <ref type="figure" target="#fig_2">Fig. 4 (left)</ref> shows that the % of seen target instances selected for entropy minimization steadily increases over time, while that selected for entropy maximization decreases. This adaptive nature is a result of the augmentation invariance regularization built into our method (Sec. 3). In <ref type="figure" target="#fig_2">Fig. 4 (middle)</ref>, we measure the % of target instances selected for entropy minimization, per-class, at the end of the first and last epoch of adaptation. Despite no explicit class-conditioning, we find that this measure increases for all classes. Precision of consistency checker. <ref type="figure" target="#fig_2">Fig 4 (right)</ref> shows the precision of our consistency and inconsistency-based selection strategies at identifying instances that are actually correct and incorrect. As seen, committee-based consistency and inconsistency are both 75-80% precise at identifying correct and incorrect instances respectively. Per-class accuracy change. In the supplementary, we report the per-class accuracy after adaptation using our method, and contrast it against InstaPBM <ref type="bibr" target="#b22">[23]</ref>. On the C?S shift, SENTRY outperforms InstaPBM across 37/40 categories.</p><p>Computational efficiency. Compared to standard entropy minimization, SENTRY requires k (for committee size k) forward passes per iteration (to determine consistency), but no additional backward passes. SENTRY thus does not add a sizeable computational overhead over prior work.  <ref type="figure">p)</ref>]. Without loss of generality, assume an incorrect prediction with y = 0 and 0.5 ? p &lt; 1. In <ref type="figure" target="#fig_3">Fig. 5</ref> we show that in this case, gradients (? p L) for entropy maximization and BCE (wrt true class) are strongly correlated. Thus, after identifying misaligned target instances based on predictive inconsistency, entropy maximization has a similar effect as supervised training with respective to the true class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose SENTRY, an algorithm for unsupervised domain adaptation (UDA) under simultaneous data and label distribution shift. Unlike prior work that suffers from error accumulation arising from unconstrained self-training, SENTRY first judges the reliability of a target instance based on its predictive consistency under a committee of random image transforms, and then selectively minimizes entropy (increasing confidence) on consistent instances, while maximizing entropy (reducing confidence) on inconsistent ones. We show that SENTRY significantly improves upon the stateof-the-art across 27/31 shifts from several UDA benchmarks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Ablating class-balancing</head><p>In Tab. 7, we include results for ablating class-balancing on both the source and target domains. As seen, both contribute a small gain: +0.5%, +0.3% on C?S (Row 1 v/s 2) for source class-balancing and +2%, +0.1% on Rw?Cl (Row 1 v/s 3) for target pseudo class-balancing. Using both together works best (Row 4). However, even without any class balancing (Row 1), SENTRY is still 3.8% and 6.2% better than the next best method on each shift (Tab. 1-2 in paper). This confirms that the gains are due to SENTRY's predictive consistency-based selective optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Role of FS-architecture</head><p>Recall that in our experiments, we matched the "fewshot" style CNN architecture used in <ref type="table">Tan</ref>   4.2 in main paper). We now quantify the effect of this choice. As before, we measure average accuracy on Do-mainNet Clipart?Sketch (C?S) and OfficeHome RS-UT Real World?Clipart (Rw?Cl). We rerun our method without the few-shot modification, and observe a 1.6% drop on C?S and a 0.03% increase in average accuracy on Rw?Cl.</p><p>Overall, this modification seems to lead to a slight gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Additional analysis</head><p>Per-class accuracy change. In <ref type="figure" target="#fig_6">Fig. 9</ref>, we report the perclass accuracy (sorted by class cardinality) after adaptation using our method on DomainNet Clipart?Sketch, and contrast it against the next best-performing method, In-staPBM <ref type="bibr" target="#b22">[23]</ref>. As seen, SENTRY outperforms InstaPBM on 37/40 categories, and is competitive on the others. We further analyze the performance of SENTRY on the SVHN?MNIST-LT (IF=20) shift, wherein the target train set has been manually long-tailed to create an imbalance factor of 20 (Sec 4.4 of main paper). In <ref type="figure" target="#fig_4">Fig 6,</ref> we show a confusion matrix of model predictions on the target test set after source training (left) and after target adaptation via SENTRY (middle). As seen, strong misalignments exist initially. However, after adaptation via our method, alignment improves dramatically across all classes. In <ref type="figure" target="#fig_4">Fig 6 (right)</ref>, we show the change in per-class accuracy after adaptation, while sorting classes in decreasing order of size. As seen, SENTRY improves performance for both head and tail classes, often   <ref type="table">sheep  scissors  bicycle  bridge  airplane  dog  bird  table  duck  elephant  truck  telephone  butterfly  bus  backpack  drums  bathtub  banana  strawberry  bed  apple  bear  fork  book  car  house  bee  fence  cat  donut  ambulance  camera  horse  chair  rabbit  dolphin  cake  calculator</ref>  t-SNE with SENTRY. Next, we use t-SNE <ref type="bibr" target="#b29">[30]</ref> to visualize features (logits) extracted by the model for the source and target train sets. In <ref type="figure" target="#fig_5">Fig. 7</ref>, we visualize the feature landscape before and after adaptation via SENTRY. As seen, significant label imbalance exists: e.g. a lot more 1's and 2's are present as compared to 0's and 9's. Further, we denote target instances that are incorrectly classified by the model as large, opaque circles. Before adaptation (left), significant misalignments exist, particularly for head classes such as 1's and 2's. However, after adaptation via SENTRY, cross-domain alignment for most classes improves significantly, as does the average accuracy on the target test set (68.1% vs 95.7%). Qualitative results. In <ref type="figure" target="#fig_7">Fig 8,</ref> we provide some qualitative examples to build intuition about our consistency-based selection. For the Clipart?Sketch shift, we visualize target (i.e. sketch) instances belonging to the ground truth cate-Most consistent instances belonging to category "bear"   <ref type="table">Table 8</ref>: Accuracies on standard OfficeHome. Bold and underscore denote the best and second-best performing methods respectively. gory "bear". On the left, we visualize a random subset of target instances for which model predictions are most consistent under augmentations over the course of adaptation. On the right, we visualize a random subset of target instances for which model predictions are most inconsistent over the course of adaptation via SENTRY. Unsurprisingly, we find highly consistent instances to be "easier" to recognize, with more canonical poses and appearances. Similarly, inconsistent instances often tend to be challenging, and may even correspond to label noise, but our method appropriately avoids increasing model confidence on such instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">OfficeHome Results</head><p>In <ref type="table" target="#tab_7">Table 3a</ref> of the main paper, we presented accuracies averaged over all 12 shifts in the standard version of Office-Home <ref type="bibr" target="#b48">[49]</ref> for our proposed method against prior work. In <ref type="table">Table 8</ref>, we include the complete table with performances on every shift. As seen, SENTRY achieves state-of-the-art performance on 9/12 shifts, and improves upon the next best method (InstaPBM <ref type="bibr" target="#b22">[23]</ref>) by 2.5% overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Additional Implementation Details</head><p>In Sec 4.2 of the main paper, we presented implementation details for our method. We describe a few additional details to aid in reproducibility. Training and Optimization. We match optimization details to Tan et al. <ref type="bibr" target="#b44">[45]</ref>. On all benchmarks other than DIGITS, we use SGD with momentum of 0.9, a learning rate of 10 ?2 for the last layer and 10 ?3 for all other layers, and weight decay of 5 ? 10 ?4 . We use the learning rate decay strategy proposed in Ganin et al. <ref type="bibr" target="#b13">[14]</ref>. On DIGITS, we use Adam with a learning rate of 2 ? 10 ?4 and no weight decay. We use a batch size of 16 on DomainNet, OfficeHome, and VisDA, and 128 on DIGITS. For data augmentation when training the source models on DomainNet, OfficeHome, and VisDA, we first resize to 256 pixels, extract a random crop of size (224x224), and randomly flip images with a 50% probability. For SENTRY, we use RandAugment <ref type="bibr" target="#b10">[11]</ref> for generating augmented images, as described in Sec. 3.3 of the main paper and do not use any additional augmentations. For L SENTRY , we average loss for consistent and inconsistent instances separately and weigh each loss by the proportion of instances assigned to each group. We select ? IE =0.  ? SENTRY =1.0 so as to approximately scale each loss term to the same order of magnitude.</p><p>Baseline implementations. For all baselines except In-staPBM <ref type="bibr" target="#b22">[23]</ref>, we directly report results from prior work. We base our InstaPBM implementation on code provided by authors and implement target information entropy, conditional entropy, contrastive, and mixup losses with loss weights 0.1, 1.0, 0.01, 0.1 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Analysis of DM-based methods under LDS</head><p>Prior work has already demonstrated the shortcomings of distribution-matching based UDA methods under additional label distribution shift <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b51">52]</ref>. Wu et al. <ref type="bibr" target="#b51">[52]</ref> show that DM-based domain adversarial methods optimize two out of a sum of three terms that bound target error as shown in Ben-David et al. <ref type="bibr" target="#b2">[3]</ref>. Under matching task label distributions across domains, the contribution of the third term is small, which is the assumption under which these methods operate; absent this, the third term is unbounded and DM-based methods are not expected to succeed in domain alignment. We refer readers to Sec 2 of their paper for a formal proof.</p><p>Under LDS, such DM-based methods are expected to primarily mis-align majority (head) classes in the target domain with other classes in the source domain. We empirically test this hypothesis on the SVHN?MNIST-LT (IF=20) domain shift for digit recognition. In <ref type="figure" target="#fig_8">Fig 10,</ref>   per-class accuracy analysis for adaptation via DANN <ref type="bibr" target="#b13">[14]</ref>, a popular distribution matching UDA algorithm that uses domain adversarial feature matching. DANN has been shown to lead to successful domain alignment in the absence of label distribution shift (LDS); we now test its effectiveness in the presence of LDS. As seen, significant misalignments exist before adaptation (left). To match the source training strategy and architecture to the original paper, we do not use the few-shot architecture we use for our method, which leads to the slightly lower starting performance observed as compared to <ref type="figure" target="#fig_4">Fig. 6</ref>. However, due to label imbalance, DANN is unable to appropriately align instances and only slightly improves performance (69.54% in <ref type="figure" target="#fig_8">Fig 10, middle)</ref>. In <ref type="figure" target="#fig_8">Fig 10,</ref> we show the change in accuracy for each class after adaptation, while sorting classes in decreasing order of size. As predicted by the theory, DANN does particularly poorly on head (majority) classes (1, 2, 3, 4), while slightly improving performance for classes with fewer examples. This is in contrast to our method SENTRY, which is able to improve performance for both head and tail classes <ref type="figure" target="#fig_4">(Fig 6, right)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7.">Dataset Details</head><p>In Sec 4.1, we described our datasets in detail. For completeness, we also include label histograms and qualitative examples from each domain in the DomainNet and Office-Home RS-UT benchmarks proposed in Tan et al. <ref type="bibr" target="#b44">[45]</ref> in Figs. 11, 12. Idiosyncrasy of the "clipart" domain. Some prior works in UDA (e.g. Tan et al. <ref type="bibr" target="#b44">[45]</ref>) use center cropping at test time on DomainNet and OfficeHome, wherein they first resize a given image to 256 pixels and then extract a 224x224 crop from the center of the image. This practice has presumably carried over from ImageNet evaluation, where images are known to have a center bias <ref type="bibr" target="#b45">[46]</ref>. <ref type="figure" target="#fig_9">Figs. 11a, 12c</ref> show qualitative examples from the clipart domain in DomainNet and OfficeHome. As seen, most clipart categories span the entire extent of the image and do not have a center bias. As a result, using center cropping at evaluation time can adversely affect performance when adapting to Clipart as a target domain. We show empirical evidence of this in Tables 9, 10when clipart is the target domain, performance drops consistently when using centercrop at test time. For SENTRY, we therefore do not use center crop at evaluation time. For comparison, we also include the performance of our strongest baseline in each setting in Tabs. 9, 10: InstaPBM <ref type="bibr" target="#b22">[23]</ref> and MDD+I.A. <ref type="bibr" target="#b19">[20]</ref>, respectively. However, we note that in both settings, with and without centercrop, SENTRY still clearly outperforms our strongest baselines on both benchmarks.</p><p>Method R ? C R ? P R ? S C ? R C ? P C ? S P ? R P ? C P ? S S ? R S ? C S ? P AVG  <ref type="table">Table 9</ref>: Idiosyncrasy of the "clipart" domain: Per-class average accuracies on DomainNet without (white rows) and with (gray rows) centercrop at test time. We highlight in red performance drops due to centercrop eval when adaptating to Clipart as a target domain. For comparison, we also include the performance of InstaPBM <ref type="bibr" target="#b22">[23]</ref>, the 2nd best method from <ref type="table" target="#tab_3">Table 1</ref>   <ref type="table" target="#tab_3">Table 10</ref>: Idiosyncrasy of the "clipart" domain: Per-class average accuracies on OfficeHome RS-UT without (white rows) and with (gray rows) centercrop at test time. We highlight in red performance drops due to centercrop eval when adaptating to Clipart as target. For comparison, we also include the performance of MDD+I.A. <ref type="bibr" target="#b19">[20]</ref>, the 2nd best method from <ref type="table" target="#tab_4">Table 2</ref> in the main paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>-entropy)L CE L IE = H([? 1 ,? 2 , ...,? qs ])</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Left: Natural label distribution shift (LDS) on the Clipart?Sketch shift from DomainNet. Right: Manually generated LDS on the Real World?Clipart shift from OfficeHome RS-UT [45].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Analysis of SENTRY on Clipart?Sketch. Left: % of seen target instances selected for entropy minimization and maximization over epochs. Middle: % of seen target data chosen for entropy minimization at the end of first and last epochs of adaptation, broken down by class. Right: Ground truth precision of SENTRY's committee consistency strategy at identifying correct and incorrect instances over epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>?pL v/s p put score p and true class y, entropy maximization loss L EM = p log p + (1 ? p) log(1 ? p), and binary crossentropy (BCE) loss L BCE = ? [y log(p) + (1 ? y) log(1 ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>SVHN?MNIST-LT (IF=20): Performance on target test set after SENTRY.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>SVHN?MNIST: We use t-SNE<ref type="bibr" target="#b29">[30]</ref> to visualize features for incorrect (large, opaque circles) and correct (partly transparent circles) model predictions on the imbalanced target train set and source train set before (left) and after (right) adaptation via SENTRY. Colors denote ground truth class, and ? and ? denote source and target instances. SENTRY is able to overcome significant misalignments for both head classes with many examples (e.g. 1's and 2's) as well as tail classes with very few examples (e.g. 0's and 9's). very significantly so.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>DomainNet C?S: Per-class accuracy gain with SENTRY over InstaPBM. Classes are sorted by size (largest ) smallest).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>DomainNet Clipart?Sketch: Visualizing most consistent and inconsistent target instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>SVHN?MNIST-LT (IF=20): Performance on target test set after DANN<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>DomainNet [33] statistics: (a)-(d): Qualitative examples from each domain. (e): Label histograms for the splits proposed in Tan et al. [45].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>OfficeHome [49] statistics: (a)-(d): Qualitative examples from each domain. (e): Label histograms for the UT target splits proposed in Tan et al. [45].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Selective Entropy Optimization via Consistency</figDesc><table><row><cell></cell><cell cols="5">Conventional: Entropy Minimization for UDA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Minimize</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>entropy</cell><cell></cell><cell></cell></row><row><cell cols="3">Ours: Poor Initialization</cell><cell></cell><cell cols="2">Error accumulation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Selective</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Entropy</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Optimization</cell><cell></cell><cell></cell></row><row><cell cols="3">Poor Initialization</cell><cell></cell><cell cols="2">Robust domain alignment</cell></row><row><cell>Classes =</cell><cell>,</cell><cell>Source Target</cell><cell>Labeled Unlabeled</cell><cell>Consistent Inconsistent</cell><cell>Current class boundary</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>(right)). VisDA. VisDA2017 [34] is a large dataset for synthetic?real adaptation with 12 classes and &gt;200k images. DIGITS. We use the SVHN [31]?MNIST [22] shift for 10-way digit recognition. Metric. On LDS DA benchmarks (DomainNet and Office-Home RS-UT), consistent with prior work in UDA under</figDesc><table /><note>LDS [20, 45], we compute a mean of per-class accuracy on the target test split as our metric, that weights perfor- mance on all classes equally. On standard DA benchmarks (OfficeHome and VisDA2017) we report standard accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>AVG source 65.75 68.84 59.15 77.71 60.60 57.87 84.45 62.35 65.07 77.10 63.00 59.72 66.80 BBSE [25] 55.38 63.62 47.44 64.58 42.18 42.36 81.55 49.04 54.10 68.54 48.19 46.07 55.25 PADA [5] 65.91 67.13 58.43 74.69 53.09 52.86 79.84 59.33 57.87 76.52 66.97 61.08 64.48 MCD [39] 61.97 69.33 56.26 79.78 56.61 53.66 83.38 58.31 60.98 81.74 56.27 66.78 65.42 DAN [26] 64.36 70.65 58.44 79.44 56.78 60.05 84.56 61.62 62.21 79.69 65.01 62.04 67.07 F-DANN [52] 66.15 71.80 61.53 81.85 60.06 61.22 84.46 66.81 62.84 81.38 69.62 66.50 69.52 UAN [55] 71.10 68.90 67.10 83.15 63.30 64.66 83.95 65.35 67.06 82.22 70.64 68.09 72.05 JAN [29] 65.57 73.58 67.61 85.02 64.96 67.17 87.06 67.92 66.10 84.54 72.77 67.51 72.48 ETN [6] 69.22 72.14 63.63 86.54 65.33 63.34 85.04 65.69 68.78 84.93 72.17 68.99 73.99 BSP [10] 67.29 73.47 69.31 86.50 67.52 70.90 86.83 70.33 68.75 84.34 72.40 71.47 74.09 DANN [14] 63.37 73.56 72.63 86.47 65.73 70.58 86.94 73.19 70.15 85.73 75.16 70.04 74.46 COAL [45] 73.85 75.37 70.50 89.63 69.98 71.29 89.81 68.01 70.49 87.97 73.21 70.53 75.89 InstaPBM [23] 80.10 75.87 70.84 89.67 70.21 72.76 89.60 74.41 72.19 87.00 79.66 71.75 77.84 SENTRY (Ours) 83.89 76.72 74.43 90.61 76.02 79.47 90.27 82.91 75.60 90.41 82.40 73.98 81.39 Per-class average accuracies on DomainNet. Bold and underscore denote the best and second-best performing methods respectively.</figDesc><table><row><cell>source</cell><cell>70.74</cell><cell>44.24</cell><cell>67.33 38.68 53.51 51.85 54.39</cell></row><row><cell>BSP [10]</cell><cell>72.80</cell><cell>23.82</cell><cell>66.19 20.05 32.59 30.36 40.97</cell></row><row><cell>PADA [5]</cell><cell>60.77</cell><cell>32.28</cell><cell>57.09 26.76 40.71 38.34 42.66</cell></row><row><cell>BBSE [25]</cell><cell>61.10</cell><cell>33.27</cell><cell>62.66 31.15 39.70 38.08 44.33</cell></row><row><cell>MCD [39]</cell><cell>66.03</cell><cell>33.17</cell><cell>62.95 29.99 44.47 39.01 45.94</cell></row><row><cell>DAN [26]</cell><cell>69.35</cell><cell>40.84</cell><cell>66.93 34.66 53.55 52.09 52.90</cell></row><row><cell>F-DANN [52]</cell><cell>68.56</cell><cell>40.57</cell><cell>67.32 37.33 55.84 53.67 53.88</cell></row><row><cell>JAN [29]</cell><cell>67.20</cell><cell>43.60</cell><cell>68.87 39.21 57.98 48.57 54.24</cell></row><row><cell>DANN [14]</cell><cell>71.62</cell><cell>46.51</cell><cell>68.40 38.07 58.83 58.05 56.91</cell></row><row><cell>MDD [56]</cell><cell>71.21</cell><cell>44.78</cell><cell>69.31 42.56 52.10 52.70 55.44</cell></row><row><cell>COAL [45]</cell><cell>73.65</cell><cell>42.58</cell><cell>73.26 40.61 59.22 57.33 58.40</cell></row><row><cell cols="2">InstaPBM [23] 75.56</cell><cell>42.93</cell><cell>70.30 39.32 61.87 63.40 58.90</cell></row><row><cell cols="2">MDD+I.A [20] 76.08</cell><cell>50.04</cell><cell>74.21 45.38 61.15 63.15 61.67</cell></row></table><note>Method Rw ) Pr Rw ) Cl Pr ) Rw Pr ) Cl Cl ) Rw Cl ) Pr AVGSENTRY (Ours) 76.12 56.80 73.60 54.75 65.94 64.29 65.25</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Per-class average accuracies on OfficeHome RS?UT (right) benchmarks. Bold and underscore denote the best and second-best performing methods respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Accuracies on standard DA benchmarks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3</head><label>3</label><figDesc>presents results on 2 standard DA benchmarks: OfficeHome and VisDA 2017. As seen, SENTRY improves mean accuracy over the next best method by 2.7% averaged over 12 shifts (full table in supp.) on OfficeHome, and by 0.4% on VisDA. 4?0.9 56.7?1.2 56.2?1.4 55.1?0.7 55.4?1.1 DANN [14] 68.0?0.9 71.5?1.0 66.9?0.5 60.6?2.2 66.8?1.5 COAL [45] 78.8?1.0 67.1?1.4 70.2?1.5 70.0?1.8 71.5?1.4 InstaPBM [23] 90.7?0.2 77.9?3.5 68.9?1.3 65.9?2.2 75.9?1.8 SENTRY (Ours) 92.9?0.3 93.9?2.2 85.6?4.5 85.6?1.1 89.5?2.0</figDesc><table><row><cell>Method source MMD [28]</cell><cell cols="2">SVHN?MNIST-LT IF=1 IF=20 IF=50 IF=100 AVG 68.1 68.1 68.1 68.1 68.1 53.1 2 3 4 5 6 7 8 0 9 IF=1 (total=14.5k) 1.5k 1.0k 0.5k 0 2.0k 4.0k 6.0k IF=50 (total=14.5k)</cell><cell>4.0k 2.0k 0 2.0k 4.0k 6.0k</cell><cell>IF=20 (total=14.5k) 1 2 3 4 5 6 7 8 0 9 IF=100 (total=14.5k)</cell></row><row><cell></cell><cell>0</cell><cell>1 2 3 4 5 6 7 8 0 9</cell><cell>0</cell><cell>1 2 3 4 5 6 7 8 0 9</cell></row></table><note>Varying degree of label imbalance. To perform a con- trolled study of adapting to targets with varying degrees of label imbalance, we use the SVHN?MNIST shift. Since MNIST is class-balanced, we manually long-tail its train-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Left: Per-class average accuracy after adapting from SVHN to manually long-tailed (-LT) training sets of MNIST (test set is unchanged). The degree of label imbalance is measured by the imbalance factor (IF). All long-tailed versions use an identical amount of data. For each IF, we construct 3 long-tailed versions and report mean and 1 standard deviation. Right: Label distribution for each IF.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>N =1 N =3 N =5 k=1 78.2 78.6 78.9</cell><cell>N =1 N =3 N =5 k=1 53.8 57.5 55.6</cell><cell cols="2">voting C?S Rw?Cl</cell></row><row><cell>k=3 76.8 79.5 77.8</cell><cell>k=3 55.3 56.8 56.2</cell><cell>maj. 79.5</cell><cell>56.8</cell></row><row><cell>k=5 77.5 78.4 77.7</cell><cell>k=5 54.7 58.4 54.5</cell><cell>unan. 77.8</cell><cell>52.2</cell></row><row><cell>(a) C?S</cell><cell>(b) Rw?Cl</cell><cell cols="2">(c) Vary voting</cell></row></table><note>Ablations of our selection strategy on DomainNet C?S and OfficeHome RS-UT Rw?Cl. Gray row is our method. Last two rows are oracle approaches that use target labels.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Ablating the consistency checker on C?S and Rw?Cl: a-b) Varying committee size (k) and num. consecutive transforms in RandAugment (N ). c) Varying voting strategy: maj. and unan. denote majority and unanimous. Gray is our method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Jordan. Bridging theory and algorithm for domain adaptation. In International Conference on Machine Learning, pages 7404-7413, 2019. 2, 5, 6, 13 [57] Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jinsong Wang. Confidence regularized self-training. In Proceedings of the IEEE International Conference on Computer Ablating class-balancing . . . . . . . . . . . 11 6.2. Role of FS-architecture . . . . . . . . . . . . 11 6.3. Additional analysis . . . . . . . . . . . . . . 11 6.4. OfficeHome Results . . . . . . . . . . . . . 13 6.5. Additional Implementation Details . . . . . . 13 6.6. Analysis of DM-based methods under LDS . 14 6.7. Dataset Details . . . . . . . . . . . . . . . . 15</figDesc><table><row><cell>Vision, pages 5982-5991, 2019. 2</cell></row><row><cell>[58] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong</cell></row><row><cell>Wang. Unsupervised domain adaptation for semantic seg-</cell></row><row><cell>mentation via class-balanced self-training. In Proceedings of</cell></row><row><cell>the European conference on computer vision (ECCV), pages</cell></row><row><cell>289-305, 2018. 4</cell></row><row><cell>6. Appendix</cell></row><row><cell>Contents</cell></row><row><cell>6.1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 7 :</head><label>7</label><figDesc>Ablating class balancing. Gray row=SENTRY. CB=class balancing. subscript=improvement v/s row 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>MethodAr ? Cl Ar ? Pr Ar ? Rw Cl ? Ar Cl ? Pr Cl ? Rw Pr ? Ar Pr ? Cl Pr ? Rw Rw ? Ar Rw ? Cl Rw ? Pr AVG</figDesc><table><row><cell>Source</cell><cell>34.9</cell><cell>50.0</cell><cell>58.0</cell><cell>37.4</cell><cell>41.9</cell><cell>46.2</cell><cell>38.5</cell><cell>31.2</cell><cell>60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9</cell><cell>46.1</cell></row><row><cell>DAN [26]</cell><cell>43.6</cell><cell>57.0</cell><cell>67.9</cell><cell>45.8</cell><cell>56.5</cell><cell>60.4</cell><cell>44.0</cell><cell>43.6</cell><cell>67.7</cell><cell>63.1</cell><cell>51.5</cell><cell>74.3</cell><cell>56.3</cell></row><row><cell>DANN [14]</cell><cell>45.6</cell><cell>59.3</cell><cell>70.1</cell><cell>47.0</cell><cell>58.5</cell><cell>60.9</cell><cell>46.1</cell><cell>43.7</cell><cell>68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8</cell><cell>57.6</cell></row><row><cell>JAN [29]</cell><cell>45.9</cell><cell>61.2</cell><cell>68.9</cell><cell>50.4</cell><cell>59.7</cell><cell>61.0</cell><cell>45.8</cell><cell>43.4</cell><cell>70.3</cell><cell>63.9</cell><cell>52.4</cell><cell>76.8</cell><cell>58.3</cell></row><row><cell>CDAN [27]</cell><cell>50.7</cell><cell>70.6</cell><cell>76.0</cell><cell>57.6</cell><cell>70.0</cell><cell>70.0</cell><cell>57.4</cell><cell>50.9</cell><cell>77.3</cell><cell>70.9</cell><cell>56.7</cell><cell>81.6</cell><cell>65.8</cell></row><row><cell>BSP [10]</cell><cell>52.0</cell><cell>68.6</cell><cell>76.1</cell><cell>58.0</cell><cell>70.3</cell><cell>70.2</cell><cell>58.6</cell><cell>50.2</cell><cell>77.6</cell><cell>72.2</cell><cell>59.3</cell><cell>81.9</cell><cell>66.3</cell></row><row><cell>MDD [56]</cell><cell>54.9</cell><cell>73.7</cell><cell>77.8</cell><cell>60.0</cell><cell>71.4</cell><cell>71.8</cell><cell>61.2</cell><cell>53.6</cell><cell>78.1</cell><cell>72.5</cell><cell>60.2</cell><cell>82.3</cell><cell>68.1</cell></row><row><cell>MCS</cell><cell>55.9</cell><cell>73.8</cell><cell>79.0</cell><cell>57.5</cell><cell>69.9</cell><cell>71.3</cell><cell>58.4</cell><cell>50.3</cell><cell>78.2</cell><cell>65.9</cell><cell>53.2</cell><cell>82.2</cell><cell>66.3</cell></row><row><cell>InstaPBM [23]</cell><cell>54.4</cell><cell>75.3</cell><cell>79.3</cell><cell>65.4</cell><cell>74.2</cell><cell>75.0</cell><cell>63.3</cell><cell>49.7</cell><cell>80.2</cell><cell>72.8</cell><cell>57.8</cell><cell>83.6</cell><cell>69.7</cell></row><row><cell cols="2">MDD+I.A [20] 56.2</cell><cell>77.9</cell><cell>79.2</cell><cell>64.4</cell><cell>73.1</cell><cell>74.4</cell><cell>64.2</cell><cell>54.2</cell><cell>79.9</cell><cell>71.2</cell><cell>58.1</cell><cell>83.1</cell><cell>69.5</cell></row><row><cell>Ours</cell><cell>61.8</cell><cell>77.4</cell><cell>80.1</cell><cell>66.3</cell><cell>71.6</cell><cell>74.7</cell><cell>66.8</cell><cell>63.0</cell><cell>80.9</cell><cell>74.0</cell><cell>66.3</cell><cell>84.1</cell><cell>72.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>source 65.75 68.84 59.15 77.71 60.60 57.87 84.45 62.35 65.07 77.10 63.00 59.72 66.80 +CC-eval 62.42 69.31 59.28 79.74 59.49 58.46 84.55 60.42 66.26 78.61 58.31 61.31 66.51 InstaPBM [23] 80.10 75.87 70.84 89.67 70.21 72.76 89.60 74.41 72.19 87.00 79.66 71.75 77.84 SENTRY (Ours) 83.89 76.72 74.43 90.61 76.02 79.47 90.27 82.91 75.60 90.41 82.40 73.98 81.39 +CC-eval 78.81 78.15 71.62 89.84 75.98 77.69 89.50 77.34 73.82 89.96 80.66 75.02 79.87</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>in the main paper.</figDesc><table><row><cell>source</cell><cell>70.74</cell><cell>44.24</cell><cell>67.33 38.68 53.51 51.85 54.39</cell></row><row><cell>+CC-eval</cell><cell>70.25</cell><cell>38.20</cell><cell>67.74 35.61 55.08 52.90 53.30</cell></row><row><cell cols="2">MDD+I.A [20] 76.08</cell><cell>50.04</cell><cell>74.21 45.38 61.15 63.15 61.67</cell></row><row><cell cols="2">SENTRY (Ours) 76.12</cell><cell>56.80</cell><cell>73.60 54.75 65.94 64.29 65.25</cell></row><row><cell>+CC-eval</cell><cell>76.35</cell><cell>52.25</cell><cell>73.08 50.60 66.69 64.19 63.86</cell></row></table><note>Method Rw ) Pr Rw ) Cl Pr ) Rw Pr ) Cl Cl ) Rw Cl ) Pr AVG</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by funding from the DARPA LwLL project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Regularized learning for domain adaptation under label shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanny</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Natural and adversarial error detection using invariance to image transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Bahat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00236</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Partial adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijia</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to transfer examples for partial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichao</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2985" to="2994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Avinash Ravichandran, and Stefano Soatto. A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guneet</forename><surname>Singh Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Implicit class-conditioned domain alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qicheng</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Matwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Havaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Rethinking distributional matching based domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13352</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting and correcting for label shift with black box predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2200" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Tasdizen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04586</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Can you trust your model&apos;s uncertainty? evaluating predictive uncertainty under dataset shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Ovadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Domain adaptation with conditional distribution matching and generalized label shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Tachet Des Combes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Motoharu Sonogashira, and Masaaki Iiyama. Partially-shared variational auto-encoders for unsupervised domain adaptation with target shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuhei</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Classimbalanced domain adaptation: An empirical odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Balanced distribution adaptation for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuji</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqi</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1129" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7029" to="7039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Domain adaptation with asymmetrically-relaxed distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ezra</forename><surname>Winston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Universal domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichao</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
