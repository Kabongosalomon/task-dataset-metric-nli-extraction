<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DyStaB: Unsupervised Object Segmentation via Dynamic-Static Bootstrapping *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
							<email>yanchaoy@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lai</surname></persName>
							<email>b4lai@g.ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
							<email>soatto@ucla.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">UCLA Vision Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">UCLA Vision Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DyStaB: Unsupervised Object Segmentation via Dynamic-Static Bootstrapping *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe an unsupervised method to detect and segment portions of images of live scenes that, at some point in time, are seen moving as a coherent whole, which we refer to as objects. Our method first partitions the motion field by minimizing the mutual information between segments. Then, it uses the segments to learn object models that can be used for detection in a static image. Static and dynamic models are represented by deep neural networks trained jointly in a bootstrapping strategy, which enables extrapolation to previously unseen objects. While the training process requires motion, the resulting object segmentation network can be used on either static images or videos at inference time. As the volume of seen videos grows, more and more objects are seen moving, priming their detection, which then serves as a regularizer for new objects, turning our method into unsupervised continual learning to segment objects. Our models are compared to the state of the art in both video object segmentation and salient object detection. In the six benchmark datasets tested, our models compare favorably even to those using pixel-level supervision, despite requiring no manual annotation. * Also ArXiv:2008.07012, August 16, 2020 ? Equal contributions. Our implementation and trained models are available at:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to segment the visual field into coherently moving regions is among the traits most broadly shared among visual animals <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Even camouflaged, moving objects are easy to spot <ref type="figure">(Fig. 1</ref>). During early development, humans spend considerable amounts of time interacting with a single moving object before losing interest <ref type="bibr" target="#b3">[4]</ref>, which may help prime object models and learn invariances <ref type="bibr" target="#b52">[60]</ref>. In contrast, the mature visual system can learn an object with a few static examples; objects do not need to move in order to be detected. This suggests using motion as a cue to bootstrap object models that can be used for detection in static images, with no need for explicit supervision. Ob- <ref type="figure">Figure 1</ref>. Dynamic-static bootstrapping. (a) A lizard is hard to detect when still thanks to camouflage (top left). However, it is easy to see once it moves (bottom left; optical flow visualized using the inset color wheel). Once learned the lizard, a never-before-seen sloth (b) can be easily detected in a static image, exploiting the static model learned from the moving lizard. jects that have never been seen moving are considered part of whatever background they are part of, at least until they move. As time goes by, more and more objects are seen moving, thus improving one's ability to detect and segment objects in static images (Tab. 6). The more objects are bootstrapped in a bottom-up fashion, the easier they are to detect top-down, priming better motion discrimination, which in turn results in more accurate object detection. This synergistic loop gradually improves both the diversity of objects that can be detected and the accuracy of the detection.</p><p>We present a method to learn object segmentation using unlabeled videos, that at test time can be used for both moving objects in videos and static objects in single images. The method uses a motion segmentation module that performs temporally consistent region separation. The resulting motion segmentation primes a detector that operates on static images, and feeds back to the motion segmentation module, reinforcing it. We call this method Dynamic-Static Bootstrapping, or DyStaB. During training, the dynamic model minimizes the mutual information between partitions of the motion field, while enforcing temporal consistency, which yields a state-of-the-art unsupervised motion segmentation method. Putative regions, along with their uncertainty approximated during the computation of mutual information in the dynamic model, are used to train a static model. The 3) iterates between these two models. In particular, once ? (static object model) is trained, it can be used as a "top-down" object prior to bias the motion segmentation network ? (dynamic model) in a feedback loop. ? is the adversarial inpainting network that enforces minimal mutual information between motion field partitions (two ?'s are identical), and losses are represented using dashed lines/boxes. static model is then fed back as a regularizer in a top-down fashion, completing the synergistic loop. One might argue that every pixel in the image backprojects to something in space that we could call an object. However, the training data biases the process towards objects that exist at a scale that is detectable relative to the size of the pixel and the magnitude of the motion. For instance, individual leaves in an outdoor video might not be seen at a resolution that allows detecting them as independent objects. Instead, the tree may be detected as moving coherently. So, the definition of objects is conditioned on the training data and, in particular, the scale and distribution of their size and relative motion.</p><p>With this caveat, our contribution is two-fold: First, a deep neural network trained with unlabeled videos that achieves state-of-the-art performance in motion segmentation. It exploits mutual information separation and temporal consistency to identify candidate objects. Second, a deep neural network to perform object segmentation in single images, bootstrapped from the first. The static model uses as input both the output of the dynamic model and its uncertainty, to avoid self-learning. The two models are trained jointly in a synergistic loop. The resulting object segmentation models outperform the state of the art by 10% in average precision in both video and static object segmentation across six standard benchmarks. Despite not requiring any manual annotation, our method also outperforms recent supervised ones by almost 5% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Motion segmentation aims to identify independently moving regions in a video. Background subtraction assumes a static camera <ref type="bibr" target="#b53">[61,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b45">53]</ref>, while scene dynamic models compensate for camera motion <ref type="bibr" target="#b37">[45,</ref><ref type="bibr" target="#b41">49,</ref><ref type="bibr" target="#b46">54]</ref>. One could directly segment or cluster pixel-wise motion vectors <ref type="bibr" target="#b62">[70,</ref><ref type="bibr" target="#b49">57,</ref><ref type="bibr" target="#b23">31,</ref><ref type="bibr" target="#b40">48]</ref>, but this approach is prone to errors due to occlusions, singularities and discontinuities of the motion field. To increase robustness, some employ pixel trajectories accumulated over multiple frames <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">27,</ref><ref type="bibr" target="#b64">72,</ref><ref type="bibr" target="#b47">55]</ref> or patches, losing discriminative power. The hard trade-off between discriminability and robustness is a key challenge in unsupervised motion segmentation. Manual pixel-level annotations help set the trade-off, but in a non-scalable manner <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b56">64,</ref><ref type="bibr" target="#b63">71,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b10">11]</ref>. Contextual Information Separation <ref type="bibr" target="#b69">[77]</ref> aims to bypass this trade-off without the need for human annotation, using a segmentation network to minimize the mutual information between the inside and the outside of putative motion regions. In the absence of temporal consistency, this procedure can be sensitive to motion errors ( <ref type="figure" target="#fig_3">Fig. 5</ref>). Furthermore, <ref type="bibr" target="#b69">[77]</ref> cannot detect stationary objects as it relies on motion segmentation ( <ref type="figure" target="#fig_5">Fig. 6</ref>). Note, <ref type="bibr" target="#b68">[76]</ref> applies <ref type="bibr" target="#b69">[77]</ref> with perceptual cycle-consistency to separating multiple objects, but they focus on learning object-centric representations.</p><p>Saliency prediction aims to detect the most salient objects relative to their background or "context." Saliency can be computed locally in a bottom-up fashion <ref type="bibr" target="#b21">[29,</ref><ref type="bibr">24,</ref><ref type="bibr" target="#b36">44]</ref> or globally <ref type="bibr" target="#b72">[80,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref>, at multiple scales <ref type="bibr" target="#b31">[39,</ref><ref type="bibr" target="#b33">41]</ref> and top-down <ref type="bibr" target="#b57">[65]</ref>.</p><p>[20] constructs the saliency map from the spectral residual and <ref type="bibr" target="#b48">[56]</ref> performs low-rank matrix decomposition to detect salient objects. Separation can also be achieved by increasing the divergence of the feature distributions <ref type="bibr" target="#b20">[28]</ref>. Despite advancements in the optimization for salient region segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">34]</ref>, the quality of the predicted saliency depends highly on the features selected. Currently, the best performing methods employ deep neural networks trained on labeled datasets <ref type="bibr" target="#b74">[82,</ref><ref type="bibr" target="#b35">43,</ref><ref type="bibr" target="#b32">40,</ref><ref type="bibr" target="#b30">38,</ref><ref type="bibr" target="#b29">37]</ref>. Recently, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5]</ref> proposed unsupervised adversarial salient object dis-Video:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Fields:</head><p>Image I~P ( u | I ) <ref type="figure">Figure 3</ref>. A single image I, renders possible motion fields u of an independently moving object as if they are sampled from the conditional distribution p(u|I). Thus, given the image, one can complete partial observations of the flow fields in <ref type="figure" target="#fig_2">Fig. 4</ref>.  covery models in single images. Due to the variability of object appearance, these models are hard to train and the process is not scalable. On the other hand, <ref type="bibr" target="#b73">[81,</ref><ref type="bibr" target="#b38">46]</ref> propose training deep networks using the pseudo-labels generated by conventional unsupervised methods. While an improvement over adversarial methods, performance still hinges on human prior knowledge through the selection of handcrafted features. We wish to avoid specifying features by instead articulating a criterion, namely that anything similar to what we have previously detected as objects should be salient. The model trained with this simple criterion outperforms the state of the art on unsupervised saliency prediction. Video object segmentation comprises a vast literature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b25">33,</ref><ref type="bibr" target="#b70">78,</ref><ref type="bibr" target="#b60">68,</ref><ref type="bibr" target="#b61">69]</ref>; we focus on unsupervised methods that are more related to our work. It should be noted that the term "unsupervised" in video object segmentation only reflects absence of annotations at test time, whereas we reserve the name for methods that involve no manual annotation during training, as well as testing. To segment moving objects, <ref type="bibr" target="#b56">[64]</ref> trains a network to directly output the segmentation from motion, which is then augmented by an appearance channel in [25]; <ref type="bibr" target="#b55">[63]</ref> proposes a layered model for detachable objects using occlusion as primary cue. <ref type="bibr" target="#b77">[85]</ref> incorporates salient motion detection with object proposals; <ref type="bibr" target="#b22">[30]</ref> segments video into "primary" objects. A pyramid dilated bidirectional ConvLSTM is proposed in <ref type="bibr" target="#b51">[59]</ref> to extract spatial features at multiple scales, and <ref type="bibr" target="#b34">[42]</ref> introduces a global co-attention mechanism to capture scene context. <ref type="bibr" target="#b75">[83]</ref> proposes an architecture that allows interaction between motion and appearance during the encoding process, while <ref type="bibr" target="#b71">[79,</ref><ref type="bibr" target="#b24">32]</ref> focuses on learning discriminative features for segmentation propagation and assumes the first frame been annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>An object that is detached from its surroundings induces an independent motion when projected onto a moving image <ref type="bibr" target="#b2">[3]</ref>. Here, we utilize this independence principle by minimizing the mutual information between the motions of an object and its context in Sec. 3.1. In contrast to the Contextual Information Separation (CIS) criterion <ref type="bibr" target="#b69">[77]</ref>, we enforce that the separation is temporally consistent in a sequence, which directly improves <ref type="bibr" target="#b69">[77]</ref> by an average of 7%. In Sec. 3.2, we instantiate a static model that enables perception of stationary objects utilizing the detection of moving ones and the confidence measure computed from the mutual information. The interaction or mutual bootstrapping between motion segmentation and static perception is described in Sec. 3.3. The overall method is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dynamic Model with Temporally Consistent Mutual Information Minimization</head><p>Given an image I ? R H?W ?3 , the motion field u defined on I is a random variable distributed according to p(u|I), which is determined by a dataset of image sequences</p><formula xml:id="formula_0">D = {I t i } i?N,t?T ,</formula><p>where N is the cardinality of the dataset and T is the maximum number of images in a sequence. Particularly, for an instance u ? R H?W ?2 , sampled from p(u|I), there exists an image? such that for any pixel x on I, I(x) =?(x + u(x)) holds up to noise and occlusions, as shown in <ref type="figure">Fig. 3</ref>.</p><p>To detect objects that move independently in the scene, a motion segmentation network ? should generate masks m = ?(u) ? {0, 1} H?W , such that the motions inside the mask m u and outside the mask (1 ? m) u are mutually independent conditioned on the image I. More explicitly, the conditional mutual information I(m u, (1 ? m) u|I) should be minimal. Since the mutual information measures the difference between the Shannon entropy of the inside and its conditional entropy (conditioned) on the outside, simply minimizing the mutual information yields a trivial solution (empty set). One solution is to normalize the conditional mutual information by the entropy H(m u|I),</p><formula xml:id="formula_1">arg min m I(m u, (1 ? m) u|I) H(m u|I) = arg max m H(m u|(1 ? m) u, I) H(m u|I)<label>(1)</label></formula><p>which is equivalent to maximizing the un-informativeness measured by the ratio on the right. Again, this ratio can be maximized by simply setting m as the whole image domain, which results in over detection ( <ref type="figure" target="#fig_2">Fig. 4 (a)</ref>). Thus, if the detection is not accurate as in <ref type="figure" target="#fig_2">Fig. 4 (c)</ref>, the context will be rendered informative, vice versa. Therefore it is necessary to add a symmetric term:</p><formula xml:id="formula_2">L(m; I) = H(m u|(1 ? m) u, I) H(m u|I) + H((1 ? m) u|m u, I) H((1 ? m) u|I)<label>(2)</label></formula><p>whose value is upper bounded by 2, and can be maximized only when m accurately segments the object. Similar to <ref type="bibr" target="#b69">[77]</ref>, we make the loss in Eq. (2) computable by assuming Gaussian conditionals, and instantiating an adversarial inpainting network ? that computes the conditional means, e.g., ?(m, (1 ? m) u, I) estimates the conditional mean of m u under p(m u|(1 ? m) u, I). Both ? and ? can be trained adversarially:</p><formula xml:id="formula_3">max ? min ? L A (?, ?; I) = u?p(u|I) m u ? ?(m, (1 ? m) u, I) u?p(u|I) m u + + u?p(u|I) (1 ? m) u ? ?(1 ? m, m u, I) u?p(u|I) (1 ? m) u +<label>(3)</label></formula><p>with m = ?(u), and ? the l 2 -norm. The constant 0 &lt; 1 is to prevent numerical instability, and ?(m, ?, I) is default to zeros.</p><p>Since Eq. (3) characterizes moving objects solely using motion, it is sensitive to variations in the motion field, resulting in label flipping and irregular segments due to failures of motion estimation shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. To resolve these issues, we introduce a temporal consistency constraint to reduce the instability in the motion segmentation model.</p><p>Temporal consistency. Given two consecutive images I 1 , I 2 from the same video sequence, we can compute the forward and backward optical flow u 12 , u 21 , and the predicted masks m 1 = ?(u 12 ), m 2 = ?(u 21 ). We would like the individually predicted masks to be temporally consistent, in the sense that if we deform one onto the other using the flow fields, the two should look similar as they are the projections of the same object. Thus, we penalize the following warping difference to enforce temporal consistency:</p><formula xml:id="formula_4">L TC (?; u 12 , u 21 ) = x / ?o |m 1 (x) ? m 2 (x + u 12 (x))| + |m 2 (x) ? m 1 (x + u 21 (x))| (4)</formula><p>with x the pixel index, and o the union of occlusions within the image domain, which can be easily estimated using the forward-backward identity criterion <ref type="bibr">[23]</ref>. The reasoning is that inconsistencies of the predictions should only be penalized in the co-visible region. Without introducing extra networks, Eq. (4) effectively reduces instabilities in the motion segmentation compared to Eq. (3), as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p><p>Initial training for the dynamic model: By enforcing temporal consistency, the dynamic model for independently moving object segmentation can be obtained through the following adversarial training:</p><formula xml:id="formula_5">max ? min ? L D (?, ?; I 1 , u 12 , u 21 ) = L A (?, ?; I 1 ) + ? TC L TC (?; u 12 , u 21 ) (5) with ? TC = ?0.1 (? TC &lt; 0 as ? maximizes L D ).</formula><p>The effectiveness of optimizing Eq. <ref type="formula">(5)</ref> is also numerically demonstrated in Tab. 1. Next, we describe the confidence-aware training for the static object model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Static Model with Confidence-Aware Update</head><p>In Sec. 3.1, we describe a model that detects moving objects in a temporally coherent manner. However, what if the objects stop moving or they have been moving in an indistinctive way? These present challenges for the dynamic model ?, which relies on motion to signal the existence of an object ( <ref type="figure" target="#fig_5">Fig. 6</ref>). Note in <ref type="figure" target="#fig_5">Fig. 6</ref>, the motion varies between frames, but the appearance of an object is temporally persistent, and when motion fades away, the image array still depicts the same object.</p><p>Our position is that, once a moving object is detected, we ought to be able to find it even in a still image. Thus, we propose to train a static object model to complement the dynamic model when there is no significant motion. We could directly train a segmentation network ? to predict objects from a single image, utilizing the output of ? as the pseudo labels, by maximizing the F-measure <ref type="bibr" target="#b38">[46]</ref> commonly used for salient object detection: </p><p>with ?, ? the precision and recall between the prediction ?(I) and the motion mask m generated by ?. And ? 2 is default to 1.5 if not explicitly mentioned. However, directly learning from all motion masks is counter-productive, as these are quite noisy especially when motion is uninformative <ref type="figure" target="#fig_5">(Fig. 6</ref>). To address this issue, we propose to use the loss L A <ref type="figure">(Eq. (3)</ref>) as a confidence measure on the reliability of the motion masks: if the motion is uninformative, the reconstruction from the context will be accurate, thus L A will be small and vice versa, if the motion is distinctive, L A will be large due to a bad reconstruction (See <ref type="figure" target="#fig_6">Fig. 7</ref> and <ref type="figure">Fig. 8</ref> for a demonstration.) We propose using the difference between the values of L A to perform a confidence-aware adaptation via the following:</p><formula xml:id="formula_7">L ? (?; I, m, ? ) = ? F F ? (?(I), ? (I)) + max(L A (m) ? L A (? (I)) ? ?, 0)F ? (?(I), m) (7)</formula><p>Note that the pseudo-label m is only effective when it has a larger L A than the one predicted by ? , which is a copy of an earlier ?. In other words, if the output of the dynamic model is not confident enough, ? retains its own prediction, and the first term (moving average) is to ensure that ? is updated smoothly. We set L A (? (I)) = 0 and ? F = 0 the first time ? is trained, then ? F = 1.0.</p><p>When updated, ? learns a model of objects based on their appearance, so we would expect ? to detect stationary objects which have been seen moving before. Indeed, we find that ? is able to detect static object that has never been observed moving as shown in <ref type="figure" target="#fig_5">Fig. 6 (3rd column)</ref>, which confirms that a general concept of objects can be learned through the observations of moving ones. Next we detail the proposed dynamic-static bootstrapping scheme for a continuous learning of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamic-Static Bootstrapping</head><p>Once the static object model is learned by ?, it can be used to modulate the detection in general scenes, even where the motion of the objects is unknown. It is also possible that the predictions of ?, which employs motion information are imperfect. The output of ? can then provide complementary information to strengthen the dynamic model. We feed ? back into the training of the dynamic model as an experiential prior of objectness based on the photometric information.</p><p>The dynamic model reinforced by the objectness prior is:</p><formula xml:id="formula_8">max ? min ? L J (?, ?; I 1 , u 12 , u 21 , ?) = L D (?, ?; I 1 , u 12 , u 21 ) + ? obj F ? (?(u 12 ), ?(I 1 )) (8)</formula><p>with L D the adversarial motion segmentation loss in Eq. (5), and the second term measures the similarity between the motion mask and the static object prior (? obj = 1.0). Besides learning from motion information to detect moving objects, ? is now able to leverage photometric cues that facilitate the detection under circumstances where objects become stationary or move extremely slowly.</p><p>Moreover, improved dynamic model could yield better pseudo-labels that help training a more accurate static object model (Eq. <ref type="formula">(7)</ref>), which can then be used to facilitate the learning of the former in a synergistic loop (Eq. (8)), thus the name "Dynamic-Static Bootstrapping." The overall training procedure is presented in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Dynamic-Static Bootstrapping</head><p>Result: ?: dynamic model; ?: static object model; ?: conditional inpainting network Initialize ?, ? by optimizing L D (Eq. (5)), set k=0; while k&lt;3 do k = k+1; Update the static model ? using L ? (Eq. <ref type="formula">(7)</ref>); Update the dynamic model ? using L J (Eq. (8));</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head><p>Dynamic model ? uses the Deeplab architecture <ref type="bibr" target="#b6">[7]</ref>, with the initialization weights as in <ref type="bibr" target="#b73">[81,</ref><ref type="bibr" target="#b38">46]</ref>. ? takes as input the estimated flow between two randomly chosen frames from the same video with the maximum interval equals to three. Optical flow is produced by PWCNet <ref type="bibr" target="#b54">[62]</ref>, which is trained on synthetic data. The output of ? is a two channel softmax score. In total, ? has 23M trainable parameters and can run in 19 fps during inference.  <ref type="figure">Figure 8</ref>. Distribution of LA for the dynamic model ? (orange) and the static model ? (blue) on the frog sequence, which shows that when the motion is not informative or erroneous, the static model can work better, thus the difference in the values of LA is small.</p><p>Static model ? uses similar architecture as the dynamic model, but takes a single RGB image as input. The output is also a two channel softmax score. The total number of parameters is 21 M, and ? can run at 22 fps at inference.</p><p>Training details. For the initial training of the dynamic model ?, we alternate between updating ? for three steps and updating ? for one step, up to 30 epochs on the training set of each dataset, using an Adam optimizer with lr=1e-4, beta1=0.9, and beta2=0.999. The static model is then trained up to 15 epochs, using an Adam optimizer with lr=2e-5, beta1=0.9, and beta2=0.999. As described in Algorithm 1, the dynamic-static bootstrapping runs for three iterations. On average, each iteration takes six hours to converge, and the whole training procedure can be finished within a day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets: For video object segmentation, we train and test our model on three commonly used video object segmentation datasets: DAVIS <ref type="bibr" target="#b42">[50]</ref>, FBMS <ref type="bibr" target="#b39">[47]</ref>, and SegTrackV2 <ref type="bibr" target="#b27">[35]</ref> (Tab. 3). We also test our trained model on two other datasets, DAVIS17 <ref type="bibr" target="#b43">[51]</ref> and Youtube-VOS <ref type="bibr" target="#b65">[73]</ref>, to check how well our method generalizes (Tab. 7).</p><p>DAVIS consists of high-resolution videos (30 for training and 20 for validation) depicting the primary object moving in the scene with pixel-wise annotations for each frame. FBMS contains videos of multiple moving objects, providing test cases for multiple object segmentation. FBMS has sparsely annotated 59 video sequences, with 30 sequences for validation. SegTrackV2 contains 14 densely annotated videos. These videos constitute the only source of training data for our unsupervised motion perception module ?. Youtube-VOS contains 4,453 videos and 94 object categories, and DAVIS17 consists of 150 videos.</p><p>To evaluate ? on static object segmentation, we test on three major saliency prediction datasets: MSRA-B [26] (5000 images), ECCSD <ref type="bibr" target="#b50">[58]</ref> (1000 images) and DUT <ref type="bibr" target="#b67">[75]</ref> (5168 images). All three datasets are annotated with pixelwise labels for each image. These saliency datasets contain objects from a much broader span of categories, such as road signs, statues, flowers, etc., that are never seen moving in the training videos We evaluate the static object model learned from only video objects on these saliency benchmarks, to check its transferability to different instances from seen categories and unseen categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effectiveness of Temporally Consistent Mutual Information Minimization</head><p>To verify the effectiveness of the temporally consistent mutual information minimization proposed in Sec. 3.1 for bottom-up motion segmentation, we compare to the baseline CIS <ref type="bibr" target="#b69">[77]</ref> that trains a segmentation network using only Eq. (3). We train both CIS <ref type="bibr" target="#b69">[77]</ref> and our model described in Eq. (5) on the unlabeled videos from DAVIS, and then test on the validation set of DAVIS. We also report the scores by directly applying the model trained on DAVIS to FBMS and SegTrackV2 in Tab. 1 to check the generalization on different domains. The performance is measured by mean-Intersection-over-Union (mIoU), and the relative weights used in our model are ? TC = 1.0. As shown in Tab. 1, our dynamic model (Eq. (5)) consistently outperforms CIS (Eq. (3)) on all three video object segmentation benchmarks by 7%, which confirms that temporal consistency is a critical component in our dynamic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS FBMS SegTV2</head><p>CIS <ref type="bibr" target="#b69">[77]</ref> 59 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effectiveness of Confidence-Aware Adaptation</head><p>Here we check the effectiveness of the confidence-aware adaptation scheme proposed in Sec. 3.2 in precluding counter-productive self-learning. We first train a motion model ? on the training data from DAVIS. Then we train two static models ?: ?(F ? ) using Eq. (6) (not confidenceaware), the other ?(L ? ) using Eq. (7) (confidence-aware). We set ? in Eq. (7) to 0.2 and ? F to 1.0, which are fixed for the future experiments. We compare the performance of <ref type="figure">Figure 9</ref>. Qualitative comparison to the top two methods from each category on the DAVIS benchmark. MatNet <ref type="bibr" target="#b75">[83]</ref>, AnDiff <ref type="bibr" target="#b71">[79]</ref> (supervised) and ARP <ref type="bibr" target="#b22">[30]</ref>, CIS <ref type="bibr" target="#b69">[77]</ref> (unsupervised). the static models on the DAVIS validation set using mIoU. Further, we perform the same evaluation on both FBMS and SegTrackV2, and report the scores in Tab. 2. As shown, with the confidence-aware adaptive bootstrapping loss Eq. <ref type="formula">(7)</ref>, the static object model ? consistently improves over its counterpart on the three benchmarks, confirming the importance of uncertainty estimation in self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS FBMS SegTV2</head><formula xml:id="formula_9">?(F ? ) 73.8 65.5 65.7 ?(L ? )</formula><p>78.2 68.7 69.3 <ref type="table">Table 2</ref>. Static model ?(F?) vs. ?(L?). The latter is trained with confidence-aware adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Improvement from Bootstrapping</head><p>In Tab. 5, we show the improvement after multiple rounds of bootstrapping for each model as described in Algorithm 1 (dynamic-static bootstrapping). The reported score is the mIoU of the segmentation on the three video object segmentation benchmarks. This demonstrates the effectiveness of training in a synergistic loop with our dynamic-static bootstrapping strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Static Model Improves with the Number of Training Videos</head><p>One characteristic of our method is that the static model ? improves over time as more and more objects are seen moving through the bottom-up motion detection module ?. To verify, we construct a collection of videos S by combining the three aforementioned video object segmentation datasets (in total there are 123 video sequences). We randomly partition them into 10 subsets {s k } 10 k=1 , each contains around 12 video sequences. Correspondingly, we train 5 static object models {? i } 4 i=0 by performing Algorithm 1. The training set for each ? i is {s k } 2i+1 k=1 , such that ? i with a larger i is exposed to more video sequences. Each ? is evaluated on the <ref type="figure">Figure 10</ref>. Visual results on the saliency prediction benchmarks. CHS <ref type="bibr" target="#b66">[74]</ref>, HS <ref type="bibr" target="#b78">[86]</ref> (unsupervised), SBF <ref type="bibr" target="#b59">[67]</ref> (unsupervised learning-based), SR <ref type="bibr" target="#b58">[66]</ref> (supervised).</p><p>union of the three saliency datasets mentioned above (in total 11,000 testing images). In Tab. 6, we report the performance of ? i 's, measured in terms of mIoU (with standard deviation computed across five runs). As shown in Tab. 6, when the number of the observed videos increases, the performance of the static object model also improves, which is consistently observed across multiple runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Video Object Segmentation Benchmarks</head><p>To check the effectiveness of the proposed dynamic-static bootstrapping in learning better motion segmentation, we evaluate on the benchmarks for video object segmentation. Since video object segmentation focuses on moving objects, we weigh the predictions from the dynamic model with the predictions from the static model to emphasize the detection of moving ones. Similar to <ref type="bibr" target="#b69">[77]</ref>, CRF postprocessing is performed to get our final results. We compare to top-performing unsupervised (no annotations are involved) and supervised (annotations are involved during the training phase) methods. The performance is measured by mIoU. As shown in Tab. 3, our method achieves the top performance on all three video object segmentation benchmarks among fully unsupervised methods. To compare with methods that utilize manual annotations (Supervised), we finetune our model on the DAVIS training set with 2000 annotations. We have also listed the number of annotations used by other supervised methods in Tab. 3. Again, our model achieves the top performance using the least amount of manual annotations among all the supervised methods. In Tab. 7 we show the results by testing our trained models on two other video object segmentation benchmarks DAVIS17 and Youtube-VOS. Our model still achieves competitive performance compared to the state of the art and demonstrates good generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Unsupervised Salient Object Detection</head><p>In Tab. 4, we evaluate the learned object prior (static object model) through the task of salient object detection in images. Note that the top-performing methods on unsupervised salient object detection all rely on handcrafted methods   either as the primary procedure, or as a subprocess. Among all top-performing ones, we are, to the best of our knowledge, the only one that does not rely on any handcrafted features.</p><p>We refine the static model ? by performing CRF on its predictions, and by one round of self-training with the CRF refined masks <ref type="bibr" target="#b38">[46]</ref>. By leveraging the object prior learned through videos, we can approach and surpass the state of the art. Even when compared with top-performing supervised methods (DSS, NDF, SR in Tab. 4), our method still achieves competitive performance with no explicit annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We have presented a method to learn how to segment objects in images that exploits temporal consistency in their motion, observed in training videos, to bootstrap a top-down model. The definition of what constitutes an object is implicit in the method and in the datasets used for training. This may appear to be a limitation, as training on different datasets may yield different outcomes. However, what constitutes an object, or even a segment of an image, is ultimately not objective: In <ref type="figure" target="#fig_3">Fig. 5</ref>, is the object a person? Or the motorcycle they are riding? The union of the two? The helmet they are wearing? All of the above? We let the evidence bootstrap the definition: If the motion at the resolution of the first video shows the human and bike moving as a whole, we do not know any better than to consider them one object. If, in later video, a human is seen without a motorcycle, they will be an independent object thereafter. Admittedly, our model does not capture the fact that a proper object model should segment instances and enable multiple memberships for each point: A pixel on the helmet is part of the object, but also of the person, and the rider, and so on. We also do not exploit side information from other modalities. Nonetheless, despite the complete absence of annotation requirements, our method edges out methods that exploit manual annotation, so we believe it to be a useful starting point for further development of more complete object segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">From Eq. 2 to Eq. 3</head><p>Under the Gaussian assumption, the Shannon entropy is proportional to the variance of the Gaussian. So Eq. 3 can be obtained by replacing Shannon entropies in Eq. 2 with the estimated variances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Additional Qualitative Comparisons</head><p>In <ref type="figure">Fig. 11</ref>, we show that the objectness prior learned by the static model ? introduced in Sec. 3.2 can effectively generalize to object categories that it has not seen during training. We perform the training using only the DAVIS dataset and directly use the static model ? to do inference on sequences from SegTrackV2.</p><p>Youtube-VOS <ref type="bibr" target="#b65">[73]</ref> has recently been used to benchmark "semi-supervised" video object segmentation methods, where one is required to feed the system a ground-truth segmentation mask for the first frame. This reduces the problem to one of correspondence and tracking, quite different than what we tackle, which is object 'discovery' without any manual input. But we still experiment on Youtube-VOS and compare to similar methods to check the generalization ability over diverse video quality, unseen object categories and annotation noise. We provide additional visual comparisons on the Youtube-VOS <ref type="bibr" target="#b65">[73]</ref> benchmark in <ref type="figure">Fig. 13</ref>. In this case, where there are many new categories that have not been seen during training, the model is still capable of generating reasonable segmentations. The use of videos to learn good object representations aligns well with the findings in <ref type="bibr" target="#b44">[52]</ref> which finds that self-supervised contrastive learning methods often perform better when videos are used in place of single images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Failure Case in Salient Object Detection</head><p>In <ref type="figure" target="#fig_0">Fig. 12</ref>, we show some examples of our static model that violate the ground-truth annotations. Since our model captures the general concept of objectness, it does not have access to or understanding of which object in the image is currently attended by the viewer. Further inclusion of attention information can help on this. <ref type="figure">Figure 11</ref>. Top: frames from sequences in DAVIS <ref type="bibr" target="#b42">[50]</ref> on which the static model is trained. Bottom: directly running the static model, trained on DAVIS without any fine-tuning, on frames from sequences in SegTrackV2 <ref type="bibr" target="#b27">[35]</ref>, which contain different object categories. The good segmentation quality in the bottom demonstrates again that our experiential definition of objectness or saliency transfers well between different object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT</head><p>Ours Ours Ours GT GT <ref type="figure" target="#fig_0">Figure 12</ref>. Failure case: multiple salient objects appear in the same image are all captured by our static model. However, the ground-truth annotations only highlight one of them.</p><p>[20] Xiaodi Hou and Liqing Zhang. Image GT AnDiff CIS Ours <ref type="figure">Figure 13</ref>. Visual comparison on Youtube-VOS <ref type="bibr" target="#b65">[73]</ref>. From left to right: Image, Ground-truth, AnDiff <ref type="bibr" target="#b71">[79]</ref>, CIS <ref type="bibr" target="#b69">[77]</ref>, Ours.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>System overview. Dynamic: motion segmentation model described in Sec. 3.1; Static: object model described in Sec. 3.2. The training ("dynamic-static bootstrapping" in Sec. 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>With image I inFig. 3, only the mask in (c) minimizes mutual information between flow fields of the object (inside mask) and the background (outside mask).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Temporally consistent mutual information minimization prevents label flipping (2nd and 3rd columns), and also improves segmentation accuracy (the motorbike).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F</head><label></label><figDesc>? (?(I), m) = (1 + ? 2 ) ?(?(I), m)?(?(I), m) ? 2 ?(?(I), m) + ?(?(I), m)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Comparison between the dynamic and static object models. After the first round of learning from noisy motion segmentations (?(u), second row) with the proposed confidence-aware adaptation, the static object model (?(I), fourth row) improves over the dynamic model on all cases where the motion is noisy (a), the object is partially static (b) or fully static (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Distribution of LA computed using the output of the dynamic model ? (orange) and the static model ? (blue) on the birdfall sequence, which shows that LA is a good indicator of reliable masks when the motion field is informative, as verified by the gap between the distributions of LA.image flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Our temporally consistent dynamic model v.s. CIS<ref type="bibr" target="#b69">[77]</ref>.</figDesc><table><row><cell></cell><cell>.2</cell><cell>36.8</cell><cell>45.6</cell></row><row><cell>Ours</cell><cell>62.4</cell><cell>40.0</cell><cell>49.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparison on video object segmentation benchmarks with both supervised and fully unsupervised methods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Supervised Methods</cell><cell></cell><cell></cell><cell cols="3">Unsupervised Methods</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6"># Annot. DAVIS FBMS(J/F) SegTV2</cell><cell></cell><cell cols="3">DAVIS FBMS SegTV2</cell></row><row><cell></cell><cell>MATNet [83]</cell><cell cols="2">14,000</cell><cell>82.4</cell><cell cols="2">76.1 / --</cell><cell>50.2</cell><cell>ARP [30]</cell><cell>76.2</cell><cell>59.8</cell><cell>57.2</cell></row><row><cell></cell><cell>AnDiff [79]</cell><cell cols="2">2,000</cell><cell>81.7</cell><cell cols="2">--/ 81.2</cell><cell>48.3</cell><cell>ELM [33]</cell><cell>61.8</cell><cell>61.6</cell><cell>-</cell></row><row><cell></cell><cell>COSNet [42]</cell><cell cols="2">17,000</cell><cell>80.5</cell><cell cols="2">75.6 / --</cell><cell>49.7</cell><cell>FST [48]</cell><cell>55.8</cell><cell>47.7</cell><cell>47.8</cell></row><row><cell></cell><cell>EPONet [13]</cell><cell cols="2">2,000+</cell><cell>80.6</cell><cell>-</cell><cell></cell><cell>70.9</cell><cell>NLC [14]</cell><cell>55.1</cell><cell>51.5</cell><cell>67.2</cell></row><row><cell></cell><cell>PDB [59]</cell><cell cols="2">17,000</cell><cell>77.2</cell><cell cols="2">74.0 / 81.5</cell><cell>60.9</cell><cell>SAGE [68]</cell><cell>42.6</cell><cell>61.2</cell><cell>57.6</cell></row><row><cell></cell><cell>LVO [64]</cell><cell cols="2">2,000+</cell><cell>75.9</cell><cell cols="2">65.1 / 77.8</cell><cell>57.3</cell><cell>STP [22]</cell><cell>77.6</cell><cell>60.8</cell><cell>70.1</cell></row><row><cell></cell><cell>FSEG [25]</cell><cell cols="2">10,500</cell><cell>70.7</cell><cell cols="2">68.4 / --</cell><cell>61.4</cell><cell>CIS [77]</cell><cell>71.5</cell><cell>63.6</cell><cell>62.0</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">2,000</cell><cell>82.8</cell><cell cols="2">75.8 / 82.0</cell><cell>74.2</cell><cell>Ours</cell><cell>80.0</cell><cell>73.2</cell><cell>74.2</cell></row><row><cell></cell><cell cols="3">Supervised Methods</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Unsupervised Methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="12">DSS [19] NDF [43] SR [66] RBD [84] DSR [36] HS [86] CHS [67] SBF [74] USD [81] DUSPS [46] Ours</cell></row><row><cell>ECCSD</cell><cell>87.9</cell><cell>89.1</cell><cell>82.6</cell><cell></cell><cell>65.2</cell><cell>63.9</cell><cell>62.3</cell><cell>68.2</cell><cell>78.7</cell><cell>87.8</cell><cell>87.4</cell><cell>88.1</cell></row><row><cell>MSRA-B</cell><cell>89.4</cell><cell>89.7</cell><cell>85.1</cell><cell></cell><cell>75.1</cell><cell>72.3</cell><cell>71.3</cell><cell>79.8</cell><cell>-</cell><cell>87.7</cell><cell>90.3</cell><cell>89.7</cell></row><row><cell>DUT</cell><cell>72.9</cell><cell>73.6</cell><cell>67.2</cell><cell></cell><cell>51.0</cell><cell>55.8</cell><cell>52.1</cell><cell>-</cell><cell>58.3</cell><cell>71.6</cell><cell>73.6</cell><cell>73.9</cell></row><row><cell></cell><cell cols="10">Table 4. Quantitative results on saliency prediction (or salient object detection) benchmarks.</cell><cell></cell></row><row><cell cols="6"># of Rounds DAVIS FBMS SegTV2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>73.8</cell><cell>65.5</cell><cell></cell><cell>65.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>79.2</cell><cell>71.7</cell><cell></cell><cell>73.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell>80.0</cell><cell>73.2</cell><cell></cell><cell>74.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>The mIoU improves over multiple rounds of dynamic-static bootstrapping.</figDesc><table><row><cell># of videos</cell><cell>12</cell><cell>36</cell><cell>60</cell><cell>84</cell><cell>108</cell></row><row><cell>mIoU</cell><cell cols="5">48.3 52.4 54.9 58.8 61.4</cell></row><row><cell>Std. Dev.</cell><cell cols="5">1.99 2.37 2.30 1.17 1.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .Table 7 .</head><label>67</label><figDesc>Static model ? improves over time as more videos are observed. Results on DAVIS17 [51] and Youtube-VOS [73] datasets.</figDesc><table><row><cell></cell><cell cols="4">MATNet [83] Andiff [79] ARP [30] CIS [77] Ours</cell></row><row><cell>DAVIS17</cell><cell>58.6</cell><cell>57.8</cell><cell>50.2</cell><cell>53.1 58.9</cell></row><row><cell>YTVOS</cell><cell>-</cell><cell>46.1</cell><cell>28.7</cell><cell>15.6 47.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Saliency detection: A spectral residual approach. In 2007 IEEE Conference on computer vision and pattern recognition, pages 1-8. Ieee, 2007. 2 [21] Ping Hu, Gang Wang, Xiangfei Kong, Jason Kuen, and Yap-Peng Tan. Motion-guided cascaded refinement network for video object segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1400-1409, 2018. 2 [22] Yuan-Ting Hu, Jia-Bin Huang, and Alexander G Schwing. Unsupervised video object segmentation using motion saliencyguided spatio-temporal propagation. In Proceedings of the European conference on computer vision (ECCV), pages 786-802, 2018. 3, 8 [23] Serdar Ince and Janusz Konrad. Occlusion-aware optical flow estimation. IEEE Transactions on Image Processing, 17(8):1443-1451, 2008. 4 [24] Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions on pattern analysis and machine intelligence, 20(11):1254-1259, 1998. 2 [25] Suyog Dutt Jain, Bo Xiong, and Kristen Grauman. Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos. In 2017 IEEE conference on computer vision and pattern recognition (CVPR), pages 2117-2126. IEEE, 2017. 3, 8 [26] Huaizu Jiang, Jingdong Wang, Zejian Yuan, Yang Wu, Nanning Zheng, and Shipeng Li. Salient object detection: A discriminative regional feature integration approach. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2083-2090, 2013. 6</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>Research supported by ARO W911NF-17-1-0304 and ONR N00014-17-1-2072.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheila</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Visual motion perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><forename type="middle">R</forename><surname>Albright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2433" to="2440" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detachable object detection: Segmentation and depth ordering from short-baseline video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alper</forename><surname>Ayvaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1942" to="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toddler-inspired visual object learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Bambach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1201" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emergence of object segmentation in perturbed generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7254" to="7264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised object segmentation by redrawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micka?l</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Arti?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12705" to="12716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorin</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards segmenting anything that moves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Background and foreground modeling using nonparametric kernel density estimation for visual surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramani</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1151" to="1163" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Exploiting geometric constraints on dense trajectories for motion saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13258</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Video segmentation by nonlocal consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient graph-based image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4083" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The ecological approach to visual perception: classic edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sensation and perception. Cengage Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Brockmole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion trajectory segmentation via minimum cost multicuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3271" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Center-surround divergence of feature statistics for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dominik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2214" to="2219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: towards the underlying neural circuitry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Matters of intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="115" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos based on region augmentation and reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7417" to="7425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning layered motion segmentations of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="319" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-supervised learning for video correspondence flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00875</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Extending layered models to 3d motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Sundaramoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="435" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Turbopixels: Fast superpixels using geometric flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Stere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kiriakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2290" to="2297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figureground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taeyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Region enhanced scaleinvariant saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1477" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">See more, know more: Unsupervised video object segmentation with co-attention siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3623" to="3632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshaya</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Marc</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer vision and pattern recognition</title>
		<meeting>the IEEE Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6609" to="6617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contrast-based image attention analysis by using fuzzy growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Fei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM international conference on Multimedia</title>
		<meeting>the eleventh ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Motion-based background subtraction using adaptive kernel density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepusps: Deep robust unsupervised saliency prediction via self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tam</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Dax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaithanya</forename><surname>Kumar Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nhung</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi Hoai Phuong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="204" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anestis</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust foreground detection in video using pixel layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilios</forename><surname>Morellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="746" to="751" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Demystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13916</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image change detection algorithms: a systematic survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Richard J Radke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Andra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badrinath</forename><surname>Al-Kofahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roysam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="294" to="307" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Background subtraction for freely moving cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1219" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Submodular trajectories for better motion segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianteng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2688" to="2700" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A unified approach to salient object detection via low rank matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="853" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Motion segmentation and tracking using normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Computer Vision (IEEE Cat. No. 98CH36271)</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1154" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="717" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="715" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Principles of object perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elizabeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spelke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="56" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning patterns of activity using real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="747" to="757" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasiliy</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4268" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning motion patterns in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3386" to="3394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Modeling visual attention via selective tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John K Tsotsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winky</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan Kei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhong</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nuflo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="507" to="545" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4019" to="4028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Saliencyaware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3395" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3978" to="3987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Smoothness in layers: Motion segmentation using nonparametric mixture estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="520" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maojun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1140" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised trajectory clustering via adaptive multi-kernelbased shrinkage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4328" to="4336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning to manipulate individual objects in an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6558" to="6567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised moving object detection via contextual information separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Self-occlusions and disocclusions in causal video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Sundaramoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4408" to="4416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Anchor diffusion for unsupervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Visual attention detection in video sequences using spatiotemporal cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Multimedia</title>
		<meeting>the 14th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep unsupervised saliency detection: A multiple noisy labeling perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9029" to="9038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Motion-attentive transition for zero-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhou</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangjiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Unsupervised online video object segmentation with motion property understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="249" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Harf: Hierarchyassociated rich features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
