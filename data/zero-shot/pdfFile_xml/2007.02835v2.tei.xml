<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<email>yu.rong@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Bian</surname></persName>
							<email>yatao.bian@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Xie</surname></persName>
							<email>weiyangxie@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>jzhuang@uta.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Graph Transformer on Large-Scale Molecular Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How to obtain informative representations of molecules is a crucial prerequisite in AI-driven drug design and discovery. Recent researches abstract molecules as graphs and employ Graph Neural Networks (GNNs) for molecular representation learning. Nevertheless, two issues impede the usage of GNNs in real scenarios:</p><p>(1) insufficient labeled molecules for supervised training; (2) poor generalization capability to new-synthesized molecules. To address them both, we propose a novel framework, GROVER, which stands for Graph Representation frOm self-superVised mEssage passing tRansformer. With carefully designed self-supervised tasks in node-, edge-and graph-level, GROVER can learn rich structural and semantic information of molecules from enormous unlabelled molecular data. Rather, to encode such complex information, GROVER integrates Message Passing Networks into the Transformer-style architecture to deliver a class of more expressive encoders of molecules. The flexibility of GROVER allows it to be trained efficiently on large-scale molecular dataset without requiring any supervision, thus being immunized to the two issues mentioned above. We pre-train GROVER with 100 million parameters on 10 million unlabelled molecules-the biggest GNN and the largest training dataset in molecular representation learning. We then leverage the pre-trained GROVER for molecular property prediction followed by task-specific fine-tuning, where we observe a huge improvement (more than 6% on average) from current state-of-the-art methods on 11 challenging benchmarks. The insights we gained are that well-designed self-supervision losses and largely-expressive pre-trained models enjoy the significant potential on performance boosting. * Equal contribution.</p><p>Despite the fruitful progress, two issues still impede the usage of deep learning in real scenarios:</p><p>(1) insufficient labeled data for molecular tasks; (2) poor generalization capability of models in the enormous chemical space. Different from other domains (such as image classification) that have rich-source labeled data, getting labels of molecular property requires wet-lab experiments which is time-consuming and resource-costly. As a consequence, most public molecular benchmarks contain far-from-adequate labels. Conducting deep learning on these benchmarks is prone to over-fitting and the learned model can hardly cope with the out-of-distribution molecules.</p><p>Indeed, it has been a long-standing goal in deep learning to improve the generalization power of neural networks. Towards this goal, certain progress has been made. For example, in Natural Language Processing (NLP), researchers can pre-train the model from large-scale unlabeled sentences via a newly-proposed technique-the self-supervised learning. Several successful self-supervised pretraining strategies, such as BERT [9] and GPT <ref type="bibr" target="#b37">[38]</ref> have been developed to tackle a variety of language tasks. By contending that molecule can be transformed into sequential representation-SMILES [59], the work by <ref type="bibr" target="#b57">[58]</ref> tries to adopt the BERT-style method to pretrain the model, and Liu  et.al. [29]  also exploit the idea from N-gram approach in NLP and conducts vertices embedding by predicting the vertices attributes. Unfortunately, these approaches fail to explicitly encode the structural information of molecules as using the SMILES representation is not topology-aware.</p><p>Without using SMILES, several works aim to establish a pre-trained model directly on the graph representations of molecules. Hu et.al. <ref type="bibr" target="#b17">[18]</ref> investigate the strategies to construct the three pretraining tasks, i.e., context prediction and node masking for node-level self-supervised learning and graph property prediction for graph-level pre-training. We argue that the formulation of pre-training in this way is suboptimal. First, in the masking task, they treat the atom type as the label. Different from NLP tasks, the number of atom types in molecules is much smaller than the size of a language vocabulary. Therefore, it would suffer from serious representation ambiguity and the model is hard to encode meaningful information especially for the highly frequent atoms. Second, the graph-level pre-training task in [18] is supervised. This limits the usage in practice since most of molecules are completely unlabelled, and it also introduces the risk of negative transfer for the downstream tasks if they are inconsistent to the graph-level supervised loss.</p><p>In this paper, we improve the pre-training model for molecular graph by introducing a novel molecular representation framework, GROVER, namely, Graph Representation frOm self-superVised mEssage passing tRansformer. GROVER constructs two types of self-supervised tasks. For the node/edge-level tasks, instead of predicting the node/edge type alone, GROVER randomly masks a local subgraph of the target node/edge and predicts this contextual property from node embeddings. In this way, GROVER can alleviate the ambiguity problem by considering both the target node/edge and its context being masked. For the graph-level tasks, by incorporating the domain knowledge, GROVER extracts the semantic motifs existing in molecular graphs and predicts the occurrence of these motifs for a molecule from graph embeddings. Since the semantic motifs can be obtained by a low-cost pattern matching method, GROVER can make use of any molecular to optimize the graph-level embedding. With self-supervised tasks in node-, edge-and graph-level, GROVER can learn rich structural and semantic information of molecules from enormous unlabelled molecular data. Rather, to encode such complex information, GROVER integrates Message Passing Networks with the Transformer-style architecture to deliver a class of highly expressive encoders of molecules. The flexibility of GROVER allows it to be trained efficiently on large-scale molecular data without requiring any supervision. We pre-train GROVER with 100 million parameters on 10 million of unlabelled molecules-the biggest GNN and the largest training dataset that have been applied. We then leverage the pre-trained GROVER models to downstream molecular property prediction tasks followed by task-specific fine-tuning. On the downstream tasks, GROVER achieve 22.4% relative improvement compared with [29] and 7.4% relative improvement compared with [18] on classification tasks. Furthermore, even compared with current state-of-the-art results for each data set, we observe a huge relative improvement of GROVER (more than 6% on average) over 11 popular benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Molecular Representation Learning. To represent molecules in the vector space, the traditional chemical fingerprints, such as ECFP <ref type="bibr" target="#b41">[42]</ref>, try to encode the neighbors of atoms in the molecule into a fix-length vector. To improve the expressive power of chemical fingerprints, some studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> introduce convolutional layers to learn the neural fingerprints of molecules, and apply the neural</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inspired by the remarkable achievements of deep learning in many scientific domains, such as computer vision <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b56">57]</ref>, natural language processing <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53]</ref>, and social networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b30">31]</ref>, researchers are exploiting deep learning approaches to accelerate the process of drug discovery and reduce costs by facilitating the rapid identification of molecules <ref type="bibr" target="#b4">[5]</ref>. Molecules can be naturally represented by molecular graphs which preserve rich structural information. Therefore, supervised deep learning of graphs, especially with Graph Neural Networks(GNNs) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b45">46]</ref> have shown promising results in many tasks, such as molecular property prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> and virtual screening <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>fingerprints to the downstream tasks, such as property prediction. Following these works, <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b61">62]</ref> take the SMILES representation <ref type="bibr" target="#b58">[59]</ref> as input and use RNN-based models to produce molecular representations. Recently, many works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> explore the graph convolutional network to encode molecular graphs into neural fingerprints. A slot of work <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b60">61]</ref> propose to learn the aggregation weights by extending the Graph Attention Network (GAT) <ref type="bibr" target="#b53">[54]</ref>. To better capture the interactions among atoms, <ref type="bibr" target="#b12">[13]</ref> proposes to use a message passing framework and <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b62">63]</ref> extend this framework to model bond interactions. Furthermore, <ref type="bibr" target="#b29">[30]</ref> builds a hierarchical GNN to capture multilevel interactions.</p><p>Self-supervised Learning on Graphs. Self-supervised learning has a long history in machine learning and has achieved fruitful progresses in many areas, such as computer vision <ref type="bibr" target="#b34">[35]</ref> and language modeling <ref type="bibr" target="#b8">[9]</ref>. The traditional graph embedding methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37]</ref> define different kinds of graph proximity, i.e., the vertex proximity relationship, as the self-supervised objective to learn vertex embeddings. GraphSAGE <ref type="bibr" target="#b14">[15]</ref> proposes to use a random-walk based proximity objective to train GNN in an unsupervised fashion. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b54">55]</ref> exploit the mutual information maximization scheme to construct objective for GNNs. Recently, two works are proposed to construct unsupervised representations for molecular graphs. Liu et.al. <ref type="bibr" target="#b28">[29]</ref> employ an N-gram model to extract the context of vertices and construct the graph representation by assembling the vertex embeddings in short walks in the graph. Hu et.al. <ref type="bibr" target="#b17">[18]</ref> investigate various strategies to pre-train the GNNs and propose three self-supervised tasks to learn molecular representations. However, <ref type="bibr" target="#b17">[18]</ref> isolates the highly correlated tasks of context prediction and node/edge type prediction, which makes it difficult to preserve domain knowledge between the local structure and the node attributes. Besides, the graph-level task in <ref type="bibr" target="#b17">[18]</ref> is constructed by the supervised property labels, which is impeded by the limited number of supervised labels of molecules and has demonstrated the negative transfer in the downstream tasks. Contrast with <ref type="bibr" target="#b17">[18]</ref>, the molecular representations derived by our method are more appropriate in terms of persevering the domain knowledge, which has demonstrated remarkable effectiveness in downstream tasks without negative transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries of Transformer-style Models and Graph Neural Networks</head><p>We briefly introduce the concepts of supervised graph learning, Transformer <ref type="bibr" target="#b52">[53]</ref>, and GNNs in this section.</p><p>Supervised learning tasks of graphs. A molecule can be abstracted as a graph G = (V, E), where |V| = n refers to a set of n nodes (atoms) and |E| = m refers to a set of m edges (bonds) in the molecule. N v is used to denote the set of neighbors of node v. We use x v to represent the initial features of node v, and e uv as the initial features of edge <ref type="bibr">(u, v)</ref>. In graph learning, there are usually two categories of supervised tasks: i) Node classification/regression, where each node v has a label/target y v , and the task is to learn to predict the labels of unseen nodes; ii) Graph classification/regression, where a set of graphs {G 1 , ..., G N } and their labels/targets {y 1 , ..., y N } are given, and the task is to predict the label/target of a new graph.</p><p>Attention mechanism and the Transformer-style architectures. The attention mechanism is the main building block of Transformer. We focus on multi-head attention, which stacks several scaled dot-product attention layers together and allows parallel running. One scaled dot-product attention layer takes a set of queries, keys, values (q, k, v) as inputs. Then it computes the dot products of the query with all keys, and applies a softmax function to obtain the weights on the values. By stacking the set of (q, k, v)s into matrices (Q, K, V), it admits highly optimized matrix multiplication operations. Specifically, the outputs can be arranged as a matrix:</p><formula xml:id="formula_0">Attention(Q, K, V) = softmax(QK / ? d)V,<label>(1)</label></formula><p>where d is the dimension of q and k. Suppose we arrange k attention layers into the multi-head attention, then its output matrix can be written as,</p><formula xml:id="formula_1">MultiHead(Q, K, V) = Concat(head 1 , ..., head k )W O , head i = Attention(QW Q i , KW K i , VW V i ),<label>(2)</label></formula><p>where W Q i , W K i , W V i are the projection matrices of head i. Graph Neural Networks (GNNs). Recently, GNNs have received a surge of interest in various domains, such as knowledge graph, social networks and drug discovery. The key operation of GNNs lies in a message passing process, which involves message passing (also called neighborhood aggregation) between the nodes in the graph. The message passing operation iteratively updates a node v's hidden states, h v , by aggregating the hidden states of v's neighboring nodes and edges. In general, the message passing process involves several iterations, each iteration can be further partitioned into several hops. Suppose there are L iterations, and iteration l contains K l hops. Formally, in iteration l, the k-th hop can be formulated as,</p><formula xml:id="formula_2">m (l,k) v = AGGREGATE (l) ({(h (l,k?1) v , h (l,k?1) u , e uv ) | u ? N v }),<label>(3)</label></formula><formula xml:id="formula_3">h (l,k) v = ?(W (l) m (l,k) v + b (l) ), where m (l,k) v</formula><p>is the aggregated message, and ?(?) is some activation function. We make the convention that h</p><formula xml:id="formula_4">(l,0) v := h (l?1,K l?1 ) v .</formula><p>There are several popular ways of choosing AGGREGATE (l) (?), such as mean, max pooling and graph attention mechanism <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b53">54]</ref>. For one iteration of message passing, there are a layer of trainable parameters (i.e., parameters inside AGGREGATE (l) (?), W (l) and b (l) . These parameters are shared across the K l hops within the iteration l. After L iterations of message passing, the hidden states of the last hop in the last iteration are used as the embeddings of the nodes, i.e., h</p><formula xml:id="formula_5">(L,K L ) v , v ? V.</formula><p>Lastly, a READOUT operation is applied to get the graph-level representation,</p><formula xml:id="formula_6">h G = READOUT({h (0,K0) v , ..., h (L,K L ) v | v ? V}).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The GROVER Pre-training Framework</head><p>This section contains details of our pre-training architecture together with the well-designed selfsupervision tasks. On a high level, the model is a Transformer-based neural network with tailored GNNs as the self-attention building blocks. The GNNs therein enable capturing structural information in the graph data and information flow on both the node and edge message passing paths. Furthermore, we introduce a dynamic message passing scheme in the tailored GNN, which is proved to boost the generalization performance of GROVER models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Details of Model Architecture</head><p>GROVER consists of two modules: the node GNN transformer and edge GNN transformer. In order to ease the exposition, we will only explain details of the node GNN transformer (abbreviated as node GTransformer) in the sequel, and ignore the edge GNN transformer since it has a similar structure. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates the overall architecture of node GTransformer. More details of GROVER are deferred to Appendix A.  GNN Transformer (GTransformer). The key component of the node GTransformer is our proposed graph multihead attention component, which is the attention blocks tailored to structural input data. A vanilla attention block, such as that in Equation <ref type="formula" target="#formula_0">(1)</ref>, requires vectorized inputs. However, graph inputs are naturally structural data that are not vectorized. So we design a tailored GNNs (dyMPN, see the following sections for details) to extract vectors as queries, keys and values from nodes of the graph, then feed them into the attention block.</p><p>This strategy is simple yet powerful, because it enables utilizing the highly expressive GNN models, to better model the structural information in molecular data. The high expressiveness of GTransformer can be attributed to its bi-level information extraction framework. It is wellknown that the message passing process captures local structural information of the graph, therefore using the outputs of the GNN model as queries, keys and values would get the local subgraph structure involved, thus constituting the first level of information extraction. Meanwhile, the Transformer encoder can be viewed as a variant of the GAT <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b53">54]</ref> on a fully connected graph constructed by V. Hence, using Transformer encoder on top of these queries,  <ref type="figure">Figure 2</ref>: Overview of the designed self-supervised tasks of GROVER. keys and values makes it possible to extract global relations between nodes, which enables the second level of information extraction. This bi-level information extraction strategy largely enhances the representational power of GROVER models.</p><p>Additionally, GTransformer applies a single long-range residual connection from the input feature to convey the initial node/edge feature information directly to the last layers of GTransformer instead of multiple short-range residual connections in the original Transformer architecture. Two benefits could be obtained from this single long-range residual connection: i) like ordinary residual connections, it improves the training process by alleviating the vanishing gradient problem <ref type="bibr" target="#b16">[17]</ref>, ii) compared to the various short-range residual connections in the Transformer encoder, our long-range residual connection can alleviate the over-smoothing <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref> problem in the message passing process.</p><p>Dynamic Message Passing Network (dyMPN). The general message passing process (see Equation (3)) has two hyperparameters: number of iterations/layers L and number of hops K l , l = 1, ..., L within each iteration. The number of hops is closely related to the size of the receptive field of the graph convolution operation, which would affect generalizability of the message passing model.</p><p>Given a fixed number of layers L, we find out that the pre-specified number of hops might not work well for different kinds of dataset. Instead of pre-specified K l , we develop a randomized strategy for choosing the number of message passing hops during training process: at each epoch, we choose K l from some random distribution for layer l. Two choices of randomization work well: i) K l ? U (a, b), drawn from a uniform distribution; ii) K l is drawn from a truncated normal distribution ?(?, ?, a, b) , which is derived from that of a normally distributed random variable by bounding the random variable from both bellow and above. Specifically, let its support be x ? [a, b], then the</p><formula xml:id="formula_7">p.d.f. is f (x) = 1 ? 2? exp [? 1 2 ( x?? ? ) 2 ] ?[?( b?? ? )??( a?? ? )] , where ?(x) = 1 2 (1 + erf( x ? 2 )</formula><p>) is the cumulative distribution of a standard normal distribution.</p><p>The above randomized message passing scheme enables random receptive field for each node in graph convolution operation. We call the induced network Dynamic Message Passing networks (abbreviated as dyMPN). Extensive experimental verification demonstrates that dyMPN enjoys better generalization performance than vanilla message passing networks without the randomization strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Self-supervised Task Construction for Pre-training</head><p>The success of the pre-training model crucially depends on the design of self-supervision tasks. Different from Hu et.al. <ref type="bibr" target="#b17">[18]</ref>, to avoid negative transfer on downstream tasks, we do not use the supervised labels in pre-training and propose new self-supervision tasks on both of these two levels: contextual property prediction and graph-level motif prediction, which are sketched in <ref type="figure">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contextual Property Prediction.</head><p>A good selfsupervision task on the node level should satisfy the following properties: 1) The prediction target is reliable and easy to get; 2) The prediction target should reflect contextual information of the node/edge. Guided by these criteria, we present the tasks on both nodes and edges. They both try to predict the context-aware properties of the target node/edge within some local subgraph. What kinds of context-aware properties shall one use? We define recurrent statistical properties of local subgraph in the following two-step manner (let us take the node subgraph in <ref type="figure">Figure 3</ref> as the example): i) Given a target node (e.g., the Carbon atom in red color), we extract its local subgraph as its k-hop neighboring nodes and edges. When k=1, it involves the Nitrogen atom, Oxygen atom, the double bond and single bond. ii) We extract statistical properties of this subgraph, specifically, we count the number of occurrence of (node, edge) pairs around the center node, which makes the term of node-edge-counts. Then we list all the node-edge counts terms in alphabetical order, which constitutes the final property: e.g., C_N-DOUBLE1_O-SINGLE1 in the example. This step can be viewed as a clustering process: the subgraphs are clustered according to the extracted properties, one property corresponds to a cluster of subgraphs with the same statistical property.</p><p>With the context-aware property defined, the contextual property prediction task works as follows: given a molecular graph, after feeding it into the GROVER encoder, we obtain embeddings of its atoms and bonds. Suppose randomly choose the atom v and its embedding is h v . Instead of predicting the atom type of v, we would like h v to encode some contextual information around node v. The way to achieve this target is to feed h v into a very simple model (such as a fully connected layer), then use the output to predict the contextual properties of node v. This prediction is a multi-class prediction problem (one class corresponds to one contextual property).</p><p>Graph-level Motif Prediction. Graph-level self-supervision task also needs reliable and cheap labels. Motifs are recurrent sub-graphs among the input graph data, which are prevalent in molecular graph data. One important class of motifs in molecules are functional groups, which encodes the rich domain knowledge of molecules and can be easily detected by the professional software, such as RDKit <ref type="bibr" target="#b26">[27]</ref>. Formally, the motif prediction task can be formulated as a multi-label classification problem, where each motif corresponds to one label. Suppose we are considering the presence of p motifs {m 1 , ..., m p } in the molecular data. For one specific molecule (abstracted as a graph G), we use RDKit to detect whether each of the motif shows up in G, then use it as the target of the motif prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fine-tuning for Downstream Tasks</head><p>After pre-training GROVER models on massive unlabelled data with the designed self-supervised tasks, one should obtain a high-quality molecular encoder which is able to output embeddings for both nodes and edges. These embeddings can be used for downstream tasks through the fine-tuning process. Various downstream tasks could benefit from the pre-trained GROVER models. They can be roughly divided into three categories: node level tasks, e.g., node classification; edge level tasks, e.g., link prediction; and graph level tasks, such as the property prediction for molecules. Take the graph level task for instance. Given node/edge embeddings output by the GROVER encoder, we can apply some READOUT function (Equation <ref type="formula" target="#formula_6">(4)</ref>) to get the graph embedding firstly, then use additional multiple layer perceptron (MLP) to predict the property of the molecular graph. One would use part of the supervised data to fine-tune both the encoder and additional parameters (READOUT and MLP). After several epochs of fine-tuning, one can expect a well-performed model for property prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Pre-training Data Collection. We collect 11 million (M) unlabelled molecules sampled from ZINC15 <ref type="bibr" target="#b47">[48]</ref> and Chembl <ref type="bibr" target="#b10">[11]</ref> datasets to pre-train GROVER. We randomly split 10% of unlabelled molecules as the validation sets for model selection.</p><p>Fine-tuning Tasks and Datasets. To thoroughly evaluate GROVER on downstream tasks, we conduct experiments on 11 benchmark datasets from the MoleculeNet <ref type="bibr" target="#b59">[60]</ref> with various targets, such as quantum mechanics, physical chemistry, biophysics and physiology. <ref type="bibr" target="#b2">3</ref> Details are deferred to Appendix B.1. In machine learning tasks, random splitting is a common process to split the dataset. However, for molecular property prediction, scaffold splitting <ref type="bibr" target="#b1">[2]</ref> offers a more challenging yet realistic way of splitting. We adopt the scaffold splitting method with a ratio for train/validation/test as 8:1:1. For each dataset, as suggested by <ref type="bibr" target="#b59">[60]</ref>, we apply three independent runs on three randomseeded scaffold splitting and report the mean and standard deviations.</p><p>Baselines. We comprehensively evaluate GROVER against 10 popular baselines from MoleculeNet <ref type="bibr" target="#b59">[60]</ref> and several state-of-the-arts (STOAs) approaches. Among them, TF_Roubust <ref type="bibr" target="#b39">[40]</ref> is a DNNbased mulitask framework taking the molecular fingerprints as the input. GraphConv <ref type="bibr" target="#b23">[24]</ref>, Weave  <ref type="bibr" target="#b22">[23]</ref> and SchNet <ref type="bibr" target="#b44">[45]</ref> are three graph convolutional models. MPNN <ref type="bibr" target="#b12">[13]</ref> and its variants DMPNN <ref type="bibr" target="#b62">[63]</ref> and MGCN <ref type="bibr" target="#b29">[30]</ref> are models considering the edge features during message passing. AttentiveFP <ref type="bibr" target="#b60">[61]</ref> is an extension of the graph attention network. Specifically, to demonstrate the power of our self-supervised strategy, we also compare GROVER with two pre-trained models: N-Gram <ref type="bibr" target="#b28">[29]</ref> and Hu et.al <ref type="bibr" target="#b17">[18]</ref>. We only report classification results for <ref type="bibr" target="#b17">[18]</ref> since the original implementation do not admit regression task without non-trivial modifications.</p><p>Experimental Configurations. We use Adam optimizer for both pre-train and fine-tuning. The Noam learning rate scheduler <ref type="bibr" target="#b8">[9]</ref> is adopted to adjust the learning rate during training. Specific configurations are:</p><p>GROVER Pre-training. For the contextual property prediction task, we set the context radius k = 1 to extract the contextual property dictionary, and obtain 2518 and 2686 distinct node and edge contextual properties as the node and edge label, respectively. For each molecular graph, we randomly mask 15% of node and edge labels for prediction. For the graph-level motif prediction task, we use RDKit <ref type="bibr" target="#b26">[27]</ref> to extract 85 functional groups as the motifs of molecules. We represent the label of motifs as the one-hot vector. To evaluate the effect of model size, we pre-train two GROVER models, GROVER base and GROVER large with different hidden sizes, while keeping all other hyper-parameters the same. Specifically, GROVER base contains ?48M parameters and GROVER large contains ?100M parameters. We use 250 Nvidia V100 GPUs to pre-train GROVER base and GROVER large . Pre-training GROVER base and GROVER large took 2.5 days and 4 days respectively. For the models depicted in Section 5.2, we use 32 Nvidia V100 GPUs to pre-train the GROVER model and its variants.</p><p>Fine-tuning Procedure. We use the validation loss to select the best model. For each training process, we train models for 100 epochs. For hyper-parameters, we perform the random search on the validation set for each dataset and report the best results. More pre-training and fine-tuning details are deferred to Appendix C and Appendix D. <ref type="table" target="#tab_1">Table 1</ref> documents the overall results of all models on all datasets, where the cells in gray indicate the previous SOTAs, and the cells in blue indicates the best result achieved by GROVER. <ref type="table" target="#tab_1">Table 1</ref> offers the following observations: (1) GROVER models consistently achieve the best performance on all datasets with large margin on most of them. The overall relative improvement is 6.1% on all datasets (2.2% on classification tasks and 10.8% on regression tasks). <ref type="bibr" target="#b4">5</ref> . This remarkable boosting validates the effectiveness of the pre-training model GROVER for molecular property prediction tasks. (2) Specifically, GROVER base outperforms the STOAs on 8/11 datasets, while GROVER large surpasses the STOAs on all datasets. This improvement can be attributed to the high expressive power of the large model, which can encode more information from the self-supervised tasks. (3) In the small dataset FreeSolv with only 642 labeled molecules, GROVER gains a 23.9% relative improvement over existing SOTAs. This confirms the strength of GROVER since it can significantly help with the tasks with very little label information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results on Downstream Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies on Design Choices of the GROVER Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">How Useful is the Self-supervised Pre-training?</head><p>To investigate the contribution of the self-supervision strategies, we compare the performances of pre-trained GROVER and GROVER without pre-training on classification datasets, both of which follow the same hyper-parameter setting. We report the comparison of classification task in <ref type="table" target="#tab_3">Table 2</ref>, it is not supervising that the performance of GROVER becomes worse without pre-training. The selfsupervised pre-training leads to a performance boost with an average AUC increase of 3.8% over the model without pre-training. This confirms that the self-supervised pre-training strategy can learn the implicit domain knowledge and enhance the prediction performance of downstream tasks. Notably, the datasets with fewer samples, such as SIDER, ClinTox and BACE gain a larger improvement through the self-supervised pre-training. It re-confirms the effectiveness of the self-supervised pre-training for the task with insufficient labeled molecules. To verify the expressive power of GTransformer, we implement GIN and MPNN based on our framework. We use a toy data set with 600K unlabelled molecules to pretrain GROVER with different backbones under the same training setting with nearly the same number of parameters (38M parameters). As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, GROVER with GTransformer backbone outperforms GIN and MPNN in both training and validation, which again verifies the effectiveness of GTransformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">How Powerful is GTransformer Backbone?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Effect of the Proposed dyMPN and GTransformer.</head><p>To justify the rationale behind the proposed GTransformer and dyMPN, we implement two variants: GROVER w/o dyMPN and GROVER w/o GTrans. GROVER w/o dyMPN fix the number of message passing hops K l , while GROVER w/o GTrans replace the GTransformer with the original Transformer. We use the same toy data set to train GROVER w/o dyMPN and GROVER w/o GTrans under the same settings in Section 5.2.2. <ref type="figure">Figure 5</ref> displays the curve of training and validation loss for three models. First, GROVER w/o GTrans is the worst one in both training and validation. It implies that trivially combining the GNN and Transformer can not enhance the expressive power of GNN. Second, dyMPN slightly harm the training loss by introducing randomness in the training process. However, the validation loss becomes better. Therefore, dyMPN brings a better generalization ability to GROVER by randomizing the receptive field for every message passing step. Overall, with new Transformer-style architecture and the dynamic message passing mechanism, GROVER enjoys high expressive power and can well capture the structural information in molecules, thus helping with various downstream molecular prediction tasks.   <ref type="figure">Figure 5</ref>: The training and validation loss of GROVER and its variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>We explore the potential of the large-scale pre-trained GNN models in this work. With well-designed self-supervised tasks and largely-expressive architecture, our model GROVER can learn rich implicit information from the enormous unlabelled graphs. More importantly, by fine-tuning on GROVER, we achieve huge improvements (more than 6% on average) over current STOAs on 11 challenging molecular property prediction benchmarks, which first verifies the power of self-supervised pretrained approaches in the graph learning area.</p><p>Despite the successes, there is still room to improve GNN pre-training in the following aspects: More self-supervised tasks. Well designed self-supervision tasks are the key of success for GNN pre-training. Except for the tasks presented in this paper, other meaningful tasks would also boost the pre-training performance, such as distance-preserving tasks and tasks that getting 3D input information involved. More downstream tasks. It is desirable to explore a larger category of downstream tasks, such as node prediction and link prediction tasks on different kinds of graphs. Different categories of downstream tasks might prefer different pre-training strategies/self-supervision tasks, which is worthwhile to study in the future. Wider and deeper models. Larger models are capable of capturing richer semantic information for more complicated tasks, as verified by several studies in the NLP area. It is also interesting to employ even larger models and data than GROVER. However, one might need to alleviate potential problems when training super large models of GNN, such as gradient vanishing and oversmoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>In this paper, we have developed a self-supervised pre-trained GNN model-GROVER to extract the useful implicit information from massive unlabelled molecules and the downstream tasks can largely benefit from this pre-trained GNN models. Below is the broader impact of our research:</p><p>-For machine learning community: This work demonstrates the success of pre-training approach on Graph Neural Networks. It is expected that our research will open up a new venue on an in-depth exploration of pre-trained GNNs for broader potential applications, such as social networks and knowledge graphs.</p><p>-For the drug discovery community: Researchers from drug discovery can benefit from GROVER from two aspects. First, GROVER has encoded rich structural information of molecules through the designing of self-supervision tasks. It can also produce feature vectors of atoms and molecule fingerprints, which can directly serve as inputs of downstream tasks. Second, GROVER is designed based on Graph Neural Networks and all the parameters are fully differentiable. So it is easy to fine-tune GROVER in conjunction with specific drug discovery tasks, in order to achieve better performance. We hope that GROVER can help with boosting the performance of various drug discovery applications, such as molecular property prediction and virtual screening.  <ref type="figure">Figure 6</ref>: Overview of the whole GROVER architecture with both node-view GTransformer (in pink background) and edge-view GTransformer (in green background) <ref type="figure">Figure 6</ref> illustrates the complete architecture of GROVER models, which contains a node-view GTransformer (in pink background) and an edge-view GTransformer (in green background). Brief presentations of the node-view GTransformer have been introduced in the main text, and the edgeview GTransformer is in a similar structure. Here we elaborate more details of the GROVER model and its associated four sets of output embeddings.</p><p>As shown in <ref type="figure">Figure 6</ref>, node-view GTransformer contains node dyMPN, which maintains hidden states of nodes h v , v ? V and performs the message passing over nodes. Meanwhile, edge-view GTransformer contains edge dyMPN, that maintains hidden states of edges h vw , h wv , (v, w) ? E and conducts message passing over edges. The edge message passing is viewed as an ordinary message passing over the line graph of the original graph, where the line graph describes the neighboring of edges in the original graph and enables an appropriate way to define message passing over edges <ref type="bibr" target="#b5">[6]</ref>. Note that edge hidden states have directions, i.e., h vw is not identical to h wv in general.</p><p>Then, after the multi-head attention, we denote the transformed node and edge hidden states byh v andh vw , respectively.</p><p>Given the above setup, we can explain why GROVER will output four sets of embeddings in <ref type="figure">Figure 6</ref>.</p><p>Let us focus on the information flow in the pink panel of <ref type="figure">Figure 6</ref>, first. Here the node hidden statesh v encounter the two components, Aggregate2Node and Aggregate2Edge, which are used to aggregate the node hidden states to node messages and edge messages, respectively. Specifically, the Aggregate2Node and Aggregate2Edge components in node-view GTransformer is formulated as follows: </p><p>Then the node-view GTransformer transforms the node messages m node-embedding-from-node-states v and edge messages m edge-embedding-from-node-states vw through Pointwise Feed Forward layers <ref type="bibr" target="#b52">[53]</ref> and Add&amp;LayerNorm to produce the final node embeddings and edge embeddings, respectively. </p><p>Then, the edge-view GTransformer transforms the node messages and edge messages through Pointwise Feed Forward layers and Add&amp;LayerNorm to produce the final node embeddings and edge embeddings, respectively.</p><p>In summary, the GROVER model outputs four sets of embeddings from two information flows. The node information flow (node GTransformer) maintains node hidden states and finally transform them into another node embeddings and edge embeddings, while the edge information flow (edge GTransformer) maintains edge hidden states and also transforms them into node and edge embeddings. The four sets of embeddings reflect structural information extracted from the two distinct views, and they are flexible to conduct downstream tasks, such as node-level prediction, edge-level prediction and graph-level prediction (via an extra READOUT component).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Fine-tuning Model for Molecular Property Prediction</head><p>As explained above, given a molecular graph G i and the corresponding label y i , GROVER produces two node embeddings, H i,node-view and H i,edge-view , from node-view GTransformer and edge-view GTransformer, respectively. We feed these two node embeddings into a shared self-attentive READ-OUT function to generate the graph-level embedding <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b53">54]</ref>:</p><formula xml:id="formula_10">S = softmax W 2 tanh W 1 H , g = Flatten(SH),<label>(9)</label></formula><p>where W 1 ? R dattn_hidden?dhidden_size and W 2 ? R dattn_out?dattn_hidden are two weight matrix and g is the final graph embedding. After the READOUT, we employ two distinct MLPs to generate two predictions: p i,node-view and p i,edge-view . Besides the supervised loss L(p i,node-view , y i ) + L(p i,edge-view , y i ), the final loss function also includes a disagreement loss <ref type="bibr" target="#b27">[28]</ref> L diss = ||p i,node-view ? p i,edge-view || 2 to retrain the consensus of two predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Constructing Contextual Properties for Edges</head><p>In Section 4.2 we describe an example of constructing contextual properties of nodes, here we present an instance of cooking edge contextual properties in order to complete the picture.</p><p>Similar to the process of node contextual property construction, we define recurrent statistical properties of local subgraph in a two-step manner. Let us take the graphs in <ref type="figure">Figure 7</ref> for instance and consider the double chemical bond in red color in the left graph.</p><p>Step I: We extract its local subgraph as its k-hop neighboring nodes and edges. When k=1, it involves the Nitrogen atom, Carbon atom and the two single bonds.</p><p>Step II: We extract statistical properties of this subgraph, specifically, we count the number of occurrence of (node, edge) pairs around the center edge, which makes the term of node-edge-counts. Then we list all the node-edge counts terms in alphabetical order, which makes the final property: e.g., DOUBLE_C_SINGLE1_N-SINGLE1 in the example.</p><p>Note that there are two graphs and two double bonds in red color in <ref type="figure">Figure 7</ref>, since their subgraphs have the same statistical property, the resulted contextual properties of the two bonds would be the same. For a different point of view, this step can be viewed as a clustering process: the subgraphs are clustered according to the extracted properties, one property corresponds to a cluster of subgraphs with the same statistical property.  <ref type="table" target="#tab_5">Table 3</ref> summaries information of benchmark datasets, including task type, dataset size, and evaluation metrics. The details of each dataset are listed bellow <ref type="bibr" target="#b59">[60]</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details about Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Dataset Description</head><p>Molecular Classification Datasets.</p><p>-BBBP <ref type="bibr" target="#b31">[32]</ref> involves records of whether a compound carries the permeability property of penetrating the blood-brain barrier.</p><p>-SIDER <ref type="bibr" target="#b25">[26]</ref> records marketed drugs along with its adverse drug reactions, also known as the Side Effect Resource .</p><p>-ClinTox <ref type="bibr" target="#b11">[12]</ref> compares drugs approved through FDA and drugs eliminated due to the toxicity during clinical trials.</p><p>-BACE <ref type="bibr" target="#b48">[49]</ref> is collected for recording compounds which could act as the inhibitors of human ?-secretase 1 (BACE-1) in the past few years. -QM8 [39] contains computer-generated quantum mechanical properties, e.g., electronic spectra and excited state energy of small molecules.</p><p>-ESOL is a small dataset documenting the solubility of compounds <ref type="bibr" target="#b7">[8]</ref>.</p><p>-Lipophilicity <ref type="bibr" target="#b10">[11]</ref> is selected from the ChEMBL database, which is an important property that affects the molecular membrane permeability and solubility. The data is obtained via octanol/water distribution coefficient experiments .</p><p>-FreeSolv <ref type="bibr" target="#b32">[33]</ref> is selected from the Free Solvation Database, which contains the hydration free energy of small molecules in water from both experiments and alchemical free energy calculations .</p><p>Dataset Splitting. We apply the scaffold splitting <ref type="bibr" target="#b1">[2]</ref> for all tasks on all datasets. It splits the molecules with distinct two-dimensional structural frameworks into different subsets. It is a more challenging but practical setting since the test molecular can be structurally different from training set. Here we apply the scaffold splitting to construct the train/validation/test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Feature Extraction Processes for Molecules</head><p>The feature extraction contains two parts: 1) Node / edge feature extraction. We use RDKit to extract the atom and bond features as the input of dyMPN.   We use Pytorch to implement GROVER and horovod <ref type="bibr" target="#b46">[47]</ref> for the distributed training. We use the Adam optimizer with learning rate 0.00015 and L2 weight decay for 10 ?7 . We train the model for 500 epochs. The learning rate warmup over the first two epochs and decreases exponentially from 0.00015 to 0.00001. We use PReLU <ref type="bibr" target="#b15">[16]</ref> as the activation function and the dropout rate is 0.1 for all layers. Both GROVER base and GROVER large contain 4 heads. We set the iteration L = 1 and sample K l ? ?(? = 6, ? = 1, a = 3, b = 9) for the embedded dyMPN in GROVER. ?(?, ?, a, b) is a truncated normal distribution with a truncation range (a, b). The hidden size for GROVER base and GROVER base are 800 and 1200 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Fine-tuning Details</head><p>For each task, we try 300 different hyper-parameter combinations via random search to find the best results. <ref type="table" target="#tab_10">Table 6</ref> demonstrates all the hyper-parameters of fine-tuning model. All fine-tuning tasks are run on a single P40 GPU.  <ref type="table" target="#tab_11">Table 7</ref> depicts the additional results of the comparison of the performance of pre-trained GROVER and GROVER without pre-training on regression tasks. In order to verify the effectiveness of the proposed self-supervised tasks, we report the fine-tuning results by Hu et al. with and without pre-training in <ref type="table" target="#tab_12">Table 8</ref>. As a comparison, we also involve the performance of GROVER with the backbone GIN and MPNN trained in Section 5.2. We find that without pre-training, our GROVER-GIN is consistent with Hu et al. on average, thus verifying the reliability of our implementations. However, after pre-training, GROVER-GIN achieves nearly 2% higher number than Hu et al., which supports the advantage of our proposed self-supervised loss. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of GTransformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Figure 3 :</head><label>13</label><figDesc>Illustration of contextual properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The training and validation losses on different backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 Figure 7 :</head><label>17</label><figDesc>Examples of constructing contextual properties for edgesSimilarly, for the information flow in the green panel, the edge hidden statesh vw encounter the two components Aggregate2Node and Aggregate2Edge as well. Their operations are formulated as follows,m node-embedding-from-edge-states v = u?Nvh uv ,(7)m edge-embedding-from-edge-states vw = u?Nv\wh uv .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The performance comparison. The numbers in brackets are the standard deviation. The methods in green are pre-trained methods.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Classification (Higher is better)</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>BBBP</cell><cell>SIDER</cell><cell>ClinTox</cell><cell>BACE</cell><cell>Tox21</cell><cell>ToxCast</cell></row><row><cell># Molecules</cell><cell>2039</cell><cell>1427</cell><cell>1478</cell><cell>1513</cell><cell>7831</cell><cell>8575</cell></row><row><cell>TF_Robust [40]</cell><cell>0.860 (0.087)</cell><cell>0.607 (0.033)</cell><cell>0.765 (0.085)</cell><cell>0.824 (0.022)</cell><cell>0.698 (0.012)</cell><cell>0.585 (0.031)</cell></row><row><cell>GraphConv [24]</cell><cell>0.877 (0.036)</cell><cell>0.593 (0.035)</cell><cell>0.845 (0.051)</cell><cell>0.854 (0.011)</cell><cell>0.772 (0.041)</cell><cell>0.650 (0.025)</cell></row><row><cell>Weave [23]</cell><cell>0.837 (0.065)</cell><cell>0.543 (0.034)</cell><cell>0.823 (0.023)</cell><cell>0.791 (0.008)</cell><cell>0.741 (0.044)</cell><cell>0.678 (0.024)</cell></row><row><cell>SchNet [45]</cell><cell>0.847 (0.024)</cell><cell>0.545 (0.038)</cell><cell>0.717 (0.042)</cell><cell>0.750 (0.033)</cell><cell>0.767 (0.025)</cell><cell>0.679 (0.021)</cell></row><row><cell>MPNN [13]</cell><cell>0.913 (0.041)</cell><cell>0.595 (0.030)</cell><cell>0.879 (0.054)</cell><cell>0.815 (0.044)</cell><cell>0.808 (0.024)</cell><cell>0.691 (0.013)</cell></row><row><cell>DMPNN [63]</cell><cell>0.919 (0.030)</cell><cell>0.632 (0.023)</cell><cell>0.897 (0.040)</cell><cell>0.852 (0.053)</cell><cell>0.826 (0.023)</cell><cell>0.718 (0.011)</cell></row><row><cell>MGCN [30]</cell><cell>0.850 (0.064)</cell><cell>0.552 (0.018)</cell><cell>0.634 (0.042)</cell><cell>0.734 (0.030)</cell><cell>0.707 (0.016)</cell><cell>0.663 (0.009)</cell></row><row><cell>AttentiveFP [61]</cell><cell>0.908 (0.050)</cell><cell>0.605 (0.060)</cell><cell>0.933 (0.020)</cell><cell>0.863 (0.015)</cell><cell>0.807 (0.020)</cell><cell>0.579 (0.001)</cell></row><row><cell>N-GRAM [29]</cell><cell>0.912 (0.013)</cell><cell>0.632 (0.005)</cell><cell>0.855 (0.037)</cell><cell>0.876 (0.035)</cell><cell>0.769 (0.027)</cell><cell>-4</cell></row><row><cell>HU. et.al[18]</cell><cell>0.915 (0.040)</cell><cell>0.614 (0.006)</cell><cell>0.762 (0.058)</cell><cell>0.851 (0.027)</cell><cell>0.811 (0.015)</cell><cell>0.714 (0.019)</cell></row><row><cell>GROVER base</cell><cell>0.936 (0.008)</cell><cell>0.656 (0.006)</cell><cell>0.925 (0.013)</cell><cell>0.878 (0.016)</cell><cell>0.819 (0.020)</cell><cell>0.723 (0.010)</cell></row><row><cell>GROVER large</cell><cell cols="4">0.940 (0.019) 0.658 (0.023) 0.944 (0.021) 0.894 (0.028)</cell><cell cols="2">0.831 (0.025) 0.737 (0.010)</cell></row><row><cell></cell><cell></cell><cell cols="3">Regression (Lower is better)</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>FreeSolv</cell><cell>ESOL</cell><cell>Lipo</cell><cell>QM7</cell><cell>QM8</cell><cell></cell></row><row><cell># Molecules</cell><cell>642</cell><cell>1128</cell><cell>4200</cell><cell>6830</cell><cell>21786</cell><cell></cell></row><row><cell>TF_Robust [40]</cell><cell>4.122 (0.085)</cell><cell>1.722 (0.038)</cell><cell>0.909 (0.060)</cell><cell>120.6 (9.6)</cell><cell>0.024 (0.001)</cell><cell></cell></row><row><cell>GraphConv [24]</cell><cell>2.900 (0.135)</cell><cell>1.068 (0.050)</cell><cell>0.712 (0.049)</cell><cell>118.9 (20.2)</cell><cell>0.021 (0.001)</cell><cell></cell></row><row><cell>Weave [23]</cell><cell>2.398 (0.250)</cell><cell>1.158 (0.055)</cell><cell>0.813 (0.042)</cell><cell>94.7 (2.7)</cell><cell>0.022 (0.001)</cell><cell></cell></row><row><cell>SchNet [45]</cell><cell>3.215 (0.755)</cell><cell>1.045 (0.064)</cell><cell>0.909 (0.098)</cell><cell>74.2 (6.0)</cell><cell>0.020 (0.002)</cell><cell></cell></row><row><cell>MPNN [13]</cell><cell>2.185 (0.952)</cell><cell>1.167 (0.430)</cell><cell>0.672 (0.051)</cell><cell>113.0 (17.2)</cell><cell>0.015 (0.002)</cell><cell></cell></row><row><cell>DMPNN [63]</cell><cell>2.177 (0.914)</cell><cell>0.980 (0.258)</cell><cell>0.653 (0.046)</cell><cell>105.8 (13.2)</cell><cell>0.0143 (0.002)</cell><cell></cell></row><row><cell>MGCN [30]</cell><cell>3.349 (0.097)</cell><cell>1.266 (0.147)</cell><cell>1.113 (0.041)</cell><cell>77.6 (4.7)</cell><cell>0.022 (0.002)</cell><cell></cell></row><row><cell>AttentiveFP [61]</cell><cell>2.030 (0.420)</cell><cell>0.853 (0.060)</cell><cell>0.650 (0.030)</cell><cell>126.7 (4.0)</cell><cell>0.0282 (0.001)</cell><cell></cell></row><row><cell>N-GRAM [29]</cell><cell>2.512 (0.190)</cell><cell>1.100 (0.160)</cell><cell>0.876 (0.033)</cell><cell>125.6 (1.5)</cell><cell>0.0320 (0.003)</cell><cell></cell></row><row><cell>GROVER base</cell><cell>1.592 (0.072)</cell><cell>0.888 (0.116)</cell><cell>0.563 (0.030)</cell><cell>72.5 (5.9)</cell><cell>0.0172 (0.002)</cell><cell></cell></row><row><cell>GROVER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>large 1.544 (0.397) 0.831 (0.120) 0.560 (0.035) 72.6 (3.8) 0.0125 (0.002)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison between GROVER with and without pre-training.</figDesc><table><row><cell>10 0</cell><cell></cell><cell cols="2">Training Loss</cell><cell></cell><cell>10 0</cell><cell></cell><cell></cell><cell cols="2">Validation Loss</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>GROVER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GROVER</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">GROVER w/o DyMPN</cell><cell></cell><cell></cell><cell></cell><cell cols="3">GROVER w/o DyMPN</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">GROVER w/o GTrans</cell><cell></cell><cell></cell><cell></cell><cell cols="3">GROVER w/o GTrans</cell></row><row><cell>Loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Loss</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>A The Overall Architecture of GROVER Model</figDesc><table><row><cell>Node Embed Node Embed</cell><cell cols="2">Edge Embed Edge Embed</cell><cell>Node Embed Node Embed</cell><cell cols="2">Edge Embed Edge Embed</cell></row><row><cell>LayerNorm LayerNorm</cell><cell></cell><cell>LayerNorm LayerNorm</cell><cell>LayerNorm LayerNorm</cell><cell></cell><cell>LayerNorm LayerNorm</cell><cell>Node-view GTransformer Node-view GTransformer</cell></row><row><cell>Feed Forward Feed Forward</cell><cell cols="2">Feed Forward Feed Forward</cell><cell>Feed Forward Feed Forward</cell><cell cols="2">Feed Forward Feed Forward</cell><cell>Edge-view GTransformer Edge-view GTransformer</cell></row><row><cell>Concat Concat</cell><cell></cell><cell>Concat Concat</cell><cell>Concat Concat</cell><cell></cell><cell>Concat Concat</cell></row><row><cell>Aggregate2Node Aggregate2Node</cell><cell cols="2">Aggregate2Edge Aggregate2Edge</cell><cell>Aggregate2Node Aggregate2Node</cell><cell cols="2">Aggregate2Edge Aggregate2Edge</cell></row><row><cell cols="2">LayerNorm LayerNorm</cell><cell></cell><cell cols="2">LayerNorm LayerNorm</cell><cell></cell></row><row><cell cols="3">Multi-Head Attention Multi-Head Attention</cell><cell cols="3">Multi-Head Attention Multi-Head Attention</cell></row><row><cell>Node DyMPN Node DyMPN Node DyMPN Q Node DyMPN Node DyMPN Node DyMPN Q</cell><cell>Node DyMPN Node DyMPN K Node DyMPN Node DyMPN Node DyMPN K Node DyMPN</cell><cell>Node DyMPN Node DyMPN V Node DyMPN Node DyMPN Node DyMPN V Node DyMPN</cell><cell>Node DyMPN Node DyMPN Edge DyMPN Q Q Node DyMPN Node Edge DyMPN DyMPN</cell><cell>Node DyMPN Node DyMPN K Edge DyMPN K Node DyMPN Node Edge DyMPN DyMPN</cell><cell>Node DyMPN Node DyMPN V Edge DyMPN V Node Node Edge DyMPN DyMPN DyMPN</cell></row><row><cell></cell><cell>Linear Linear</cell><cell></cell><cell></cell><cell>Linear Linear</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Input Graph Input Graph</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Dataset information</figDesc><table><row><cell>Type</cell><cell>Category</cell><cell>Dataset</cell><cell cols="2"># Tasks # Compounds Metric</cell></row><row><cell></cell><cell>Biophysics</cell><cell>BBBP</cell><cell>1</cell><cell>2039 ROC-AUC</cell></row><row><cell></cell><cell></cell><cell>SIDER</cell><cell>27</cell><cell>1427 ROC-AUC</cell></row><row><cell></cell><cell></cell><cell>ClinTox</cell><cell>2</cell><cell>1478 ROC-AUC</cell></row><row><cell></cell><cell></cell><cell>BACE</cell><cell>1</cell><cell>1513 ROC-AUC</cell></row><row><cell>Classification</cell><cell>Physiology</cell><cell>Tox21 ToxCast</cell><cell>12 617</cell><cell>7831 ROC-AUC 8575 ROC-AUC</cell></row><row><cell></cell><cell></cell><cell>FreeSolv</cell><cell>1</cell><cell>642 RMSE</cell></row><row><cell></cell><cell></cell><cell>ESOL</cell><cell>1</cell><cell>1128 RMSE</cell></row><row><cell></cell><cell>Physical chemistry</cell><cell>Lipophilicity</cell><cell>1</cell><cell>4200 RMSE</cell></row><row><cell>Regression</cell><cell>Quantum mechanics</cell><cell>QM7 QM8</cell><cell>1 12</cell><cell>6830 MAE 21786 MAE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>-</head><label></label><figDesc>Tox21 [1] is a public database measuring the toxicity of compounds, which has been used in the 2014 Tox21 Data Challenge. -ToxCast [41] contains multiple toxicity labels over thousands of compounds by running high-throughput screening tests on thousands of chemicals. QM7 [4] is a subset of GDB-13, which records the computed atomization energies of stable and synthetically accessible organic molecules, such as HOMO/LUMO, atomization energy, etc. It contains various molecular structures such as triple bonds, cycles, amide, epoxy, etc .</figDesc><table><row><cell>Molecular Regression Datasets.</cell></row><row><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 and</head><label>4</label><figDesc>Tabel 5show the atom and bond feature we used in GROVER. 2) Molecule-level feature extraction. Following the same protocol of<ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b62">63]</ref>, we extract additional 200 molecule-level features by RDKit for each molecule and concatenate these features to the output of self-attentive READOUT, to go through MLP for the final prediction.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Atom features.</figDesc><table><row><cell>features</cell><cell cols="2">size description</cell></row><row><cell>atom type</cell><cell cols="2">100 type of atom (e.g., C, N, O), by atomic number</cell></row><row><cell>formal charge</cell><cell>5</cell><cell>integer electronic charge assigned to atom</cell></row><row><cell>number of bonds</cell><cell>6</cell><cell>number of bonds the atom is involved in</cell></row><row><cell>chirality</cell><cell>5</cell><cell>number of bonded hydrogen atoms</cell></row><row><cell>number of H</cell><cell>5</cell><cell>number of bonded hydrogen atoms</cell></row><row><cell>atomic mass</cell><cell>1</cell><cell>mass of the atom, divided by 100</cell></row><row><cell>aromaticity</cell><cell>1</cell><cell>whether this atom is part of an aromatic system</cell></row><row><cell>hybridization</cell><cell>5</cell><cell>sp, sp2, sp3, sp3d, or sp3d2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Bond features.</figDesc><table><row><cell>features</cell><cell cols="2">size description</cell></row><row><cell>bond type</cell><cell>4</cell><cell>single, double, triple, or aromatic</cell></row><row><cell>stereo</cell><cell>6</cell><cell>none, any, E/Z or cis/trans</cell></row><row><cell>in ring</cell><cell>1</cell><cell>whether the bond is part of a ring</cell></row><row><cell>conjugated</cell><cell>1</cell><cell>whether the bond is conjugated</cell></row><row><cell cols="3">C Implementation and Pre-training Details</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>The fine-tuning hyper-parameters ratio of Noam learning rate scheduler. The real initial learning rate is max_lr / init_lr. 10 max_lr maximum learning rate of Noam learning rate scheduler. 0.0001 ? 0.001 final_lr final learning rate ratio of Noam learning rate scheduler. The real final learning rate is max_lr / final_lr.</figDesc><table><row><cell>hyper-parameter Description</cell></row></table><note>E.1 Effect of Self-supervised Pre-training on Regression Tasks</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Comparison between GROVER with and without pre-training on regression tasks GROVER No Pre-training Absolute Improvement</figDesc><table><row><cell></cell><cell>FreeSolv</cell><cell>1.544</cell><cell>1.987</cell><cell>0.443</cell></row><row><cell>RMSE</cell><cell>ESOL Lipo</cell><cell>0.831 0.560</cell><cell>0.911 0.643</cell><cell>0.080 0.083</cell></row><row><cell>MAE</cell><cell>QM7 QM8</cell><cell>72.600 0.013</cell><cell>89.408 0.017</cell><cell>16.808 0.004</cell></row></table><note>E.2 GROVER Fine-tuning Tasks with Other Backbones</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Comparison between different methods. The metric is AUC-ROC. The numbers in brackets are the standard deviation.</figDesc><table><row><cell></cell><cell cols="2">Hu. et al.</cell><cell cols="2">GROVER-GIN</cell><cell cols="2">GROVER-MPNN</cell></row><row><cell></cell><cell>w pre-train</cell><cell cols="2">w/o pre-train w pre-train</cell><cell cols="2">w/o pre-train w pre-train</cell><cell>w/o pre-train</cell></row><row><cell>BBBP</cell><cell>0.915 (0.040)</cell><cell cols="2">0.899 (0.035) 0.925 (0.036)</cell><cell cols="2">0.901 (0.051) 0.929 (0.029)</cell><cell>0.917 (0.027)</cell></row><row><cell>SIDER</cell><cell>0.614 (0.006)</cell><cell cols="2">0.615 (0.007) 0.648 (0.015)</cell><cell cols="2">0.627 (0.016) 0.650 (0.003)</cell><cell>0.637 (0.030)</cell></row><row><cell>BACE</cell><cell>0.851 (0.027)</cell><cell cols="2">0.837 (0.028) 0.862 (0.020)</cell><cell cols="2">0.823 (0.050) 0.872 (0.031)</cell><cell>0.852 (0.034)</cell></row><row><cell>Average</cell><cell>0.793</cell><cell>0.784</cell><cell>0.812</cell><cell>0.784</cell><cell>0.817</cell><cell>0.802</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">All datasets can be downloaded from http://moleculenet.ai/datasets-1</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The result is not presented since N-Gram on ToxCast is too time consuming to be finished in time.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We use relative improvement<ref type="bibr" target="#b51">[52]</ref> to provide the unified descriptions.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements and Disclosure of Funding</head><p>This work is jointly supported by Tencent AI Lab Rhino-Bird Visiting Scholars Program (VS202006), China Postdoctoral Science Foundation (Grant No.2020M670337), and the National Natural Science Foundation of China (Grant No. 62006137). The GPU resources and distributed training optimization are supported by Tencent Jizhi Team. We would thank the anonymous reviewers for their valuable suggestions. Particularly, Yu Rong wants to thank his wife, Yunman Huang, for accepting his proposal for her hand in marriage.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tox21 challenge</title>
		<ptr target="https://tripod.nih.gov/tox21/challenge/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The properties of known drugs. 1. molecular frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Bemis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murcko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2887" to="2893" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rumor detection on social media with bi-directional graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">970 million druglike small molecules for virtual screening in the chemical universe database GDB-13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Chem. Soc</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page">8732</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The rise of deep learning in drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ola</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Blaschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Drug discovery today</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1241" to="1250" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08415</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional embedding of attributed molecular graphs for physical property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klavs F</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Esol: estimating aqueous solubility directly from molecular structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Delaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1000" to="1005" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chembl: a large-scale bioactivity database for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gaulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Bellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Bento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Hersey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaun</forename><surname>Light</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcglinchey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bissan</forename><surname>Michalovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al-Lazikani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="1100" to="1107" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A data-driven approach to predicting successes and failures of clinical trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaitlyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gayvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Madhukar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elemento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell chemical biology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1294" to="1301" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Pre-training graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Tackling oversmoothing for general graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2008</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Le?niak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06289</idno>
		<title level="m">Learning to smile (s)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Transformers are graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Joshi</surname></persName>
		</author>
		<ptr target="https://graphdeeplearning.github.io/post/transformers-are-gnns/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janek</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The sider database of drugs and side effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivica</forename><surname>Letunic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">Juhl</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peer</forename><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="1075" to="1079" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rdkit: Open-source cheminformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semisupervised graph classification: A hierarchical graph perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="972" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">N-gram graph: Simple unsupervised representation for graphs, with applications to molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mehmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Demirel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8464" to="8476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Molecular property prediction: A multilevel quantum interactions modeling perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1052" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detect rumors on twitter by promoting information campaigns with generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3049" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A bayesian approach to in silico blood-brain barrier penetration modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><forename type="middle">Filipa</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><forename type="middle">L</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre O</forename><surname>Falcao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1686" to="1697" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Freesolv: a database of experimental and calculated hydration free energies, with input files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mobley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter Guthrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">On asymptotic behaviors of graph cnns from dynamical systems perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10947</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Audio-visual scene analysis with self-supervised multisensory features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Time Salimans, and Ilya Sutskever. Improving language understanding with unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Electronic spectra from tddft and machine learning in chemical space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Tapavicza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O Anatole Von</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">84111</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Massively multitask networks for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Konerding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02072</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Toxcast chemical landscape: paving the road to 21st century toxicology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Judson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Houck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patra</forename><surname>Christopher M Grulke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inthirany</forename><surname>Volarath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chihae</forename><surname>Thillainadarajah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rathman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wambaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical research in toxicology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1225" to="1251" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deeply learning molecular structure-property relationships using attention-and gate-augmented graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongok</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaechang</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung Hwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo Youn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10988</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huziel Enoc Sauceda</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhad</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Horovod: fast and easy distributed deep learning in TensorFlow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Del</forename><surname>Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Zinc 15-ligand discovery for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Computational modeling of ?-secretase 1 (bace-1) inhibitors using ligand based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Govindan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiah Aldrin</forename><surname>Denny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1936" to="1949" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">How should relative changes be measured?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>T?rnqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pentti</forename><surname>Vartia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yrj? O</forename><surname>Vartia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="46" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10341</idno>
		<title level="m">Deep graph infomax</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Atomnet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhar</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Dzamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Heifets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02855</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Smiles-bert: Large scale unsupervised pre-training for molecular property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</title>
		<meeting>the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Smiles. 2. algorithm for generation of unique smiles notation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Weininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Weininger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="101" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of molecular representation for drug discovery with the graph attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoping</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhe</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hualiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BCB</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Analyzing learned molecular representations for property prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Eiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Guzman-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Mathea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3370" to="3388" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Onionnet: a multiple-layer intermolecularcontact-based convolutional neural network for protein-ligand binding affinity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuguang</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS omega</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="15956" to="15965" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
