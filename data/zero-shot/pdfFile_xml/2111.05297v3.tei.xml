<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sliced Recursive Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
							<email>zhiqiangshen@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
							<email>zechunliu@fb.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Reality Labs</orgName>
								<orgName type="institution">Meta Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
							<email>epxing@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of Artificial Intelligence</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sliced Recursive Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a neat yet effective recursive operation on vision transformers that can improve parameter utilization without involving additional parameters. This is achieved by sharing weights across depth of transformer networks. The proposed method can obtain a substantial gain (?2%) simply using na?ve recursive operation, requires no special or sophisticated knowledge for designing principles of networks, and introduces minimal computational overhead to the training procedure. To reduce the additional computation caused by recursive operation while maintaining the superior accuracy, we propose an approximating method through multiple sliced group self-attentions across recursive layers which can reduce the cost consumption by 10?30% without sacrificing performance. We call our model Sliced Recursive Transformer (SReT), a novel and parameter-efficient vision transformer design that is compatible with a broad range of other designs for efficient ViT architectures. Our best model establishes significant improvement on ImageNet-1K over state-ofthe-art methods while containing fewer parameters. The proposed weight sharing mechanism by sliced recursion structure allows us to build a transformer with more than 100 or even 1000 shared layers with ease while keeping a compact size (13?15M), to avoid optimization difficulties when the model is too large. The flexible scalability has shown great potential for scaling up models and constructing extremely deep vision transformers. Code is available at https://github.com/szq0214/SReT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The architectures of transformer have achieved substantively breakthroughs recently in the fields of natural language processing (NLP) <ref type="bibr" target="#b49">[54]</ref>, computer vision (CV) <ref type="bibr" target="#b12">[16]</ref> and speech <ref type="bibr" target="#b11">[15,</ref><ref type="bibr" target="#b51">56]</ref>. In the vision area, Dosovitskiy et al. <ref type="bibr" target="#b12">[16]</ref> introduced a vision transformer (ViT) model that splits a raw image into a patch sequence as input, and they directly adopt transformer model <ref type="bibr" target="#b49">[54]</ref>    task. ViT achieved impressive results and has inspired many follow-up works. However, the benefits of a transformer often come with a large number of parameters and computational cost and it is always of great challenge to achieve the optimal trade-off between the accuracy and model complexity. In this work, we are motivated by the following question: How can we improve the parameter utilization of a vision transformer, i.e., the representation ability without increasing the model size? We observe recursive operation, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, is a simple yet effective way to achieve this purpose. Our recursion-based vision transformer models significantly outperform state-of-the-art approaches while containing fewer parameters and FLOPs, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Intrinsically, the classifier requires high-level abstracted features from the neural network to perform accurate classification, while the extraction of these features often requires multiple layers and deeper networks. This introduces parameter overhead into the model. Our motivation of this work stems from an interesting phenomenon of latent representation visualization. We observed that in the deep vision transformer network, the weights and activations of adjacent layers are similar with not much difference (a similar phenomenon is also discovered in <ref type="bibr" target="#b58">[63]</ref>), which means they can be reused. The transformer with a fixed stack of distinct layers loses the inductive bias in the recurrent neural network which inspires us to share those weights in a recursive manner, forming an iterative or recursive vision transformer. Recursion can help extract stronger features without the need of increasing the parameters, and further improve the accuracy. In addition, this weight reuse or sharing strategy partially regularizes the training process by reducing the number of parameters to avoid overfitting and ill-convergence challenges, which will be discussed in the later sections. Why do we need to introduce sliced recursion, i.e., the group selfattention, into transformers? (advantages and drawbacks) We usually push towards perfection on weight utilization of a network under a bounded range of parameters, thus, it can be used practically in the resource-limited circumstances like embedded devices. Recursion is a straightforward way to compress the feature representation in a cyclic scheme. The recursive neural networks also allow the branching of connections and structures with hierarchies. We found that it is intriguingly crucial for learning better representations on vision data in a hierarchical manner, as we will introduce in <ref type="figure" target="#fig_0">Fig. 10</ref> of our experiments. Also, even the most simplistic recursive operation still improves the compactness of utilizing parameters without requiring to modify the transformer block structure, unlike others <ref type="bibr" target="#b45">[50,</ref><ref type="bibr" target="#b56">61,</ref><ref type="bibr" target="#b19">24,</ref><ref type="bibr" target="#b50">55,</ref><ref type="bibr" target="#b52">57,</ref><ref type="bibr" target="#b32">37,</ref><ref type="bibr" target="#b26">31,</ref><ref type="bibr" target="#b54">59]</ref>, that add more parameters or involve additional fine-grained information from input <ref type="bibr" target="#b14">[19]</ref>. However, such a recursion will incur more computational cost by its loops, namely, it sacrifices the executing efficiency for better parameter representation utilization. To address this shortcoming, we propose an approximating method for global self-attention through decomposing into multiple sliced group self-attentions across recursive layers, meanwhile, enjoying similar FLOPs and better representations, we also apply the spatial pyramid design to reduce the complexity of the network. Feed-forward Networks, Recurrent Neural Networks and Recursive Neural Networks. Feed-forward networks, such as CNNs and transformers, are directed acyclic graphs (DAG), so the information path in the feed-forward processing is unidirectional. Recurrent networks (RNNs) are usually developed to process the time-series and other sequential data, and predict using current input and past memory. Recursive network is a less frequently used term compared to other two counterparts. Recursive refers to repeating or reusing a certain piece of a network 5 . Different from RNNs that repeat the same block throughout the whole network, recursive network selectively repeats critical blocks for particular purposes. The recursive transformer iteratively refines its representations for all patches in the sequence. We found that, through the designed recursion into the feed-forward transformer, we can dramatically enhance feature representation especially for structured data without including additional parameters.</p><p>The strong experimental results show that integrating the proposed sliced recursive operation in the transformer strike a competitive trade-off among accuracy, model size and complexity. To the best of our knowledge, there are barely existing works studying the effectiveness of recursive operation in vision transformers and proposing the approximation of self-attention method for reducing the complexity of recursive operation. We have done extensive experiments to derive a set of guidelines for the new design on vision task, and hope it is useful for future research. Moreover, since our method does not involve the sophisticated knowledge for modification of transformer block or additional input information, it is orthogonal and friendly to most of existing ViT designs and approaches. Our contributions.</p><p>-We investigate the feasibility of leveraging recursive operation with sliced group self-attention in the vision transformers, which is a promising direction for establishing efficient transformers and has not been well-explored before. We conducted in-depth studies on the roles of recursion in transformers and conclude an effective scheme to use them for better parameter utilization.</p><p>-We provide design principles, including the concrete format and comprehensive comparison to variants of SReT architectures, computational equivalency analysis, modified distillation, etc., in hope of enlightening future studies in compact transformer design and optimization.</p><p>-We verify our method across a variety of scenarios, including vision transformer, all-MLP architecture of transformer variant, and neural machine translation (NMT) using transformers. Our model outperforms the state-of-the-art methods by a significant margin with fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>(i) Transformer <ref type="bibr" target="#b49">[54]</ref> was originally designed for natural language processing tasks and has been the dominant approach <ref type="bibr" target="#b10">[14,</ref><ref type="bibr" target="#b55">60,</ref><ref type="bibr" target="#b36">41,</ref><ref type="bibr" target="#b5">9,</ref><ref type="bibr" target="#b30">35]</ref> in this field. Recently, Vision Transformer (ViT) <ref type="bibr" target="#b12">[16]</ref> demonstrates that such multi-head self attention blocks can completely replace convolutions and achieve competitive performance on image classification. While it relied on pre-training on large amounts of data and transferring to downstream datasets. DeiT <ref type="bibr" target="#b47">[52]</ref> explored the training strategies and various data augmentation on ViT models, to train them on ImageNet-1K directly. Basically, DeiT can be regarded as a framework of ViT backbone + massive data augmentation + hyper-parameter tuning + hard distillation with tokens. After that, many extensions and variants of ViT models have emerged on image classification task, such as Bottleneck Transformer <ref type="bibr" target="#b45">[50]</ref>, Multimodal Transformer <ref type="bibr" target="#b17">[22]</ref>, Tokens-to-Token Transformer <ref type="bibr" target="#b56">[61]</ref>, Spatial Pyramid Transformer <ref type="bibr" target="#b19">[24,</ref><ref type="bibr" target="#b50">55]</ref>, Class-Attention Transformer <ref type="bibr" target="#b48">[53]</ref>, Transformer in Transformer <ref type="bibr" target="#b14">[19]</ref>, Convolution Transformer <ref type="bibr" target="#b52">[57]</ref>, Shifted Windows Transformer <ref type="bibr" target="#b32">[37]</ref>, Co-Scale Conv-Attentional Transformer <ref type="bibr" target="#b54">[59]</ref>, etc. (ii) Recursive operation has been explored in NLP <ref type="bibr" target="#b29">[34,</ref><ref type="bibr" target="#b8">12,</ref><ref type="bibr" target="#b2">6,</ref><ref type="bibr" target="#b1">5,</ref><ref type="bibr" target="#b3">7,</ref><ref type="bibr" target="#b23">28,</ref><ref type="bibr" target="#b7">11]</ref> and vision <ref type="bibr" target="#b27">[32,</ref><ref type="bibr" target="#b21">26,</ref><ref type="bibr" target="#b13">18,</ref><ref type="bibr" target="#b31">36]</ref> areas. In particular, DEQ <ref type="bibr" target="#b1">[5]</ref> proposed to find equilibrium points via root-finding in the weight-tied feedforward models like transformers and trellis for constant memory. UT <ref type="bibr" target="#b8">[12]</ref> presented the transformer with recurrent inductive bias of RNNs which is similar to our SReT format. However, these works ignored the complexity increased by recursive operation in designing networks. In this paper, we focus on utilizing recursion properly by approximating self-attention through multiple group self-attentions for building compact and efficient vision transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recursive Transformer</head><p>Vanilla Transformer Block. A basic transformer block F consists of a Multihead Self Attention (MHSA), Layer Normalization (LN), Feed-forward Network (FFN), and Residual Connections (RC). It can be formulated as:</p><formula xml:id="formula_0">z = MHSA (LN (z ?1 )) + z ?1 ; z = FFN (LN (z )) + z ; i.e., z = F ?1 (z ?1 )</formula><p>(1) where z and z ?1 are the intermediate representations. F indicates the transformer block at -th layer. ? {0, 1, . . . , L} is the layer index and L is the number of hidden layers. The self-attention module is realized by the inner products with a scaling factor and a softmax operation, which is written as:</p><formula xml:id="formula_1">Attention(Q, K, V ) = Softmax QK / d k V (2)</formula><p>where Q, K, V are query, key and value vectors, respectively. 1/ ? d k is the scaling factor for normalization. Multi-head self attention further concatenates the parallel attention layers to increase the representation ability:</p><formula xml:id="formula_2">MHSA(Q, K, V ) = Concat (head 1 , . . . , head h ) W O , where W O ? R hdv?d model . head i = Attention QW Q i , KW K i , V W V i are the projections with parameter matrices W Q i ? R d model ?d k , W K i ? R d model ?d k , W V i ? R d model ?dv .</formula><p>The FFN contains two linear layers with a GELU non-linearity <ref type="bibr" target="#b18">[23]</ref> in between</p><formula xml:id="formula_3">FFN(x) = (GELU (zW 1 + b 1 ))W 2 + b 2 (3)</formula><p>where z is the input. W 1 , b 1 , W 2 , b 2 are the two linear layers' weights and biases. Recursive Operation. In the original recursive module <ref type="bibr" target="#b44">[49]</ref> for the language modality, the shared weights are recursively applied on a structured input which is among the complex inherent chains, so it is capable of learning deep structured knowledge. Recursive neural networks are made of architectural data and class, which is majorly proposed for model compositionality on NLP tasks. Here, we still use the sequence of patch tokens from the images as the inputs following the ViT model <ref type="bibr" target="#b12">[16]</ref>. And, there are no additional inputs used for feeding into each recursive loop of recursive block as used on structured data. Take two loops as an example for building the network, the recursive operation can be simplified:</p><formula xml:id="formula_4">z = F ?1 (F ?1 (z ?1 ))<label>(4)</label></formula><p>The na?ve recursive operation tends to learn a simple and trivial solution like the identity mapping by the optimizer, since the F ?1 's output and input are identical at the adjacent two depths (layers). Non-linear Projection Layer (NLL). NLL is placed between two recursive operations to enable the non-linear transformation between each block's output and input, to avoid learning trivial status for these recursive blocks by forcing nonequivalence on neighboring output and input. NLL can be formulated as:</p><formula xml:id="formula_5">NLL(z ?1 ) = MLP LN z ?1 + z ?1<label>(5)</label></formula><p>where MLP is a multi-layer projection as FFN, but has different mlp ratio for hidden features. We also use residual connection in it for better representation. As shown in <ref type="table" target="#tab_1">Table 1</ref>, more recursions will not improve accuracy without NLL. Recursive Transformer. A recursive transformer with two loops in every block is:</p><formula xml:id="formula_6">z = NLL 2 (F ?1 (NLL 1 (F ?1 (z ?1 ))))<label>(6)</label></formula><p>where z ?1 and z are each recursive block's input and output. Different from MHSA and FFN that share parameters across all recursive operations within a block, NLL 1 and NLL 2 use the non-shared weights independently regardless of positioning within or outside the recursive blocks.</p><p>Recursive All-MLP <ref type="bibr" target="#b46">[51]</ref> (an extension). We can formulate it as:</p><formula xml:id="formula_7">U * ,i = X * ,i + W 2 * GELU (W 1 * LN (X) * ,i ) , Y j, * = U j, * + W 4 * GELU (W 3 * LN (U) j, * ) , Y j, * = M ?1 (M ?1 (X * ,i ))<label>(7)</label></formula><p>where the first and second lines are token-mixing and channel-mixing from <ref type="bibr" target="#b46">[51]</ref>. M ?1 is a MLP block, C is the hidden dimension and S is the number of nonoverlapping image patches. NLL is not used here for simplicity. Gradients in A Recursive Block. Here, we simply use explicit backpropagation through the exact operations in the forward pass like gradient descent method since SReT has no constraint to obtain the equilibrium of input-output in recursions like DEQ <ref type="bibr" target="#b1">[5]</ref> and the number of loops can be small to control the network computation and depth. Our backward pass is more like UT <ref type="bibr" target="#b8">[12]</ref>. In general, the gradient of the parameters in each recursive block can be:</p><formula xml:id="formula_8">?L ?W F = ?L ?z N ?z N ?W F + ?L ?z N ?z N ?z N ?1 ?z N ?1 ?W F + . . . ?L ?z N ?z N ?z N ?1 . . . ?z 2 ?z 1 ?z 1 ?W F = N i=1 ?L ?z N ? ? N ?1 j=i ?z j+1 ?z j ? ? ?z i ?W F<label>(8)</label></formula><p>where W F is the parameters of recursive block. L is the objective function. Learnable Residual Connection (LRC) for Recursive Vision Transformers. He et al. <ref type="bibr" target="#b16">[21]</ref> studied various strategies of shortcut connections on CNNs and found that the original residual design with pre-activation performs best. Here, we found simply adding learnable coefficients on each branch of residual connection can benefit to the performance of ViT following the similar discovery of literature <ref type="bibr" target="#b28">[33]</ref>. Formally, Eq. 1 and Eq. 5 can be reformulated as:</p><formula xml:id="formula_9">z = ? * MHSA (LN (z ?1 )) + ? * z ?1 ; z = ? * FFN (LN (z )) + ? * z ;<label>(9)</label></formula><formula xml:id="formula_10">NLL(z ?1 ) = ? * MLP LN z ?1 + ? * z ?1<label>(10)</label></formula><p>where ?, ?, ?, ?, ?, ? are the learnable coefficients. They are initialized as 1 and trained with other model's parameters simultaneously without restrictions. Extremely Deep Transformers. Weight-sharing mechanism allows us to build a transformer with more than 100 layers still keeping a small model. We demonstrate empirically that the proposed method can significantly simplify the optimization when the transformer is scaled up to an exaggerated number of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approximating Global MHSA via Multi-Group MHSA</head><p>Though recursive operation is adequate to provide better representation using the same number of parameters, the additional forward loop makes the overhead in training and inference increasing unnegligibly. To address the extra computational cost caused by recursion while maintaining the improved accuracy, we introduce an approximating method through multiple group self-attentions which is surprisingly effective in reducing FLOPs without compromising accuracy. Approximating Global Self-Attention in SReT. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, a regular self-attention layer can be decoupled through multiple group self-attentions in a recursion manner with similar or even smaller computational cost. In general, the number of groups in different recursion can be the same or different depending on the requirements of FLOPs and accuracy trade-off. Such a strategy will not change the number of parameters while more groups can enjoy lower FLOPs but slightly inferior performance. We empirically verified that the decoupling scheme can achieve similar performance with significantly fewer FLOPs if using proper splitting of self-attention in a tolerable scope, as shown in Appendix. Computational Equivalency Analysis. In this subsection, we analyze the complexity of global (i.e., original) and sliced group self-attentions and compare with different values of groups in a vision transformer.</p><p>Theorem 1. (Equivalency of global and multiple group self-attentions with recursion on FLOPs.</p><formula xml:id="formula_11">) Let {N , G } ? R 1 , when N = G , FLOPs(1 V-SA)= FLOPs(N ?Recursion with G ?G-SAs).</formula><p>The complexity C of global and group self-attentions can be calculated as: (For simplicity, here we assume #groups and vector dimensions in each recursive operation are the same.)</p><formula xml:id="formula_12">C G-SA = N G ? C V-SA<label>(11)</label></formula><p>where N and G are the numbers of recursion and group MHSA in layer , i.e., -th recursive block. V-SA and G-SA represent the vanilla and group MHSA.</p><p>The proof is provided in Appendix. The insight provided by Theorem 1 is at the core of our method to control the complexity and its various benefits on better representations. Importantly, the computation of self-attention through the "slice" paralleling is equal to the vanilla self-attention. We can observe that We employ ex-tiny model to evaluate the performance of global self-attention and sliced group self-attention with recursion. As shown in <ref type="table" target="#tab_2">Table 2</ref>, we empirically verify that, with the similar computation, group self-attention with recursion can obtain better accuracy than vanilla self-attention. Analysis: Where is the benefit from in SReT? Theoretical analysis on recursion could further help understand the advantage behind, while it is difficult and prior literature on this always proves it empirically. Here, we provide some basic theoretical explanations from the optimization angle for better understanding this approach. One is the enhanced gradients accumulation. Let g t =? ? f t (?) denote the gradient, we take Adam optimizer <ref type="bibr" target="#b22">[27]</ref> as an example, na?ve param-</p><formula xml:id="formula_13">when N = G , C V-SA ? C G-SA 6 and if N &lt; G , C G-SA &lt; C V-SA ,</formula><formula xml:id="formula_14">eter update is ? t ? ? t?1 ?? ? m t / ? v t + where the gradients w.r.t. stochastic objective at timestep t is g t ? ? ? f t (? t?1 )</formula><p>, here we omit first and second moment estimate formulae. After involving recursion (here NLL guarantees</p><formula xml:id="formula_15">m i t , v i t 's dis- crepancy), the new updating is: ? t ? ? t?1 ? N i=1 ? ? m i t / v i t + where N is</formula><p>the number of recursion loops. Basically, recursion enables more updating/tuning of parameters in the same iteration, so that the learned weights are more aligned to the loss function, and the performance is naturally better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we first empirically verify the proposed SReT on image classification task with self-attention <ref type="bibr" target="#b49">[54]</ref> and all-MLP <ref type="bibr" target="#b46">[51]</ref> architectures, respectively. We also perform detailed ablation studies to explore the optimal hyper-parameters of our proposed network. Then, we extend it to the neural machine translation (NMT) task to further verify the generalization ability of the proposed approach. Finally, we visualize the evolution of learned coefficients in LRC and intermediate activation maps to better understand the behaviors and properties of our proposed model. Our experiments are conducted on CIAI cluster. . We use the same setup as <ref type="bibr" target="#b33">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Experimental Settings</head><formula xml:id="formula_16">(i) ImageNet-1K [13]: ImageNet-1K</formula><p>Settings: Our detailed training settings and hyper-parameters are shown in Appendix. On ImageNet-1K, our backbone network is a spatial pyramid <ref type="bibr" target="#b19">[24]</ref> architecture with stem structure following <ref type="bibr" target="#b42">[47]</ref>. Soft distillation strategy. On vision transformer, DeiT <ref type="bibr" target="#b47">[52]</ref> proposed to distill tokens together with hard predictions from the teacher. They stated that using one-hot label with hard distillation can achieve the best accuracy. This seems counterintuitive since soft labels can provide more subtle differences and finegrained information of the input. In this work, through a proper distillation design, our soft label based distillation framework (one-hot label is not used) consistently obtained better performance than DeiT 7 . Our loss is a soft version of cross-entropy between teacher and student's outputs as used in <ref type="bibr" target="#b41">[46,</ref><ref type="bibr" target="#b39">44,</ref><ref type="bibr" target="#b40">45,</ref><ref type="bibr" target="#b0">4]</ref>:</p><formula xml:id="formula_17">L CE (S W ) = ? 1 N N i=1 P T W (z) logP S W (z),</formula><p>where P T W and P S W are the outputs of teacher and student, respectively. More details can be referred to Appendix. In this section, we examine the effectiveness of proposed recursion using DeiT training strategies. We verify the following two fashions of recursion. Internal and external loops. As illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>, there are two possible recursion designs on transformer networks. One is the internal loop that repeats every block separately. Another one is the external loop that cyclically executes all blocks together. Although external loop design can force the model being more compact as it shares parameters across all blocks with fewer non-shared NLL layers, we found such a structure is inflexible with limited representation ability. We conducted a comparison with 12 layers of basic transformers with 2? recursive operation and the results are: external 67.0% (3.2M) vs. internal 67.6% (3.0M) | 70.3% (3.9M). In the following experiments, we use the internal recursive design as our default setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Na?ve Recursion on Transformer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>The overview of our ablation studies is shown in <ref type="table" target="#tab_5">Table 3</ref>. The first row presents the baseline, the second group is the different structures indicated by the used factors. The last is the comparison of KD. We also verify the following designs.  Architecture configuration. As in <ref type="table" target="#tab_7">Table 5</ref>, SReT-T is our tiny model which has mlp ratio = 3.6 in FFN and 4.0 for SReT-TL. More details about these architectures are provided in our Appendix. To examine the effectiveness of recursive operation, we conduct different loops of na?ve recursion on DeiT-T.</p><p>The results of accuracy curves on validation data are shown in <ref type="figure" target="#fig_5">Fig. 5</ref> (1), we can see 2? is slightly better than 1? and the further boost is marginal, while the 1? is much faster for executing. Thus, we use this in the following experiments. NLL configuration. NLL is a crucial factor for size and performance since the weights in it are not shared. To find an optimal trade-off between model compactness and accuracy, we explore the NLL ratios in <ref type="figure" target="#fig_1">Fig. 5 (2, 3)</ref>. Generally, a larger NLL ratio can achieve better performance but the model size increases accordingly. We use 1.0 in our SReT-T and SReT-TL, and 2.0 in our SReT-S. Different permutation designs and groups numbers. We explore the different permutation designs and the principle of choosing group numbers for better accuracy-FLOPs trade-off. We propose to insert permutation and inverse permutation layers to preserve the input's order information after the sliced group self-attention operation. The detailed formulation of this module, together with recursions and their result analyses are given in our Appendix. Distillation. To examine the effectiveness of our proposed soft distillation method, we conduct the comparison of one-hot label + hard distillation and soft distillation only. The backbone network is SReT-T, all hyper-parameters are the same except the loss functions. The accuracy curves are shown in our Appendix. Our result 77.7% is significantly better than the baseline 77.1%.</p><p>(1) (2) (3)  Throughput evaluation. In <ref type="table" target="#tab_6">Table 4</ref>, we provide the throughput comparisons with DeiT and Swin on one NVIDIA GeForce RTX 3090 which can directly reflect the real inference speed and time consumption. We highlight that our method obtains significantly fewer params and FLOPs with better throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with State-of-the-art Approaches</head><p>A summary of our main results is shown in <ref type="table" target="#tab_7">Table 5</ref>, our SReT-ExT is better than PiT-T by 1.0% with 18.4%? parameters. SReT-T also outperforms DeiT-T by 3.8% with 15.8%? parameters and 15.4%? FLOPs. Distillation can help improve the accuracy by 1.6% and fine-tuning on large resolution further boosts to 79.6%. Moreover, our SReT-S is consistently better than state-of-the-art Swin-T, T2T, etc., on accuracy, model size and FLOPs, which demonstrates the superiority and potential of our architectures in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">All-MLP Architecture</head><p>MLP-Mixer <ref type="bibr" target="#b46">[51]</ref> (Baseline), MLP-Mixer+Recursion and MLP-Mixer+Recursion +LRC: Mixer is a recently proposed plain design that is based entirely on multilayer perceptrons (MLPs). We apply our recursive operation and LRC on MLP-Mixer to verify their generalization. Results are shown in <ref type="figure" target="#fig_7">Fig. 6</ref> (1), our method is consistently better than the baseline using the same training protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Neural Machine Translation</head><p>In this section, we compare the BLEU scores <ref type="bibr" target="#b34">[39]</ref> of vanilla transformer <ref type="bibr" target="#b49">[54]</ref> and ours on the WMT14 En-De and IWSLT'14 De-En (Appendix) using fairseq toolkit <ref type="bibr">[17]</ref>. IWSLT'14 De-En is a relatively small dataset so the improvement is   denotes the model is trained without the proposed group self-attention approximation. Fine-tuning on large resolution is highlighted by gray color. not as significant as on WMT14 En-De. The results are shown in <ref type="figure">Fig. 7</ref>, we can see our method is favorably better than the baseline. Without LRC, the model slightly converges faster, but the final accuracy is inferior to using LRC. Also, LRC makes the training process more stable, as shown in the red dashed box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Landscape Visualizations of DeiT and Our Mixed-depth SReT</head><p>Explicit mixed-depth training. The recursive neural network enables to train the model in a mixed-depth scheme. As shown in <ref type="figure">Fig. 8 (d)</ref>, the left branch is the subnetwork containing recursive blocks, while the right is the blocks without sharing the weights on depth, but their weights are re-used with the left branch. In this structure, the two branches take inputs from the same stem block. Mixed- depth training offers simplified optimization by performing operations parallelly and prevents under-optimizing when the network is extremely deep. Benefits of mixed-depth training. The spin-off benefit of sliced recursion is the feasibility of mixed-depth training, which essentially is an explicit deep supervision scheme as the shallow branch receives stronger supervision that is closer to the final loss layer, meanwhile, weights are shared with the deep branch. Inspired by <ref type="bibr" target="#b25">[30]</ref>, we visualize the landscape of baseline DeiT-108 and our SReT-108 &amp; SReT-108 mixed-depth models to examine and analyze the difficulty of optimization on these three architectures. The results are illustrated in <ref type="figure">Fig. 9</ref>, we can observe that DeiT-108 is more chaotic and harder for optimization with a deeper local minimum than our mixed-depth network. This verifies the advantage of our proposed network structure for simpler optimization.</p><p>(1) DeiT-108</p><p>(3) SReT w/ Mixed-depth-108 (Ours) (2) SReT-108 (Ours) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Analysis and Understanding</head><p>Here, we provide two visualizations regarding LRC and learned response maps. Evolution of LRC coefficients. As shown in <ref type="figure" target="#fig_7">Fig. 6</ref> (2), we plot the evolution of learned coefficients in the first block. We can observe that the coefficients on the identity mapping (?, ?, ?) first go up and then down as the training continues. This phenomenon indicates that, at the beginning of model training, the identify mapping plays a major role in the representations. After ?50 epochs of training, the main branch is becoming increasingly important. Once the training is complete, in FFN and NLL, the main branch exceeds the residual connection branch while on MHSA it is the opposite. We believe this phenomenon can inspire us to design a more reasonable residual connection structure in ViT. Learned response maps. We visualize the activation maps of DeiT-T and our SReT-T model at shallow and deep layers. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, DeiT is a network with uniform resolution of feature maps (14?14). While, our spatial pyramid structure has different sizes of feature maps along with the depth of the network, i.e., the resolution of feature maps decreases when the depth increases. More interesting observations are discussed in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>It is worthwhile considering how to improve the efficiency of parameter utilization for a vision transformer with minimum overhead. In this work, we have summarized and explained several behaviors observed while training such networks. We focused on building an efficient vision transformer with a compact model size through the recursive operation, and the proposed group self-attention approximation method allows us to train in a more efficient manner with recursive transformers. We highlight such a training scheme has not been well-explored yet in previous literature. We attributed the superior performance of sliced recursive transformer to its ability of intensifying the representation quality of intermediate features. We conducted comprehensive experiments to establish the success of our method on the image classification and neural machine translation tasks, not just verifying it in the vision domain, but proving the capability to generalize for multiple modalities and architectures, such as MLP-Mixer.</p><p>In this appendix, we provide details omitted in the main text, including:</p><p>? Section A: Proof for equivalency of global self-attention and sliced group self-attention with recursive operation on FLOPs. (Sec. 4 "Approximating Global Self-Attention via Permutation of Group/Local Self-Attentions" of the main paper.)</p><p>? Section B: Results of SReT on ImageNet ReaL <ref type="bibr" target="#b4">[8]</ref> and ImageNetV2 <ref type="bibr" target="#b38">[43]</ref> datasets. (Sec. 5 "Experiments and Analysis" of the main paper.)</p><p>? Section C: More ablation results on different permutation designs and numbers of groups when approximating global self-attention on ImageNet-1K. (Sec. 5.3 "Ablation Studies" of the main paper.)</p><p>? Section D: Pseudocode for implementing sliced group self-attention. (Sec. 4 "Approximating Global Self-Attention via Permutation of Group/Local Self-Attentions" of the main paper.)</p><p>? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A FLOPs Analysis</head><p>One of the key benefits of our SReT is to control the complexity of a recursive network. We analyze the FLOPs of global (i.e., original) and sliced group selfattentions and compare them with different circumstances of groups in a vision transformer. In this section, we provide a proof to Theorem 1 which we restate below.</p><p>Theorem 1. (Equivalency of global self-attention and group selfattention with recursive operation on FLOPs.)</p><formula xml:id="formula_18">Let {N , G } ? R 1 , when N = G , FLOPs(1 V-SA) = FLOPs(N ? Recursive with G ? G-SAs).</formula><p>The complexity of regular and group self-attentions can be calculated as: (For simplicity, here we assume #groups and vector dimensions in each recursive operation are the same.)</p><formula xml:id="formula_19">C G-SA = N G ? C V-SA (12)</formula><p>where N is the number of recursive operation and G is the number of group self-attentions in layer , i.e., -th recursive block. V-SA and G-SA represent the vanilla and group self-attentions, respectively.</p><p>Proof. (Theorem 1) The complexity C of regular self-attention can be calculated as:</p><formula xml:id="formula_20">C V-SA = O(L 2 ? D )<label>(13)</label></formula><p>where L is the sequence length and D is the dimensionality of the latent representations.</p><p>The complexity of simple recursive operation without group will be:</p><formula xml:id="formula_21">C recursive = O(N ? L 2 ? D )<label>(14)</label></formula><p>where N is the number of recursive operation. The complexity of sliced group self-attentions with a recursive block can be calculated as:</p><formula xml:id="formula_22">C G-SA = O( N i (g i ? ( L g i ) 2 ? d i )) = O( N i ( L 2 g i ? d i ))<label>(15)</label></formula><p>where</p><formula xml:id="formula_23">g i ? {G }, d i ? {D }, i = 1, . . . , N .</formula><p>Consider the condition of #groups g i and vector dimension d i in each recursive operation are the same. The complexity of group self-attentions can be re-formulated as:</p><formula xml:id="formula_24">C G-SA = O(N ? L 2 G ? D ) = N G ? C V-SA<label>(16)</label></formula><p>where G is the number of group self-attentions. When N = G ,</p><formula xml:id="formula_25">C V-SA = C G-SA and if N &lt; G , C G-SA &lt; C V-SA .</formula><p>B More Results and Comparisons on ImageNet ReaL <ref type="bibr" target="#b4">[8]</ref> and ImageNetV2 <ref type="bibr" target="#b38">[43]</ref> Datasets</p><p>In this section, we provide results on ImageNet ReaL <ref type="bibr" target="#b4">[8]</ref> and ImageNetV2 <ref type="bibr" target="#b38">[43]</ref> datasets. On ImageNetV2 <ref type="bibr" target="#b38">[43]</ref>, we verify our SReT models on three metrics "Top-Images", "Matched Frequency", and "Threshold 0.7". The results are shown in <ref type="table" target="#tab_8">Table 6</ref>, we achieve consistent improvement over DeiT on various network architectures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Ablation Results on Different Permutation Designs and Groups Numbers</head><p>In this section, we explore the different permutation designs and the principle of choosing group numbers for the best accuracy-FLOPs trade-off. We propose to insert an inverse permutation layer to preserve the input order information after the sliced group self-attention operation. The formulation of this operation is shown in <ref type="figure" target="#fig_0">Fig. 11</ref> and the ablation results for this design are given in <ref type="table" target="#tab_10">Table 7</ref> of the first group. In the table, "P" represents the permutation layer, "I" represents the inverse permutation layer and "L" indicates that we did not involve permutation and inverse permutation in the last stage of models when the number of groups equals 1. We use SReT-T and SReT-TL as the base structures for the ablation of different groups. In the Groups column of the table, we applied two loops of recursion in each recursive block according to the ablation study in <ref type="table" target="#tab_1">Table 1</ref> of our main text. In each pair of the square brackets, the values denote the number of groups for each recursion, and each pair of square brackets represents one stage of blocks in the spatial pyramid based backbone network. We use <ref type="bibr" target="#b4">[8,</ref><ref type="bibr">2]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Pseudocode for Sliced Group Self-attention</head><p>The PyTorch pseudocode for implementation of our sliced group self-attention is shown in Algorithm 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Training Details on ImageNet-1K</head><p>On ImageNet-1K, we conduct experiments on three training schemes: (1) conventional training with one-hot labels;</p><p>(2) distillation with soft labels from a pre-trained teacher; (3) finetuning from distilled parameters with higher resolution. Our training settings and hyper-parameters mainly follow the designs of DeiT <ref type="bibr" target="#b47">[52]</ref>. A detailed introduction of these settings is shown in <ref type="table" target="#tab_11">Table 8</ref>, 9 and 10 with an item-by-item comparison.</p><p>Conventional Training from Scratch with One-hot Label. As shown in <ref type="table" target="#tab_11">Table 8</ref>, we use batch-sizes of 512/1024 for training our models and the default initial learning rate is 1e-3, while from our experiments, larger initial lr of 2e-3 with more warmup epochs of 30 can favorably improve the accuracy. Other settings are following <ref type="bibr" target="#b47">[52]</ref>.   (2) (1)</p><p>(3) <ref type="figure" target="#fig_0">Fig. 12</ref>: Comprehensive ablation study on different design factors.</p><p>Distillation Strategy. Knowledge distillation <ref type="bibr" target="#b20">[25]</ref> is a popular way to boost the performance of a student network. Recently, many promising results <ref type="bibr" target="#b35">[40,</ref><ref type="bibr" target="#b43">48,</ref><ref type="bibr" target="#b53">58]</ref> have been achieved using this technique. On vision transformer, DeiT <ref type="bibr" target="#b47">[52]</ref> proposed to distill tokens together with hard predictions from the teacher, and it claimed that using one-hot label with hard distillation can achieve the best accuracy. This seems counterintuitive since soft labels can provide more subtle differences and fine-grained information of the input. In this work, through a proper distillation scheme, our soft label based distillation framework (one-hot label is not used) consistently obtained better performance than DeiT. Our loss is a soft version of cross-entropy between teacher and student's outputs as used in <ref type="bibr" target="#b39">[44,</ref><ref type="bibr" target="#b0">4,</ref><ref type="bibr" target="#b41">46]</ref>:</p><formula xml:id="formula_26">L CE (S W ) = ? 1 N N i=1 P T W (z) logP S W (z)<label>(17)</label></formula><p>where P T W and P S W are the outputs of teacher and student, respectively. Distillation from Scratch. As shown in <ref type="table" target="#tab_12">Table 9</ref>, we use soft predictions solely from RegNetY-16GF <ref type="bibr" target="#b37">[42]</ref> as a teacher instead of one-hot label + hard distillation used in <ref type="bibr" target="#b47">[52]</ref>. The ablation study on this point is provided in <ref type="figure" target="#fig_0">Fig. 12 (1)</ref> with SReT-T. Spatial Pyramid (SP) Design. Pyramids <ref type="bibr" target="#b24">[29,</ref><ref type="bibr" target="#b15">20]</ref> are an effective design in conventional vision tasks. The resolution of the shallow stage in a network is usually large, SP can help to redistribute the computation from shallow to deep stages of a network according to their representation ability. Here, we follow the construction principles <ref type="bibr" target="#b19">[24]</ref> but replacing the first patch embedding layer with a Stem block (i.e., a stack of three 3?3 convolution layers with stride = 2) following <ref type="bibr" target="#b42">[47]</ref>. Other Small Modifications. Considering the unique properties of vision modality compared to the language, we further apply some minor modifications on our network design, some of them have been proven useful on CNNs in the vision domain, including: (i) We remove the class token and replace with a global average pooling (GAP) on the last output together with a fullyconnected layer; (ii) We also remove the distillation token if the training process involves KD, which means we use the same feature embedding for both the groundtruth labels in standard training, and distillation with soft labels from the teacher. (iii) When fine-tuning from low resolution (224?224) to high resolution (384?384) <ref type="bibr" target="#b47">[52]</ref>, following the perspective of <ref type="bibr" target="#b43">[48]</ref> that to increase the capacity of a model, we do not apply weight decay (set it as 0) during fine-tuning. Generally, the above modifications can slightly save parameters, boost the performance and significantly improve the simplicity of the whole framework. The illustration of these modifications is shown in <ref type="figure" target="#fig_0">Fig. 13</ref>.</p><p>(1) WMT14 De-En</p><p>(2) IWSLT14 De-En  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Hyper-parameter Settings of Language Models</head><p>We test our proposed method on two public language datasets: IWSLT14 De-En and WMT14 En-De translation tasks. We describe experimental settings in detail in <ref type="table" target="#tab_1">Table 11</ref>. Network Configurations. We use the Transformer <ref type="bibr" target="#b49">[54]</ref> implemented in Fairseq [17] that shares the decoder input and output embedding as the basic NMT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Details of Our SReT Architectures</head><p>The details of our SReT-T, SReT-TL, SReT-S and SReT-B architectures are shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H All-MLP Structure</head><p>We use B/16 in Mixer architectures <ref type="bibr" target="#b46">[51]</ref> as our backbone network. In particular, it contains 12 layers, the patch resolution is 16 ? 16, the hidden size C is 768, the sequence length S is 196, the MLP dimension D C and D S are 3072 and 384, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Ablation Study on Different LRC Designs</head><p>In this section, we verify the effectiveness of different LRC designs as shown in <ref type="figure" target="#fig_0">Fig. 15, including:</ref> (1) learnable coefficients on the identity mapping branch; (2) learnable coefficients on the main self-attention/MLP branch; (3) our used design in the main text, i.e., including learnable coefficients on both branches. The quantitative results of different LRC designs are shown in <ref type="table" target="#tab_1">Table 13</ref>, we can observe that strategy (1) is slightly better than (2), while, (3) can achieve consistent improvement over (1) and (2), and it is applied in our main text. We further visualize more evolution visualizations on various layers/depths of our SReT-TL architecture. The results are shown in <ref type="figure" target="#fig_0">Fig. 16</ref> and the analysis is provided in Sec. K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Observations of Response Maps</head><p>We have a few interesting observations on the visualizations of <ref type="figure">Fig. 8</ref> (main text):</p><p>(1) In the uniform size of transformer DeiT, information in the shallow layers is basically vague, blurry and lacks details. In contrast, the high-level layers contain stronger semantic information and are more aligned with the input. However, our model has a completely different behavior: first, in the same block but with different recursive operations, we can observe that the features are hierarchical (in <ref type="figure" target="#fig_1">Fig. 8 of main text (2)</ref>). Taken as a whole, shallow layers can capture more details like edges, shapes and contours and deep layers focus on the high-level semantic information, which is similar to CNNs. We emphasize such hierarchical representation enabled by recursion and spatial pyramid is critical for vision modality like images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K More Evolution Visualization of LRC Coefficients on ImageNet-1K Dataset</head><p>The visualizations of coefficients evolution at different recursive blocks and layers are shown in <ref type="figure" target="#fig_0">Fig. 15</ref>. Intriguingly, we can observe in the deep layers of recursive blocks, ? tends to be one stably during the whole training. Other coefficients on the identity mapping (? and ?) are holding fixed values that are also close to one during the training. This phenomenon indicates that the identity mapping branch tends to pass the original signal with small scaling. Moreover, it seems the contributions of the two branches have a particular proportion for the particular depth of layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Evolution Visualization of LRC Coefficients on Language Model</head><p>The visualization of coefficients evolution on the language model is shown in <ref type="figure" target="#fig_0">Fig. 17</ref>. Different from the evolution in vision transformer models, the coefficients </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M More Ablation Results on Directly Enlarging Depth of Baseline DeiT Model</head><p>In this section, we provide the results by directly expanding the depth of baseline DeiT model, as shown in <ref type="table" target="#tab_1">Table 14</ref>. We can see deeper na?ve DeiT could not bring additional gain on performance since the deeper and heavier network is usually more difficult to learn meaningful and diverse intermediate features, while our recursive operation through sharing/reusing parameters is an effective way to enlarge the depth of a transformer, meanwhile, obtaining extra improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N More Definitions and Explanations to Prior Arts</head><p>Feed-forward Networks, Recurrent Neural Networks and Recursive Neural Networks. To clarify the definition of proposed recursive operation, we distinct recursive neural networks from feed-forward networks and recurrent neural networks. Feed-forward networks, such as CNNs and transformers, are directed acyclic graphs (DAG). The information path in the feed-forward processing is unidirectional, making the feed-forward networks hard to tackle the structured data with long-span correlations. Recurrent neural networks (RNNs) are usually developed to process the time-series and other sequential data. They output predictions based on the current input and past memory, so they are capable of processing data that contains long-term interdependent compounds like language. Recursive network is a less frequently used term compared to other two counterparts. Recursive refers to repeating or reusing a certain piece of a network. Different from RNNs that repeat the same block throughout the whole network, recursive neural network selectively repeats critical blocks for particular purposes. The recursive transformer iteratively refines its representations for all image patches in the sequence. Difference to Prior Arts: On CNNs, ShuffleNet <ref type="bibr" target="#b57">[62]</ref> uses inerratic shuffle for efficient design while it is not truly random. Thus, there is no inverse operation involved. In contrast, our permutation is entirely stochastic and inverse is crucial since self-attention is sensitive to tokens' order. The na?ve group selfattention only has interaction within the window, Swin <ref type="bibr" target="#b32">[37]</ref> addresses this using shifted windows across different layers. While, we solve it by integrating "slice+permutation+recursion" on the same layer's parameters, so each layer enables to interact with all other windows, not across layers as Swin.</p><p>Algorithm 1 PyTorch-like Code for Sliced Group MHSA with 2? Recursion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer_1</head><p>Layer_2 Recursive 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer_3</head><p>Layer_4 Recursive 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer_17</head><p>Layer_18 Recursive 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer_19</head><p>Layer_20 Recursive 10  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Params/FLOPs vs. ImageNet-1K Acc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Atomic Recursive Operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Approximating global MHSA via sliced group MHSA with permutation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>we can use this property to reduce the FLOPs in designing ViT. Empirical observation: When FLOPs(recursion +G-SA)?FLOPs(V-SA), Acc.(recursion + G-SAs) &gt; Acc.(V-SA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Paradigms of recursive designs in transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>A comprehensive ablation study on different design factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>(1) ImageNet-1K results on All-MLP. (2) Evolution of coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Comparison of BLEU, training loss and val loss on WMT14 En-De. Illustration of recursive transformer with different designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :Fig. 10 :</head><label>910</label><figDesc>The actual optimization landscape from DeiT-108, our SReT-108 and SReT-108 mixed-depth models. Illustration of activation distributions on shallow, middle and deep layers of DeiT-Tiny and our SReT-T networks. Under each subfigure, 14 ? 14, 28 ? 28 and 7 ? 7 are the resolutions of feature maps. "R1/2" indicates the index of recursive operations in each block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Section E: Implementation details of training on ImageNet-1K. (Sec. 5.1 "Datasets and Experimental Settings" of the main paper.) ? Section F: Hyper-parameters setting for training language models on WMT14 En-De and IWSLT14 De-En datasets. (Sec. 5.1 "Datasets and Experimental Settings" and Sec. 5.6 "Neural Machine Translation" of the main paper.) ? Section G: Details of our SReT-T, SReT-TL, SReT-S and SReT-B architectures. (Sec. 3 "Recursive Transformer" and Sec. 5.3. "Ablation Studies" of the main paper.) ? Section H: Details of All-MLP structure. (Sec. 5.5 "All-MLP Architecture" of the main paper.) ? Section I: Ablation study on different LRC designs. (Sec. 3 "Recursive Transformer" and Sec. 5.8 "Analysis and Understanding" of the main paper.) ? Section J: Observations of Response Maps. (Sec. 5.8 "Analysis and Understanding" of the main paper.) ? Section K: More evolution visualization of LRC coefficients on ImageNet-1K dataset. (Sec. 5.8 "Analysis and Understanding" of the main paper.) ? Section L: Evolution visualization of LRC coefficients in language model on WMT14 En-De dataset. (Sec. 5.6 "Neural Machine Translation" and Sec. 5.8 "Analysis and Understanding" of the main paper.) ? Section M: More ablation results on directly expanding the depth of baseline DeiT model on ImageNet-1K dataset. (Sec. 5.8 "Analysis and Understanding" of the main paper.) ? Section N: More definitions of "Feed-forward Networks, Recurrent Neural Networks and Recursive Neural Networks" and explanations of difference to prior arts. (Sec. 1 "Introduction" and Sec. 2 "Related Work" of the main paper.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 :</head><label>11</label><figDesc>Details of group self-attention with permutation designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 :</head><label>13</label><figDesc>Our modifications by removing class token and distillation token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 :</head><label>14</label><figDesc>Comparison of BLEU, training loss and val loss on WMT14 En-De (top) and IWSLT14 De-En datasets (bottom). The red dashed box indicates that LRC makes training more stable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 :</head><label>15</label><figDesc>Ablation study on different LRC designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>#</head><label></label><figDesc>num_groups1 and num_groups2: numbers of groups in different recursions # recursion: recursive indicator class SG_Attention(nn.Module): def __init__(self, dim, num_groups1=8, num_groups2=4, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.): super().__init__() self.num_heads = num_heads # numbers of groups in different recursions self.num_groups1 = num_groups1 self.num_groups2 = num_groups2 head_dim = dim // num_heads self.scale = qk_scale or head_dim ** -0.5 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) def forward(self, x, recursion): B, N, C = x.shape if recursion == False: num_groups = self.num_groups1 else: num_groups = self.num_groups2 # we will not do permutation and inverse permutation if #group=1 if num_groups != 1: idx = torch.randperm(N) # perform permutation x = x[:,idx,:] # prepare for inverse permutation inverse = torch.argsort(idx) qkv = self.qkv(x).reshape(B, num_groups, N // num_groups, 3, self. num_heads, C // self.num_heads).permute(3, 0, 1, 4, 2, 5) q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) attn = self.attn_drop(attn) x = (attn @ v).transpose(2, 3).reshape(B, num_groups, N // num_groups, C) x = x.permute(0, 3, 1, 2).reshape(B, C, N).transpose(1, 2) if recursion == True and num_groups != 1: # perform inverse permutation x = x[:,inverse,:] x = self.proj(x) x = self.proj_drop(x) return x ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 16 :</head><label>16</label><figDesc>Evolution of coefficients at different recursive blocks and layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 17 :</head><label>17</label><figDesc>Evolution of coefficients on language of WMT14 En-De dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>for the image classification arXiv:2111.05297v3 [cs.CV] 26 Jul 2022</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">N? Recursive Operation</cell><cell></cell><cell></cell></row><row><cell>Embeddings</cell><cell>Patch</cell><cell>Norm</cell><cell>Norm</cell><cell>Block</cell><cell>Block Self-Att</cell><cell>Self-Att</cell><cell>+</cell><cell>+</cell><cell>Norm</cell><cell>Norm</cell><cell>MLP</cell><cell>MLP</cell><cell>+</cell><cell>+</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Transformer Block</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">Recursive Transformer Block</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results using different numbers N of na?ve recursive operation on ImageNet-1K dataset.</figDesc><table><row><cell>Method</cell><cell cols="3">Layers #Params (M) Top-1 Acc. (%)</cell></row><row><cell>DeiT-Tiny [52]</cell><cell>12</cell><cell>5.7</cell><cell>72.2</cell></row><row><cell cols="2">+ 1? na?ve recursion 24</cell><cell>5.7</cell><cell>74.0</cell></row><row><cell cols="2">+ 2? na?ve recursion 36</cell><cell>5.7</cell><cell>74.1</cell></row><row><cell cols="2">+ 3? na?ve recursion 48</cell><cell>5.7</cell><cell>73.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Representation ability with global/group self-attentions.</figDesc><table><row><cell>Method</cell><cell cols="3">#Params (M) FLOPs (B) Top-1 Acc. (%)</cell></row><row><cell>Baseline (PiT [24])</cell><cell>4.9</cell><cell>0.7</cell><cell>73.0</cell></row><row><cell>SReT (global self-attention w/o loop)</cell><cell>4.0</cell><cell>0.7</cell><cell>73.6</cell></row><row><cell>SReT (group self-attentions w/ loops)</cell><cell>4.0</cell><cell>0.7</cell><cell>74.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>is a standard image classification dataset, which contains 1K classes with a total number of 1.2 million training images and 50K validation images. Our models are trained on this dataset solely without additional images; (ii) IWSLT'14 German to English (De-En) dataset [2]: It contains about 160K sentence pairs as the training set. We train and evaluate models following the protocol [1]; (iii) WMT'14 English to German (En-De) dataset [3]: The WMT'14 training data consists of 4.5M sentences pairs (116M English words, 110M German words)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Effectiveness</cell></row><row><cell>of various designs on</cell></row><row><cell>ImageNet-1K val set.</cell></row><row><cell>Please refer to Sec. 5.3</cell></row><row><cell>and our Appendix for</cell></row><row><cell>more details. In this</cell></row><row><cell>ablation study, the</cell></row><row><cell>backbone is SReT-TL</cell></row><row><cell>model using spatial</cell></row><row><cell>pyramid architecture.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Throughput evaluation of SReT and baselines. SReT-ExT FLOPs: 0.7B ?46.2% #Params: 4.0M ?29.8% Acc.: 74.0% ?1.8% Throughput: 3473.43 img/s FLOPs: 4.2B ?6.7% #Params: 20.9M ?27.9% Acc.: 81.9% ?0.6% Throughput: 1101.84 img/s</figDesc><table><row><cell>DeiT-T</cell><cell>FLOPs: 1.3B</cell><cell>#Params: 5.7M</cell><cell>Acc.: 72.2%</cell><cell>Throughput: 3283.49 img/s</cell></row><row><cell>Swin-T</cell><cell>FLOPs: 4.5B</cell><cell>#Params: 29.0M</cell><cell>Acc.: 81.3%</cell><cell>Throughput: 1071.43 img/s</cell></row><row><cell>SReT-S</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparison of Top-1 (%) on ImageNet-1K with state-of-the-art methods.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>More Comparison of SReT on ReaL<ref type="bibr" target="#b4">[8]</ref> and ImageNetV2<ref type="bibr" target="#b38">[43]</ref> datasets.</figDesc><table><row><cell>Method</cell><cell cols="5">Network #Parames FLOPs ImageNet ReaL</cell><cell cols="3">ImageNetV2 Top-images Matched-frequency Threshold-0.7 ImageNetV2 ImageNetV2</cell></row><row><cell>DeiT [52]</cell><cell>Tiny</cell><cell>5.7</cell><cell>1.3</cell><cell>72.2</cell><cell>80.1</cell><cell>74.4</cell><cell>59.9</cell><cell>68.5</cell></row><row><cell>SReT</cell><cell>Tiny</cell><cell>4.8</cell><cell>1.1</cell><cell>76.0</cell><cell>83.1</cell><cell>77.9</cell><cell>64.0</cell><cell>72.8</cell></row><row><cell cols="2">DeiT [52] Tiny+Distill</cell><cell>5.7</cell><cell>1.3</cell><cell>74.5</cell><cell>82.1</cell><cell>77.0</cell><cell>62.3</cell><cell>71.1</cell></row><row><cell>SReT</cell><cell>Tiny+Distill</cell><cell>4.8</cell><cell>1.1</cell><cell>77.6</cell><cell>84.4</cell><cell>79.6</cell><cell>65.7</cell><cell>74.2</cell></row><row><cell>DeiT [52]</cell><cell>Small</cell><cell>22.1</cell><cell>4.6</cell><cell>79.8</cell><cell>85.7</cell><cell>81.0</cell><cell>68.1</cell><cell>76.4</cell></row><row><cell>SReT</cell><cell>Small</cell><cell>20.9</cell><cell>4.2</cell><cell>81.9</cell><cell>86.7</cell><cell>82.8</cell><cell>70.3</cell><cell>78.1</cell></row><row><cell cols="2">DeiT [52] Small+Distill</cell><cell>22.1</cell><cell>4.6</cell><cell>81.2</cell><cell>86.8</cell><cell>82.5</cell><cell>69.7</cell><cell>77.5</cell></row><row><cell>SReT</cell><cell>Small+Distill</cell><cell>20.9</cell><cell>4.2</cell><cell>82.7</cell><cell>88.1</cell><cell>84.0</cell><cell>72.3</cell><cell>79.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Ablation results of SReT-T and SReT-TL with different group designs.</figDesc><table><row><cell>Groups</cell><cell>Net</cell><cell cols="4">Layers Params (M) #FLOPs (B) Top-1 (%)</cell></row><row><cell>[8,8][4,4][1,1]</cell><cell>P</cell><cell>20</cell><cell>4.99</cell><cell>1.08</cell><cell>75.41</cell></row><row><cell>[8,8][4,4][1,1]</cell><cell>P+I</cell><cell>20</cell><cell>4.99</cell><cell>1.08</cell><cell>75.94</cell></row><row><cell>[8,8][4,4][1,1]</cell><cell>P+I-L</cell><cell>20</cell><cell>4.99</cell><cell>1.08</cell><cell>76.06</cell></row><row><cell>[1,1][1,1][1,1]</cell><cell>SReT-T</cell><cell>20</cell><cell>4.76</cell><cell>1.38</cell><cell>76.07</cell></row><row><cell>[8,8][4,4][1,1]</cell><cell>SReT-T</cell><cell>20</cell><cell>4.76</cell><cell>1.03</cell><cell>75.73</cell></row><row><cell>[16,2][4,2][1,1]</cell><cell>SReT-T</cell><cell>20</cell><cell>4.76</cell><cell>1.01</cell><cell>75.79</cell></row><row><cell>[8,2][4,1][1,1]</cell><cell>SReT-T</cell><cell>20</cell><cell>4.76</cell><cell>1.12</cell><cell>75.97</cell></row><row><cell>[1,1][1,1][1,1]</cell><cell>SReT-TL</cell><cell>20</cell><cell>4.99</cell><cell>1.43</cell><cell>76.78</cell></row><row><cell>[8,8][4,4][1,1]</cell><cell>SReT-TL</cell><cell>20</cell><cell>4.99</cell><cell>1.08</cell><cell>76.06</cell></row><row><cell>[8,4][4,2][1,1]</cell><cell>SReT-TL</cell><cell>20</cell><cell>4.99</cell><cell>1.14</cell><cell>76.16</cell></row><row><cell>[8,2][4,1][1,1]</cell><cell>SReT-TL</cell><cell>20</cell><cell>4.99</cell><cell>1.18</cell><cell>76.65</cell></row><row><cell>[8,1][4,1][1,1]</cell><cell>SReT-TL</cell><cell>20</cell><cell>4.99</cell><cell>1.25</cell><cell>76.72</cell></row><row><cell>[16,1][14,1][1,1]</cell><cell>SReT-TL</cell><cell>20</cell><cell>4.99</cell><cell>1.24</cell><cell>76.56</cell></row><row><cell>[49,1][28,1][1,1]</cell><cell>SReT-TL</cell><cell>20</cell><cell>4.99</cell><cell>1.23</cell><cell>76.30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameter details of conventional training.</figDesc><table><row><cell>Method</cell><cell cols="3">SReT-T SReT-TL SReT-S</cell></row><row><cell>Epoch</cell><cell>300</cell><cell>300</cell><cell>300</cell></row><row><cell>Batch size</cell><cell>1024</cell><cell>1024</cell><cell>512</cell></row><row><cell>Optimizer</cell><cell cols="3">AdamW AdamW AdamW</cell></row><row><cell>Learning rate</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>Weight decay</cell><cell>0.05</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>Warmup epochs</cell><cell>5</cell><cell>5</cell><cell>5</cell></row><row><cell>Label smoothing</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Stoch. Depth</cell><cell>0.1</cell><cell>0.1</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Hyper-parameter details of soft distillation training.</figDesc><table><row><cell>Method</cell><cell>DeiT</cell><cell>SReT</cell></row><row><cell>Label</cell><cell cols="2">one-hot+hard distillation soft distillation</cell></row><row><cell>Epoch</cell><cell>300</cell><cell>300</cell></row><row><cell>Batch size</cell><cell>1024</cell><cell>1024</cell></row><row><cell>Optimizer</cell><cell>AdamW</cell><cell>AdamW</cell></row><row><cell>Learning rate</cell><cell>0.001</cell><cell>0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Hyper-parameter details of higher-resolution finetuning.</figDesc><table><row><cell>Method</cell><cell>DeiT SReT</cell></row><row><cell>Resolution</cell><cell>384 384</cell></row><row><cell cols="2">Weight decay 1e-8 0.0</cell></row><row><cell>Learning rate</cell><cell>5e-6 5e-6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Training details of our language models. The architectures we used are in Fairseq [17].</figDesc><table><row><cell>Method</cell><cell>IWSLT14 De-En</cell><cell>WMT14 En-De</cell></row><row><cell>arch</cell><cell>transformer iwslt de en</cell><cell>transformer wmt en de</cell></row><row><cell>share decoder input output embed</cell><cell>True</cell><cell>True</cell></row><row><cell>optimizer</cell><cell>Adam</cell><cell>Adam</cell></row><row><cell>adam-betas</cell><cell>(0.9, 0.98)</cell><cell>(0.9, 0.98)</cell></row><row><cell>clip-norm</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>learning rate</cell><cell>5e-4</cell><cell>5e-4</cell></row><row><cell>lr scheduler</cell><cell>inverse sqrt</cell><cell>inverse sqrt</cell></row><row><cell>warmup updates</cell><cell>4K</cell><cell>4K</cell></row><row><cell>dropout</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell>weight decay</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>criterion</cell><cell cols="2">label smoothed cross-entropy label smoothed cross-entropy</cell></row><row><cell>label smoothing</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>max tokens</cell><cell>4096</cell><cell>4096</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 .</head><label>12</label><figDesc>In each recursive transformer block [[.] ? A] ? B, A is the number of blocks with self-contained (non-shared) parameters, B is the number of recursive operations for each block. For C?FFN and D?NLL, C and D are the dimensions (ratios) of hidden features between the two fully-connected layers.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>SReT architectures (Input size is 3?224?224, sliced group self-attention is not included for simplicity.)</figDesc><table><row><cell></cell><cell>Layers</cell><cell cols="2">Output Size</cell><cell cols="2">SReT-T</cell><cell>SReT-TL</cell></row><row><cell></cell><cell>Conv-BN-ReLU</cell><cell cols="2">32?112?112</cell><cell cols="3">3?3 conv, stride 2</cell><cell>3?3 conv, stride 2</cell></row><row><cell>Stem</cell><cell>Conv-BN-ReLU</cell><cell cols="2">64?56?56</cell><cell cols="3">3?3 conv, stride 2</cell><cell>3?3 conv, stride 2</cell></row><row><cell></cell><cell>Conv-BN-ReLU</cell><cell cols="2">64?28?28</cell><cell cols="3">3?3 conv, stride 2</cell><cell>3?3 conv, stride 2</cell></row><row><cell></cell><cell>Recursive T Block (1)</cell><cell cols="2">64?28?28</cell><cell cols="2">64-dim MHSA 3.6?FFN/1.0?NLL</cell><cell>? 2 ? 2</cell><cell>64-dim MHSA 4.0?FFN/1.0?NLL</cell><cell>? 2 ? 2</cell></row><row><cell cols="2">Conv-Pooling Layer (1)</cell><cell cols="2">128?14?14</cell><cell cols="3">3?3 conv, stride 2, group 64</cell><cell>3?3 conv, stride 2, group 64</cell></row><row><cell></cell><cell>Recursive T Block (2)</cell><cell cols="2">128?14?14</cell><cell cols="2">128-dim MHSA 3.6?FFN/1.0?NLL</cell><cell>? 5 ? 2</cell><cell>128-dim MHSA 4.0?FFN/1.0?NLL</cell><cell>? 5 ? 2</cell></row><row><cell cols="2">Conv-Pooling Layer (2)</cell><cell cols="2">256?7?7</cell><cell cols="3">3?3 conv, stride 2, group 128</cell><cell>3?3 conv, stride 2, group 128</cell></row><row><cell></cell><cell>Recursive T Block (3)</cell><cell cols="2">256?7?7</cell><cell cols="2">256-dim MHSA 3.6?FFN/1.0?NLL</cell><cell>? 3 ? 2</cell><cell>256-dim MHSA 4.0?FFN/1.0?NLL</cell><cell>? 3 ? 2</cell></row><row><cell cols="2">Global Average Pooling</cell><cell cols="2">256?1?1</cell><cell cols="3">AdaptiveAvgPool</cell><cell>AdaptiveAvgPool</cell></row><row><cell></cell><cell>Linear Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1000</cell></row><row><cell></cell><cell>#Params (M)</cell><cell></cell><cell></cell><cell cols="2">4.8 M</cell><cell>5.0 M</cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell></cell><cell></cell><cell cols="2">76.1</cell><cell>76.8</cell></row><row><cell cols="2">Distilled Accuracy (%)</cell><cell></cell><cell></cell><cell cols="2">77.7</cell><cell>77.9</cell></row><row><cell cols="3">Finetuning Accuracy ?384 (%)</cell><cell></cell><cell cols="2">79.7</cell><cell>80.0</cell></row><row><cell></cell><cell>Layers</cell><cell>Output Size</cell><cell></cell><cell>SReT-S</cell><cell></cell><cell>Output Size</cell><cell>SReT-B</cell></row><row><cell></cell><cell>Conv-BN-ReLU</cell><cell>63?112?112</cell><cell></cell><cell cols="2">3?3 conv, stride 2</cell><cell>96?112?112</cell><cell>3?3 conv, stride 2</cell></row><row><cell>Stem</cell><cell>Conv-BN-ReLU</cell><cell>126?56?56</cell><cell></cell><cell cols="2">3?3 conv, stride 2</cell><cell>168?56?56</cell><cell>3?3 conv, stride 2</cell></row><row><cell></cell><cell>Conv-BN-ReLU</cell><cell>126?28?28</cell><cell></cell><cell cols="2">3?3 conv, stride 2</cell><cell>336?28?28</cell><cell>3?3 conv, stride 2</cell></row><row><cell></cell><cell>Recursive T Block (1)</cell><cell>126?28?28</cell><cell cols="2">126-dim MHSA 3.0?FFN/2.0?NLL</cell><cell cols="2">? 2 ? 2 336?28?28</cell><cell>336-dim MHSA 3.0?FFN/2.0?NLL</cell><cell>? 2 ? 2</cell></row><row><cell cols="2">Conv-Pooling Layer (1)</cell><cell cols="5">252?14?14 3?3 conv, stride 2, group 126 672?14?14 3?3 conv, stride 2, group 336</cell></row><row><cell></cell><cell>Recursive T Block (2)</cell><cell>252?14?14</cell><cell cols="2">252-dim MHSA 3.0?FFN/2.0?NLL</cell><cell cols="2">? 5 ? 2 672?14?14</cell><cell>672-dim MHSA 3.0?FFN/2.0?NLL</cell><cell>? 5 ? 2</cell></row><row><cell cols="2">Conv-Pooling Layer (2)</cell><cell>504?7?7</cell><cell cols="3">3?3 conv, stride 2, group 252</cell><cell>1344?7?7</cell><cell>3?3 conv, stride 2, group 672</cell></row><row><cell></cell><cell>Recursive T Block (3)</cell><cell>504?7?7</cell><cell cols="2">504-dim MHSA 3.0?FFN/2.0?NLL</cell><cell cols="2">? 3 ? 2 1344?7?7</cell><cell>1344-dim MHSA 3.0?FFN/2.0?NLL</cell><cell>? 3 ? 2</cell></row><row><cell cols="2">Global Average Pooling</cell><cell>504?1?1</cell><cell></cell><cell cols="2">AdaptiveAvgPool</cell><cell>1344?1?1</cell><cell>AdaptiveAvgPool</cell></row><row><cell></cell><cell>Linear Layer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1000</cell></row><row><cell></cell><cell>#Params (M)</cell><cell></cell><cell></cell><cell>20.9 M</cell><cell></cell><cell>71.2 M</cell></row><row><cell></cell><cell>Accuracy (%)</cell><cell></cell><cell></cell><cell>82.0</cell><cell></cell><cell>82.7</cell></row><row><cell cols="2">Distilled Accuracy (%)</cell><cell></cell><cell></cell><cell>82.8</cell><cell></cell><cell>83.7</cell></row><row><cell cols="2">Finetuning Accuracy ?384 (%)</cell><cell></cell><cell></cell><cell>83.8</cell><cell></cell><cell>84.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 :</head><label>13</label><figDesc>Ablation study on different LRC designs.</figDesc><table><row><cell>Method</cell><cell cols="2">#Params (M) Top-1 Acc. (%)</cell></row><row><cell>Baseline (SReT-TL w/o LRC)</cell><cell>5.0</cell><cell>74.7</cell></row><row><cell>on x branch (1)</cell><cell>5.0</cell><cell>75.0</cell></row><row><cell>on f branch (2)</cell><cell>5.0</cell><cell>74.9</cell></row><row><cell>on both (3)</cell><cell>5.0</cell><cell>75.2</cell></row><row><cell cols="3">in language model are more stable during training with small variance. Also, they</cell></row><row><cell>are symmetrical with value one.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14 :</head><label>14</label><figDesc>More ablation results on directly expanding depth of baseline DeiT model. * indicates that the total number layers of our network is 20 (recursive transformer blocks) + 10 (NLL) + 3 (image patch embeddings). Permutation and inverse permutation layers are not included.</figDesc><table><row><cell>Method</cell><cell cols="3">#Layers #Params (M) Top-1 Acc. (%)</cell></row><row><cell>DeiT-Tiny [52]</cell><cell>12</cell><cell>5.7</cell><cell>72.20</cell></row><row><cell>+ extend depth</cell><cell>24</cell><cell>11.55</cell><cell>77.35</cell></row><row><cell>+ extend depth</cell><cell>36</cell><cell>16.39</cell><cell>77.18</cell></row><row><cell>+ extend depth</cell><cell>48</cell><cell>21.73</cell><cell>75.89</cell></row><row><cell>Ours (SReT-S)</cell><cell>33*</cell><cell>20.90</cell><cell>81.90</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In a broader sense, the recurrent neural network is a type of recursive neural network.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In practice, the FLOPs of the two forms are not identical as self-attention module includes extra operations like softmax, multiplication with scale and attention values, which will be multiples by the recursive operation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">We observed a minor issue of soft distillation implementation in DeiT (https: //github.com/facebookresearch/deit/blob/main/losses.py#L56). Basically, it is unnecessary to use logarithm for teacher's output (logits) according to the formulation of KL-divergence or cross-entropy. Adding log on both teacher and student's logits will make the results of KL to be extremely small and intrinsically negligible. We argue that soft labels can provide fine-grained information for distillation, and consistently achieve better results using soft labels in a proper way than one-hot label + hard distillation, as shown in Sec. 5.3.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Zhiqiang Shen et al.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02641</idno>
		<title level="m">Label refinery: Improving imagenet classification through label progression</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep equilibrium models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems</title>
		<meeting>the International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiscale deep equilibrium models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems</title>
		<meeting>the International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language models are few-shot learners</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Autoformer: Searching transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling hierarchical structures with continuous recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caragea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speech-transformer: A no-recurrence sequence-tosequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://github.com/pytorch/fairseq11" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (2021) 1, 4, 5</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic recursive neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<title level="m">Transformer in transformer</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Decoupling the role of data, attention, and losses in multimodal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nematzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00529</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16302</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-adaptive scaling for learnable residual structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A recursive recurrent neural network for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cbnet: A novel composite backbone network architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10580</idno>
		<title level="m">Meta pseudo labels</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno>PMLR</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet? In: International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Meal: Multi-model ensemble via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Is label smoothing truly incompatible with knowledge distillation: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dsod: Learning deeply supervised object detectors from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
		<title level="m">Meal v2: Boosting vanilla resnet-50 to 80%+ top-1 accuracy on imagenet without tricks</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note>NeurIPS Workshop</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<title level="m">Mlp-mixer: An all-mlp architecture for vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS (2017) 1, 4, 8</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Transformer in action: a comparative study of transformer-based acoustic models for large scale speech recognition applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06399</idno>
		<title level="m">Co-scale conv-attentional image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokensto-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
