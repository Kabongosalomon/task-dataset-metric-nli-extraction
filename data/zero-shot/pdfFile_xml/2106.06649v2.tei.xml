<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">st Place Solution for YouTubeVOS Challenge 2021: Video Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thuy</forename><forename type="middle">C</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CyberCore AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan</forename><forename type="middle">N</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CyberCore AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Lh</forename><surname>Phan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CyberCore AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuong</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
							<email>chuong.nguyen@cybercore.co.jp</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Yamazaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CyberCore AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Motor Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Yamanaka</surname></persName>
							<email>masao.yamanaka@toyota-tokyo.tech</email>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Motor Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">st Place Solution for YouTubeVOS Challenge 2021: Video Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video Instance Segmentation (VIS) is a multi-task problem performing detection, segmentation, and tracking simultaneously. Extended from image set applications, video data additionally induces the temporal information, which, if handled appropriately, is very useful to identify and predict object motions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this technical report, we present a solution for the task of Video Instance Segmentation (VIS), specifically targeting the VIS dataset hold by the CVPR2021-YoutubeVOS 2021 Workshop. VIS, first introduced in the YoutubeVOS 2019 challenge <ref type="bibr" target="#b19">[20]</ref>, aims to perform object detection, instance segmentation, and object tracking across video frames. There are 2883 videos with 40 categories in the original 2019 version. In 2021, the dataset is enriched with more than 3800 videos, each has about 30 frames, and the categories are also refined. ? equal contribution VIS by its nature is a multi-task learning problem, and generally there two main approaches. A straightforward way is to perform each individual task separately and sequentially <ref type="bibr" target="#b14">[15]</ref>. However, since the components are trained and inferred independently, the full pipeline is complicated, slow, and sub-optimal. The second approach <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref> aims to build a single model that jointly learns and performs all the tasks simultaneously. This not only simplifies the pipeline, reduces the inference latency, but potentially improves the final performance.</p><p>Our solution also follows the unified direction but is designed to address several specific technical challenges for the dataset. We also hope that it can serve as a strong baseline for more general applications. Our main contributions for the challenge are summarized as follows:</p><p>? Our data analysis shows that only a small portion (17%) of the training images are useful, while the rest (83%) are ineffective. We hence propose a training mechanism, named Multi-Source Data (MSD), which could both increase the diversity of data and improves model generalization.</p><p>? We exploit multi-task learning and propose the Temporally Correlated Instance Segmentation (TCIS) module to learn the temporal relation between instance masks over adjacent frames.</p><p>? We suggest a Bidirectional Tracking (BiTrack) postprocessing step to track objects in both forward and backward order to recall more objects, before merging two sets of tracks to obtain a final result.</p><p>? Our method secures the 1st rank on the YoutubeVOS-VIS2021, with the score 0.575 mAP on the public validation set, and 0.541 mAP on the private test set. Evaluating the YouTubeVOS-VIS2019 dataset, our solution also obtains 0.543 mAP, setting a new record for the benchmark. Our baseline framework is based on Mask-RCNN <ref type="bibr" target="#b8">[9]</ref>. It has a Backbone, a Feature Pyramid Network with 6 levels, an RPN head in the first stage. The second stage has three branches: object bounding box detection, instance mask segmentation, and object track heads.</p><p>The paper is organized as follows. Section 2 summarizes our baseline method, including the model architecture, training, and inference pipeline. Section 3 describes our main solutions, including the data analysis and techniques to improve data diversity, the TCIS component, and a bag of useful tricks to further improve the results. The implementation details, ablation study for different components, and comparison with other methods are presented in section 4. Section 5 concludes our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Baseline</head><p>Network architecture Our model is built upon Mask-RCNN <ref type="bibr" target="#b8">[9]</ref>, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. The network includes a backbone and a Feature Pyramid Network (FPN) <ref type="bibr" target="#b10">[11]</ref> to extract features. A Regional Proposal Network (RPN) is used in the first stage to detect object regions. Given the proposal boxes, the second stage uses a RoI-Align operator to crop features and feed to 3 sub-networks, namely the Box Head for detection, Mask Head for Segmentation, and Track Head <ref type="bibr" target="#b16">[17]</ref> to extract embedding vector for object association. Here, we also add an extra level P7 to the FPN, resulting in 6 levels in total. Training pipeline To train the tracking module, we feed a pair of frames (X t , X t ), where X t is the key frame at time t, and X t is randomly sampled within the interval [t ? ?, t + ?]. Since they are significantly overlapped, either one of the frames is sufficient for training the detection and the segmentation modules. Inference pipeline Given a video, the inference is sequentially performed to obtain object attributes (box, label, mask, and embedding). Meanwhile, the data association is conducted online to link the same objects across frames. Finally, we can construct series of unique object masks in the video to output final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data analysis</head><p>The YoutubeVOS-VIS2021 dataset has about 90k images, extracted from 3k different videos. However, because the camera may be fixed, and the objects can stay idle or move slowly, the frames in a video can be extremely overlapped, as illustrated in <ref type="figure" target="#fig_1">Fig. 2c</ref>. Therefore, we conduct two experiments to analyze the data efficiency.</p><p>Firstly, we study the severity of overlapping due to object's slow motion and fixed camera. Specifically, we calculate the Intersection over Union (IoU) of each object's bounding box in two consecutive frames, and then take the average IoU score over the video. The IoU histogram of the dataset is then shown in <ref type="figure" target="#fig_1">Fig. 2a</ref>. We see that the portion of objects having IoU overlap above 0.8 is dominant, verifying that the object displacement is indeed trivial and the overlap is severe. In the second experiment, we uniformly sample different number of frames (eg. 1,2,5,10) in a video to train the model and compare with the results using all frames. For each key-frame, we apply affine transforms to generate a pseudo reference frame for tracking. As shown in <ref type="figure" target="#fig_1">Fig. 2b</ref>, using only 1 frame in the video already achieves 23.6% mAP, while 5 frames can reach 30.7% mAP, almost equal if using all frames (30.9% mAP). This confirms that 83.3% of the data is redundant and basically ineffectual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Source Data</head><p>To enrich the dataset, we utilize a subset from Open-Image <ref type="bibr" target="#b0">[1]</ref>, that has common object categories with YoutubeVOS-VIS2021, such as bird, fish, turtle. This adds 14k images to the dataset. We also combine with the MS COCO 2017 dataset <ref type="bibr" target="#b11">[12]</ref>, yielding approximately 221k images in total. However, using a heterogeneous dataset brings some technical issues due to the label difference, that is, no tracking labels in both OpenImage and COCO, low quality or missing ground truth mask in OpenImage, and class mismatch between Youtube VIS and COCO. We address the problems as follows.</p><p>Semi-supervised Tracking learning To overcome the absence of ground truth tracking labels, we generate pseudo track-ids by applying augmentations such as shift, rotate, and flip on the key-frame to get its transformed version. Boxes of the same object in different transformed images are assigned with the same and unique track-id.</p><p>Weakly-supervised Segmentation learning Images in OpenImage dataset can have no or noisy segmentation mask. Hence, we ignore the segmentation loss of these samples and use them only for detection and tracking training.</p><p>Dataset Fusion with Auxiliary Classes The YoutubeVOS-VIS2021 and the MS COCO 2017 datasets have 40 and 80 classes, respectively, and they share 22 categories in common. Conveniently, we can simply ignore the objects of the remaining classes. However, COCO has high-quality labels, especially segmentation masks. Ignoring these classes discards a majority of the dataset while learning all of them will shift our model's target attention. Therefore, to utilize all the available labeled samples, we propose to relax their categories, casting the problem as dataset fusion with auxiliary classes. Following <ref type="bibr" target="#b15">[16]</ref>, we assume that the remaining 58 classes from COCO can be grouped into K auxiliary classes, leading to predict 40 + K classes totally. However, we do not manually assign the category for these K classes, but let the network learn the concept of auxiliary classes implicitly and automatically. The proposed method is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Dataset fusion with auxiliary classes</head><formula xml:id="formula_0">Input: Batch size N ; Predicted probs? = {?i} N i=1 ; Labels l = {li} N i=1 ; Output: New labels y = {yi} N i=1 for i in range(N) do if li = C + 1 (#if it is auxiliary class) then if C &lt; argmax(?i) ? C + K then # it is doing correct, continue yi = argmax(?i) else # randomly pickup among K classes yi = uniform(C + 1, C + K) end else yi = li end end</formula><p>Concretely, in the case of auxiliary classes, the category is selected based on the corresponding prediction. If the predicted index falls into the auxiliary indices, the predicted index is the label, otherwise, the label is randomly sampled in range [41, 40 + K]. This mechanism can benefit from two aspects. First, if K is set to 1, the number of samples of this class will be significantly imbalanced with our target classes. In addition, the concept of this class is hard to learn, due to the inconsistency of the feature. Secondly, by randomly sampling class indices, we ensure that the model does not bias to a specific class index, resulting in the extreme case K = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporally Correlated Instance Segmentation</head><p>Simply applying the techniques from image to video is generally less effective, since the temporal correlation is not taken into account. In fact, we observe that an instance mask in a reference frame is highly related to the corresponding instance mask in a key frame. Consequently, we introduce the module named Temporally Correlated Instance Segmentation (TCIS) to exploit this feature. <ref type="figure" target="#fig_2">Figure 3</ref> depicts the TCIS architecture, in which the Correlation Transform is a standard ResNet block. Let F t i and F t+? i be the RoI feature of the same instance i th at frame t and t + ?, respectively. F t mi is the ground truth mask of instance i at frame t. The TCIS module operates by the following steps:</p><p>1. We multiply the feature F t i with its ground truth mask F t mi to create the attention-masked feature M t i . Mean- while, we subtract F t i for F t+? i , and fed the difference to the Correlation Transform block to compute the spatial shift S t,t+? i . <ref type="bibr" target="#b3">[4]</ref> receives the spatial shift S t,t+? i as the offset input and the attention-masked M t i as the feature input, outputs the temporally correlated feature F t,t+? i . The offset S t,t+? i helps TCIS pay attention to motion of the instance i between the two frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Deformable Convolution</head><p>3. Finally, F t,t+? i and F t+? i is added together and used as the input for the mask head. We share the same mask head between TCIS and the main branch.</p><p>Our TCIS is borrowed from the module MaskProp <ref type="bibr" target="#b1">[2]</ref>. The difference is that, objects' features in MaskProp are jointly computed on the whole image, while our approach processes each ROI instance independently. Moreover, TCIS is employed as an auxiliary task during training only, hence induces no additional computation at inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Bidirectional Tracking</head><p>Since a motion can happen both forward and backward in time, we propose a post-processing step called Bidirectional Tracking (BiTrack) to further enhance the prediction consistency, as described in Algorithm 2.</p><p>Concretely, we first predict objects' bounding boxes, class scores, segmentation masks, and embeddings for all frames in a video. We then run the object ID association backward and forward, matching new objects with existing objects stored in a buffer. If the new object is matched with the existing object, ID of the existing object is assigned to the new object. Otherwise, a new ID will be assigned. The process continues until all frames are checked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Bidirectional Tracking</head><p>Input:</p><p>? Forward tracklets F . ? Backward tracklets B.</p><p>? </p><formula xml:id="formula_1">m = Merge(f ,b) ; M ? M ? m ; B ?B ? b ; F ?F ? f ; else continue end end end M ? M ? (F \F ) ? (B \B) ; return M</formula><p>Consequently, we obtain the tracklets F from the forward order, and the tracklets B from the backward order. We consider a frame as valid if it has objects detected in the both tracklets. For the same instance, the forward tracklet f and the backward tracklet b may be different. BiTrack module is applied to merge high overlapping tracklets into a final one. Concretely, two tracklets are merged together if the average of IoU between their boxes in valid frames is greater than a threshold thr. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates an example of how a forward tracklets (first row) and a backward tracklets (second row) are merged together into the final result (third row). In the forward tracklets, as seen in the lower half of the images, it is hard to track the Frisbee disc (red box) in the forward path, but easier if doing it backward. Vice versa for another Frisbee disc appearing on the upper half of the images. As a result, two tracklets compensate each other, yielding more robust tracking results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Bag of tricks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Multi-task learning</head><p>Besides main tasks, we also train the model with the auxiliary tasks, described as follows.</p><p>? Semantic segmentation The mask head for predicting instance masks only focuses on local information belonging to instances without learning a global concept. Therefore, we suggest adding a semantic segmentation branch to predict masks on a global scale. Specifically, the feature output from the P3 level of FPN is forwarded to stacked convolutions to predict a semantic segmentation mask with 40 channels.</p><p>? Multi-label classification We propose to add a multilabel classification sub-network to predict categories. Concretely, the backbone feature at the C5 level is fed into the sub-network to predict a 40-class vector. To allow multi-label prediction, we use the Binary Cross Entropy loss during the training.</p><p>? Mask scoring <ref type="bibr" target="#b9">[10]</ref> We further predict the instance segmentation quality in terms of mask IoU. In inference, the mask score is multiplied with the classification score to improve prediction confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Ensemble</head><p>We ensemble the predictions of different models into the final results as follows.</p><p>? Detection We apply Greedy Auto Ensemble <ref type="bibr" target="#b22">[23]</ref> to merge predicted boxes of models. Note that, to ensure the merged detection score would be well calibrated, we do not average scores of merged boxes. Instead, we perform the max operation so that the final score would be inherited from the dominant box.</p><p>? Segmentation and Tracking The bounding boxes obtained from the ensembled models are treated as proposals, which are then fed to different models to extract segmentation mask and embedding representation. Finally, we average masks and embeddings of models to obtain the final ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Pseudo label</head><p>We take the advantage of the ensemble to generate pseudo labels on the detection of the valset. Additionally, we feed these boxes through the tracking module to obtain the most confident ones. Our motivation is that if we can match boxes over frames, these boxes are likely to represent a  <ref type="table">Table 2</ref>. Ablation study on the bag of tricks with two backbones ResNeSt101 (S101) and SwinS on the YoutubeVOS-VIS2021 valset.</p><p>foreground object. Thus, they are more reliable than nontrackable ones. Afterward, we combine the trainset and the pseudo-label (only detection) valset to train new networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.4">Label voting</head><p>In the tracking-by-detection approach, if the detector misclassifies object labels, the tracking consequently fails to track the object. Consequently, this can break a tracklet into many fragments, and damage the results. To ease this issue, we relax the label consistency criterion. Therefore, detected objects with different categories could be matched together based on their visual embeddings. To this end, a track may still contain different labels, hence, we select the one with highest frequency as the final label for that track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.5">Multi-scale testing</head><p>We utilize Multi-scale testing for further boosting network performance. Alongside the 1? image scale, we also exploit 0.7? and 1.3? scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>All models are trained with Synchronized BatchNorm of batch size 16 on 4 GPUs (4 images per GPU). We use two types of backbone: ResNeSt <ref type="bibr" target="#b23">[24]</ref> and SwinTransformer <ref type="bibr" target="#b12">[13]</ref>. For ResNeSt backbone, the optimizer is SGD with momentum 0.9 and initial learning rate 1e ?2 , while AdamW <ref type="bibr" target="#b13">[14]</ref> with initial learning rate 5e ?5 is used for SwinTransformer. Each experiment is trained by 12 epochs, in which, the learning rate is dropped 10 times at the end of epoch 8 and 11. For fast training, we use the image size of 360x640. In our experiments, training with double image size only provides negligible improvement, so this image scale is sufficient. Our code is based on MMDetection <ref type="bibr" target="#b2">[3]</ref>, and networks are pretrained on the MS COCO 2017 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>Proposed components At first, we use the model with backbone ResNeSt50 to evaluate the effects of components including Temporally Correlated Instance Segmentation (TCIS), Multi-task learning (MaskScoring, SemSeg, and Multi-label classification), Multi-Source Data (MSD), and Bidirectional Tracking (BiTrack). <ref type="table" target="#tab_0">Table 1</ref> lists results on the YoutubeVOS-VIS2021 valset. In Exp. A1, we start by a baseline model and achieve 0.309 mAP. By adding TCIS in Exp. A2, the metric is improved by 2.2% mAP. Multi-task add-ins (Exp. A3) only give a small gain by 0.7% mAP. Then, when applying MSD (Exp. A4), the mAP reaches 0.364. Finally, BiTrack (Exp. A5) increases the result to 0.388 mAP. These experiments reveal that the three main proposed components constantly leverage the model performance by more than 2% mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bag of tricks</head><p>We combine all mentioned techniques in Exp. A5 and increase model capacity by training two models with larger backbones ResNeSt101 (Exp. B1) and SwinS (Exp. B2), yielding 0.418 and 0.440 mAP (see <ref type="table">Tab.</ref> 2), respectively. By ensembling these two models, we obtain an improved mAP at 0.464 in Exp. B3. After generating pseudo data for the detection part in valset, we combine the trainset and valset to re-train the two models and reach boosted performance with mAP 0.539 (Exp. B4) and 0.560 (Exp. B5). Finally, by ensembling B1, B2, B4, and B5, we achieve the state-of-the-art with mAP 0.575. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison</head><p>YoutubeVOS-VIS2021 We use the solution in Exp. B6 to benchmark on both YoutubeVOS-VIS2021 valset and testset. Results in Tab. 3 and Tab. 4 show that our method surpasses others by a large margin. Specifically, in the valset, our solution achieves 0.575 mAP, which is a large gap of more than 3% to the second method. While transferring to the testset, we preserve the first rank to be the Stateof-the-art with 0.541 mAP. This indicates that the proposed method has a strong and stable performance on the VIS task. <ref type="figure">Figure 5</ref> shows sample predictions of our model on the testset. The model can accurately detect categories, segment instance masks, and track objects over frames.</p><p>YoutubeVOS-VIS2019 The model used for this benchmark contains backbone SwinS, TCIS, MultiTask, and Bi-Track. The result is shown in Tab. 5.</p><p>Method mAP AP 50 AP 75 AR1 AR10 Ours 0.543 0.766 0.656 0.470 0.579 MaskProp <ref type="bibr" target="#b1">[2]</ref> 0.425 -0.456 --VisTR <ref type="bibr" target="#b18">[19]</ref> 0.401 0.640 0.450 0.383 0.449 CrossVIS <ref type="bibr" target="#b20">[21]</ref> 0.366 0.573 0.397 0.360 0.420 CompFeat <ref type="bibr" target="#b6">[7]</ref> 0.353 0.560 0.386 0.331 0.403 <ref type="table">Table 5</ref>. Comparison with other methods on the YoutubeVOS-VIS2019 valset. Bold symbols represent the best metrics.</p><p>It can be seen that, without MSD, our method can still surpass recent ones with a remarkable gap, i.e., we achieve 0.543 mAP, which is around 11.0% better than the second method (MaskProp). This again demonstrates the effectiveness and generalization of the proposed solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we design a unified approach to perform the VIS task within a single model. The success of our solution mainly comes from three proposed components, namely Temporally Correlated Instance Segmentation, Multi-Source Data, and Bidirectional Tracking, as well as applying several practical tricks, e.g. ensemble and pseudo label. Specifically, the three proposed components can boost the performance by approximately 8% mAP with the standard backbone, while the bag of tricks give us a significant improvement by more than 15% mAP with stronger backbones. Leveraging this robust performance, our method outperforms the others significantly and makes new records on the YoutubeVOS VIS 2019 and 2021 datasets. <ref type="figure">Figure 5</ref>. Our model predictions on the YoutubeVOS-VIS2021 testset. Each row is a video. The same color represents the same object.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our baseline framework is based on Mask-RCNN [9]. It has a Backbone, a Feature Pyramid Network with 6 levels, an RPN head in the first stage. The second stage has three branches: object bounding box detection, instance mask segmentation, and object track heads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Data efficiency analysis. (a) Histogram of bounding boxes's IoU of the same objects appearing in two adjacent-frames. The higher IoU values, the more static objects. (b) Accuracy of models trained with different number of frames. (c) Many frames in a video are almost identical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of TCIS module (best viewed in color).Step 1 computes attention-masked feature M t i and spatial shift S t,t+? i as the feature and deformable-offsets inputs, which are fed to the deformable convolution to compute correlated feature F t,t+? i in Step 2. Step3 predicts mask for the instance in the reference frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>IsOverlap(f, b): the function used to check if tracklet f and tracklet b are overlapped. Output: Final tracklets M Initialization // Init empty matched lists for forward tracklets and backward tracklet? F ? ?,B ? ? end for f in range(F) do for b in range(B) do if b ?B and IsOverlap(f,b) then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Example of forward tracklets (first row), backward tracklets (second row), and merged tracklets(bottom row). The missing Frisbee discs are marked by red boxes. Forward tracklets and backward tracklets compensate each other by being able to keep track of the objects that the other missed, resulting in the merged tracklets with a better result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Experiments TCIS MultiTask MSD BiTrack mAP AP 50 AP 75 AR1 AR10 ?mAP (%) Ablation study on the proposed components using the backbone ResNeSt50 on the YoutubeVOS-VIS2021 valset.</figDesc><table><row><cell>A1</cell><cell></cell><cell>0.309</cell><cell>0.501</cell><cell>0.338</cell><cell>0.269 0.346</cell><cell>-</cell></row><row><cell>A2</cell><cell></cell><cell>0.331</cell><cell>0.535</cell><cell>0.354</cell><cell>0.285 0.368</cell><cell>2.2</cell></row><row><cell>A3</cell><cell></cell><cell>0.338</cell><cell>0.546</cell><cell>0.356</cell><cell>0.287 0.374</cell><cell>0.7</cell></row><row><cell>A4</cell><cell></cell><cell>0.364</cell><cell>0.570</cell><cell>0.402</cell><cell>0.299 0.397</cell><cell>2.6</cell></row><row><cell>A5</cell><cell></cell><cell>0.388</cell><cell>0.589</cell><cell>0.438</cell><cell>0.320 0.436</cell><cell>2.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TCIS: Temporal</cell></row><row><cell cols="7">Correlated Instance Segmentation, MultiTask: Multi-task learning, MSD: Multi-Source Data, BiTrack: Bi-directional tracking.</cell></row><row><cell>Experiments</cell><cell>Method</cell><cell cols="4">mAP AP 50 AP 75 AR1 AR10</cell></row><row><cell>B1</cell><cell>S101</cell><cell>0.418</cell><cell>0.652</cell><cell>0.464</cell><cell>0.340 0.454</cell></row><row><cell>B2</cell><cell>SwinS</cell><cell>0.440</cell><cell>0.666</cell><cell>0.504</cell><cell>0.359 0.476</cell></row><row><cell>B3</cell><cell>Ensemble (B1, B2)</cell><cell>0.464</cell><cell>0.698</cell><cell>0.515</cell><cell>0.376 0.505</cell></row><row><cell>B4</cell><cell>S101 + Pseudo</cell><cell>0.539</cell><cell>0.777</cell><cell>0.628</cell><cell>0.421 0.578</cell></row><row><cell>B5</cell><cell>SwinS + Pseudo</cell><cell>0.560</cell><cell>0.792</cell><cell>0.644</cell><cell>0.430 0.594</cell></row><row><cell>B6</cell><cell cols="2">Ensemble (B1,B2, B4, B5) 0.575</cell><cell>0.806</cell><cell>0.671</cell><cell>0.441 0.609</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alldrin</forename><forename type="middle">J</forename><surname>Uijlings I. Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying, Segmenting, and Tracking Object Instances in Video with Mask Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9736" to="9745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deformable Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10-764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal feature augmented network for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiping</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dual embedding learning for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peike</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Compfeat: Comprehensive feature aggregation for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03400</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Msn: Efficient online mask selection network for video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidit</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhika</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10452</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video instance segmentation 2019: A winning approach for combined detection, segmentation, classification and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised learning for generalizable out-ofdistribution detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Mohseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Pitale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jbs</forename><surname>Yadawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quasi-dense similarity learning for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An empirical study of detection-based video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Endto-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5188" to="5197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Crossover learning for fast online video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05970</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shusheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11963</idno>
		<title level="m">Bin Feng, and Wenyu Liu. Tracking instances as queries</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Guanglu Song and Xiaogang Wang. 1st place solutions for openimage2019 -object detection and instance segmentation</title>
		<idno type="arXiv">arXiv:2003.07557</idno>
		<editor>Yuhang Zang Yan Gao Enze Xie Junjie Yan-Chen Change Loy Yu Liu</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
