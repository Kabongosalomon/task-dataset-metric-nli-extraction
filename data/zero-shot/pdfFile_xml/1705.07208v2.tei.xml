<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PIXCOLOR: PIXEL RECURSIVE COLORIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Dahl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bieber</surname></persName>
							<email>dbieber@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
							<email>mnorouzi@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
							<email>shlens@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
							<email>kpmurphy@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">PIXCOLOR: PIXEL RECURSIVE COLORIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel approach to automatically produce multiple colorized versions of a grayscale image. Our method results from the observation that the task of automated colorization is relatively easy given a low-resolution version of the color image. We first train a conditional PixelCNN to generate a low resolution color for a given grayscale image. Then, given the generated low-resolution color image and the original grayscale image as inputs, we train a second CNN to generate a high-resolution colorization of an image. We demonstrate that our approach produces more diverse and plausible colorizations than existing methods, as judged by human raters in a "Visual Turing Test".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Building a computer system that can automatically convert a black and white image to a plausible color image is useful for restoring old photographs, videos <ref type="bibr" target="#b33">[34]</ref>, or even assisting cartoon artists <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref>. From a computer vision perspective, this may appear like a straightforward image-toimage mapping problem, amenable to a convolutional neural network (CNN). We denote this by y = f (x), where x is the input grayscale image, y is the predicted color image, and f is a CNN. This approach has been pursued in several recent papers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref> which leverages the fact that one may obtain unlimited labeled training pairs by converting color images to grayscale.</p><p>Removing the chromaticity from an image is a surjective operation, thus restoring color to an image is a one-to-many operation <ref type="figure" target="#fig_0">(Figure 1</ref>). We can express this ambiguity as a conditional probability model y ? p (y | x) to capture multiple possible outputs, rather than predicting a single image (see Section 2 for review of generative models).</p><p>In this paper, we propose a new method, that employs a PixelCNN <ref type="bibr" target="#b35">[36]</ref> probabilistic model to produce a coherent joint distribution over color images given a grayscale input. PixelCNNs have several advantages over other conditional generative models: <ref type="bibr" target="#b0">(1)</ref> they capture dependencies between the pixels to ensure that colors are selected consistently; (2) the log-likelihood can be computed exactly and training is stable unlike other generative models. The main disadvantage of PixelCNNs, however, is that they are slow to sample from, due to their inherently sequential (autoregressive) structure. In this paper we leverage the fact that the chrominance of an image (especially as perceived by humans) is of much lower spatial frequency than the luminance. In fact, some image storage formats, such as JPEG, exploit this intuition and store the color channels at lower resolution than the intensity channel. This means that it is sufficient for the PixelCNN to predict a low resolution color image, which may be done quite quickly. We then train a Figure 2: Diverse colorizations generated by our PixColor method. For each group of 4 images, the first is the grayscale input, and the rest are 3 samples from the model. second CNN-based "refinement network", which combines the predicted low resolution color image with the high resolution grayscale input to produce a high resolution color image.</p><p>Formally, our approach can be thought of as a conditional latent variable model of the form p(y |</p><formula xml:id="formula_0">x) = z ?(y = f (x, z))p(z | x)</formula><p>, where x is the input grayscale image, y is the output color image, z is the latent low-dimensional color image. The PixelCNN estimates p(z | x), and the refinement CNN estimates y = f (x, z). At test time, rather than summing over z's, we sample a few z s. During training, we use the ground truth low resolution color image for z, so that we can fit the two conditional models independently. See Section 3 for the details.</p><p>Our proposed method, called Pixel Recursive Colorization (PixColor), produces diverse, high quality colorizations. <ref type="figure">Figure 2</ref> depicts some examples with high diversity. In Section 4, we describe how we quantitatively evaluate the performance of colorization using human raters. We report our results in Section 5, where we show that PixColor significantly outperforms existing methods. Section 6 concludes the paper and discusses some future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Early approaches to colorization relied on some amount of human effort, either to identify a relevant source color image from which the colors could be transferred <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3]</ref>, or to get a rough coloring from a human annotator to serve as a set of "hints" <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b10">11]</ref>. More recently, there has been a surge of interest in developing fully automated solutions, which do not require human interaction (see <ref type="table" target="#tab_1">Table 1</ref>).</p><p>Most recent methods train a CNN to map a gray input image to a single color image <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref>. When such models are trained with L2 or L1 loss, the colorization results often look somewhat "washed out", since the model is encouraged to predict the average color. Some recent papers (e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b40">41]</ref>) discretize the color space, and use a per-pixel cross-entropy loss on the softmax outputs of a CNN, resulting in more colorful pictures, especially if rare colors are upweighted during training (e.g., <ref type="bibr" target="#b40">[41]</ref>). However, since the model predicts each pixel independently, the one-to-many nature of the task is not captured properly, e.g., all of the pixels in a region cannot be constrained to have the same color.</p><p>Previous work has proposed several ways to ensure that multiple colorizations generated by a model are globally coherent. One approach is to use a conditional random field (CRF) <ref type="bibr" target="#b2">[3]</ref>, although inference in such models can be slow. A second approach is to use a CNN with multiple output "heads", corresponding to different colorizations of an image. One can additionally train a "gating" network   <ref type="bibr" target="#b2">[3]</ref> requires that the user specify one or more training images that are similar to the input gray image. Although the CRF is is capable of generating multiple solutions, <ref type="bibr" target="#b2">[3]</ref> uses graph-cuts to produce a single MAP estimate. Similarly, although the GAN method of <ref type="bibr" target="#b16">[17]</ref> is capable of producing multiple solutions, they report that their GAN ignores the noise, and always predicts the same answer for each input. This problem is fixed in <ref type="bibr" target="#b1">[2]</ref> by introducing noise at multiple levels of the generator.</p><p>to select the best head for a given image. This mixture of experts (MOE) approach was used in <ref type="bibr" target="#b0">[1]</ref> mainly for image compression, rather than colorization per se.</p><p>A third approach is to use a (conditional) variational autoencoder (VAE) <ref type="bibr" target="#b17">[18]</ref> to capture dependencies amongst outputs via a low dimensional latent space. To capture the dependence on the input image, <ref type="bibr" target="#b8">[9]</ref> proposes to use a mixture density network (MDN) to learn a mapping from a gray input image to a distribution over the latent codes, which is then converted to a color image using the VAE's decoder. Unfortunately, this method often produces sepia toned results <ref type="table" target="#tab_4">(Table 3)</ref>.</p><p>A fourth approach is to use a (conditional) generative adversarial network (GAN) <ref type="bibr" target="#b11">[12]</ref> to train a generative model jointly with a discriminative model. The goal of the discriminative model is detect synthesized images, while the goal of the generative model is a fool the discriminator. This approach results sharp images, but <ref type="bibr" target="#b16">[17]</ref> reports that a GAN-based colorization results underperform previous CNN approaches <ref type="bibr" target="#b40">[41]</ref>. One of their failure modes "mode collapse" problem, whereby the resulting model correctly predicts one mode of a distribution but fails the full diversity of the data <ref type="bibr" target="#b22">[23]</ref>. More recently, <ref type="bibr" target="#b1">[2]</ref> have applied a slightly different GAN to colorization. Although the authors claim to avoid the mode collapse problem, it is hard to compare against previous results because the authors only employ the LSUN-bedrooms dataset for evaluation. Most papers (including ours) employ the "ctest10k" split of the ImageNet validation dataset from <ref type="bibr" target="#b19">[20]</ref> (see Section 4 for more details).</p><p>We propose a novel approach that uses a PixelCNN <ref type="bibr" target="#b35">[36]</ref> to produce multiple low resolution color images, which are then deterministically converted to high resolution color images using a CNN refinement network. By using multiple low resolution color "hints" to the CNN, we capture the one-to-many nature of the task and prevent the CNN from producing sepia toned outputs.</p><p>Very recently, in a concurrent submission, <ref type="bibr" target="#b28">[29]</ref> proposed an approach which is similar to ours. However, instead of passing the output of a pixelCNN into a refinement CNN, they do the opposite, and pass the output a CNN into a pixelCNN. The visual quality and diversity of their results look good, but, unlike us, they do not perform any human evaluation, so we do not have a quantitative comparison. The primary disadvantage of their approach is that it is slow for a pixelCNN to generate high resolution images; indeed, their method only generates 32 ? 32 color images, which are then deterministically upscaled to 128 ? 128. By contrast, our CNN refinement network learns to upscale from 28 ? 28 to the same size as the input, which works much better than deterministic upscaling, as we will show. We mostly focus on generating 256 ? 256 images, to be comparable to prior work, but we also show some non-square examples, which is important in practice, since many grayscale photos of interest are in portrait or landscape mode.  <ref type="bibr" target="#b3">[4]</ref>. Then, the conditioning network and the adaptation network convert the brightness channel Y into a set of features providing the necessary conditioning signal to the PixelCNN. The PixelCNN is optimized jointly with the conditioning and adaptation networks to predict a low spatial resolution version of the color image in a discretized space. The low spatial resolution image is subsequently supplied to a refinement network, which is trained to produce a full resolution colorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PIXEL RECURSIVE COLORIZATION (PIXCOLOR)</head><p>The key intuition behind our approach is that it suffices to predict a plausible low resolution color image, since color is much lower spatial frequency than intensity. To illustrate this point, suppose we take the ground truth chrominance of an image, downsample it to 28?28, upsample it back to the original size, and then combine it with the original luminance. <ref type="figure">Figure 3</ref> shows some examples of this process. It is clear that the resulting colorized images look very close to the original color images.</p><p>In the sections below, we describe how we train a model to predict multiple plausible low resolution color images, and then how we train a second model to combine these predictions with the original grayscale input to produce a high resolution color output. See <ref type="figure">Figure 4</ref> for an overview the approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PIXELCNN FOR LOW-RESOLUTION COLORIZATION</head><p>Inspired by the success of autoregressive models for unconditional image generation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> and super resolution <ref type="bibr" target="#b7">[8]</ref>, we use a conditional PixelCNN <ref type="bibr" target="#b35">[36]</ref> to produce multiple low resolution color images. That is, we turn colorization into a sequential decision making task, where pixels are colored sequentially, and the color of each pixel is conditioned on the input image and previously colored pixels.</p><p>Although sampling from a PixelCNN is in general quite slow (since it is inherently sequential), we only need to generate a low-resolution image (28x28), which is reasonably fast. In addition, there are various additional speedup tricks we can use (see e.g., <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b18">19]</ref>) if necessary.</p><p>Our architecture is based on <ref type="bibr" target="#b7">[8]</ref> who used PixelCNNs to perform super resolution (another one-tomany problem). We use the YCbCr colorspace, because it is linear, simple and widely used (e.g., by JPEG). We discretize the Cb and Cr channels separately into 32 bins. Thus the model has the following form:</p><formula xml:id="formula_1">p(y|x) = i p(y(i, r) | y(1 : i ? 1, :), x) p(y(i, b) | y(i, r), y(1 : i ? 1, :), x)</formula><p>where y(i, r) is the Cr value for pixel i, and y(i, b) is the Cb value. We performed some preliminary experiments using Logistic mixture models to represent the output values as suggested by the Pix-elCNN++ of <ref type="bibr" target="#b30">[31]</ref>, as opposed to using multinomials over discrete values <ref type="bibr" target="#b35">[36]</ref>. However, we did not see a meaningful improvement, so for simplicity, we stick to a multinomial prediction model. We train this model using maximum likelihood, with a cross-entropy loss per pixel. Because of the sequential nature of the model, each prediction is conditioned on previous pixels. During training, we "clamp" all the previous pixels to the ground truth values (an approach known as "teacher forcing" <ref type="bibr" target="#b38">[39]</ref>), and just train the network to predict a single pixel at a time. This can be done efficiently in parallel across pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FEEDFORWARD CNN FOR HIGH-RESOLUTION REFINEMENT</head><p>A simple way to use the low resolution output of the colorization network is to upsample it (e.g., using bilinear or nearest neighbor interpolation), and then to concatenate the result with the original luminance channel. This can work quite well given groundtruth color, as we showed in <ref type="figure">Figure 3</ref>. However, it is possible to do better by learning how to combine the predicted low resolution color image with the original high resolution grayscale image.</p><p>For this, we use an image-to-image CNN which we call the refinement network. It is similar in architecture to the network used in <ref type="bibr" target="#b14">[15]</ref> but with more layers in the decoding part. In addition, we use bilinear interpolation for upsampling instead of learned upsampling.</p><p>The refinement network is trained on a 28x28 downsampling of the ground truth chroma images. The reason we do not train it end-to-end with the PixelCNN is the following: the PixelCNN can generate multiple samples, all of which might be quite far from the true chroma image; if we forced the refinement network to map these to the true RGB image, it might learn to ignore these "irrelevant" color "hints", and just use the input grayscale image. By contrast, when we train using the true low-resolution chroma images, we force the refinement network to focus its efforts on learning how to combine these "hints" with the edge boundaries which are encoded in the grayscale image.</p><p>We show some qualitative examples of the benefits of the refinement network on the left of <ref type="figure" target="#fig_2">Figure 5</ref>. At first glance, the benefits seem small, but if you zoom in you will notice that the refinement network's outputs are much more plausible, since they better adhere to segment boundaries, etc.</p><p>The results of a quantitative human evaluation of the refinement network, using the "Visual Turing Test" metric explained in Section 4, are shown in the table on the right of <ref type="figure" target="#fig_2">Figure 5</ref>. The increase from the Sample-Unrefined score (19.9%) to the Sample-Refined score (33.9%) shows the value added by the refinement network. The GT-Refined score (43.6%) shows the upper limit of our method could achieve with our refinement network (the maximum expected score for VTT is 50%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION METHODOLOGY</head><p>Since the mapping from gray to color is one-to-many, we cannot evaluate performance by comparing the predicted color image to the "ground truth" color image in terms of mean squared error or even other perceptual similarity metrics such as SSIM <ref type="bibr" target="#b36">[37]</ref>. Instead, we follow the approach of <ref type="bibr" target="#b40">[41]</ref> and conduct a "Visual Turing Test" (VTT) using a crowd sourced human raters. In this test, we present two different color versions of an image, one the ground truth and one corresponding to the predicted colors generated by some method. We then ask the rater to pick the image which has the "true colors". A method that always produces the ground truth colorization would score 50% by this metric.</p><p>To be comparable with <ref type="bibr" target="#b40">[41]</ref>, we show the two images sequentially for 1 second each. (We randomize which image is shown first.) Following standard practice, we train on the 1.2M training images from the ILSVRC-CLS dataset <ref type="bibr" target="#b29">[30]</ref>, and use 500 images from the "ctest10k" split of the 50k ILSVRC-CLS validation dataset proposed in <ref type="bibr" target="#b19">[20]</ref>. Each image is shown to 5 different raters. We then compute the fraction of times the generated image is preferred to ground truth; we will call this the "VTT score" for short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We assess the effectiveness of our technique by comparing against several recent colorization methods, both qualitatively and quantitatively. <ref type="table" target="#tab_4">Table 3</ref> shows a qualitative comparison of various recent methods applied to a few randomly chosen test images. Based on these examples, it seems that the best methods include our method (PixColor), and several recent CNN-based methods, namely LTBC <ref type="bibr" target="#b14">[15]</ref>, LRAC <ref type="bibr" target="#b19">[20]</ref>, and CIC <ref type="bibr" target="#b40">[41]</ref>. Therefore, we conduct a more costly "Visual Turing Test" (VTT) on these four systems, as explained in Section 4. <ref type="figure">Figure 6</ref> summarizes the VTT scores. We see that our method significantly outperforms the previous state of the art methods, with an average VTT score of 33.9%.</p><p>One reason we think our results are better is that the colors they produce are more "natural", and are placed in the "right" places. To assess the first issue, <ref type="figure">Figure 7</ref> plots the marginal statistics of the a and b channels (of CIELab) derived from the images generated from each image. We see that our model matches the empirical distribution (derived from the true color images) more closely than the  other methods, without needing to do any explicit reweighting of color bins, as was done in previous work <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SAMPLE DIVERSITY</head><p>Our model can produce multiple samples for each input, so for we run it 3 times, with 3 different seeds, and evaluate the outputs of each run independently. From <ref type="figure">Figure 6</ref>, we see that all of the samples are fairly good, but are they different from each other? That is, are the samples diverse? <ref type="figure">Figure 2</ref> suggests that our method can generate diverse samples. To quantitatively assess how different these samples are from each other, we compute the multiscale SSIM <ref type="bibr" target="#b36">[37]</ref> measure between pairs of samples. The results are shown in <ref type="figure" target="#fig_4">Figure 8</ref>. We see that most pairs have an SSIM score in the 0.95-0.99 range, meaning that they are very similar, but differ in a few places, corresponding to subtle details, such as the color of a person's shirt. The pairs which have the lowest SSIM score are the ones where large objects are given different colors (see the pair of birds on the left hand side).</p><p>In an ideal world, we could automatically select the single best sample, and just show that to the user. To get a sense of how well this could perform, we decided to use humans to perform the task of picking the best sample. More precisely, for each of the 3 samples for a given image, we picked the one that the most raters liked. We then computed the VTT score for these single samples using a different set of raters. The VTT score jumps to 38.3%. This suggests that an algorithmic way to pick a good sample from the set could yield significantly better results.</p><p>We did some preliminary experiments where we used the likelihood score (according to the Pixel-CNN model) to pick the best sample, but this did not yield good correlation with human judgement. It may be possible to train a separate ranking model, but we leave that to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We showed PixColor produces diverse colorizations and found that on average the outputs of our model perform better than other published methods in a crowd sourced human evaluation. We avoid the problem of slow inference in PixelCNN by only sampling low-resolution color channels and use a standard image-to-image CNN to refine the result. We justified the necessity of the refinement network with ablation studies and we showed that PixColor outputs more closely match the marginal SSIM = 0.80 SSIM = 0.85 SSIM = 0.90 SSIM = 0.95 SSIM = 0.99 0.80 0.85 0.90 0.95 0.99 color distributions when compared to other methods. The model exhibits a variety of failure modes, as illustrated in <ref type="figure" target="#fig_0">Figure 10</ref>, which we will address in our future work.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Grayscale on the left with three colorizations from our model and the original.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>All you need is a few bits of color. The top row is the original color image. The middle row is the true chroma image downsampled to have smallest size 28 pixels. The bottom row is the result of bilinear upsampling the middle row, and combining with the original grayscale image. Diagram of Pixel Recursive Colorization (PixColor) method. We first pre-train the conditioning network on COCO image segmentation following</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Left: The intermediate stages of PixColor. The column labeled "sample" is an output from Pix-elCNN, upsampled to the size of the image for visualization purposes. The column labeled "refined" is the output of the refinement network, before being combined with the grayscale input. Right: Results of a human evaluation using the "Visual Turing Test" metric explained in Section 4. We compare four systems: ground truth (GT) chroma image vs generated sample, passed directly into bilinear upsampling (unrefined) or passed into the refinement network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Results of the Visual Turing Test (VTT) study on the ImageNet test set. We report the fraction of times raters picked the generated color image over the ground truth with error ranges produced by bootstrapping the mean. Our study includes 500 test images and 5 raters per image. The column labeled "Oracle" is the score of the single best sample per image chosen by human raters. Marginal statistics of the color channels in Lab color space. Left: each method's histogram is shown in blue against ImageNet's test set histogram in black. Right: Histogram intersection on the color channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>To demonstrate that our model produces diverse samples, we compare two outputs from the same input with multiscale SSIM. A histogram of the SSIM distances from the ImageNet test set is shown above. Representative pairs are shown at at various SSIM distances. An SSIM value of 1.0 means the images are identical.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Selected high resolution and non-square samples. Images illustrating each possible VTT score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Summary of related methods. Columns comprise name of method; reference; model type (MOE</cell></row><row><cell>= mixture of experts, VAE = variational autoencoder, MDN = mixture density network, GAN = generative</cell></row><row><cell>adversarial network); color space; loss (CE = cross entropy, Mahal = Mahalanobis distance, Adv = adversarial);</cell></row></table><note>multiple diverse outputs or not; and the dataset used to train the model. The CRF method of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Qualitative side by side comparison of colorizations produced by various methods (LTBC:<ref type="bibr" target="#b14">[15]</ref>, pix2pix:<ref type="bibr" target="#b16">[17]</ref>, VAE:<ref type="bibr" target="#b8">[9]</ref>, LRAC:<ref type="bibr" target="#b19">[20]</ref>, CIC:<ref type="bibr" target="#b40">[41]</ref>, PixColor: this paper, G. Truth: original color). These images are randomly sampled from the ImageNet test set.</figDesc><table><row><cell>LTBC</cell><cell>pix2pix</cell><cell>cVAE</cell><cell>LRAC</cell><cell>CIC</cell><cell>PixColor</cell><cell>G. Truth</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Stephen Mussmann and Laurent Dinh for work and discussion on earlier versions of this project; Julia Winn, Jingyu Cui and Dhyanesh Narayanan for help with an earlier prototype; A?ron van den Oord for advice and guidance employing PixelCNN architectures; the TensorFlow team for technical and infrastructure assistance.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple hypothesis colorization and its application to image compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised diverse colorization via generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06674</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automatic image colorization via multimodal predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deeplab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<title level="m">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zezhou</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.00075</idno>
		<title level="m">Qingxiong Yang, and Bin Sheng. Deep colorization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic colorization with internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex Yong-Sang</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Kumar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu-Yeung</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Dahl</surname></persName>
		</author>
		<ptr target="http://tinyclouds.org/colorize/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00783</idno>
		<title level="m">Pixel recursive super resolution</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning diverse image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao-Chuang</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01958</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning large-scale automatic image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Outline colorization through tandem adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Frans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-04-28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial nets. NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ee Sin Ng, and Huang Zhiyong. Image colorization using similar images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><surname>Kumar Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex Yong-Sang</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Rajan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An adaptive edge detection based colorization algorithm and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Shin</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ja-Ling</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Let there be Color!: Joint Endto-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Colorization by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Revital</forename><surname>Irony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurographics Symposium on Rendering</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>arxiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Auto-Encoding variational bayes. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08185</idno>
		<title level="m">Latent variable PixelCNNs for natural image modeling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Qing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<title level="m">Natural image colorization. Eurographics Symposium on Rendering</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<title level="m">Unrolled generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic colorization of grayscale images using multiple images on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Taguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Naemura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated colour grading using colour distribution transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Piti?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rozenn</forename><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manga colorization. ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fast generation for convolutional autoregressive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooya</forename><surname>Khorrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR workshop</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ashikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Probabilistic image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amelie</forename><surname>Royer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-05-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised colorization of black-and-white cartoons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sykora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burianek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International symposium on Non-photorealistic animation and rendering</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Local color transfer via probabilistic segmentation by expectation-maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A novel visualization tool for art history and conservation: Automated colorization of black and white archival photographs of works of art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsaftaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casadio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Andral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Conservation</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05328</idno>
		<title level="m">Conditional Image Generation with PixelCNN Decoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A C</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Asilomar Conference on Signals</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Systems, Computers</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transferring color to greyscale images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomihisa</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ashikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast image and video colorization using chrominance blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yatziv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Colorful image colorization. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Real-time user-guided image colorization with learned deep priors. SIG-GRAPH</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhe</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LTBC pix2pix cVAE LRAC CIC PixColor G. Truth LTBC pix2pix cVAE LRAC CIC PixColor G. Truth</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
