<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PlotQA: Reasoning over Scientific Plots</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><surname>Methani</surname></persName>
							<email>nmethani@cse.iitm.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Robert Bosch Centre for Data Science and AI (RBC-DSAI</orgName>
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritha</forename><surname>Ganguly</surname></persName>
							<email>prithag@cse.iitm.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Robert Bosch Centre for Data Science and AI (RBC-DSAI</orgName>
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
							<email>miteshk@cse.iitm.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Robert Bosch Centre for Data Science and AI (RBC-DSAI</orgName>
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyush</forename><surname>Kumar</surname></persName>
							<email>pratyush@cse.iitm.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Robert Bosch Centre for Data Science and AI (RBC-DSAI</orgName>
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<settlement>Chennai</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PlotQA: Reasoning over Scientific Plots</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* The first two authors have contributed equally</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing synthetic datasets (FigureQA, DVQA) for reasoning over plots do not contain variability in data labels, real-valued data, or complex reasoning questions. Consequently, proposed models for these datasets do not fully address the challenge of reasoning over plots. In particular, they assume that the answer comes either from a small fixed size vocabulary or from a bounding box within the image. However, in practice, this is an unrealistic assumption because many questions require reasoning and thus have real-valued answers which appear neither in a small fixed size vocabulary nor in the image. In this work, we aim to bridge this gap between existing datasets and real-world plots. Specifically, we propose PlotQA with 28.9 million question-answer pairs over 224,377 plots on data from realworld sources and questions based on crowd-sourced question templates. Further, 80.76% of the out-of-vocabulary (OOV) questions in PlotQA have answers that are not in a fixed vocabulary. Analysis of existing models on PlotQA reveals that they cannot deal with OOV questions: their overall accuracy on our dataset is in single digits. This is not surprising given that these models were not designed for such questions. As a step towards a more holistic model which can address fixed vocabulary as well as OOV questions, we propose a hybrid approach: Specific questions are answered by choosing the answer from a fixed vocabulary or by extracting it from a predicted bounding box in the plot, while other questions are answered with a table questionanswering engine which is fed with a structured table generated by detecting visual elements from the image. On the existing DVQA dataset, our model has an accuracy of 58%, significantly improving on the highest reported accuracy of 46%. On PlotQA, our model has an accuracy of 22.52%, which is significantly better than state of the art models. * The first two authors have contributed equally</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data plots such as bar charts, line graphs, scatter plots, etc. provide an efficient way of summarizing numerical information. Recently, in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">12]</ref> two datasets containing plots and deep neural models for question answering over the generated plots have been proposed. In both the datasets, the plots are synthetically generated with data values and labels drawn from a custom set. In the FigureQA dataset <ref type="bibr" target="#b12">[13]</ref>, all questions are binary wherein answers are either Yes or No, (see <ref type="figure" target="#fig_1">Figure 1a</ref> for an example). The DVQA dataset <ref type="bibr">[12]</ref>, generalizes this to include questions which can be answered either by (a) fixed vocabulary of 1000 words, or (b) extracting text (such as tick labels) from the plot. An example question could seek the numeric value represented by a bar of a specific label in a bar plot (see <ref type="figure" target="#fig_1">Figure 1b</ref>). Given that all data values in the DVQA dataset are chosen to be integers and from a fixed range, the answer to this question can be extracted from the appropriate tick label.</p><p>While these datasets have initiated the research questions on plot reasoning, realistic questions over plots are much more complex. For instance, consider the question in <ref type="figure" target="#fig_1">Figure 1c</ref>, where we are to compute the average of floating point numbers represented by three bars of a color specified by the label. The answer to this question is neither in a fixed vocabulary nor can it be extracted from the plot itself. Answering such questions requires a combination of perception, language understanding, and reasoning, and thus poses a significant challenge to existing systems. Furthermore, this task is harder if the training set is not synthetic, but instead is sourced from real-world data with large variability in floating-point values, large diversity in axis and tick labels, and natural complexity in question templates.</p><p>To address this gap between existing datasets and realworld plots, we introduce the PlotQA 1 dataset with <ref type="bibr">28.9</ref> million question-answer pairs grounded over 224,377 plots. PlotQA improves on existing datasets on three fronts. First, <ref type="bibr">1</ref>   Fixed vocabulary How are the legend labels stacked?</p><p>What is the label or title of the X-axis ? In how many years, is the price of diesel greater than 0.6 units?</p><p>Open vocabulary -What is the price of diesel in Lebanon in the year 2008? What is the ratio of the price of diesel in Lebanon in 2010 to that in 2014?  roughly 80.76% of the questions have answers which are not present in the plot or in a fixed vocabulary. Second, the plots are generated from data sourced from World Bank, government sites, etc., thereby having a large vocabulary of axis and tick labels, and a wide range in data values. Third, the questions are complex as they are generated based on 74 templates extracted from 7,000 crowd-sourced questions asked by workers on a sampled set of 1,400 plots. Questions are categorized into 9 (=3x3) cells based on the question type: 'Structural Understanding', 'Data Retrieval', or 'Reasoning <ref type="bibr">'</ref> and and the answer type: 'Yes/No', 'From Fixed Vocabulary', or 'Out Of Vocabulary (OOV)' (see <ref type="table">Table 1</ref>).</p><p>We first evaluate three state of the art models on PlotQA, viz., SAN-VQA <ref type="bibr" target="#b35">[36]</ref>, Bilinear attention network (BAN) <ref type="bibr">[16]</ref> and LoRRA <ref type="bibr" target="#b32">[33]</ref>. Note that, by design none of these models are capable of answering OOV questions. In particular, SAN-VQA and BAN treat plot reasoning as a classification task and expect the answer to lie in a small vocabulary whereas in our dataset the answer vocabulary is prohibitively large (?5M words). Similarly, LoRRA assumes that the answer is present in the image itself as a text and the task is to just extract this region containing the text followed by OCR (optical character recognition). Again, such a model will be unable to answer questions such as the one shown in <ref type="figure" target="#fig_1">Figure 1c</ref>, which form a significant segment of real-world use-cases and our dataset. As a result, these these models give an accuracy of less than 8% on our dataset. On the other hand, existing models (in particular, SAN) perform well on questions with answers from a fixed vocabulary, which was the intended purpose of these models.</p><p>Based on the above observations, we propose a hybrid model with a binary classifier which given a question decides if the answer would lie in a small top-k vocabulary or if the answer is OOV. For the former, the question is passed through a classification pipeline which predicts a distribution over the top-k vocabulary and selects the most probable answer. For the latter (arguably harder questions), we pass the question through a pipeline of four modules: Visual element detection, Optical character recognition, Extraction into a structured table, and Structured table question answering. This proposed hybrid model significantly outperforms the existing models and has an aggregate accuracy of 22.52% on the PlotQA dataset. We also evaluate our model on the DVQA dataset where it gives an accuracy of 58%, improving on the best-reported result of SANDY <ref type="bibr">[12]</ref> of 46%. In summary, we make two major contributions:  (1) We propose PlotQA dataset with plots on data sourced from the real-world and questions based on templates sourced from manually curated questions. The dataset exposes the need to train models for questions that have answers from an Open Vocabulary.</p><p>(2) We propose a hybrid model with perception and QA modules for questions that have answers from an Open Vocabulary. This model gives the best performance not only on our dataset but also on the existing DVQA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Datasets: Over the past few years several large scale datasets for Visual Question Answering have been released. These include datasets such as COCO-QA <ref type="bibr" target="#b27">[28]</ref>, DAQUAR <ref type="bibr" target="#b22">[23]</ref>, VQA <ref type="bibr">[1,</ref><ref type="bibr">7]</ref> which contain questions asked over natural images. On the other hand, datasets such as CLEVR <ref type="bibr" target="#b10">[11]</ref> and NVLR <ref type="bibr" target="#b34">[35]</ref> contain complex reasoning based questions on synthetic images having 2D and 3D geometric objects.</p><p>There are some datasets <ref type="bibr">[14,</ref><ref type="bibr" target="#b14">15]</ref> which contain questions asked over diagrams found in text books but these datasets are smaller and contain multiple-choice questions.  is another dataset which contains images extracted from research papers but this is also a relatively small (60,000 images) dataset. Further, FigureSeer focuses on answering questions based on line plots as opposed to other types of plots such as bar charts, scatter plots, etc. as seen in FigureQA <ref type="bibr" target="#b12">[13]</ref> and DVQA <ref type="bibr">[12]</ref>. There is also the recent TextVQA <ref type="bibr" target="#b32">[33]</ref> dataset which contains questions which require models to read the text present in natural images. This dataset does not contain questions requiring numeric reasoning. Further, the answer is contained as a text in the image itself. Thus, no existing dataset contains plot images with complex questions which require reasoning and have answers from an Open Vocabulary. Models: The availability of the above mentioned datasets has facilitated the development of complex end-to-end neural network based models ( <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr">[12]</ref>, <ref type="bibr" target="#b32">[33]</ref>). These end-to-end networks contain (a) encoders to compute a representation for the image and the question, (b) attention mechanisms to focus on important parts of the question and image, (c) interaction components to capture the interactions between the question and the image, (d) OCR module to extract the image specific text and (e) a classification layer for selecting the answer either from a fixed vocabulary or from a OCR appended vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The PlotQA dataset</head><p>In this section, we describe the PlotQA dataset and the process to build it. Specifically, we discuss the four main stages, viz., (i) curating data such as year-wise rainfall statistics, country-wise mortality rates, etc., (ii) creating different types of plots with a variation in the number of elements, legend positions, fonts, etc., (iii) crowd-sourcing to generate questions, and (iv) extracting templates from the crowd-sourced questions and instantiating these templates using appropriate phrasing suggested by human annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Collection and Curation</head><p>We considered online data sources such as World Bank Open Data, Open Government Data, Global Terrorism Database, etc. which contain statistics about various indicator variables such as fertility rate, rainfall, coal production, etc. across years, countries, districts, etc. We crawled data from these sources to extract different variables whose relations could then be plotted (for example, rainfall v/s years across countries, or movie v/s budget, or carbohydrates v/s food item). There are a total of 841 unique indicator variables (CO2 emission, Air Quality Index, Fertility Rate, Revenue generated, etc.) with 160 unique entities (cities, states, districts, countries, movies, food, etc.). The data ranges from 1960 to 2016, though not all indicator variables have data items for all years. The data contains positive integers, floating point values, percentages, and values on a linear scale. These values range from 0 to 3.50e+15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Plot Generation</head><p>We included 3 different types of plots in this dataset, viz., bar plots, line plots, and scatter plots. Within bar plots, we have grouped them by orientation as either horizontal or vertical. <ref type="figure" target="#fig_3">Figure 2</ref> shows one sample of each plot type. Each of these plot types can compactly represent 3-dimensional data. For instance, in <ref type="figure" target="#fig_3">Figure 2b</ref>, the plot compares the diesel prices across years for different countries. To enable the development of supervised modules for various sub-tasks, we provide bounding box annotations for legend boxes, legend names, legend markers, axes titles, axes ticks, bars, lines, and title. By using different combinations of indicator variables and entities (years, countries, etc.) we created a total of 224, 377 plots.</p><p>To ensure variety in the plots, we randomly chose the following parameters: grid lines (present/absent), font size, notation used for tick labels (scientific-E notation or standard notation), line style (solid, dashed, dotted, dash-dot), marker styles for marking data points </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Sample Question Collection by Crowd-sourcing</head><p>As the source data of PlotQA dataset is significantly richer in comparison to FigureQA and DVQA, we found it necessary to ask a larger set of annotators to create questions over these plots. However, creating questions for all the plots in our dataset would have been prohibitively expensive. We sampled 1, 400 plots across different types and asked workers on Amazon Mechanical Turk to create questions for these plots. We showed each plot to 5 different workers resulting in a total of 7, 000 questions. We specifically instructed the workers to ask complex reasoning questions which involved reference to multiple plot elements in the plots. We paid the workers USD 0.1 for each question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Question Template Extraction &amp; Instantiation</head><p>We manually analyzed the questions collected by crowdsourcing and divided them into a total of 74 templates. These templates were divided into 3 question categories. These question categories along with a few sample templates are shown below. See <ref type="table" target="#tab_5">Table 4</ref> for statistics of dif-ferent question and answer types in our dataset (please refer to the Supplementary material for further details). Structural Understanding: These are questions about the overall structure of the plot and do not require any quantitative reasoning. Examples: "How many different coloured bars are there?", "How are the legend labels stacked?". Data Retrieval: These questions seek data item for a single element in the plot. Examples: "What is the number of tax payers in Myanmar in 2015?". Reasoning: These questions either require numeric reasoning over multiple plot elements or a comparative analysis of different elements of the plot, or a combination of both to answer the question. Examples: "In which country is the number of threatened bird species minimum?", "What is the median banana production?",</p><p>We abstracted the questions into templates such as "In how many &lt;plural form of X label&gt;, is the &lt;Y label&gt; of/in &lt;legend label&gt; greater than the average &lt;Y label&gt; of/in &lt;legend label&gt; taken over all &lt;plural form of X label&gt;?". We could then generate multiple questions for each template by replacing X label, Y label, legend label, etc. by indicator variables, years, cities etc. from our curated data. However, this was a tedious task requiring a lot of manual intervention. For example, consider the indicator variable "Race of students" in <ref type="figure" target="#fig_1">Figure 1c</ref>. If we substitute this indicator variable as it is in the above template, it would result in a question, "In how many cities, is the race of the students(%) of Asian greater than the average race of the students (%) of Asian taken over all cities?", which sounds unnatural. To avoid this, we asked in-house annotators to carefully paraphrase these indicator variables and question templates. The paraphrased version of the above example was "In how many cities, is the percentage of Asian students greater than the average percentage of Asian students taken over all cities?". Such paraphrasing for every question template and indicator variable required significant manual effort. Using this semi-automated process we generated  <ref type="table" target="#tab_2">Train  52,463 52,700 25,897  26,010  871,782  2,784,041 16,593,656  784,115  3,095,774  16,369,590  Validation  11,249 11,292  5,547  5,571  186,994  599,573  3,574,081  167,871  600,424  3,592,353  Test  11,242 11,292  5,549  5,574  186,763  596,359  3,559,392  167,727  667,742</ref> 3,507,045  a total of 28, 952, 641 questions. This approach of creating questions on real-world plot data with carefully curated question templates followed by manual paraphrasing is a key contribution of our work. The resultant PlotQA dataset is much closer to the real-world challenge of reasoning over plots, significantly improving on existing datasets. <ref type="table" target="#tab_2">Table 2</ref> summarizes the differences between PlotQA and these existing datasets such as FigureQA and DVQA. Note that (a) the number of unique answers in PlotQA is very large, (b) the questions in PlotQA are much longer, and (c) the vocabulary of PlotQA is more realistic than FigureQA or DVQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Model</head><p>Existing models for VQA are of two types: (i) read the answer from the image (as in LoRRA) or (ii) pick the answer from a fixed vocabulary (as in SAN and BAN). Such models work well for datasets such as DVQA where indeed all answers come from a fixed vocabulary (global or plot specific) but are not suited for PlotQA with a large number of OOV questions. Answering such questions involves various sub-tasks: (i) detect all the elements in the plot (bars, legend names, tick labels, etc), (ii) read the values of these elements, (iii) establish relations between the plot elements, e.g., creating tuples of the form {country=Angola, year=2006, price of diesel = 0.4 }, and (iv) reason over this structured data. Expecting a single end-to-end model to be able to do all of this is unreasonable. Hence, we propose a multi-staged pipeline to address each of the sub-tasks.</p><p>We further note that for simpler questions which do not require reasoning and can be answered from a small fixed size vocabulary, such an elaborate pipeline is an overkill. As an illustration consider the question "How many bars are there in the image?". This does not require reasoning and can be answered based on visual properties of the image. For such questions, we have a simpler QA-as-classification pipeline. As shown in <ref type="figure" target="#fig_5">Figure 3</ref>, our overall model is thus  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Visual Elements Detection (VED)</head><p>The data bearing elements of a plot are of 10 distinct classes: the title, the labels of the x and y axes, the tick labels or categories (e.g., countries) on the x and y axis, the data markers in the legend box, the legend names, and finally the bars and lines in the graph. Following existing literature ( <ref type="bibr">[4]</ref>, <ref type="bibr">[12]</ref>), we refer to these elements as the visual elements of the graph. The first task is to extract all these visual elements by drawing bounding boxes around them and classifying them into the appropriate class. To this end, we can either apply object detection models such as RCNN, Fast-RCNN <ref type="bibr">[6]</ref>, YOLO <ref type="bibr" target="#b26">[27]</ref>, SSD <ref type="bibr" target="#b20">[21]</ref>, etc. or instance segmentation models such as Mask-RCNN <ref type="bibr">[9]</ref>. Upon comparing all methods, we found that Faster R-CNN <ref type="bibr" target="#b28">[29]</ref> model along with Feature Pyramid Network(FPN) <ref type="bibr" target="#b19">[20]</ref> performed the best and hence we used it as our VED module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Character Recognition (OCR)</head><p>Some of the visual elements such as title, legends, tick labels, etc. contain numeric and textual data. For extracting this data from within these bounding boxes, we use a stateof-the-art OCR model <ref type="bibr" target="#b33">[34]</ref>. We crop the detected visual element to its bounding box, convert the cropped textual image into gray-scale, resize and deskew it, and then pass it to an OCR module. Existing OCR modules perform well for machine-written English text, and indeed we found that a pre-trained OCR module 2 works well on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Elements Detection (VED)</head><p>Optical Character Recognition (OCR) Semi-structured information extraction (SIE)  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semi-Structured Information Extraction (SIE)</head><p>The next stage of extracting the data into a semistructured table is best explained with an example shown in <ref type="figure" target="#fig_5">Figure 3</ref>. The desired output of SIE is shown in the table where the rows correspond to the ticks on the x-axis <ref type="bibr">(1996,</ref><ref type="bibr">1997,</ref><ref type="bibr">1998,</ref><ref type="bibr">1999)</ref>, the columns correspond to the different elements listed in the legend (Brazil, Iceland, Kazakhstan, Thailand) and the i,j-th cell contains the value corresponding to the x-th tick and the y-th legend. The values of the xtick labels and the legend names are available from the OCR module. The mapping of legend name to legend marker or color is done by associating a legend name to the marker or color whose bounding box is closest to the bounding box of the legend name. Similarly, we associate each tick label to the tick marker whose bounding box is closest to the bounding box of the tick label. For example, we associate the legend name Brazil to the color "Dark Cyan" and the tick label 1996 to the corresponding tick mark on the xaxis. With this we have the 4 row headers and 4 column headers, respectively. To fill in the 16 values in the table, there are again two smaller steps. First we associate each of the 16 bounding boxes of the 16 bars to their corresponding x-ticks and legend names. A bar is associated with an x-tick label whose bounding box is closest to the bounding box of the bar. To associate a bar to a legend name, we find the dominant color in the bounding box of the bar and match it with a legend name corresponding to that color. Second, we need to find the value represented by each bar. We extract the height of the bar using bounding box information from the VED module and then search for the y-tick labels immediately above and below that height. We then interpolate the value of the bar based on the values of these bounding ticks. With this we have the 16 values in the cells and thus have extracted all the information from the plot into a semi-structured table. The output of each stage is discussed in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Table Question Answering (QA)</head><p>The final stage of the pipeline is to answer questions on the semi-structured table. As this is similar to answering questions from the WikiTableQuestions dataset <ref type="bibr" target="#b25">[26]</ref>, we adopt the same methodology as proposed in <ref type="bibr" target="#b25">[26]</ref>. In this method, the table is converted to a knowledge graph and the question is converted to a set of candidate logical forms by applying compositional semantic parsing. These logical forms are then ranked using a log-linear model and the highest ranking logical form is applied to the knowledge graph to get the answer. Note that with this approach the output is computed by a logical form that operates on the numerical data. This supports complex reasoning questions and also avoids the limitation of using a small answer vocabulary for multi-class classification as is done in existing work on VQA. There are recent neural approaches for answering questions over semi-structured tables such as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr">8]</ref>. Individually these models do not outperform the relatively simpler model of <ref type="bibr" target="#b25">[26]</ref>, but as an ensemble they show a small improvement of only (1-2%). To the best of our knowledge, there is only one neural method <ref type="bibr" target="#b18">[19]</ref> which outperforms <ref type="bibr" target="#b25">[26]</ref>, but the code for this model is not available which makes it hard to reproduce the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Train-Valid-Test Splits</head><p>By using different combinations of 841 indicator variables and 160 entities (years, countries, etc), we created a total of 224, 377 plots. Depending on the context and type of the plot, we instantiated the 74 templates to create mean-  ingful {question, answer} pairs for each of the plots. We created train (70%), valid (15%) and test (15%) splits ( <ref type="table" target="#tab_7">Table 5</ref>). The dataset and the crowd-sourced questions can be downloaded from the link: bit.ly/PlotQA</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Models Compared</head><p>We compare the performance of the following models: -IMG-only: This is a simple baseline where we just pass the image through a VGG19 and use the embedding of the image to predict the answer from a fixed vocabulary.</p><p>-QUES-only: This is a simple baseline where we just pass the question through a LSTM and use the embedding of the question to predict the answer from a fixed vocabulary.</p><p>-SAN [36]: This is an encoder-decoder model with a multilayer stacked attention <ref type="bibr">[2]</ref> mechanism. It obtains a representation for the image using a deep CNN and a representation for the query using LSTM. It then uses the query representation to locate relevant regions in the image and uses this to pick an answer from a fixed vocabulary.</p><p>-SANDY <ref type="bibr">[12]</ref>: This is the best performing model on the DVQA dataset and is a variant of SAN. Unfortunately, the code for this model is not available and the description in the paper was not detailed enough for us to reimplement it. <ref type="bibr">3</ref> Hence, we report the numbers for this model only on DVQA (from the original paper).</p><p>-LoRRA <ref type="bibr" target="#b32">[33]</ref>: This is the recently proposed model on the TextVQA dataset. It concatenates the image features extracted from pre-trained ResNet-152 <ref type="bibr" target="#b9">[10]</ref> model with the re- <ref type="bibr">3</ref> We have contacted the authors and while they are helpful in sharing various details, they do not have access to the original code now. gion based features extracted from Faster-RCNN [5] model. It then reads the text present in the image using a pre-trained OCR module and incorporates an attention mechanism to reason about the image and the text. Finally, it does multiclass classification where the answer either comes from a fixed vocabulary or is copied from the text in the image.</p><p>-BAN [16]: This model exploits bilinear interactions between two groups of input channels, i.e., between every question word (GRU <ref type="bibr">[3]</ref> features) and every image region (pre-trained Faster-RCNN <ref type="bibr" target="#b28">[29]</ref> object features). It then uses low-rank bilinear pooling <ref type="bibr" target="#b16">[17]</ref> to extract the joint distribution for each pair of channels. BAN accumulates 8 such bilinear attention maps which are then fed to a two-layer perceptron classifier to get the final joint distribution over answers from a fixed vocabulary. -Our Model: This proposed model shown in <ref type="figure" target="#fig_6">Figure 4</ref> with two model paths. The training data for the binary classification is generated by comparing the performance of the individual models: For a given question, the label is set to 1 if the performance of QA-as-classification model is better than the multi-stage pipeline, and 0 otherwise. We use an LSTM to represent the input question and then perform binary classification on this representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Training Details SAN:</head><p>We used an existing implementation of SAN 4 for the initial baseline results. Image features are extracted from the last pooling layer of VGG19 network. Question features are the last hidden state of the LSTM. Both the LSTM hidden state and 512-d image feature vector at each location are transferred to a 1024-d vector by a fully connected layer, and added and passed through a non-linearity (tanh). The model was trained using Adam <ref type="bibr" target="#b17">[18]</ref> with an initial learning rate of 0.0003 and a batch size of 128 for 25,000 iterations. Our model: The binary question classifier in the proposed model contains a 50-dimensional word embedding layer followed by an LSTM with 128 hidden units. The output of the LSTM is projected to 256 dimensions and this is then fed to the output layer. The model is trained for 10 epochs using RMSProp with an initial learning rate of 0.001. Accuracy on the validation set is 87.3%. Of the 4 stages of the multi-stage pipeline, only two require training, viz., Visual Elements Detection (VED) and <ref type="table" target="#tab_8">Table Question</ref> Answering (QA). As mentioned earlier, for VED we train a variant of Faster R-CNN <ref type="bibr" target="#b19">[20]</ref> with FPN using the bounding box annotations available in PlotQA. We trained the model with a batch size of 32 for 200, 000 steps. We used RMSProp with an initial learning rate of 0.004. For <ref type="table">Table QA</ref>, we trained the model proposed in <ref type="bibr" target="#b25">[26]</ref> using questions from our dataset and the corresponding ground truth tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Evaluation Metric</head><p>We used accuracy as the evaluation metric. Specifically, for textual answers (such as India, CO2, etc.) the model's output was considered to be correct only if the predicted answer exactly matches the true answer. However, for numeric answers with floating point values, an exact match is a very strict metric We relax the measure to consider an answer to be correct as if it is within 5% of the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Human Accuracy on PlotQA dataset</head><p>To assess the difficulty of the PlotQA dataset, we report human accuracy on a small subset of the Test split of the dataset. With the help of in-house annotators, we were able to evaluate 5, 860 questions grounded in 160 images. Human accuracy on this subset is found to be 80.47%. We used the evaluation metric as defined in section 5.4. Most human errors were due to numerical precision as it is difficult to find the exact value from the plot even with a 5% margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Observations and Results</head><p>1. Evaluating models on PlotQA dataset ( <ref type="table">Table 7)</ref>:</p><p>The baselines IMG-only and QUES-only performed poorly with an accuracy of 4.84% and 5.35% respectively. Existing models (SAN, BAN, LoRRA) perform poorly on this dataset. In particular, BAN and LoRRA have an abysmal accuracy of less than 1%. This is not surprising given that both models are not designed to answer OOV questions. Further, the original VQA tasks for which BAN was proposed does not have any complex numerical reasoning questions as found in PlotQA. Similarly, LoRRA was designed only for text based answers and not for questions requiring numeric reasoning. Note that we have used the original code <ref type="bibr" target="#b31">[32]</ref> released by the authors of these models. Given the specific focus and limited capabilities of these existing models it may even seem unfair to evaluate these models on our dataset but we still do so for the sake of completeness and to highlight the need for better models. Lastly, our model gives the best performance of 22.52% on the PlotQA dataset. Ablation Study of proposed method: <ref type="table" target="#tab_13">Table 8</ref> presents the details of the ablation study of the proposed method for each question type (structural, data retrieval, reasoning) and each answer type (binary, fixed vocabulary, OOV). QA-asclassification performs very well on Yes/No questions and moderately well on Fixed vocab. questions with a good baseline aggregate accuracy of 7.76%. It performs poorly on Open vocab. question, failing to answer almost all the 3,507,045 questions in this category. On the other hand, the QA-as-multi-stage pipeline fails to answer correctly any of the Yes/No questions, performs moderately well on Fixed vocab. questions, and answers correctly some of the hard Open vocab. questions. Our model combines the complementary strengths of QA-as-classification and QA-as-multistage pipeline achieving the highest accuracy of 22.52%. In particular, the performance improves significantly for all Fixed Vocab. questions, while retaining the high accuracy of QA-as-classification on Yes/No questions and QA-asmulti-stage pipeline's performance on Open vocab. We acknowledge that the accuracy is significantly lower than human performance. This establishes that the dataset is challenging and raises open questions on models for visual reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Analysis of the pipeline</head><p>We analyze the performance of VED, OCR and SIE modules in the pipeline. VED: <ref type="table" target="#tab_15">Table 9</ref> shows that the VED module performs reasonably well at an Intersection Over Union (IOU) of 0.5. For higher IOUs of 0.75 and 0.9, the accuracy falls drastically. For instance, at IOU of 0.9, dotlines are detected with an accuracy of under 20%. Clearly, such inaccuracies would lead to incorrect table generation and subsequent QA. This brings out an interesting difference between this task and other instance segmentation tasks where the margin of error is higher (where IOU of 0.5 is accepted). A small error in visual element detection as indicated by mAP scores of 75% is considered negligible for VQA tasks, however for PlotQA small errors can cause significantly misaligned table generation and subsequent QA. We illustrate this with an example given in <ref type="figure" target="#fig_7">Figure 5</ref>. The predicted red box having an IOU of 0.58 estimates the bar size as 760 as opposed to ground truth of 680, significantly impacting downstream QA accuracy. OCR: We evaluate the OCR module in standalone/oracle mode and pipeline mode in <ref type="table" target="#tab_16">Table 10</ref>. In the oracle mode, we feed ground truth boxes to the OCR model whereas in    <ref type="table">Table 7</ref>: Accuracy (in %) of different models on PlotQA.</p><p>the pipeline model we perform OCR on the output of the VED module. We observe only a small drop in performance from 97.06% (oracle) to 93.10% (after VED), which indicates that the OCR module is robust to the reduction in VED module's accuracy at higher IOU as it does not depend on the class label or the exact position of bounding boxes. SIE: We now evaluate the performance of the SIE module. We consider each cell in the table to be a tuple of the form {row header, column header, value } (e.g., {Poland, 1964, 10000 tractors). We consider all the tuples extracted by the SIE module with the tuples present in the ground truth table to compute the F1-score. Even though <ref type="table" target="#tab_15">Table 9</ref> suggests that the VED model is very accurate with a mAP@0.5 of 96.43%, we observe that the F1-score for table extraction is only 0.68. This indicates that many values are not being extracted accurately due to the kind of errors shown in <ref type="figure" target="#fig_7">Figure 5</ref> where the bounding box has a high overlap with the true box. We thus need better plot VED modules which can predict tighter bounding boxes (higher mAP at IOU of 0.9) around the plot's visual and textual elements. Inaccurate VED module leads to erroneous tables which further affects the downstream QA accuracy.</p><p>-In summary, a highly accurate VED for structured images is an open challenge to improve reasoning over plots.</p><p>3. Evaluating new models on the existing DVQA dataset ( <ref type="table" target="#tab_11">Table 6</ref>): The proposed model performs better than the existing models (SAN and SANDY-OCR) establishing a new SOTA result on DVQA. The higher performance of the proposed hybrid model in comparison to SAN (in contrast to the PlotQA results) suggests that the extraction of the structured table is more accurate on the DVQA dataset. This is because of the limited variability in the axis and tick labels and shorter length (one word only) of labels.   <ref type="table">Table 1</ref>).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We introduce the PlotQA dataset to reduce the gap between existing synthetic plot datasets and real-world plots and question templates. Analysis of existing VQA models on PlotQA reveals that they perform poorly for Open Vocabulary questions. This is not surprising as these models were not designed to handle complex questions which require numeric reasoning and OOV answers. We propose a hybrid model with separate pipelines for handling (i) simpler questions which can be answered from a fixed vocabulary and (ii) complex questions with OOV answers. For OOV questions, we propose a pipelined approach that combines visual element detection and OCR with QA over tables. The proposed model gives state-of-the-art results on both the DVQA and PlotQA datasets. Further analysis of our pipeline reveals the need for more accurate visual element detection to improve reasoning over plots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pass Appendices</head><p>The supplementary material is organised in the following manner: Section 8 describes the methodology of knowledge graph creation from structured data. In Section 9, we further analyse our proposed pipeline and discuss the errors in each stage. In Section 10, we provide sample plots from the PlotQA dataset. In Section 11, we list all the 74 question templates that were formulated from the crowd sourced questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Construction of knowledge graph from structured data</head><p>Following <ref type="bibr" target="#b25">[26]</ref>, we convert the semi-structured table into a knowledge graph which has two types of nodes viz. row nodes and entity nodes. The rows of the table become row nodes, whereas the cells of each row become the entity nodes in the graph. Directed edges exist from the row nodes to the entity nodes of that column and the corresponding table column header act as edge-labels. An example of knowledge graph of the semi-structured table given in <ref type="figure" target="#fig_8">Figure 6a</ref> is shown in <ref type="figure" target="#fig_8">Figure 6b</ref>. For reasoning on the knowledge graph, we adopted the same methodology as given in <ref type="bibr" target="#b25">[26]</ref>. The questions are converted to a set of candidate logical forms by applying compositional semantic parsing. Each of these logical forms is then ranked using a log-linear model and the highest ranking logical form is applied to the knowledge graph to get the final answer.</p><p>(a) Semi-Structured <ref type="table" target="#tab_11">Table   1996</ref> 13.174405 7.895492 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Some failure cases</head><p>In this section, we visualize the output of each stage in our multistage pipeline and show some interesting failure cases with the help of an example.</p><p>-VED Stage: Although the bounding boxes predicted by Faster R-CNN coupled with Feature Pyramid Network (FPN) fit reasonably well at an IOU of 0.5, it is not acceptable as the values extracted from these bounding boxes will lead to incorrect table generation and subsequent QA. Example: In <ref type="figure" target="#fig_10">Figure 7b</ref>, consider the bar representing the "Indoor User Rating" value for "Vodafone". The overlap between the ground-truth box (blue) and the predicted box (red) is higher than 0.5 but the values extracted from the detected box is 4.0 as opposed to the actual value which is 3.73. Another interesting failure case is shown in <ref type="figure">Figure</ref> 9b. There are multiple overlapping data points and the model is able to detect only one of the points. This leads to incomplete table generation as shown in <ref type="figure" target="#fig_1">Figure 10b</ref> where the values for Liberia for the years 2008, 2009 and 2010 could not be extracted. This small error might be acceptable for other VQA tasks but for PlotQA these small errors will escalate to multiple incorrect answers.</p><p>-OCR stage: A slight misalignment in the bounding boxes predicted by VED module causes significant errors while extracting the text. Example: In <ref type="figure" target="#fig_10">Figure 7b</ref>, consider the box enclosing the legend label "Indoor". The rightmost edge of the predicted bounding box is drawn over the letter "r", which makes the OCR module incorrectly recognize the text as "Indoo". A similar error is made while performing OCR on the X-axis title which is read as "Dperator" instead of "Operator". Consider another example where there are misaligned bounding boxes on axes tick-labels as shown in <ref type="figure">Figure 9b</ref>. The values extracted are 200B, -2009 and -100 as opposed to the ground-truth values 2008, 2009 and 100. This slight error leads to incorrect column name in the subsequent generated tables <ref type="figure">(Figure 8b</ref> and <ref type="figure" target="#fig_1">Figure 10b</ref>) and incorrect answers to all the questions pertaining to these labels as shown in <ref type="table">Table 11</ref> and Tale 12.</p><p>-SIE stage: <ref type="figure">Figure 8a</ref> shows the oracle table which is generated by using the ground-truth annotations and <ref type="figure">Figure 8b</ref> shows the table generated after passing the plot image through the different stages of our proposed multistage pipeline. It is evident from the generated table that the errors propagated from the VED and the OCR stage has lead to an incorrect table generation.</p><p>-QA stage: In <ref type="table" target="#tab_2">Table 11 and Table 12</ref> we compare the answer predictions made by different models with the groundtruth answer on randomly sampled questions. Note that, our proposed model combines the complementary strengths of both, QA-as-classification and QA as multistage pipeline, models.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Samples from the PlotQA dataset</head><p>Few examples of the {plot, question, answer} triplets from the PlotQA dataset are shown in <ref type="figure" target="#fig_1">Figure 12</ref>. For each of the plots, most of the question templates discussed in section 11 are applicable but depending on the context of the plot, their language varies from it's surface form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Question Templates</head><p>In this section, we present the 74 question templates which we have used for the question generation. Note that, not all question templates are applicable to each and every type of plot. Also depending on the context of the plot, the question varies from the template's surface form. 15. What is the difference between the &lt;Y label&gt; of/in &lt;legend label1&gt; in &lt; i th x tick&gt; and the &lt;Y label&gt; of/in &lt;legend label2&gt; in &lt; j th x tick&gt; ? <ref type="bibr">16</ref>. What is the average &lt;Y label&gt; of/in &lt;legend label&gt; per &lt;singular form of X label&gt; ?</p><p>17. In the year &lt; i th x tick&gt;, what is the difference between the &lt;Y label&gt; of/in &lt;legend label1&gt; and &lt;Y label&gt; of/in &lt;legend label2&gt; ?</p><p>18. What is the difference between the &lt;Y label&gt; of/in &lt;legend label1&gt; and &lt;Y label&gt; of/in &lt;legend label2&gt; in &lt; i th x tick&gt; ? <ref type="bibr" target="#b18">19</ref>. In how many &lt;plural form of X label&gt;, is the &lt;Y label&gt; greater than &lt;N&gt; units ? <ref type="bibr" target="#b19">20</ref>. Do a majority of the &lt;plural form of X label&gt; between &lt; i th x tick&gt; and &lt; j th x tick? (inclusive/exclusive) have &lt;Y label&gt; greater than N &lt;units&gt; ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21.</head><p>What is the ratio of the &lt;Y label&gt; in &lt; i th x tick&gt; to that in &lt; j th x tick&gt; ?</p><p>22. Is the &lt;Y label&gt; in &lt; i th x tick&gt; less than that in &lt; j th x tick&gt; ?</p><p>23. In how many &lt;plural form of X label&gt;, is the &lt;Y label&gt; of/in &lt;legend label&gt; greater than &lt;N&gt; &lt;units&gt;?</p><p>24. What is the ratio of the &lt;Y label&gt; of/in &lt;legend label1&gt; in &lt; i th x tick&gt; to that in &lt; j th x tick&gt;?</p><p>25. Is the &lt;Y label&gt; of/in &lt;legend label&gt; in &lt; i th x tick&gt; less than that in &lt; j th x tick&gt; ?</p><p>26. Is the difference between the &lt;Y label&gt; in &lt; i th x tick&gt; and &lt; j th x tick&gt; greater than the difference between any two &lt;plural form of X label&gt; ?</p><p>27. What is the difference between the highest and the second highest &lt;Y label&gt; ?</p><p>28. Is the sum of the &lt;Y label&gt; in &lt; i th x tick&gt; and &lt; (i + 1) th x tick&gt; greater than the maximum &lt;Y label&gt; across all &lt;plural form of X label&gt; ? <ref type="bibr" target="#b28">29</ref>. What is the difference between the highest and the lowest &lt;Y label&gt; ?</p><p>30. In how many &lt;plural form of X label&gt;, is the &lt;Y label&gt; greater than the average &lt;Y label&gt; taken over all &lt;plural form of X label&gt; ?</p><p>31. Is the difference between the &lt;Y label&gt; of/in &lt;legend label1&gt; in &lt; i th x tick&gt; and &lt; j th x tick&gt; greater than the difference between the &lt;Y label&gt; of/in &lt;legend label2&gt; in &lt; i th x tick&gt; and &lt; j th x tick&gt; ? 32. What is the difference between the highest and the second highest &lt;Y label&gt; of/in &lt;legend label&gt; ? <ref type="bibr" target="#b32">33</ref>. What is the difference between the highest and the lowest &lt;Y label&gt; of/in &lt;legend label&gt; ? <ref type="bibr" target="#b33">34</ref>. In how many &lt;plural form of X label&gt;, is the &lt;Y label&gt; of/in &lt;legend label&gt; greater than the average &lt;Y label&gt; of/in &lt;legend label&gt; taken over all &lt;plural form of X label&gt; ? <ref type="bibr" target="#b34">35</ref>. Is it the case that in every &lt;singular form of X label&gt;, the sum of the &lt;Y label&gt; of/in &lt;legend label1&gt; and &lt;legend label2&gt; is greater than the &lt;Y label&gt; of/in &lt;legend label3&gt; ? <ref type="bibr" target="#b35">36</ref>. Is the sum of the &lt;Y label&gt; of/in &lt;legend label1&gt; in &lt; i th x tick&gt; and &lt; j th x tick&gt; greater than the maximum &lt;Y label&gt; of/in &lt;legend label2&gt; across all &lt;plural form of X label&gt;? 37. Is it the case that in every &lt;singular form of X label&gt;, the sum of the &lt;Y label&gt; of/in &lt;legend label1&gt; and &lt;legend label2&gt; is greater than the sum of &lt;Y label&gt; of &lt;legend label3&gt; and &lt;Y label&gt; of &lt;legend label4&gt; ?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Dataset can be downloaded from bit.ly/PlotQA arXiv:1909.00997v3 [cs.CV] 1 Feb 2020 Q: Is Light Green the minimum? A: 1 (a) FigureQA Q: What is the value of mad in drop? A: 7 (b) DVQA Q: What is the average number of Hispanic students in schools? A: 51.67 (c) PlotQA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>A sample {plot, question, answer} triplet from FigureQA, DVQA, and PlotQA (our) datasets. the price of diesel in Barbados monotonically increase over the years? Is the difference between the price of diesel in Angola in 2002 and 2004 greater than the difference between the price of diesel in Lebanon in 2002 and 2004?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>Sample questions for 9 different question-answer types in PlotQA. The example questions are with respect to the plot in Figure 2b. Note that there are no open vocabulary answers for Structural Understanding questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Sample plots of different types in the PlotQA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(asterisk, circle, diamond, square, triangle, inverted triangle), position of legends (bottom-left, bottom-centre, bottom-right, centerright, top-right), and colors for the lines and bars from a set of 73 colors. The number of discrete elements on the x-axis varies from 2 to 12 and the number of entries in the legend box varies from 1 to 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Our proposed multi-staged modular pipeline for QA on scientific plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Our proposed model containing (i) a question classifier for deciding whether the question can be answered from a fixed vocabulary (orange) or needs more complex reasoning (green), (ii) QA-as-classification model to answer questions of the former type, and (iii) multi-staged model as a pipeline of perception and QA modules for answering complex questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Ground-truth (cyan) and predicted (red) boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>An example of the knowledge graph constructed from the semi-structured table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Input plot image (b) Few examples of the predicted bounding boxes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Errors made by the VED stage (highlighted in red).(a) Oracle table generated using ground-truth annotations (b) Generated Semi-Structured Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Q1: 2 Q3: 5 Q4: 5 Q8: 2 Figure 12 :</head><label>255212</label><figDesc>What is the difference between price of gasoline in Switzerland and price of gasoline in Macedonia in 2008? A: 0.15 Q2: In how many years is the number of neonatal deaths in Cuba greater than 500? A: What is the difference between the highest and the second highest amount of earnings from goods? A: 0.9e + What is the ratio of the sodium content in Sample 37 to that in Sample 33? A: 1.086 Q5: What is the average air quality index value for NHMC per hour? A: 0.167 Q6: Does the amount of banana production monotonically increase over the years? A: Yes Q7: What is the difference between two consecutive major ticks on the Y-axis? A: 2.000e + In how many cases, is the number of bars for a given year not equal to the number of legend labels? A: 0 Q9: In how many countries, is the mineral rent (as % of GDP) in 1970 greater than the average mineral rent (as % of GDP) in 1970 taken over all countries? A: 1 Q10: What is the total tuberculosis detection rate in Indonesia? A: 101 Q11: Is it the case that in every year, the sum of the number of tourists in Costa Rica and Serbia greater than the number of tourists in Bhutan? A: Yes Q12: How many bars are there on the 2 nd tick from the top? A: Sample {plot, question, answer} triplet present in the PlotQA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison between the existing datasets (FigureQA and DVQA) and our proposed dataset (PlotQA).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Detailed Statistics for different splits of the PlotQA dataset.</figDesc><table><row><cell></cell><cell></cell><cell>Question (Q) Type</cell><cell></cell></row><row><cell>Answer (A) Type</cell><cell>Structure</cell><cell>Data Retrieval</cell><cell>Reasoning</cell></row><row><cell>Yes/No</cell><cell>36.99%</cell><cell>5.19%</cell><cell>2.05%</cell></row><row><cell>Fixed vocabulary</cell><cell>63.01%</cell><cell>18.52%</cell><cell>15.92%</cell></row><row><cell>Open vocabulary</cell><cell>0.00%</cell><cell>76.29%</cell><cell>82.03%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Overall distribution of Q and A types in PlotQA.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>PlotQA Dataset Statistics a hybrid model containing the following elements: (i) a binary classifier for deciding whether the given question can be answered from a small fixed vocabulary or needs more complex reasoning, and (ii) a simpler QA-as-classification model to answer questions of the former type, and (iii) a multi-staged model containing four components as described below to deal with complex reasoning questions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table Question</head><label>Question</label><figDesc></figDesc><table><row><cell>A: 9.008</cell></row><row><cell>Answering (QA)</cell></row><row><cell>Q: What is the average percentage</cell></row><row><cell>of imports from different countries</cell></row><row><cell>in Thailand per year?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>How many bars are there in the image? Q: What is the average number of neonatal deaths in Cuba per year?</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>QA as classification</cell><cell></cell></row><row><cell></cell><cell cols="2">Image Feature Vectors</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>S</cell><cell></cell></row><row><cell>LSTM CNN</cell><cell></cell><cell>N T T A</cell><cell>A M T F O</cell><cell>A: 8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>X</cell><cell></cell></row><row><cell></cell><cell cols="2">Question Embeddings</cell><cell></cell><cell></cell></row><row><cell>Question</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>classifier</cell><cell></cell><cell></cell><cell>Multi-staged model</cell><cell></cell></row><row><cell>Visual Elements Detection (VED)</cell><cell>Optical Character Recognition (OCR)</cell><cell>Semi-structured (SIE) extraction information</cell><cell>Table (QA) Answering Question</cell><cell>A: 514.95</cell></row></table><note>Q:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Accuracy of different models on DVQA dataset.</figDesc><table><row><cell>Models</cell><cell>IMG</cell><cell>QUES</cell><cell>BAN</cell><cell>LoRRA</cell><cell>SAN</cell><cell>Our Model</cell></row><row><cell>Accuracy</cell><cell>4.84</cell><cell>5.35</cell><cell>0.01</cell><cell>0.02</cell><cell>7.76</cell><cell>22.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of proposed method on PlotQA. Note that there are no open vocab. answers for Structural Understanding question templates (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>VED Module's Accuracy on PlotQA dataset</figDesc><table><row><cell></cell><cell>Oracle</cell><cell>After VED</cell></row><row><cell>Title</cell><cell>99.31%</cell><cell>94.6%</cell></row><row><cell>X-axis Label</cell><cell>99.94%</cell><cell>95.5%</cell></row><row><cell>Y-axis Label</cell><cell>98.43%</cell><cell>97.07%</cell></row><row><cell>X-tick Label</cell><cell>94.8%</cell><cell>91.38%</cell></row><row><cell>Y-tick Label</cell><cell>93.38%</cell><cell>88.07%</cell></row><row><cell>Legend Label</cell><cell>98.53%</cell><cell>91.99%</cell></row><row><cell>Total</cell><cell>97.06%</cell><cell>93.10%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>OCR Module Accuracy on the PlotQA dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Figure 8 :Table 11 :</head><label>811</label><figDesc>Errors made by the OCR and SIE stage (highlighted in red). Note that most of these errors have been propagated from the VED stage. Answers predicted by different models on the sample questions. Errors made by the OCR and SIE stage (highlighted in red). Note that most of these errors have been propagated from the VED stage.</figDesc><table><row><cell>(a) Input plot image</cell><cell></cell><cell cols="7">(b) Few examples of the predicted bounding boxes</cell></row><row><cell cols="6">Figure 9: Errors made by the VED stage (highlighted in red).</cell><cell></cell><cell></cell></row><row><cell>(a) Oracle table generated using ground-truth annotations</cell><cell></cell><cell></cell><cell cols="5">(b) Generated Semi-Structured Table</cell></row><row><cell>Figure 10: Question</cell><cell cols="2">Ground Truth</cell><cell cols="2">QA as classification</cell><cell cols="2">Multistage Pipeline</cell><cell cols="2">Our Model</cell></row><row><cell>Q1. How are the legend-labels stacked?</cell><cell cols="2">horizontal</cell><cell></cell><cell>horizontal</cell><cell cols="4">horizontal horizontal</cell></row><row><cell>Q2. What is the time (in hours) required to prepare and pay taxes in Kuwait in 2008?</cell><cell></cell><cell>98</cell><cell></cell><cell>100</cell><cell></cell><cell>100-</cell><cell></cell><cell>100-</cell></row><row><cell cols="2">Q3. What is the difference between in time (in Question Q1. What is the average indoor user rating per operator? hours) required to prepare and pay taxes in Kuwait in 2009 and the time (in hours) required</cell><cell cols="2">Ground Truth 3.62 -52</cell><cell cols="2">QA as classification 500 -0.5</cell><cell cols="2">Multistage Pipeline 3.54 100-</cell><cell>Our Model 3.54 100-</cell></row><row><cell cols="2">Q2. What is the total user rating for Vodafone in the graph? to prepare and pay taxes in Liberia in 2008?</cell><cell></cell><cell>7.24</cell><cell></cell><cell>5</cell><cell></cell><cell>7.35</cell><cell>7.35</cell></row><row><cell>Q3. What is the label or title of the X-axis? Q4. What is the difference between the highest</cell><cell></cell><cell cols="2">Operator</cell><cell cols="2">Years</cell><cell cols="3">Dperator Dperator</cell></row><row><cell>Q4. What is the indoor user rating of Airtel? and the lowest time (in hours) required to</cell><cell></cell><cell>131</cell><cell>3.62</cell><cell>500</cell><cell>0</cell><cell>125</cell><cell>3.60</cell><cell>3.60 125</cell></row><row><cell>Q5. How many groups of bars are there? prepare and pay taxes in Peru?</cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell>4</cell><cell></cell><cell>4</cell><cell>4</cell></row><row><cell cols="2">Q6. What is the ratio of indoor user rating of Airtel to that of the outdoor user rating of Vodafone? Q5. Is it the case that every year the sum of time (in hours) required to prepare and pay taxes in</cell><cell></cell><cell>1.03</cell><cell></cell><cell>No</cell><cell></cell><cell>3.35</cell><cell>3.35</cell></row><row><cell cols="2">Q7. What is the difference between the highest and lowest outdoor user rating? time (in hours) required to prepare and pay Peru and Kuwait is greater than the sum of the</cell><cell>Yes</cell><cell>0.13</cell><cell>Yes</cell><cell>1.5</cell><cell>320</cell><cell>0.10</cell><cell>0.10 Yes</cell></row><row><cell cols="2">Q8. Does "Indoor" appear as one of the legend-labels in the graph? taxes in Liberia and Malaysia?</cell><cell></cell><cell>Yes</cell><cell></cell><cell>Yes</cell><cell></cell><cell>1.0</cell><cell>Yes</cell></row><row><cell>Q9. For how many operators are the indoor user rating</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>greater than the average outdoor user rating taken</cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell>4</cell><cell></cell><cell>3.4</cell><cell>4</cell></row><row><cell>over all operators?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Q10. Is the sum of outdoor user rating in Reliance and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Idea greater than the maximum outdoor user rating</cell><cell></cell><cell></cell><cell>Yes</cell><cell></cell><cell>Yes</cell><cell></cell><cell>6.85</cell><cell>Yes</cell></row><row><cell>across all operators?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 12 :</head><label>12</label><figDesc>Answers predicted by different models on the sample questions.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/tesseract-ocr/tesseract</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/TingAnChien/san-vqa-tensorflow</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scatteract: Automated extraction of data from scatter plots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cliche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madeka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R-Cnn</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1504.08083</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural multi-step reasoning for question answering on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grnarova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval -40th European Conference on IR Research</title>
		<meeting><address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-03-26" />
			<biblScope unit="page" from="611" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas</title>
		<meeting><address><addrLine>NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">DVQA: understanding data visualizations via question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<idno>abs/1801.08163</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Figureqa: An annotated figure dataset for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>K?d?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1710.07300</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A diagram is worth a dozen images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salvato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="1571" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="1516" to="1526" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning a natural language interface with neural programmer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno>abs/1611.08945</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Training recurrent answering units with joint loss minimization for VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1606.03647</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Image question answering: A visual semantic embedding model and a new dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1505.02074</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G T</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Figureseer: Parsing result-figures in research papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pythia-a platform for vision &amp; language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SysML Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards vqa models that can read</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An overview of the tesseract ocr engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A corpus of natural language for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Structural Understanding : 1. Does the graph contain any zero values? 2. Does the graph contain grids ?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Where does the legend appear in the graph ?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">How many legend labels are there?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">How are the legend labels stacked? 6. How many &lt;plural form of X label&gt; are there in the graph?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">How many &lt;figure-type&gt;s are there? 8. How many different colored &lt;figure-type&gt;s are there?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Are the number of bars on each tick equal to the number of legend labels? 11. Are the number of bars in each group equal? 12. How many bars are there on the i th tick from the left? 13</title>
	</analytic>
	<monogr>
		<title level="m">How many groups of &lt;figure-type&gt;s are there? 10</title>
		<imprint/>
	</monogr>
	<note>How many bars are there on the i th tick from the right? 14. How many bars are there on the i th tick from the top? 15. How many bars are there on the i th tick from the bottom</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Are all the bars in the graph horizontal? 17. How many lines intersect with each other? 18. Is the number of lines equal to the number of legend labels? 2. Data Retrieval : 1. What does the i th bar from the left in each group represent?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">What does the i th bar from the right in each group represent? 3. What does the i th bar from the top in each group represent? 4. What does the i th bar from the bottom in each group represent?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">What is the label of the j th group of bars from the left?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">What is the label of the j th group of bars from the top?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Does the &lt;Y label&gt; of/in &lt;legend-label&gt; monotonically increase over the &lt;plural form of X label&gt; ?</title>
		<imprint/>
	</monogr>
	<note>8. What is the difference between two consecutive major ticks on the Y-axis</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Are the values on the major ticks of Y-axis written in scientific E-notation ? 10</title>
		<imprint/>
	</monogr>
	<note>What is the title of the graph ? 11. Does &lt;legend label&gt; appear as one of the legend labels in the graph</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">In how many cases, is the number of &lt;figure type&gt; for a given &lt;X label&gt; not equal to the number of legend labels ? 15. What is the &lt;Y value&gt; in/of &lt; i th X tick&gt; ? 16. What is the &lt;Y value&gt; of the i th &lt;legend label&gt; in &lt; i th X tick&gt; ? 17. Does the &lt;Y label&gt; monotonically increase over the &lt;plural form of X label&gt; ? 18. Is the &lt;Y label&gt; of/in &lt;legend label1&gt; strictly greater than the &lt;Y label&gt; of/in &lt;legend label2&gt; over the &lt;plural form of X label&gt; ? 19. Is the &lt;Y label&gt; of/in &lt;legend label1&gt; strictly less than the &lt;Y label&gt; of/in &lt;legend label2&gt; over the &lt;plural form of X label&gt; ? 3. Reasoning : 1. Across all &lt;plural form of X label&gt;</title>
		<imprint/>
	</monogr>
	<note>What is the label or title of the X-axis ? 13. What is the label or title of the Y-axis ? 14. what is the maximum &lt;Y label&gt; ? 2. Across all &lt;plural form of X label&gt;, what is the minimum &lt;Y label&gt;</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<title level="m">which &lt;X label&gt; was the &lt;Y label&gt; maximum ? 4. In which &lt;X label&gt; was the &lt;Y label&gt; minimum ? 5. Across all &lt;plural form of X label&gt;, what is the maximum &lt;Y label&gt; of/in &lt;legend label&gt;</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Across all &lt;plural form of X label&gt;, what is the minimum &lt;Y label&gt; of/in &lt;legend label&gt; ?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">In which &lt;singular form of X label&gt; was the &lt;Y label&gt; of/in &lt;legend label&gt; maximum ?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">In which &lt;singular form of X label&gt; was the &lt;Y label&gt; of/in &lt;legend label&gt; minimum ?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">What is the sum of &lt;title&gt; ? 10. What is the difference between the &lt;Y label&gt; in &lt; i th x tick&gt; and &lt; j th x tick&gt; ? 11. What is the average &lt;Y label&gt; per &lt;singular form of X label&gt; ?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">What is the median &lt;Y label&gt; ? 13. What is the total &lt;Y label&gt; of/in &lt;legend label&gt; in the graph?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">What is the difference between the &lt;Y label&gt; of/in &lt;legend label&gt; in &lt; i th x tick&gt; and that in &lt; j th x tick&gt; ?</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
