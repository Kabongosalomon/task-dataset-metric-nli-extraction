<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Channel Splitting Network for Single MR Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaole</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Zou</surname></persName>
						</author>
						<title level="a" type="main">Channel Splitting Network for Single MR Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Convolutional neural network</term>
					<term>channel splitting</term>
					<term>feature fusion</term>
					<term>magnetic resonance imaging</term>
					<term>super-resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High resolution magnetic resonance (MR) imaging is desirable in many clinical applications due to its contribution to more accurate subsequent analyses and early clinical diagnoses. Single image super-resolution (SISR) is an effective and cost efficient alternative technique to improve the spatial resolution of MR images. In the past few years, SISR methods based on deep learning techniques, especially convolutional neural networks (CNNs), have achieved state-of-the-art performance on natural images. However, the information is gradually weakened and training becomes increasingly difficult as the network deepens. The problem is more serious for medical images because lacking high quality and effective training samples makes deep models prone to underfitting or overfitting. Nevertheless, many current models treat the hierarchical features on different channels equivalently, which is not helpful for the models to deal with the hierarchical features discriminatively and targetedly. To this end, we present a novel channel splitting network (CSN) to ease the representational burden of deep models. The proposed CSN model divides the hierarchical features into two branches, i.e., residual branch and dense branch, with different information transmissions. The residual branch is able to promote feature reuse, while the dense branch is beneficial to the exploration of new features. Besides, we also adopt the merge-and-run mapping to facilitate information integration between different branches. Extensive experiments on various MR images, including proton density (PD), T1 and T2 images, show that the proposed CSN model achieves superior performance over other state-of-the-art SISR methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S PATIAL resolution is one of the most important imaging parameters for magnetic resonance imaging (MRI). In many clinical applications and research work, high resolution (HR) MRI is usually preferred because it can provide more <ref type="bibr">Manuscript</ref>    The test results vs. the number of model parameters on 70 PD volumes (6720 2D slices, Bicubic: 35.04 dB). The symbols , , $ and ? represent models with less than 1M, 10M, 20M and more than 20M parameters respectively. C1 indicates that the training sample is a single slice, and C96 indicates that the model treats 96 slices of a 3D volume as 96 channels. significant structure and texture details with a smaller voxel size <ref type="bibr" target="#b0">[1]</ref>, thus promoting accurate subsequent analysis and early diagnosis. However, it is limited by several factors, e.g., hardware device, imaging time, desired signal-to-noise ratio (SNR) and body motion etc, and increasing spatial resolution of magnetic resonance (MR) images typically reduces image SNR and/or increases imaging time <ref type="bibr" target="#b1">[2]</ref>.</p><p>Image super-resolution (SR) is a typical ill-posed inverse problem in computer vision community, which mainly aims at inferring a HR image from one or more low resolution (LR) images. It is a well-studied problem in both natural image (NI) and MR image processing. High resolution means that the pixel density of an image is higher than its LR counterpart. Thus, HR images can offer more details that may be critical in various applications such as medical imaging <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, aerial spectral imaging <ref type="bibr" target="#b4">[5]</ref> and remote sensing imaging <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and security and surveillance <ref type="bibr" target="#b7">[8]</ref>, where high frequency details are very important and greatly desired. Up to now, many SR methods have been studied and proposed. Early methods include: (i) interpolation methods, e.g., bicubic, Lanczos-? <ref type="bibr" target="#b8">[9]</ref>; (ii) modeling and reconstruction methods, e.g., iterative back projection (IBP) <ref type="bibr" target="#b9">[10]</ref>, projection onto convex set (POCS) <ref type="bibr" target="#b10">[11]</ref> etc.; (iii) traditional shallow learning methods, e.g., example learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, dictionary learning <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> etc. The performance of these methods is inherently limited because the additional information available for solving this ill-posed inverse problem is also very limited, e.g., the interpolation methods make use of the basic smoothing priori by implicitly assuming that the image signal is continuous and bandwidth limited, and traditional machine learning-based methods can learn insufficient information due to the limited representa-arXiv:1810.06453v3 [cs.CV] <ref type="bibr" target="#b14">15</ref> Sep 2019 tional capacity of these shallow models.</p><p>In recent years, various advanced SR methods have emerged with the rapid development of deep learning techniques <ref type="bibr" target="#b15">[16]</ref> and greatly promoted the best state of SR performance. Superresolution convolutional neural network (SRCNN) <ref type="bibr" target="#b16">[17]</ref> and fast super-resolution convolutional neural network (FSRCNN) <ref type="bibr" target="#b17">[18]</ref> are two pioneering contributions that utilize convolutional neural networks (CNNs) <ref type="bibr" target="#b18">[19]</ref> to solve SR tasks. The further improvement based on these pioneering work mainly focused on increasing model depth or sharing model parameters at the beginning, for example, deeply recursive convolutional network (DRCN) <ref type="bibr" target="#b19">[20]</ref>, deep recursive residual network (DRRN) <ref type="bibr" target="#b20">[21]</ref>, super-resolution using very deep convolutional networks (VDSR) <ref type="bibr" target="#b21">[22]</ref> and memory network (MemNet) <ref type="bibr" target="#b22">[23]</ref> etc. These methods, however, are mainly aimed at the SR task of natural images, not specially at medical images.</p><p>The medical image processing community has noticed these advances and some medical image SR methods based on deep learning have also appeared <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. For MR images, training deep models with a large amount of parameters and extremely deep structures is usually more difficult because high-quality and effective training samples are relatively scarce and unavailable. It is worth noting that the challenge is not the availability of training data itself, but the acquisition or the quality of relevant annotations/labeling for these data <ref type="bibr" target="#b29">[30]</ref>. Therefore, some SR models that aim at NI are likely to fail when directly trained with MR images despite sufficient training samples, e.g., <ref type="figure" target="#fig_1">Fig.1</ref> displays the peak signalto-noise ratio (PSNR) performance of several recent single image super-resolution (SISR) models and the proposed channel splitting network (CSN) on proton density (PD) images of IXI dataset (http://brain-development.org/ixi-dataset/) for SR?2, where enhanced deep super-resolution network (EDSR) <ref type="bibr" target="#b30">[31]</ref> and residual dense network (RDN) <ref type="bibr" target="#b31">[32]</ref> are advanced models on NI. But it is failed to train the EDSR model (the same configuration as <ref type="bibr" target="#b30">[31]</ref>) with 48000 2D PD images. This problem of training failure caused by the degradation of sample quality will get worse as the network depth (or width) and the number of model parameters increase.</p><p>Thus, in the context of MR image super-resolution based on deep learning techniques, the dilemma has become more apparent: on the one hand, models with shallow structures and fewer parameters are easy to train, but their SR performance is usually unsatisfactory; on the other hand, models with deeper structures and more parameters are promising to improve SR performance, but it is more difficult for them to be fully trained with MR images. An effective way to alleviate the difficulty of model training is residual learning, which is initially proposed for image recognition <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. It has been widely proved to be helpful for feature reuse and model convergence, thus making it possible to build extremely deep models. However, residual learning strategy alone is still insufficient to train the model with a very deep structure and a very large number of parameters in case of MR images, e.g., EDSR <ref type="bibr" target="#b30">[31]</ref> with about 43M parameters is a typical residual network, but the original configuration can hardly be well trained with 2D MR images in our settings. The problem of training failure can be addressed by concatenating multiple 2D MR images into a single multi-channel training sample at the expense of performance, e.g., we train the original EDSR <ref type="bibr" target="#b30">[31]</ref> model by taking all 96 slices of a 3D volume as 96 channels of a single training sample, as shown in <ref type="figure" target="#fig_1">Fig.1</ref> (marked as EDSR (C96)).</p><p>In this paper, we improve the above dilemma by introducing a deep channel splitting network (CSN) framework. It assumes that the hierarchical features of deep models have certain clustering properties, and explicitly discriminating them is beneficial to ease the representational burden of deep models and further improve the SR performance. Therefore, instead of transferring the feature maps of the previous layer completely to the next layer, we split the feature maps into two different parts (branches) with different information transmissions. Each branch can be structured differently, e.g., in this work, we use propagation mechanisms similar to residual network (ResNet) <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> and dense network (DenseNet) <ref type="bibr" target="#b34">[35]</ref> (or RDN <ref type="bibr" target="#b31">[32]</ref>) on each branch. Besides, the merge-and-run (MAR) mapping <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> is also applied to facilitate the information integration of different branches. Thus, our model has two notable characteristics: (1) channel splitting discriminatorily limits the hierarchical features into different clusters and reduces the representational redundancy of the model by curtailing the internal connections; (2) the merge-and-run mapping can promote information sharing and integration between the hierarchical features and therefore help to improve the information flow through the entire network.</p><p>To make full use of the hierarchical features, we also adopt the global feature fusion (GFF) technique proposed by <ref type="bibr" target="#b31">[32]</ref>, as shown in <ref type="figure" target="#fig_3">Fig.2(a)</ref>. Moreover, multilevel residual mechanism and constant scaling technique <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b37">[38]</ref> are also applied to our models to further stabilize the model training. To verify the effectiveness of the proposed model, a set of standard datasets for the task of single MR image SR is generated from the IXI dataset, which includes three types of MR images (i.e., PD, T1 and T2, each of which can exhibit different contrasts for the same image content.) and two kinds of degeneration (bicubic downsampling and k-space truncation). The quantitative and qualitative experiments on the datasets display the superiority of the proposed model over other advanced methods.</p><p>The rest of this paper is organized as follows. In section II, we present some previous contributions related the present work. The proposed method and the experimental results are detailed in section III and section IV, respectively. Section V gives some discussion and future work. Finally, we conclude the whole work in section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Super-Resolution with Deep Learning</head><p>Although the work using artificial neural networks (ANNs) to solve SR problems has emerged as early as 2006 <ref type="bibr" target="#b38">[39]</ref>, the pioneering work with deep learning techniques in the modern sense is SRCNN <ref type="bibr" target="#b16">[17]</ref>. Subsequently, some advanced methods based on the increase of network depth and parameter sharing are proposed. Kim et al. <ref type="bibr" target="#b21">[22]</ref> increased network depth by stacking multiple conv layers and the usage of global residual learning (GRL), and firstly introduced recursive learning trick in a deep network for parameter sharing <ref type="bibr" target="#b19">[20]</ref>. Another network  introduced by Tai et al. <ref type="bibr" target="#b20">[21]</ref> has utilized recursive blocks to reuse parameters. Motivated by the fact that human thoughts have persistency, a deep persistent memory network (MemNet) which consists of the so-called memory block, has also been proposed by the same author <ref type="bibr" target="#b22">[23]</ref>. To improve information flow and capture more sufficient knowledge for reconstructing the high frequency details, Hu et al. <ref type="bibr" target="#b36">[37]</ref> proposed a cascaded multi-scale cross network (CMSCN) in which a sequence of subnetworks ( <ref type="figure" target="#fig_4">Fig.3(b)</ref>) is cascaded to infer HR features in a coarse-to-fine manner. To some extent, these methods promote the design of new structures for image generation tasks. There is a common feature among the above methods: they use the bicubic-interpolated image of the original LR image as input to their models. This preprocessing is convenient for keeping the size of the output image consistent with the target HR image. However, it places the nonlinear inference of the network in HR image space, resulting in great computation and memory consumption. There exist two solutions for this issue currently: deconvolution or transpose convolution <ref type="bibr" target="#b17">[18]</ref>, and the efficient sub-pixel convolutional neural network (ESPCNN) <ref type="bibr" target="#b39">[40]</ref>. Both of them can effectively solve the above problem by shifting the HR reference to the LR image space. Benefiting from the nonlinear mapping within the LR image space, these methods are capable of increasing the scale of deep models and thus boost the SR performance greatly, e.g., EDSR/MDSR <ref type="bibr" target="#b30">[31]</ref>. Further, Zhang et al. <ref type="bibr" target="#b31">[32]</ref> combined the idea of residual learning <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> with densely connected DenseNet <ref type="bibr" target="#b34">[35]</ref> and proposed a novel residual dense network (RDN) to fully utilize the hierarchical features of deep models.</p><p>Contrary to the pursuit of high performance, some methods  <ref type="bibr" target="#b36">[37]</ref>. (c) The proposed mapping. c is the channel number of feature maps, and g is the growth of a dense connection <ref type="bibr" target="#b34">[35]</ref>. BN and ReLU are omitted for simplification, and the red arrow indicates the dense connection.</p><formula xml:id="formula_0">C 3?3 Conv + Skip Add M Mean C 5?5 Conv (c)</formula><p>aim to improve the tradeoff between SR performance and time efficiency to improve the practicality of the model, e.g., <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b58">[58]</ref>. Overall, although these methods favor relatively fast inference, they are still lightweight and small-scale so that their representational capacity is limited to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MR Image Super-Resolution</head><p>The early application of SR techniques to medical images mainly focuses on multi-frame image super-resolution (MFSR) tasks. For example, IBP <ref type="bibr" target="#b9">[10]</ref> was adopted to generate a new image with increased spatial resolution from several spatially shifted, single-shot and diffusion-weighted brain MR images <ref type="bibr" target="#b41">[41]</ref>. Greenspan et al. <ref type="bibr" target="#b42">[42]</ref> and Shilling et al. <ref type="bibr" target="#b43">[43]</ref> employed IBP and POCS <ref type="bibr" target="#b10">[11]</ref> to produce a 3D MR volume with isotropic resolution from several 2D slices, respectively. These methods are usually accompanied with specific data acquisitions (e.g., rotation, scaling and translation) to simulate the generation of LR images. However, recovering a HR image from multiple degraded LR images usually needs to calibrate and fuse these LR images, which is a very challenging task in itself.</p><p>To avoid the difficulty of calibration and fusion between multiple LR images, Rousseau <ref type="bibr" target="#b44">[44]</ref> first proposed to enhance MR image resolution with single image SR techniques. In this method, the extra information was introduced into the reconstruction process by referring to another HR image. A similar method was proposed by Manj?n et al. <ref type="bibr" target="#b45">[45]</ref>. They also used a HR image as reference but with a different strategy to produce HR images. These methods introduce very limited extra information because they learn knowledge from only one external HR image. SR methods based on conventional machine learning, e.g., sparse representation <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref> and compressive sensing <ref type="bibr" target="#b48">[48]</ref>, are also applied to medical images subsequently. Recently, more advanced SR methods based on deep learning <ref type="bibr" target="#b15">[16]</ref> have also been applied to MR image SR tasks <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b49">[49]</ref>. However, these methods simply use deep learning techniques to deal with the SR tasks of MR images without considering the differences between natural images and medical images. Contrarily, the proposed CSN model aims at dealing with the hierarchical features discriminatively and reducing the representational burden of the model to adapt the degradation of MR training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-Stream Networks</head><p>Multi-stream networks are widely adopted in the image SR community to boost the model performance by assembling the information from different streams (paths). Wang et al. <ref type="bibr" target="#b50">[50]</ref> explored an end-to-end CNN architecture by jointly training both deep and shallow CNN networks, where the shallow one stabilizes model training and the deep one ensures an accurate HR reconstruction. Ren et al. <ref type="bibr" target="#b51">[51]</ref> proposed a context-wise network fusion approach to integrate the outputs of individual networks by extra convolutional layers. Yamanaka et al. <ref type="bibr" target="#b52">[52]</ref> combined skip connection layers and parallelized CNNs into a single CNN architecture. CMSCN <ref type="bibr" target="#b36">[37]</ref> is another multi-stream structure, in which complementary information under different receptive fields is integrated by the merge-and-run mechanism <ref type="bibr" target="#b35">[36]</ref>  <ref type="figure" target="#fig_4">(Fig.3</ref>). There are also multi-stream structures for medical image SR tasks, e.g., Oktay et al. <ref type="bibr" target="#b49">[49]</ref> developed a multi-input cardiac image SR network, which is capable of assembling information from different viewing planes to improve the SR performance. These methods are fundamentally different from the proposed CSN model, in that they form the multi-stream structure by the reuse of the preceding features, while our CSN network construct the multi-stream structure by splitting the preceding features into different branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overall Network Architecture</head><p>The overall structure of the proposed CSN model is outlined in <ref type="figure" target="#fig_3">Fig.2</ref>. Similar to other deep models for image SR, it consists mainly of 3 parts: feature extraction, nonlinear mapping and image recovery. Firstly, the feature extraction network (FEN) is employed to express the input image x as a set of shallow features. These shallow features are then transmitted to the nonlinear mapping network (NMN), which contains a series of stacked channel splitting blocks (CSB). Subsequently, the hierarchical features from all CSBs are concatenated together to produce the final output of the NMN. This operation is also called global feature fusion (GFF) <ref type="bibr" target="#b31">[32]</ref>. Finally, the collected deep hierarchical features are fed into the image reconstruction network (IRN) to generate the final HR predication y of the entire CSN model.</p><p>1) Feature Extraction: The FEN contains two 3?3 conv layers with a 1?1 conv layer in the middle. Denote F E (?) as the corresponding mapping function, then the shallow features x E extracted by the FEN can be represented as:</p><formula xml:id="formula_1">x E = F E (x),<label>(1)</label></formula><p>where x is the original LR input. The 1?1 conv layer indicates a point-to-point linear transformation of the features extracted by the first 3?3 conv layer. This 1?1 conv layer is considered to be helpful to further improve the robustness of the extracted features because the features on different channels also contain spatial information in the context of image SR.</p><p>2) Nonlinear Mapping: The entire NMN net is denoted as F M (?). Therefore, the output of the NMN is given by F M (x E ). Supposing we have n CSBs in the entire network and x 0 = x E is the input of the first CSB, then the output x i of the i-th CSB can be obtained by: where the function F i c (?) corresponds to the operations of the i-th CSB. More details about F i c (?) will be presented in section III-B. Therefore, the output of the last CSB can be iteratively formulated as follow:</p><formula xml:id="formula_2">x i = F i c (x i?1 ), i = 1, 2, . . . , n,<label>(2)</label></formula><formula xml:id="formula_3">c 2 c 2 c 2 c 2 C C C C M + + g g (f) CSN-D3D3 c 2 c 2 c 2 c 2 C C C C M + + (b) CSN-R3R3 c 2 c 2 c 2 c 2 C C C C M + + g (g) CSN-R5D3 c 2 c 2 c 2 c 2 C C C C M + + g (c) CSN-R3D5 c 2 c 2 c 2 c 2 C C C C M + + (d) CSN-R3R5 c 2 c 2 c 2 c 2 C C C C M + + g (h) CSN-D3D5 g C 3?3 Conv + Skip Add M Mean C 5?5 Conv c 2 c 2 c 2 c 2 C C C C g (e) CSN-SP C c c (a) baseline</formula><formula xml:id="formula_4">x n = F n c (x n?1 ) = F n c (F n?1 c (? ? ? (F 1 c (x 0 )) ? ? ? )). (3)</formula><p>The output tensor of the i-th CSB x i is produced by a series of operations (e.g., convolution, ReLU and constant scaling etc.) within the block, so it is viewed as a set of local feature maps <ref type="bibr" target="#b31">[32]</ref>. These local feature maps constitute the final output of our nonlinear mapping network. It should be noted that the output of the preceding CSB is directly used as the input of the next CSB. This is similar to the so-called continuous memory (CM) mechanism <ref type="bibr" target="#b31">[32]</ref> and contributes to the information propagation in the network <ref type="bibr" target="#b33">[34]</ref>.</p><p>3) Image Reconstruction: This phase includes two related parts: global fusion of the local features x i and the restoration of HR images based on the fused features. To fuse the local features, the outputs of all CSBs are first concatenated into a single tensor (red rectangle in <ref type="figure" target="#fig_3">Fig.2(a)</ref>):</p><formula xml:id="formula_5">x M = [x 0 , x 1 , . . . , x n ],<label>(4)</label></formula><p>where [. . .] implies the concatenation. Next, the global features are extracted by fusing all local features from all the preceding channel splitting blocks. This is completed by a 1?1 conv followed by a 3?3 conv (F F (?) in <ref type="figure" target="#fig_3">Fig.2</ref>). Finally, the global residual learning (GRL) is used to stabilize the training of the model <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, which is simply implemented via a global skip connection (GSC):</p><formula xml:id="formula_6">x R = F F (x M ) + x 0 = F F ([x 0 , x 1 , . . . , x n ]) + x 0 ,<label>(5)</label></formula><p>where x R is the fused features that will be used to recover the HR image y. As for HR image restoration, it is mainly made up of a pixel shuffle layer followed by a 3?3 convolutional layer, and an external residual learning (ERL). Formally, it can be represented as:</p><formula xml:id="formula_7">y = F R (x R ) + x,<label>(6)</label></formula><p>where F R (?) is the function corresponding to the pixel shuffle layer and the following 3?3 convolutional layer, and x is the original LR input to the model. Note that the pixel shuffle layer is implemented by ESPCNN <ref type="bibr" target="#b39">[40]</ref> in the way of <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Channel Splitting Block</head><p>The CMSCN network explored by <ref type="bibr" target="#b36">[37]</ref> is a multi-stream structure that integrates the complementary information under different receptive fields. Moreover, it has been proved that residual learning enables feature reuse and dense learning enables new features exploration, both of which are important for learning good representations <ref type="bibr" target="#b53">[53]</ref>. Inspired by this, we present a two-way channel splitting block (CSB) to incorporate different information with different propagation mechanisms. As shown in <ref type="figure" target="#fig_3">Fig.2(b)</ref>, the distinctive features of the proposed CSB are channel splitting and merging, and fusion of residual learning and dense learning. Besides, local residual learning (LRL) is also applied to further improve the information propagation. It has been shown that LRL is helpful to stabilize the training process and improve the representational capacity of the model, resulting better SR performance <ref type="bibr" target="#b31">[32]</ref>.</p><p>1) Channel Splitting and Merging: As for the i-th CSB, the input tensor x i?1 is first equally split into two tensors along the channel direction, x ? i?1 and x i?1 , which are the inputs of the lower (dense) branch and the upper (residual) branch respectively. It could be formally expressed as:</p><formula xml:id="formula_8">x ? i?1 , x i?1 = S c (x i?1 ), i = 1, 2, . . . , n,<label>(7)</label></formula><p>where S c (?) is the channel splitting function. It can be viewed as a unary operator that splits the input tensors into two parts along the channel direction. Through this channel splitting operation, we can apply different information transmission mechanisms on each branch. For example, we adopt residual learning and dense learning on the upper and lower branches respectively. Correspondingly, there exists a channel merging operation, M c (?), at the end of each CSB:</p><formula xml:id="formula_9">x i?1 = M c (x ? i?1 , x i?1 ), i = 1, 2, . . . , n.<label>(8)</label></formula><p>It should be noted that M c (?) is a bivariate function, while the [. . .] in <ref type="formula" target="#formula_5">(4)</ref> and <ref type="formula" target="#formula_6">(5)</ref>   The baseline stage mapping is a single convolutional layer followed by a ReLU operation ( <ref type="figure" target="#fig_6">Fig.5(a)</ref>), which emphasizes the impact of channel splitting.</p><p>interfere with the information flow within the network, which helps the model to process the hierarchy features with different properties in a targeted way. In addition, it is also an effective manner to maintain the scale of model parameters and increase the depth of the network.</p><p>2) Feature Reuse and New Feature Exploration: A CSB module assembles two residual-like branches in parallel with a merge-and-run mapping <ref type="bibr" target="#b35">[36]</ref> but each branch has different structures. Suppose the i-th CSB contains m stage mappings and each branch in a stage includes two convolutional layers with a ReLU operation in the middle <ref type="figure" target="#fig_3">(Fig.2(b)</ref>). Denote H ? i,j (?) and H i,j (?) as the transition functions of the lower and the upper residual branches in the j-th stage mapping respectively. Then the transition function of the j-th stage mapping can be represented in matrix form as below:</p><formula xml:id="formula_10">x i?1,j x ? i?1,j = H i,j (x i?1,j?1 ) H ? i,j (x ? i?1,j?1 ) + 1 2 I I I I x i?1,j?1 x ? i?1,j?1 ,<label>(9)</label></formula><p>where i = 1, 2, . . . , n and j = 1, 2, . . . , m are the index of the i-th CSB and the j-th stage mapping in this CSB. x ? i?1,j?1 and x i?1,j?1 (x ? i?1,j and x i?1,j ) are the inputs (outputs) of the j-th stage mapping in the i-th CSB, and x ? i?1,0 = x ? i?1 and x i?1,0 = x i?1 are the input tensors of the lower and upper branches respectively. I denotes identity matrix. Therefore, the coefficient matrix</p><formula xml:id="formula_11">C = 1 2 I I I I ,<label>(10)</label></formula><p>is an idempotent transformation matrix of the merge-and-run mapping <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. The idempotent property can promote the information flow across the different modules and encourage gradient back-propagation during model training, similar to identity mapping <ref type="bibr" target="#b33">[34]</ref>. It is worth noting that the upper branch is a residual-like structure similar to EDSR <ref type="bibr" target="#b30">[31]</ref> and the lower branch is a simplified dense-like structure similar to DenseNet <ref type="bibr" target="#b34">[35]</ref> or RDB <ref type="bibr" target="#b31">[32]</ref>, which uses only one skip dense connection to explore new features. Through merge-and-run mapping, we can effectively integrate the superiority of feature reuse and new feature exploration provided by the residual branch and the dense branch.</p><p>3) Local Residual Learning (LRL): The feature maps from these two branches, x ? m and x m , are merged together after m stages of merge-and-run mappings. Next, a local residual learning (LRL) <ref type="bibr" target="#b31">[32]</ref> is also introduced in the CSB to further improve the information flow. The output of this CSB module is thus given by:</p><formula xml:id="formula_12">x i = L(M c (x ? i?1,m , x i?1,m )) + x i?1 ,<label>(11)</label></formula><p>where L(?) corresponds to a 1?1 convolutional operation at the end of the CSB, as shown in <ref type="figure" target="#fig_3">Fig.2(b)</ref>. Unlike <ref type="bibr" target="#b31">[32]</ref>, the local residual features are derived from our CSB module, instead of the densely connected block <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multilevel Residual Mechanism</head><p>Normally, the LR images and the corresponding HR images share same information to a large extent, which indicates that a large part of the topological structure of their high-dimensional manifolds are similar to each other. Therefore, it is beneficial to explicitly allow the model to learn the residual between the original LR input and the HR output <ref type="bibr" target="#b21">[22]</ref>. However, because LR and HR images have different sizes, the residual between them cannot be directly obtained. We adopt a bicubic interpolated version of the LR image to match the size of the HR image, and use it to approximate the residual between the original LR image and the HR image. This is implemented by simply adding the interpolated image to the output of the last convolutional layer of the entire network, which we term as external skip connection (ESC) <ref type="figure" target="#fig_3">(Fig.2(a)</ref>). Thus, (6) should be rewritten as:</p><formula xml:id="formula_13">y = F R (x R ) +x,<label>(12)</label></formula><p>wherex is the interpolated version of the original LR input x.</p><p>Although we use bicubic interpolation here, one can use any other interpolation algorithm (e.g., nearest neighbour, bilinear and B-Spline etc.).</p><p>Combined with GSC and LRL, the whole network shows a characteristic of multilevel residual learning. Our experience shows that this can further stabilize the training process, and even help to slightly improve the model performance. Because the degeneration of MR training samples causes model training more unstable, it is especially helpful for the task of single MR image super-resolution.  <ref type="table" target="#tab_2">Layer  conv1  conv2  conv3  conv1  conv2  conv1  conv2  conv1  conv1  conv2  conv1  conv1,2  conv1  Filter  256  256  256  128  128  64  128  256  256  256  256?r 2  256?2 2  1|96  Kernel  3?3  1?1  3?3  3?3  3?3  3?3  3?3  1?1  1?1  3?3  3?3  3?3  3?3  Act  /  /  /  ReLU  /  ReLU  /  /  /  /  /  /  /   TABLE II  THE TESTING</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training Objective and Network Depth</head><p>Our model is a typical end-to-end mapping from LR images to HR images. The estimation of model parameters is achieved by minimizing the loss between the reconstructed HR images and the ground truth HR images. Given a training dataset D = {x (i) , y (i) }, i = 1, 2, . . . , |D|, where |D| is the total number of training samples, we use l 1 loss for model training:</p><formula xml:id="formula_14">L(?) = 1 |D| |D| i=1 ||y (i) ? F CSN (x (i) ; ?)|| 1 ,<label>(13)</label></formula><p>where ? indicates the set of model parameters, and F CSN (?) is the mapping function of the entire CSN model. y (i) is the HR target corresponding to the LR input x (i) . Despite that minimizing l 2 loss is generally preferred since it maximizes the peak signal to noise ratio (PSNR), l 1 loss provides better convergence for model training <ref type="bibr" target="#b30">[31]</ref>. This is especially helpful in case of the degradation of training samples. The depth of a deep network is usually defined as the longest path from the input to the output. Thus, the depth of the overall CSN model is given by:</p><formula xml:id="formula_15">D = n(2m + 1) + s + 6,<label>(14)</label></formula><p>where n is the number of CSBs in the entire network and m is the number of stage mappings in each CSB. s represents the depth of the pixel shuffle layer. Note that s depends on the scaling factor <ref type="bibr" target="#b30">[31]</ref>, i.e., s = 1 for SR?2 and SR?3, and s = 2 for SR?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section, we first introduce the generation of training examples and the implementation details. Then we investigate the impact of different configurations of CSB and the whole network on SR performance. Next, our CSN model is compared with several typical SISR methods under two common image degradations: bicubic downsampling (BD) and k-space truncation (TD). We use PSNR and structural similarity index metric (SSIM) <ref type="bibr" target="#b54">[54]</ref> as the metrics of quantitative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Sample Generation</head><p>The IXI dataset is used to construct our SR datasets, and it contains three types of MR images: 581 T1 volumes, 578 T2 volumes and 578 PD volumes. Firstly, we take the intersection of these three subsets, resulting in 576 3D volumes for each type of MR images. These 3D volumes are then clipped to the size of 240?240?96 (height?width?depth) to fit 3 scaling factors (?2, ?3 and ?4). In this work, we only focus on the in-plane SR of 2D MR slices. Therefore, each 3D MR volume contains 96 training samples with a single channel. The LR images are generated according to bicubic downsampling and k-space truncation. As for truncation degradation, the HR images are first converted into k-space by discrete Fourier transform (DFT) and then truncated along both height and width directions <ref type="figure" target="#fig_5">(Fig.4)</ref>. We randomly selected 500 volumes for training (D), 70 volumes for testing (T ) and the remaining At present, the model only targets at the task of single 2D MR image super-resolution. Thus, we have 500 ? 96 = 48000 training examples in a single training dataset. The generated datasets can be conveniently applied to develop 3D algorithms as each dimension is clipped to the common multiple of 2, 3 and 4, which will be a part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The configuration of the model is shown in <ref type="figure" target="#fig_3">Fig.2</ref> with n = m = 4. The size of minibatch and the number of feature maps are set to 16 and 256 respectively. For the dense branch within a CSB, the growth (g in <ref type="figure" target="#fig_4">Fig.3 and Fig.5</ref>) is set to 64. If not specified, the kernel size follows the annotation of <ref type="figure" target="#fig_3">Fig.2</ref>.</p><p>We train the models by using image patches of size 24?24 randomly extracted from LR slices with the corresponding HR patches. Data augmentation is simply implemented by random horizontal flips and 90 ? rotations, as <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b31">[32]</ref>. All models are implemented (or reimplemented) in TensorFlow 1.7.0 and trained on a NVIDIA GeForce GTX 1080 Ti GPU for one million iterations. We adopt Xavier initialization <ref type="bibr" target="#b56">[56]</ref> for all model parameters and Adam optimizer <ref type="bibr" target="#b55">[55]</ref> to minimize the loss by setting ? 1 = 0.9, ? 2 = 0.999 and = 10 ?8 . Learning rate is initialized as 10 ?4 for all layers and halved at every 2 ? 10 5 iterations i.e., piecewise constant decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Analysis</head><p>In this section, we study several components of the proposed model, including the structure of stage mapping, multilevel residual learning, global feature fusion and building block utilization. The structure of the entire network and the building block refers to <ref type="figure" target="#fig_3">Fig.2.</ref> 1) Channel Splitting Block: The proposed stage mapping can be configured in several ways, thus equipping different CSB modules. For comparison, we have studied the structure of different stage mappings from the following aspects: * If without channel splitting, four convolutional layers in a stage mapping correspond to a single convolutional layer with nearly the same number of model parameters. It is a reference structure of a stage mapping and we take it as the baseline, as shown in <ref type="figure" target="#fig_6">Fig.5(a)</ref>. * To investigate the role of the MAR mapping, we remove it from the proposed CSB and obtain the structure shown in <ref type="figure" target="#fig_6">Fig.5(e</ref>). We term it as CSN-SP, where S means splitting and P means plain. * We also design two stage mappings shown in <ref type="figure" target="#fig_6">Fig.5(b)</ref> and <ref type="figure" target="#fig_6">Fig.5(f)</ref> to check the effect of different branch structures on the performance of the model. They are referred as CSN-R3R3 and CSN-D3D3 respectively, where R and D represent residual branch and dense branch, and the numbers indicate the kernel size. * To study the impact of different kernel sizes, four other structures are designed. They are termed as CSN-R3D5 ( <ref type="figure" target="#fig_6">Fig.5(c)</ref>), CSN-R5D3 ( <ref type="figure" target="#fig_6">Fig.5(g)</ref>), CSN-R3R5 ( <ref type="figure" target="#fig_6">Fig.5(d)</ref>) and CSN-D3D5 ( <ref type="figure" target="#fig_6">Fig.5(h)</ref>), respectively.</p><p>The proposed stage mapping structure (as shown in <ref type="figure" target="#fig_3">Fig.2(b)</ref> and <ref type="figure" target="#fig_4">Fig.3(c)</ref>) is marked as CSN-R3D3. Both m and n are set to 4 for all experiments in this section. The performance of the compared stage mapping structures on V(T1, TD) for SR?2 is shown in <ref type="figure" target="#fig_7">Fig.6</ref>. It can be seen from <ref type="figure" target="#fig_7">Fig.6</ref>(a) that both channel splitting and the MAR mapping can significantly improve the model performance. According to <ref type="figure" target="#fig_7">Fig.6(b)</ref>, the performance of CSN-R3D3 is slightly better than that of CSN-R3R3 and CSN-D3D3. However, it is noteworthy that the model parameters R3R3&gt;R3D3&gt;D3D3, and the depth of these networks is the same, implying that mixing different branch structures is indeed helpful to boost the performance of the model, although slightly. It is observed from <ref type="figure" target="#fig_7">Fig.6(c)</ref> that the model performance R3R5&gt;R5D3&gt;R3D5&gt;D3D5&gt;R3D3. However, the parameters of the first four structures are about 1.6 times that of CSN-R3D3, causing a worse tradeoff between model performance and model scale. We can assume that their performance improvement on CSN-R3D3 is mainly due to the increase of model parameters. In addition, we can find that the residual branch favors better performance. The conclusions are further verified by the testing results shown in <ref type="table" target="#tab_2">Table II</ref>.</p><p>2) External Skip Connection: To investigate the impact of external skip connections (ESC), we build three other models according to our CSN model, two of which use nearest neighbor (NN) and bilinear respectively to approximate the residual between the original LR input x and the corresponding HR target y, and the other does not use ESC. They are termed as ESC-None, ESC-NN, ESC-Bilinear, and the one we use is termed as ESC-Bicubic. We train these models on D(PD, BD) and the validation performance is plotted in <ref type="figure" target="#fig_9">Fig.7</ref>. It can be easily observed that ESC-Bicubic perform significantly better than other models. The corresponding results on T (PD, BD) also illustrate this conclusion <ref type="table" target="#tab_2">(Table III)</ref>. Another important observation is that the ESC contributes to stable model training, no matter which interpolation method is used. Therefore, the ESC can reduce the possibility of training failure, which is also beneficial in the case of the degradation of training examples.</p><p>3) The Number of Stage Mappings and Blocks: It can be seen from <ref type="bibr" target="#b13">(14)</ref> that the network depth D is mainly determined by the number of CSBs m and the number of stage mappings n. We examine the impact of these two hyperparameters on the performance of the model. Firstly, we fix m to 4 and change n from 1 to 4. <ref type="figure" target="#fig_10">Fig.8(a)</ref> displays the evolution curves of PSNR performance on V(T2, TD) for SR?2. It can be seen that the performance is improved gradually with the increased number of building blocks, but at the expense of increased parameters. Next, we fix n to 4 and change m from 1 to 4. The PSNR curves of the models on the same dataset are plotted in <ref type="figure" target="#fig_10">Fig.8(b)</ref>. We observe a similar trend of the curves as m changes. The result is unsurprising because increasing m or n increases the network depth and model parameters.</p><p>Finally, we show the final SR performance of all compared models on the corresponding testing dataset T (T2, TD) in <ref type="figure" target="#fig_10">Fig.8(c)</ref>, versus the number of parameters. It is worth noting that the models with t building blocks and 4 stage mappings perform better than the models with 4 building blocks and t stage mappings (t = 1, 2, 3), although the former has fewer model parameters. In next experiments, we choose n = m = 4 for our CSN model. Therefore, the network depth is 43 for SR?2 and SR?3, and 44 for SR?4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with Other Methods</head><p>To further illustrate the effectiveness of the proposed CSN model, we compare it with several advanced SISR methods quantitatively and qualitatively, including NLM <ref type="bibr" target="#b57">[57]</ref>, SRCNN <ref type="bibr" target="#b16">[17]</ref>, VDSR <ref type="bibr" target="#b21">[22]</ref>, RDN <ref type="bibr" target="#b31">[32]</ref>, CMSCN <ref type="bibr" target="#b36">[37]</ref>, FSCWRN <ref type="bibr" target="#b27">[28]</ref> and EDSR <ref type="bibr" target="#b30">[31]</ref>. These models are retrained on the generated datasets with all image types and scaling factors. As mentioned earlier, the degradation of training samples may lead to training failure of some models, especially for those with extremely deep structure and large number of parameters, e.g., EDSR <ref type="bibr" target="#b30">[31]</ref>. To solve the problem, we train the EDSR by taking 96 slices of a 3D volume as 96 channels of a 2D sample. This can effectively avoid the training failure problem but at the cost of accuracy reduction. We attach C1 and C96 to the model name to mark these two cases. For fair comparison, we train both CSN (C1) and CSN (C96).  <ref type="bibr" target="#b21">[22]</ref>, RDN <ref type="bibr" target="#b31">[32]</ref>, CMSCN <ref type="bibr" target="#b36">[37]</ref>, FSCWRN <ref type="bibr" target="#b27">[28]</ref>, EDSR <ref type="bibr" target="#b30">[31]</ref> and our CSN) present great advantages over the traditional methods (Bicubic and NLM <ref type="bibr" target="#b57">[57]</ref>). However, the proposed CSN model gives the best SR performance in both C1 and C96 cases although it has fewer parameters and shallower model structures than RDN <ref type="bibr" target="#b31">[32]</ref> and EDSR <ref type="bibr" target="#b30">[31]</ref>. <ref type="figure">Fig.9</ref> displays the visual comparison of these methods under the bicubic degradation. The top row shows the result on a PD image with scaling factor SR?3. It can be observed that our CSN model has successfully restored the black area and it has the most similar shape to the ground truth. However, several other methods, such as Bicubic, NLM <ref type="bibr" target="#b57">[57]</ref>, SRCNN <ref type="bibr" target="#b16">[17]</ref> and even VDSR <ref type="bibr" target="#b21">[22]</ref> almost lost the black area. The middle row is the result on a T1 image with SR?4. There is a gray ridge at the position marked by the red arrow, which can hardly be recognized in the results of other methods, but our model gives a more credible indication of the ridge. Similar results can also be observed from the bottom row, which shows the comparison on a T2 image with SR?4. There is a dark ditch at the position marked by the red arrow, and only our CSN model has succeeded in restoring this information.</p><p>2) Truncation Degradation (TD): k-space truncation of HR images is a process that simulates the real image acquisition process where a LR image is scanned by reducing acquisition lines in both phase and slice encoding directions. The missing information is therefore in k-space and the degradation pattern of LR images is different from simply shrinking the size of HR images in the image domain by, e.g., bicubic interpolation <ref type="bibr" target="#b24">[25]</ref>. Table IV also shows the quantitative comparison between different methods under the truncation degradation. Again, the proposed CSN model presents the best SR performance in both C1 and C96 cases. It is worth noting that the performance of Bicubic, NLM <ref type="bibr" target="#b57">[57]</ref>, SRCNN <ref type="bibr" target="#b16">[17]</ref> and VDSR <ref type="bibr" target="#b21">[22]</ref> is slightly worse than that of these methods in case of BD, e.g., SR?2 on T2 images. On the contrary, the performance of other methods in case of TD is better than that in case of BD. This is probably <ref type="bibr" target="#b25">26</ref>  <ref type="bibr" target="#b30">[31]</ref> (b) k-space truncation (TD) <ref type="figure" target="#fig_1">Fig. 11</ref>. The visual comparison of EDSR <ref type="bibr" target="#b30">[31]</ref> and the proposed CSN model in case of C96. (a) a T1 image with scaling factor SR?2. (b) a T2 image with scaling factor SR?4. In this case, each testing example is a 3D volume. For both (a) and (b), a randomly selected slice is used for display purposes. because TD degrades image quality more seriously than BD and models such as RDN <ref type="bibr" target="#b31">[32]</ref>, CMSCN <ref type="bibr" target="#b36">[37]</ref>, FSCWRN <ref type="bibr" target="#b27">[28]</ref> and our CSN have better representational capacity than models such as SRCNN <ref type="bibr" target="#b16">[17]</ref> and VDSR <ref type="bibr" target="#b21">[22]</ref>. <ref type="figure" target="#fig_1">Fig.10</ref> presents the visual effects of the compared methods in case of TD and the proposed CSN model presents obvious advantages over other models. For instance, the bottom row is the results on a T2 image with SR?4. Our CSN model is able to reconstruct the dark contour at the position indicated by the red arrow, which cannot be clearly observed in the results of other models. The top and middle rows show the results on a PD and a T1 image, also highlighting the advantages of the proposed CSN model. <ref type="figure" target="#fig_1">Fig.11</ref> shows the visual comparison between the EDSR <ref type="bibr" target="#b30">[31]</ref> and our CSN (C96) model in case of C96. Both BD and TD are presented. In this case, our model has less obvious advantages over EDSR <ref type="bibr" target="#b30">[31]</ref>, but still performs better on the whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION AND FUTURE WORK A. Multiple Branches</head><p>Like the original MAR mapping in <ref type="bibr" target="#b35">[36]</ref>, the stage mapping in our CSB can also be easily extended to multiple branches <ref type="bibr" target="#b2">( 3)</ref>. The difference is that we branch the network by channel splitting, instead of feature reuse. In extreme cases, it can be extended to c branches with each branch occupying one channel of the input feature. This means that we explicitly differentiate the hierarchical features rather than having the network learn to distinguish between different features. Therefore, when the training samples are degraded and the model is complex, it helps to ease model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth and Width</head><p>Branching the network by reusing the entire feature tensor makes the model much wider, like <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. This significantly increases model parameters when the network depth is the same. Since EDSR <ref type="bibr" target="#b30">[31]</ref> is a typical network with very wide structure and causes training failure, while RDN <ref type="bibr" target="#b31">[32]</ref> is a deeper but less wide network and can be successfully trained. Therefore, we speculate that the width of the model may also be one of the reasons for training failure in case of training sample degradation. Our work can be regarded as a manner to going deeper with nearly unchanged model width and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Branch Structure</head><p>Currently, we only utilize the structures similar to ResNet <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b32">[33]</ref> and DenseNet <ref type="bibr" target="#b34">[35]</ref> (or RDN <ref type="bibr" target="#b31">[32]</ref>) for different branches. The experimental results show that mixing different branch structures is helpful to improve the performance of the model, but it is not conspicuous. This is probably because the structural difference between the two branches is relatively small. We conjecture that as the structural difference of the branches increases, so does the performance difference. The further investigation will be a part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. 3D Extension</head><p>The present work only aims at the task of 2D MR image super-resolution, and the further extension could be in 3D case. However, since many types of medical images are in 3D format, it is intuitively possible to further enhance SR performance if the 3D structural information can be reasonably utilized <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>. A prominent problem in the 3D settings is that the number of parameters will increase dramatically as the network depth increases, leading model training more difficult. Our model can deepen the network without significantly increasing the model width and parameters, which also helps to extend 3D models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Information Sharing</head><p>In this paper, we only deal with the SR task for a single type of 2D MR images and a single scaling factor. However, there is evidence that combining the information from different image types and scaling factors is helpful to improve the performance of deep models <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b30">[31]</ref>. The medical images SR framework combined multi-type and multi-scale information is also expected to further improve the SR performance of deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>A major problem with using deep models to super-resolve MR images is the lack of high-quality and effective training samples, which probably leads to performance degradation or even training failure of deep models. In this work, we have presented a novel deep channel splitting network (CSN) for the task of 2D MR image super-resolution, which is primarily made up of a series of cascaded channel splitting blocks (CSBs). The hierarchical features are split into two branches with different information propagations (residual branch and dense branch), which helps the model to discriminate different features explicitly. To integrate branch information, the MAR <ref type="bibr" target="#b35">[36]</ref> mapping is also applied to merge the hierarchical features on different branches.</p><p>Channel splitting helps to increase the depth of the network and the diversity of processing the hierarchical features. We conjecture that the performance improvement of the proposed model benefits from both two parts and additional performance can be further gained by exploring other branch structures and information fusion strategies. As it improves the dilemma between improving model performance and easing model training to some extent, it also has the potential to deal with other types of medical images, such as CT, ultrasound and PET etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The performance comparison between several SISR models on proton density (PD) volumes of IXI dataset for SR?2. (a) The validation results on 6 PD volumes (576 2D slices). (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) The overall architecture of the proposed two-way channel splitting network (CSN) The internal structure of each channel splitting block (CSB)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>The diagram of the proposed CSN model. (a) The overall structure consists of three parts: feature extraction F E (?), nonlinear mapping F M (?) and image reconstruction F F (?) + F R (?). (b) Channel splitting block (CSB). The intermediate feature maps within a CSB are split into two branches along the channel direction. One is built as a residual-like structure (residual branch, top), and the other is built as a dense-like structure (dense branch, bottom). Red arrows in GFF<ref type="bibr" target="#b31">[32]</ref> and dense branch indicate dense connection (channel concatenation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>The merge-and-run stage mappings in a building block. (a) Original mapping<ref type="bibr" target="#b35">[36]</ref>. (b) Multi-scale cross (MSC) mapping</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>The illustration of k-space truncation degradation (TD) for SR?4. Different from bicubic downsampling, the generated LR images are sometimes accompanied by Gibbs-ringing artifacts. The result in (d) is generated by zero-padding in k-space and inverse Fourier transform (IFT) for display purpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Several stage mapping structures for comparison. The nonlinear function ReLU between two conv layers is omitted for simplification, and red arrows represent dense connection. NoteFig.3(a)is different from CSN-R3R3 and there is a ReLU layer after the conv layer in (a). Please refer toFig.2(b)for the overall structure of a CSB and the detailed structure of CSN-R3D3 stage mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>The performance comparison between different structures shown in Fig.5 on V(T1, TD) for SR?2 (bicubic = 31.72dB). (a) Channel splitting and the merge-and-run mapping. (b) Different branch structures. (c) Different kernel sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>PERFORMANCE OF DIFFERENT STRUCTURES SHOWN IN FIG.5 ON T (T1, TD) FOR SR?2. THE MAXIMAL VALUES OF EACH COLUMN ARE IN RED, AND THE SECOND ONES ARE IN BLUE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>UnstableFig. 7 .</head><label>7</label><figDesc>The impact of different ESC approximations on model training and performance. The comparison is carried out on D(PD, BD) for SR?2. Note that ESC-None shows obvious instability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Comparison on T (T2, TD) The performance comparison between the models with different number of stage mappings and building blocks. (a) and (b): The validation performance of the models on V(T2, TD) (SR?2, Bicubic: 31.92dB). (c) The testing performance of all compared models on T (T2, TD) (SR?2, Bicubic: 33.06dB).for quick validation (V). We employ the convention: dataset name (MR image type, image degradation model), to indicate a specific sub dataset for convenience. For instance, D(T2, BD) represents the T2 training dataset with bicubic degradation and T (PD, TD) represents the PD testing dataset with k-space truncation degradation. The processed datasets are available at: https://pan.baidu.com/s/1Ak3GiJk5H1Pdn3igzElb7w (kn3d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>received October 12, 2018; revised March 17, 2019; accepted May 30, 2019. Date of publication XX XX, 2019; date of current version XX XX, 2019. This work is supported in part by Sichuan Science and Technology Program under grant 2019YJ0181, and in part by National Key Research and Development Program of China under grants 2016YFC0100800 and 2016YFC0100802. (Corresponding author: Tao Zhang.) X. Zhao is with the School of Life Science and Technology, University of Electronic Science and Technology of China (UESTC), Chengdu, Sichuan 611731, China (e-mail: zxlation@foxmail.com). T. Zhang and X. Zou are with the High Field Magnetic Resonance Brain Imaging Laboratory of Sichuan and Key Laboratory for NeuroInformation of Ministry of Education, Chengdu, Sichuan 611731, China; They are also with the School of Life Science and Technology, University of Electronic Science and Technology of China (UESTC), Chengdu, Sichuan 611731, China (e-mail: taozhangjin@gmail.com; mark.zou@alltechmed.com). Y. Zhang is with the Department of Electrical and Computer Engineering, Northeastern University, Boston, MA 02115, USA (e-mail: yu-lun100@gmail.com).</figDesc><table /><note>Digital Object Identifier XXXX</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I THE</head><label>I</label><figDesc>DETAILED CONFIGURATION OF THE PROPOSED CSN MODEL. ONLY ONE SINGLE STAGE MAPPING IS SHOWN IN NMN DUE TO THE EXACTLY SAME STRUCTURE OF EACH CSB AND EACH STAGE MAPPING. ALL CONV LAYERS ARE PADDED TO KEEP THE SIZE OF FEATURE MAPS UNCHANGED.</figDesc><table><row><cell>Config</cell><cell>FEN (three conv layers) General convolution</cell><cell>NMN (stage mapping ? m = CSB, CSB ? n) Residual branch Dense branch Merge</cell><cell>IRN (Upscaling depends on the scaling factor r) Feature Fusion Upscale (r = 2|3, 4) Recover</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>OF THE MODELS WITH DIFFERENT ESC APPROXIMATIONS ON T (PD, BD). THE MAXIMAL PSNR AND SSIM VALUES OF EACH ROW ARE IN RED, AND THE SECOND ONES ARE IN BLUE.</figDesc><table><row><cell cols="2">Network Configuration</cell><cell cols="2">Network Parameters</cell><cell>Network Depth</cell><cell>PSNR (dB)</cell><cell>SSIM</cell></row><row><cell cols="2">Bicubic</cell><cell>/</cell><cell></cell><cell>/</cell><cell>33.38</cell><cell>0.9460</cell></row><row><cell cols="2">baseline</cell><cell cols="2">13643521</cell><cell>27</cell><cell>38.37</cell><cell>0.9803</cell></row><row><cell cols="2">CSN-SP</cell><cell cols="2">13646593</cell><cell>43</cell><cell>38.46</cell><cell>0.9807</cell></row><row><cell cols="2">CSN-R3D3</cell><cell cols="2">13646593</cell><cell>43</cell><cell>38.62</cell><cell>0.9813</cell></row><row><cell cols="2">CSN-R3R3</cell><cell cols="2">13647614</cell><cell>43</cell><cell>38.61</cell><cell>0.9813</cell></row><row><cell cols="2">CSN-D3D3</cell><cell cols="2">13645566</cell><cell>43</cell><cell>38.59</cell><cell>0.9812</cell></row><row><cell cols="2">CSN-R3D5</cell><cell cols="2">22035198</cell><cell>43</cell><cell>38.67</cell><cell>0.9815</cell></row><row><cell cols="2">CSN-R5D3</cell><cell cols="2">22035198</cell><cell>43</cell><cell>38.68</cell><cell>0.9816</cell></row><row><cell cols="2">CSN-R3R5</cell><cell cols="2">22036225</cell><cell>43</cell><cell>38.70</cell><cell>0.9817</cell></row><row><cell cols="2">CSN-D3D5</cell><cell cols="2">22034177</cell><cell>43</cell><cell>38.64</cell><cell>0.9814</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE III</cell></row><row><cell cols="3">ESC-TEST RESULTS scale None</cell><cell></cell><cell>ESC-NN</cell><cell>ESC-Bilinear</cell><cell>ESC-Bicubic</cell></row><row><cell>?2</cell><cell cols="2">41.20/0.9893</cell><cell cols="2">41.19/0.9893</cell><cell>41.18/0.9893</cell><cell>41.28/0.9895</cell></row><row><cell>?3</cell><cell cols="2">35.80/0.9688</cell><cell cols="2">35.79/0.9688</cell><cell>35.82/0.9689</cell><cell>35.87/0.9693</cell></row><row><cell>?4</cell><cell cols="2">33.32/0.9478</cell><cell cols="2">33.31/0.9482</cell><cell>33.34/0.9483</cell><cell>33.40/0.9486</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV QUANTITATIVE</head><label>IV</label><figDesc>COMPARISON BETWEEN DIFFERENT METHODS ON 6 TEST DATASETS (2 IMAGE DEGRADATIONS AND 3 MR IMAGE TYPES). THE MAXIMAL PSNR (DB) AND SSIM VALUES OF EACH COMPARISON GROUP ARE MARKED IN RED, AND THE SECOND ONES ARE MARKED IN BLUE (PSNR / SSIM).</figDesc><table><row><cell>method \ dataset</cell><cell>mode</cell><cell>scale</cell><cell>PD</cell><cell cols="3">bicubic downsampling T (:, BD) T1</cell><cell>T2</cell><cell>PD</cell><cell>k-space truncation T (:, TD) T1</cell><cell>T2</cell></row><row><cell>Bicubic [2D]</cell><cell></cell><cell>?2</cell><cell cols="2">35.04 / 0.9664</cell><cell>33.80 / 0.9525</cell><cell cols="2">33.44 / 0.9589</cell><cell cols="2">34.65 / 0.9625</cell><cell>33.38 / 0.9460</cell><cell>33.06 / 0.9541</cell></row><row><cell>NLM [57]</cell><cell></cell><cell>?2</cell><cell cols="2">37.26 / 0.9773</cell><cell>35.80 / 0.9685</cell><cell cols="2">35.58 / 0.9722</cell><cell cols="2">36.18 / 0.9707</cell><cell>34.71 / 0.9581</cell><cell>34.56 / 0.9641</cell></row><row><cell>SRCNN [17]</cell><cell></cell><cell>?2</cell><cell cols="2">38.96 / 0.9836</cell><cell>37.12 / 0.9761</cell><cell cols="2">37.32 / 0.9796</cell><cell cols="2">38.23 / 0.9802</cell><cell>36.52 / 0.9705</cell><cell>37.04 / 0.9773</cell></row><row><cell>VDSR [22] RDN [32]</cell><cell>C1</cell><cell>?2 ?2</cell><cell cols="2">39.97 / 0.9861 40.31 / 0.9870</cell><cell>37.67 / 0.9783 37.95 / 0.9795</cell><cell cols="2">38.65 / 0.9836 38.75 / 0.9838</cell><cell cols="2">39.89 / 0.9850 40.39 / 0.9862</cell><cell>37.58 / 0.9760 38.08 / 0.9784</cell><cell>38.74 / 0.9823 40.02 / 0.9826</cell></row><row><cell>CMSCN [37]</cell><cell></cell><cell>?2</cell><cell cols="2">40.84 / 0.9883</cell><cell>38.06 / 0.9800</cell><cell cols="2">39.54 / 0.9857</cell><cell cols="2">41.14 / 0.9882</cell><cell>38.23 / 0.9795</cell><cell>39.63 / 0.9845</cell></row><row><cell>FSCWRN [28]</cell><cell></cell><cell>?2</cell><cell cols="2">40.72 / 0.9880</cell><cell>37.98 / 0.9797</cell><cell cols="2">39.44 / 0.9855</cell><cell cols="2">40.91 / 0.9876</cell><cell>38.04 / 0.9786</cell><cell>39.82 / 0.9851</cell></row><row><cell>CSN [Ours]</cell><cell></cell><cell>?2</cell><cell cols="2">41.28 / 0.9895</cell><cell>38.27 / 0.9810</cell><cell cols="2">39.71 / 0.9863</cell><cell cols="2">41.77 / 0.9897</cell><cell>38.62 / 0.9813</cell><cell>40.47 / 0.9868</cell></row><row><cell>EDSR [31] CSN [Ours]</cell><cell>C96</cell><cell>?2 ?2</cell><cell cols="2">39.87 / 0.9857 40.15 / 0.9865</cell><cell>37.56 / 0.9774 37.60 / 0.9778</cell><cell cols="2">38.28 / 0.9824 38.53 / 0.9831</cell><cell cols="2">39.47 / 0.9837 39.50 / 0.9839</cell><cell>37.09 / 0.9741 36.99 / 0.9737</cell><cell>38.11 / 0.9803 38.20 / 0.9807</cell></row><row><cell>Bicubic [2D]</cell><cell></cell><cell>?3</cell><cell cols="2">31.20 / 0.9230</cell><cell>30.15 / 0.8900</cell><cell cols="2">29.80 / 0.9093</cell><cell cols="2">30.88 / 0.9167</cell><cell>29.79 / 0.8793</cell><cell>29.50 / 0.9016</cell></row><row><cell>NLM [57]</cell><cell></cell><cell>?3</cell><cell cols="2">32.81 / 0.9436</cell><cell>31.74 / 0.9216</cell><cell cols="2">31.28 / 0.9330</cell><cell cols="2">32.02 / 0.9324</cell><cell>30.83 / 0.9027</cell><cell>30.57 / 0.9197</cell></row><row><cell>SRCNN [17]</cell><cell></cell><cell>?3</cell><cell cols="2">33.60 / 0.9516</cell><cell>32.17 / 0.9276</cell><cell cols="2">32.20 / 0.9440</cell><cell cols="2">32.90 / 0.9432</cell><cell>31.72 / 0.9187</cell><cell>31.80 / 0.9381</cell></row><row><cell>VDSR [22] RDN [32]</cell><cell>C1</cell><cell>?3 ?3</cell><cell cols="2">34.66 / 0.9599 35.08 / 0.9628</cell><cell>32.91 / 0.9378 33.31 / 0.9430</cell><cell cols="2">33.47 / 0.9559 33.91 / 0.9591</cell><cell cols="2">34.27 / 0.9555 35.00 / 0.9609</cell><cell>32.57 / 0.9304 33.33 / 0.9416</cell><cell>33.23 / 0.9515 33.99 / 0.9576</cell></row><row><cell>CMSCN [37]</cell><cell></cell><cell>?3</cell><cell cols="2">35.26 / 0.9641</cell><cell>33.25 / 0.9423</cell><cell cols="2">34.16 / 0.9613</cell><cell cols="2">35.41 / 0.9638</cell><cell>33.18 / 0.9398</cell><cell>34.45 / 0.9611</cell></row><row><cell>FSCWRN [28]</cell><cell></cell><cell>?3</cell><cell cols="2">35.37 / 0.9653</cell><cell>33.24 / 0.9423</cell><cell cols="2">34.27 / 0.9618</cell><cell cols="2">35.30 / 0.9636</cell><cell>33.09 / 0.9390</cell><cell>34.34 / 0.9603</cell></row><row><cell>CSN [Ours]</cell><cell></cell><cell>?3</cell><cell cols="2">35.87 / 0.9693</cell><cell>33.53 / 0.9464</cell><cell cols="2">34.64 / 0.9647</cell><cell cols="2">36.09 / 0.9697</cell><cell>33.68 / 0.9464</cell><cell>34.95 / 0.9653</cell></row><row><cell>EDSR [31] CSN [Ours]</cell><cell>C96</cell><cell>?3 ?3</cell><cell cols="2">34.39 / 0.9578 34.68 / 0.9598</cell><cell>32.76 / 0.9347 32.83 / 0.9360</cell><cell cols="2">33.15 / 0.9528 33.36 / 0.9547</cell><cell cols="2">33.97 / 0.9531 34.12 / 0.9540</cell><cell>32.27 / 0.9274 32.25 / 0.9266</cell><cell>32.89 / 0.9482 33.00 / 0.9490</cell></row><row><cell>Bicubic [2D]</cell><cell></cell><cell>?4</cell><cell cols="2">29.13 / 0.8799</cell><cell>28.28 / 0.8312</cell><cell cols="2">27.86 / 0.8611</cell><cell cols="2">28.82 / 0.8713</cell><cell>27.96 / 0.8182</cell><cell>27.60 / 0.8511</cell></row><row><cell>NLM [57]</cell><cell></cell><cell>?4</cell><cell cols="2">30.27 / 0.9044</cell><cell>29.31 / 0.8655</cell><cell cols="2">28.85 / 0.8875</cell><cell cols="2">29.27 / 0.8906</cell><cell>28.68 / 0.8439</cell><cell>28.37 / 0.8718</cell></row><row><cell>SRCNN [17]</cell><cell></cell><cell>?4</cell><cell cols="2">31.10 / 0.9181</cell><cell>29.90 / 0.8796</cell><cell cols="2">29.69 / 0.9052</cell><cell cols="2">30.52 / 0.9078</cell><cell>29.31 / 0.8616</cell><cell>29.32 / 0.8960</cell></row><row><cell>VDSR [22] RDN [32]</cell><cell>C1</cell><cell>?4 ?4</cell><cell cols="2">32.09 / 0.9311 32.73 / 0.9387</cell><cell>30.57 / 0.8932 31.05 / 0.9042</cell><cell cols="2">30.79 / 0.9240 31.45 / 0.9324</cell><cell cols="2">31.69 / 0.9244 32.64 / 0.9362</cell><cell>30.14 / 0.8818 31.00 / 0.9018</cell><cell>30.51 / 0.9162 31.49 / 0.9301</cell></row><row><cell>CMSCN [37]</cell><cell></cell><cell>?4</cell><cell cols="2">32.53 / 0.9374</cell><cell>30.83 / 0.8997</cell><cell cols="2">31.32 / 0.9312</cell><cell cols="2">32.23 / 0.9321</cell><cell>30.55 / 0.8920</cell><cell>31.28 / 0.9278</cell></row><row><cell>FSCWRN [28]</cell><cell></cell><cell>?4</cell><cell cols="2">32.91 / 0.9415</cell><cell>30.96 / 0.9022</cell><cell cols="2">31.71 / 0.9359</cell><cell cols="2">32.78 / 0.9387</cell><cell>30.79 / 0.8973</cell><cell>31.71 / 0.9334</cell></row><row><cell>CSN [Ours]</cell><cell></cell><cell>?4</cell><cell cols="2">33.40 / 0.9486</cell><cell>31.23 / 0.9093</cell><cell cols="2">32.05 / 0.9413</cell><cell cols="2">33.51 / 0.9489</cell><cell>31.27 / 0.9092</cell><cell>32.28 / 0.9421</cell></row><row><cell>EDSR [31] CSN [Ours]</cell><cell>C96</cell><cell>?4 ?4</cell><cell cols="2">31.80 / 0.9284 32.19 / 0.9325</cell><cell>30.46 / 0.8902 30.53 / 0.8915</cell><cell cols="2">30.52 / 0.9198 30.81 / 0.9231</cell><cell cols="2">31.44 / 0.9219 31.72 / 0.9246</cell><cell>30.04 / 0.8803 30.07 / 0.8794</cell><cell>30.31 / 0.9137 30.54 / 0.9163</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Bicubic downsampling is a widely used simulation of LR image generation in image SR 33.39 / 0.9325 34.66 / 0.9464 35.65 / 0.9575 36.47 / 0.9632 36.84 / 0.9658 36.78 / 0.9657 37.04 / 0.9673 37.45 / 0.9703 PSNR / SSIM .8811 29.48 / 0.9064 30.56 / 0.9242 31.91 / 0.9427 32.77 / 0.9507 32.65 / 0.9498 33.15 / 0.9543 33.65 / 0.9596 PSNR / SSIMFig. 9. The visual effect of the compared methods on a PD (top), T1 (middle) and T2 (bottom) image with SR?3, SR?4 and SR?4, respectively. The image degradation is bicubic downsampling (C1). The maximal PSNR (dB) and SSIM values for each displayed image are in red, and the second ones are in blue.settings which simply shrinks HR images to a smaller size with the bicubic kernel. We examine this image degradation in this section first. Columns 4 to 6 ofTable IVshow that the quantitative results of the compared methods over the testing datasets under this image degradation. Overall, all the deep learning based methods (SRCNN<ref type="bibr" target="#b16">[17]</ref>, VDSR</figDesc><table><row><cell>Bicubic</cell><cell>NLM [57]</cell><cell>SRCNN [17]</cell><cell>VDSR [22]</cell><cell>RDN [32]</cell><cell>CMSCN [37] FSCWRN [28]</cell><cell>CSN [Ours]</cell><cell>Ground Truth</cell></row><row><cell cols="8">25.52 / 0.7978 26.53 / 0.8391 27.03 / 0.8542 27.61 / 0.8681 28.08 / 0.8817 27.84 / 0.8763 27.98 / 0.8795 28.24 / 0.8884 PSNR / SSIM</cell></row><row><cell>28.38 / 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1) Bicubic Degradation (BD):</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>.76 / 0.8433 27.69 / 0.8698 28.47 / 0.8901 29.47 / 0.9089 30.31 / 0.9208 30.01 / 0.9179 30.45 / 0.9243 31.30 / 0.9378 PSNR / SSIM .8139 26.12 / 0.8356 26.94 / 0.8606 27.97 / 0.8830 28.84 / 0.8994 28.69 / 0.8969 29.04 / 0.9035 29.58 / 0.9153 PSNR / SSIM 25.70 / 0.8022 26.41 / 0.8290 27.16 / 0.8513 28.14 / 0.8741 28.79 / 0.8932 28.46 / 0.8838 28.62 / 0.8888 29.02 / 0.8994 PSNR / SSIMFig. 10. The visual effect of the compared methods on a PD (top), T1 (middle) and T2 (bottom) image with scaling factor SR?4. The image degradation is k-space truncation (C1). The maximal PSNR (dB) and SSIM values for each displayed image are in red, and the second ones are in blue.</figDesc><table><row><cell>Bicubic</cell><cell>NLM [57]</cell><cell>SRCNN [17]</cell><cell>VDSR [22]</cell><cell cols="2">RDN [32]</cell><cell cols="3">CMSCN [37] FSCWRN [28]</cell><cell>CSN [Ours]</cell><cell>Ground Truth</cell></row><row><cell>25.46 / 0Bicubic</cell><cell>EDSR [31]</cell><cell>CSN [Ours]</cell><cell>Ground Truth</cell><cell></cell><cell></cell><cell>Bicubic</cell><cell>EDSR</cell><cell>CSN [Ours]</cell><cell>Ground Truth</cell></row><row><cell cols="5">31.29 / 0.9432 34.92 / 0.9728 34.99 / 0.9734 PSNR / SSIM</cell><cell cols="4">26.93 / 0.8286 29.30 / 0.8949 29.42 / 0.8963 PSNR / SSIM</cell></row><row><cell></cell><cell cols="2">(a) bicubic downsampling (BD)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Resolution enhancement in MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Carmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fiat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imag</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="154" />
			<date type="published" when="2006-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super-resolution methods in MRI: can they improve the trade-off between resolution, signal-tonoise ratio, and acquisition time?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Plenge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H J</forename><surname>Poot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1983" to="1993" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Single image super resolution of 3D MRI using local regression and intermodality priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Digit. Image Process</title>
		<meeting>Int. Conf. Digit. Image ess</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10033</biblScope>
			<biblScope unit="page">100334</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cardiac image superresolution with global correspondence using multi-atlas patchmatch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image. Comput. Comput. Assist. Interv</title>
		<editor>Med</editor>
		<imprint>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Aerial spectral super-resolution using conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangnekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mokashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ientilucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<ptr target="http://cn.arxiv.org/abs/1712.08690" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sub-pixel mapping of rural land cover objects from fine spatial resolution satellite sensor imagery using super-resolution pixel-swapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="491" />
			<date type="published" when="2006-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Superresolution based on compressive sensing and structural self-similarity for remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4864" to="4876" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An integrated interpolation-based super resolution reconstruction algorithm for video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Commun</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="464" to="472" />
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the gibbs phenomenon and its resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Review</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="644" to="668" />
			<date type="published" when="1997-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Graph. Model. Image Process</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="239" />
			<date type="published" when="1991-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High resolution image recovery from image plane arrays using convex projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Oskoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1715" to="1726" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Example-based superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Example-based super-resolution via structure analysis of patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Ra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="407" to="410" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coupled Dictionary Training for Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analy. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2790" to="2798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Memnet: a persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="4549" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Brain MRI super-resolution using deep 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ducournau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fablet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Biomed. Imag. (ISBI)</title>
		<meeting>IEEE Int. Symp. Biomed. Imag. (ISBI)</meeting>
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="197" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Brain MRI super resolution using 3D deep densely connected neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://cn.arxiv.org/abs/1801.02728" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Self super-resolution for magnetic resonance images using deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Dewey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.09431" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient and accurate MRI super-resolution using a generative adversarial network and 3D multi-level densely connected network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.01417" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MR image superresolution via wide residual networks with fixed skip connection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Super-resolution reconstruction of MR image with a novel residual learning network Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics in Medicine and Biology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">85011</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops. (CVPRW)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. Workshops. (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.08797" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1608.06993" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep convolutional neural networks with merge-and-run mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Single image super-resolution via cascaded multi-scale cross network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1602.07261" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Super-resolution using neural networks based on the optimal recovery theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Electronics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="281" />
			<date type="published" when="2006-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<ptr target="http://cn.arxiv.org/abs/1609.05158v2" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Superresolution in MRI: application to human white matter fiber tract visualization by diffusion tensor imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mag. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="35" />
			<date type="published" when="2001-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">MRI inter-slice reconstruction using super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Oz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kiryati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peled</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imag</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="437" to="446" />
			<date type="published" when="2002-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A super-resolution framework for 3-D highresolution and high-contrast imaging using 2-D multislice MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Z</forename><surname>Shilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="633" to="644" />
			<date type="published" when="2009-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Brain hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="497" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MRI super-resolution using self-similarity and image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Manj?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Coup?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robles</surname></persName>
		</author>
		<idno>ID: 425891</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Biomed. Imaging</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
	<note>11 pages</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Single-image super-resolution of brain MR images using over complete dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Malpica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="132" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sparse representation-based MRI super-resolution reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Roddick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measurement</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="946" to="953" />
			<date type="published" when="2014-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Superresolution MRI images using Compressive Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noorhosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahmati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iranian Conference on Electrical Engineering (ICEE2012)</title>
		<imprint>
			<date type="published" when="2012-05" />
			<biblScope unit="page" from="1618" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-input cardiac image super-resolution using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Image Comput</title>
		<meeting>Int. Conf. Med. Image Comput</meeting>
		<imprint>
			<date type="published" when="2016-10" />
			<biblScope unit="page" from="246" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">End-to-end image superresolution via deep and shallow convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image super resolution based on fusing multiple convolution neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elkhamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis. Pattern Recognit. Workshops</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="1050" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Fast and accurate image super resolution by deep CNN with skip connection and network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kuwashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.05425" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Dual path networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1707.01629" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proces</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.6980v9" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS10</title>
		<meeting>AISTATS10</meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Nonlocal MRI upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Manjon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Coupe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Robles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="784" to="792" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Fast, accurate, and lightweight superresolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="256" to="272" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
