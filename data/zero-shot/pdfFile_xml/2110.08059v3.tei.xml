<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FLEXCONV: CONTINUOUS KERNEL CONVOLUTIONS WITH DIFFERENTIABLE KERNEL SIZES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert-Jan</forename><surname>Bruintjes</surname></persName>
							<email>r.bruintjes@tudelft.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">Delft University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hoogendoorn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Delft University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FLEXCONV: CONTINUOUS KERNEL CONVOLUTIONS WITH DIFFERENTIABLE KERNEL SIZES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When designing Convolutional Neural Networks (CNNs), one must select the size of the convolutional kernels before training. Recent works show CNNs benefit from different kernel sizes at different layers, but exploring all possible combinations is unfeasible in practice. A more efficient approach is to learn the kernel size during training. However, existing works that learn the kernel size have a limited bandwidth. These approaches scale kernels by dilation, and thus the detail they can describe is limited. In this work, we propose FlexConv, a novel convolutional operation with which high bandwidth convolutional kernels of learnable kernel size can be learned at a fixed parameter cost. FlexNets model long-term dependencies without the use of pooling, achieve state-of-the-art performance on several sequential datasets, outperform recent works with learned kernel sizes, and are competitive with much deeper ResNets on image benchmark datasets. Additionally, FlexNets can be deployed at higher resolutions than those seen during training. To avoid aliasing, we propose a novel kernel parameterization with which the frequency of the kernels can be analytically controlled. Our novel kernel parameterization shows higher descriptive power and faster convergence speed than existing parameterizations. This leads to important improvements in classification accuracy. * Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The kernel size of a convolutional layer defines the region from which features are computed, and is a crucial choice in their design. Commonly, small kernels (up to 7px) are used almost exclusively and are combined with pooling to model long term dependencies <ref type="bibr" target="#b52">(Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b19">He et al., 2016;</ref><ref type="bibr" target="#b57">Tan &amp; Le, 2019)</ref>. Recent works indicate, however, that CNNs benefit from using convolutional kernels (i) of varying size at different layers , and (ii) at the same resolution of the data <ref type="bibr" target="#b42">(Peng et al., 2017;</ref><ref type="bibr" target="#b9">Cordonnier et al., 2019;</ref><ref type="bibr" target="#b44">Romero et al., 2021)</ref>. Unfortunately, most CNNs represent convolutional kernels as tensors of discrete weights and their size must be fixed prior to training. This makes exploring different kernel sizes at different layers difficult and time-consuming due to (i) the large search space, and (ii) the large number of weights required to construct large kernels.</p><p>A more efficient way to tune different kernel sizes at different layers is to learn them during training. Existing methods define a discrete weighted set of basis functions, e.g., shifted Delta-Diracs <ref type="figure">(Fig. 2b</ref>, <ref type="bibr" target="#b10">Dai et al. (2017)</ref>) or Gaussian functions <ref type="figure">(Fig. 2c</ref>, <ref type="bibr" target="#b21">Jacobsen et al. (2016)</ref>; <ref type="bibr" target="#b49">Shelhamer et al. (2019)</ref>; ). During training they learn dilation factors over the basis functions to increase the kernel size, which crucially limits the bandwidth of the resulting kernels.</p><p>In this work, we present the Flexible Size Continuous Kernel Convolution (FlexConv), a convolutional layer able to learn high bandwidth convolutional kernels of varying size during training <ref type="figure" target="#fig_0">(Fig. 1)</ref>. Instead of using discrete weights, we provide a continuous parameterization of convolutional kernels via a small neural network <ref type="bibr" target="#b44">(Romero et al., 2021)</ref>. This parameterization allows us to model continuous functions of arbitrary size with a fixed number of parameters. By multiplying the response of the neural network with a Gaussian mask, the size of the kernel can be learned during training <ref type="figure">(Fig. 2a</ref>). This allows us to produce detailed kernels of small sizes <ref type="figure">(Fig. 3)</ref>, and tune kernel sizes efficiently.  <ref type="bibr">(FlexConv)</ref>. FlexConv defines convolutional kernels as the multiplication of a continuous convolutional kernel MLP ? , with a Gaussian mask of local support w gauss : ?(x, y) = w gauss (x, y; ? mask ) ? MLP ? (x, y). By learning the parameters of the mask, the size of the convolutional kernel can be optimized during training. See also <ref type="figure" target="#fig_2">Fig. 7</ref>.</p><p>FlexConvs can be deployed at higher resolutions than those observed during training, simply by using a more densely sampled grid of kernel indices. However, the high bandwidth of the kernel can lead FlexConv to learn kernels that show aliasing at higher resolutions, if the kernel bandwidth exceeds the Nyquist frequency. To solve this problem, we propose to parameterize convolutional kernels as Multiplicative Anisotropic Gabor Networks (MAGNets). MAGNets are a new class of Multiplicative Filter Networks <ref type="bibr" target="#b13">(Fathony et al., 2021</ref>) that allows us to analyze and control the frequency spectrum of the generated kernels. We use this analysis to regularize FlexConv against aliasing. With this regularization, FlexConvs can be directly deployed at higher resolutions with minimal accuracy loss. Furthermore, MAGNets provide higher descriptive power and faster convergence speed than existing continuous kernel parameterizations <ref type="bibr" target="#b48">(Sch?tt et al., 2017;</ref><ref type="bibr" target="#b14">Finzi et al., 2020;</ref><ref type="bibr" target="#b44">Romero et al., 2021)</ref>. This leads to important improvements in classification accuracy <ref type="bibr">(Sec. 4)</ref>.</p><p>Our experiments show that CNNs with FlexConvs, coined FlexNets, achieve state-of-the-art across several sequential datasets, match performance of recent works with learnable kernel sizes with less compute, and are competitive with much deeper ResNets <ref type="bibr" target="#b19">(He et al., 2016)</ref> when applied on image benchmark datasets. Thanks to the ability of FlexConvs to generalize across resolutions, FlexNets can be efficiently trained at low-resolution to save compute, e.g., 16 ? 16 CIFAR images, and be deployed on the original data resolution with marginal accuracy loss, e.g., 32 ? 32 CIFAR images.</p><p>In summary, our contributions are:</p><p>? We introduce the Flexible Size Continuous Kernel Convolution (FlexConv), a convolution operation able to learn high bandwidth convolutional kernels of varying size end-to-end. ? Our proposed Multiplicative Anisotropic Gabor Networks (MAGNets) allow for analytic control of the properties of the generated kernels. This property allows us to construct analytic alias-free convolutional kernels that generalize to higher resolutions, and to train FlexNets at low resolution and deploy them at higher resolutions. Moreover, MAGNets show higher descriptive power and faster convergence speed than existing kernel parameterizations. ? CNN architectures with FlexConvs (FlexNets) obtain state-of-the-art across several sequential datasets, and match recent works with learnable kernel size on CIFAR-10 with less compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Adaptive kernel sizes. <ref type="bibr" target="#b36">Loog &amp; Lauze (2017)</ref> regularize the scale of convolutional kernels for filter learning. For image classification, adaptive kernel sizes have been proposed via learnable pixel-wise offsets <ref type="bibr" target="#b10">(Dai et al., 2017)</ref>, learnable padding operations <ref type="bibr" target="#b18">(Han et al., 2018)</ref>, learnable dilated Gaussian functions <ref type="bibr" target="#b49">(Shelhamer et al., 2019;</ref><ref type="bibr" target="#b65">Xiong et al., 2020;</ref><ref type="bibr">Tabernik et al., 2020;</ref><ref type="bibr">Nguyen, 2020)</ref> and scalable Gaussian derivative filters <ref type="bibr" target="#b35">Lindeberg, 2021)</ref>. These approaches either dilate discrete kernels ( <ref type="figure">Fig. 2b)</ref>, or use discrete weights on dilated basis functions ( <ref type="figure">Fig. 2c</ref>). Using dilation crucially limits the bandwidth of the resulting kernels. In contrast, FlexConvs are able to construct high bandwidth convolutional kernels of varying size with a fixed parameter count. Larger kernels are obtained simply by passing more positions to the kernel network ( <ref type="figure" target="#fig_0">Fig. 1)</ref>.</p><p>Continuous kernel convolutions. Discrete convolutional kernel parameterizations assign an independent weight to each specific position in the kernel. Continuous convolutional kernels, on the (a) FlexConv kernels (ours) (b) Dilation / deformation <ref type="bibr" target="#b10">(Dai et al., 2017)</ref> (c) (Learnable) parametric dilation  Figure 2: Existing approaches increase the size of convolutional kernels via (learnable) parametric dilations, e.g., by deformation (b) or by Gaussian blur (c). However, dilation limits the bandwidth of the dilated kernel and with it, the amount of detail it can describe. Contrarily, FlexNets extend their kernels by passing a larger vector of positions to the neural network parameterizing them. As a result, FlexConvs are able to learn high bandwidth convolutional kernels of varying size end-to-end (a).</p><p>(a) Ground Truth (b) Reconstructions at varying degrees of localization <ref type="figure">Figure 3</ref>: The importance of dynamic sizes in continuous kernel convolutions. Consider a neural network predicting pixel values at each position. If the entire image is considered, the network must use part of its capacity to learn to predict zeros outside of the flower region, which in turn degrades the quality of the approximation in the region of interest (b). Importantly, the better the localization of the flower, the higher the approximation fidelity becomes. FlexNets learn the size of their convolutional kernels at each layer during training, and thus (i) use the capacity of the kernel efficiently, (ii) converge faster to good approximations, and (iii) are faster in execution -via dynamic cropping-.</p><p>other hand, view convolutional kernels as continuous functions parameterized via a small neural network MLP ? ? R D ? R Nout?N in , with D the data dimensionality. This defines a convolutional kernel for which arbitrary input positions can be queried. Continuous kernels have primarily been used to handle irregularly-sampled data locally, e.g., molecular data <ref type="bibr" target="#b51">(Simonovsky &amp; Komodakis, 2017;</ref><ref type="bibr" target="#b48">Sch?tt et al., 2017)</ref> and point-clouds <ref type="bibr" target="#b58">(Thomas et al., 2018;</ref><ref type="bibr" target="#b50">Shi et al., 2019)</ref>.</p><p>Recently, <ref type="bibr" target="#b44">Romero et al. (2021)</ref> introduced the Continuous Kernel Convolution (CKConv) as a tool to model long-term dependencies. CKConv uses a continuous kernel parameterization to construct convolutional kernels as big as the input signal with a constant parameter cost. Contrarily, FlexConvs jointly learn the convolutional kernel as well as its size. This leads to important advantages in terms of expressivity ( <ref type="figure">Fig. 3)</ref>, convergence speed and compute costs of the operation.</p><p>Implicit neural representations. Parameterizing a convolutional kernel via a neural network can be seen as learning an implicit neural representation of the underlying convolutional kernel <ref type="bibr" target="#b44">(Romero et al., 2021)</ref>. Implicit neural representations construct continuous data representations by encoding data in the weights of a neural network <ref type="bibr" target="#b41">(Park et al., 2019;</ref><ref type="bibr" target="#b53">Sitzmann et al., 2020;</ref><ref type="bibr" target="#b13">Fathony et al., 2021)</ref>.</p><p>We replace the SIREN <ref type="bibr" target="#b53">(Sitzmann et al., 2020)</ref> kernel parameterization used in <ref type="bibr" target="#b44">Romero et al. (2021)</ref> by our Multiplicative Anisotropic Gabor Networks: a new class of Multiplicative Filter Networks <ref type="bibr" target="#b13">(Fathony et al., 2021)</ref>. MFNs allow for analytic control of the resulting representations, and allow us to construct analytic alias-free convolutional kernels. The higher expressivity and convergence speed of MAGNets lead to accuracy improvements in CNNs using them as kernel parameterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we introduce our approach. First, we introduce FlexConv and the Gaussian mask. Next, we introduce our Multiplicative Anisotropic Gabor Networks (MAGNets) and provide a description of our regularization technique used to control the spectral components of the generated kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FLEXIBLE SIZE CONTINUOUS KERNEL CONVOLUTION (FLEXCONV)</head><p>To learn the kernel size during training, FlexConvs define their convolutional kernels ? as the product of the output of a neural network MLP ? with a Gaussian mask of local support. The neural network MLP ? parameterizes the kernel, and the Gaussian mask parameterizes its size ( <ref type="figure" target="#fig_0">Fig. 1)</ref>.</p><p>Anisotropic Gaussian mask. Let G(x; ? X , ? 2 X )?= exp ? 1 2 ? ?2 X (x ? ? X ) 2 be a Gaussian function parameterized by a mean-variance tuple (? X , ? 2 X ). The anisotropic Gaussian mask is defined as:</p><formula xml:id="formula_0">w gauss (x, y; {? X , ? 2 X , ? Y , ? 2 Y }) = G(x; ? X , ? 2 X )G(y; ? Y , ? 2 Y ).<label>(1)</label></formula><p>By learning (? X , ? 2 X ) and (? Y , ? 2 Y ) independently, anisotropic non-centered windows can be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MULTIPLICATIVE ANISOTROPIC GABOR NETWORKS (MAGNETS)</head><p>In this section, we formalize our proposed parameterization for the kernel MLP ? . We start by introducing Multiplicative Filter Networks <ref type="bibr" target="#b13">(Fathony et al., 2021)</ref>, and present our MAGNets next.</p><p>Multiplicative Filter Networks (MFNs). Recently, <ref type="bibr" target="#b13">Fathony et al. (2021)</ref> proposed to construct implicit neural representations as the linear combination of exponentially many basis functions g:</p><formula xml:id="formula_1">h (1) = g [x, y]; ? (1) g ? R 2 ? R N hid (2) h (l) = W (l) h (l?1) + b (l) ? g [x, y]; ? (l) W (l) ? R N hid ?N hid , b (l) ? R N hid (3) ?(x, y) = W (L) h (L?1) + b (L) W (L) ? R N?N hid , b (L) ? R N<label>(4)</label></formula><p>where ? (l) , W (l) , b (l) depict the learnable parameters of the bases and the affine transformations, and N, N hid depict the number of output and hidden channels, respectively. Depending on the selection of g, MFNs obtain approximations comparable to those of SIRENs <ref type="bibr" target="#b53">(Sitzmann et al., 2020)</ref> with faster convergence rate. The most successful instantiation of MNFs are the Multiplicative Gabor Network (MGN): MFNs constructed with isotropic Gabor functions as basis g (in Eq. 2):</p><formula xml:id="formula_2">g [x, y]; ? (l) = exp ? ? (l) 2 x ? ? (l) 2 + y ? ? (l) 2 Sin W (l) g ? [x, y] + b (l) g ,<label>(5)</label></formula><formula xml:id="formula_3">? (l) = ? (l) ? R N hid , ? (l) ? R N hid , W (l) g ? R N hid ?2 , b (l) g ? R N hid .</formula><p>(6) Note that, by setting N=N out ?N in , an MFN can parameterize a convolutional kernel with N in input and N out output channels. <ref type="bibr" target="#b13">Fathony et al. (2021)</ref> show that MFNs are equivalent to a linear combination of exponentially many basis functions g. This allows us to analytically derive properties of MFN representations, and plays a crucial role in the derivation of alias-free MAGNets (Sec. 3.3).</p><p>Multiplicative Anisotropic Gabor Networks (MAGNets). Our MAGNet formulation is based on the observation that isotropic Gabor functions, i.e., with equal ? for the horizontal and vertical directions, are undesirable as basis for the construction of MFNs. Whenever a frequency is required along a certain direction, an isotropic Gabor function automatically introduces that frequency in both directions. As a result, other bases must counteract this frequency in the direction where the frequency is not required, and thus the capacity of the MFN is not used optimally <ref type="bibr" target="#b11">(Daugman, 1988)</ref>.</p><p>Following the original formulation of the 2D Gabor functions <ref type="bibr" target="#b11">(Daugman, 1988)</ref>, we alleviate this limitation by using anisotropic Gabor functions instead:</p><formula xml:id="formula_4">g [x, y]; ? (l) = exp ? 1 2 ? (l) X x ? ? (l) X 2 + ? (l) Y y ? ? (l) Y 2 Sin W (l) g [x, y] + b (l) g (7) ? (l) = ? (l) X ? R N hid , ? (l) Y ? R N hid , ? (l) X ? R N hid , ? (l) Y ? R N hid , W (l) g ? R N hid ?2 , b (l) g ? R N hid . (8)</formula><p>The resulting Multiplicative Anisotropic Gabor Network (MAGNet) obtains better control upon frequency components introduced to the approximation, and demonstrates important improvements in terms of descriptive power and convergence speed (Sec. 4).</p><p>MAGNet initialization. <ref type="bibr" target="#b13">Fathony et al. (2021)</ref> proposes to initialize MGNs by drawing the size of the Gaussian envelopes, i.e., the ? (l) term, from a Gamma(? ? L ?1 , ?) distribution at every layer l ? [1, .., L ? 1]. We observe however that this initialization does not provide much variability on the initial extension of the Gaussian envelopes and in fact, most of them cover a large portion of the space at initialization. To stimulate diversity, we initialize the {?</p><formula xml:id="formula_5">(l) X , ? (l)</formula><p>Y } terms by a Gamma(?l ?1 , ?) distribution at the l-th layer. We observe that our proposed initialization consistently leads to better accuracy than the initialization of <ref type="bibr" target="#b13">Fathony et al. (2021)</ref> across all tasks considered. (Sec. 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ANALYTIC ALIAS-FREE MAGNETS</head><p>FlexConvs can be deployed at higher resolutions than those observed during training, simply by sampling the underlying continuous representation of the kernel more densely, and accounting for the change in sampling rate. Consider a D-dimensional input signal f r (1) with resolution r (1) . FlexConv learns a kernel ? r (1) that can be inferred at a higher resolution r (2) <ref type="bibr" target="#b44">(Romero et al., 2021)</ref>:</p><formula xml:id="formula_6">f r (2) * ? r (2) ? r (1) r (2) D f r (1) * ? r (1) .<label>(9)</label></formula><p>Note however, that Eq. 9 holds approximately. This is due to aliasing artifacts which can appear if the frequencies in the learned kernel surpass the Nyquist criterion of the target resolution. Consequently, an anti-aliased parameterization is vital to construct kernels that generalize well to high resolutions.</p><p>Towards alias-free implicit neural representations. We observe that SIRENs as well as unconstrained MFNs and MAGNets exhibit aliasing when deployed on resolutions higher than the training resolution, which hurts performance of the model. An example kernel with aliasing is shown in <ref type="figure" target="#fig_4">Fig. 8</ref>.</p><p>To combat aliasing, we would like to control the representation learned by MAGNets. MAGNets -and MFNs in general-construct implicit neural representations that can be seen as a linear combination of basis functions. This property allows us to analytically derive and study the properties of the resulting neural representation. Here, we use this property to derive the maximum frequency of MAGNet-generated kernels, so as to regularize MAGNets against aliasing during training. We analytically derive the maximum frequency of a MAGNet, and penalize it whenever it exceeds the Nyquist frequency of the training resolution. We note that analytic derivations are difficult for other implicit neural representations, e.g., SIRENs, due to stacked layer-wise nonlinearities.</p><p>Maximum frequency of MAGNets. The maximum frequency component of a MAGNet is given by:</p><formula xml:id="formula_7">f + MAGNet = L l=1 max i l ? ? ? ? max j W (l) g,i l ,j 2? ? ? + ? cut min{? (l) X,i l , ? (l) Y,i l } 2? ? ? ,<label>(21)</label></formula><p>where L corresponds to the number of layers, W</p><formula xml:id="formula_8">(l) g , ? (l) X , ? (l)</formula><p>Y to the MAGNet parameters as defined in Eq. 8, and ? cut =2 ? stdev to the cut-off frequency of the Gaussian envelopes in the Gabor filters. A formal treatment as well as the derivations can be found in Appx. A.1.</p><p>Effect of the FlexConv mask. The Gaussian mask used to localize the response of the MAGNet also has an effect on the frequency spectrum. Hence, the maximum frequency of a FlexConv kernel is:</p><formula xml:id="formula_9">f + FlexConv = f + MAGNet + f + wgauss , with f + wgauss = ? cut max{? X , ? Y }2? .<label>(22)</label></formula><p>Here, ? X , ? Y correspond to the mask parameters (Eq. 1). Intuitively, multiplication with the mask blurs in the frequency domain, as it is equivalent to convolution with the Fourier transform of the mask.</p><p>Aliasing regularization of FlexConv kernels. With the analytic derivation of f + FlexConv we penalize the generated kernels to have frequencies smaller or equal to their Nyquist frequency f Nyq (k) via:</p><formula xml:id="formula_10">L HF = max{f + FlexConv , f Nyq (k)} ? f Nyq (k) 2 , with f Nyq (k) = k?1 4 .<label>(25)</label></formula><p>Here, k depicts the size of the FlexConv kernel before applying the Gaussian mask, and is equal to the size of the input signal. In practice, we implement Eq. 25 by regularizing the individual MAGNet layers, as is detailed in Appx. A.2. To verify our method, <ref type="figure" target="#fig_4">Fig. 8</ref> (Appx. A.1) shows that the frequency components of FlexNet kernels are properly regularized for aliasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate FlexConv across classification tasks on sequential and image benchmark datasets, and validate the ability of MAGNets to approximate complex functions. A complete description of the datasets used is given in Appx. B. Appx. D.2 reports the parameters used in all our experiments. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">WHAT KIND OF FUNCTIONS CAN MAGNETS APPROXIMATE?</head><p>Bandwidth of methods with learnable sizes. First, we compare the bandwidth of MAGNet against N-Jet (Pintea et al., 2021) by optimizing each to fit simple targets: (i) Gabor filters of known frequency, (ii) random noise and (iii) an a 11 ? 11 AlexNet kernel from the first layer <ref type="bibr" target="#b29">(Krizhevsky et al., 2012)</ref>. <ref type="figure" target="#fig_1">Fig. 4</ref> shows that, even with 9 orders of Gaussian derivatives, N-Jets cannot fit high frequency signals in large kernels. Crucially, N-Jet models require many Gaussian derivative orders to model high frequency signals in large kernels: a hyperparameter which proportionally increases their inference  Expressivity of MLP parameterizations. Next, we compare the descriptive power and convergence speed of MAGNets, Gabor MFNs, Fourier MFNs and SIRENs for image approximation. To this end, we fit the images in the Kodak dataset <ref type="bibr" target="#b27">(Kodak, 1991)</ref> with each of these methods. Our results (Tab. 5) show that MAGNets outperform all other methods, and converge faster to good approximations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CLASSIFICATION TASKS</head><p>Network specifications. Here, we specify our networks for all our classification experiments. We parameterize all our convolutional kernels as the superposition of a 3-layer MAGNet and a learnable anisotropic Gaussian mask. We construct two network instances for sequential and image datasets respectively: FlexTCNs and FlexNets. Both are constructed by taking the structure of a baseline network -TCN <ref type="bibr" target="#b1">(Bai et al., 2018a)</ref> or CIFARResNet <ref type="bibr" target="#b19">(He et al., 2016</ref>)-, removing all internal pooling layers, and replacing convolutional kernels by FlexConvs. The FlexNet architecture is shown in <ref type="figure" target="#fig_0">Fig. 10</ref> and varies only in the number of channels and blocks, e.g., FlexNet-16 has 7 blocks. Akin to <ref type="bibr" target="#b44">Romero et al. (2021)</ref> we utilize the Fourier theorem to speed up convolutions with large kernels.</p><p>Mask initialization. We initialize the FlexConv masks to be small. Preliminary experiments show this leads to better performance, faster execution, and faster training convergence. For sequences, the mask center is initialized at the last kernel position to prioritize the last information seen.</p><p>Time series and sequential data. First we evaluate FlexTCNs on sequential classification datasets, for which long-term dependencies play an important role. We validate our approach on intrinsic    <ref type="bibr">et al., 2009)</ref>. Additional experiments on Imagenet-32, MNIST and STL-10 can be found in Appx. C. <ref type="table" target="#tab_2">Table 3</ref> shows our results on CIFAR-10. FlexNets are competitive with pooling-based methods such as CIFARResNet <ref type="bibr" target="#b19">(He et al., 2016)</ref> and outperform learnable kernel size method DCNs . In addition, we compare using N-Jet layers of order three (as in Pintea et al. <ref type="formula" target="#formula_0">(2021)</ref>) in FlexNets against using MAGNet kernels. We observe that N-Jet layers lead to worse performance, and are significantly slower than FlexConv layers with MAGNet kernels. The low accuracy of N-Jet layers is likely to be linked to the fact that FlexNets do not use pooling. Consequently, N-Jets are forced to learn large kernels with high-frequencies, which we show N-Jets struggle learning in Sec. 4.1.</p><formula xml:id="formula_11">-? ji 0.47M 89.7 ? 0.3* - N-Jet-CIFARResNet32 0.52M 92.3 ? 0.3* - N-Jet-ALLCNN 1.07M 92.5 ? 0.1* - FlexNet-16 w/ conv. (k = 3) 0.17M 89.5 ? 0.3 41 FlexNet-16 w/ conv. (k = 33) 20.0M 78.0 ? 0.</formula><p>To illustrate the effect of learning kernel sizes, we also compare FlexNets against FlexNets with large and small discrete convolutional kernels (Tab. 3). Using small kernel sizes is parameter efficient, but is not competitive with FlexNets. Large discrete kernels on the other hand require a copious amount of parameters and lead to significantly worse performance. These results indicate that the best solution is somewhere in the middle and varying kernel sizes can learn the optimal kernel size for the task at hand.</p><p>Similar to the sequential case, we conduct ablation studies on image data with learnable, nonlearnable kernel sizes and different kernel parameterizations. Accuracy change after fine-tuning <ref type="figure">Figure 5</ref>: Alias-free FlexNet-16 on CIFAR-10. We report change in accuracy between source and target resolutions, directly after upsampling (left) and after fine-tuning (right) (means over five runs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ALIAS-FREE FLEXNETS</head><p>Regularizing the FlexConv mask. Though including f + wgauss in the frequency analysis of MAGNets is crucial for the accuracy of the derivation, including the FlexConv mask in aliasing regularization is undesirable, as it steers the model to learn large kernels in order to minimize the loss (see Eq. 25). However, excluding the mask from regularization could compromise the ability of FlexNet to generalize to higher resolutions. Here, we experiment with this trade-off.  <ref type="figure">Figure 5</ref> shows accuracy change between ten source and target resolution combinations on CIFAR-10, both for including and excluding the FlexConv mask in the aliasing regularization. We train at the source resolution for 100 epochs, before testing the model at the target resolution with the upsampling described in Sec. 3.3. Next, we adjust f Nyq (k) to the target resolution, and finetune each model for 100 epochs at the target resolution.</p><p>We find that regularizing just f + MAGNet yields a trade-off. It increases the accuracy difference between low and high resolution inference, but also increases the fine-tune accuracy at the target resolution.We therefore choose to, by default, regularize f + MAGNet only. Results of our alias-free FlexNet training on CIFAR-10 are in <ref type="table" target="#tab_5">Table 4</ref>. We observe that the performance of a FlexNet trained without aliasing regularization largely breaks down when the dataset is upscaled. However, with our aliasing regularization most of the performance is retained.</p><p>Comparatively, FlexNet retains more of the source resolution performance than FlexNets with N-Jet layers, while baselines degrade drastically at the target resolution. <ref type="figure" target="#fig_4">Fig. 8</ref> shows the effect of aliasing regularization on the frequency components of FlexConv.</p><p>Training at lower resolutions saves compute. We can train alias-free FlexNets at lower resolutions. To verify that this saves compute, we time the first 32 batches of training a FlexNet-7 on CIFAR-10. We compare against training on 16 ? 16 images (downsampled before training). On 16x16 images, each batch takes 179ms (? 7ms). On 32x32 images, each batch takes 222ms (? 9ms). Therefore, we save 24% training time when training FlexNets alias-free at half the native CIFAR-10 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Learned kernel sizes match conventional priors. Commonly, CNNs use architectures of small kernels and pooling layers. This allows convolutions to build a progressively growing receptive field. With learnable kernel sizes, FlexNet could learn a different prior over receptive fields, e.g., large kernels first, and small kernels next. However, FlexNets learn to increase kernel sizes progressively <ref type="figure">(Fig. 6)</ref>, and match the network design that has been popular since AlexNet <ref type="bibr" target="#b29">(Krizhevsky et al., 2012)</ref>.</p><p>Mask initialization as a prior for feature importance. The initial values of the FlexConv mask can be used to prioritize information at particular input regions. For instance, initializing the center of mask on the first element of sequential FlexConvs can be used to prioritize information from the far past. This prior is advantageous for tasks such as npCIFAR10. We observe that using this prior on npCIFAR10 leads to much faster convergence and better results (68.33% acc. w/ FlexTCN-2). <ref type="figure">Figure 6</ref>: Learned FlexConv masks for FlexNets with 3, 5 and 7 residual blocks. FlexNets learn very small kernels at shallow layers, which become larger as a function of depth.</p><p>MAGNet regularization as prior induction. MAGNets allow for analytic control of the properties of the resulting representations. We use this property to generate alias-free kernels. However, other desiderata could be induced, e.g., smoothness, for the construction of implicit neural representations.</p><p>Benefits of cropping and the influence of PyTorch. Dynamic cropping adjust the computational cost of the convolutions on the fly. For a signal of size M and a cropped kernel size k, this incurs in</p><formula xml:id="formula_12">savings from O(M 2 D ) to O(M D k D ) relative to using global kernel sizes (O(M 4 ) to O(M 2 k 2 ) in 2D)</formula><p>. We test this theoretical speed up in a controlled environment for the Speech Commands and CIFAR-10 datasets. Cropping reduces the per-epoch run time by a factor of 11.8x and 5.5x for Speech Commands and CIFAR-10, respectively. Interestingly, however, both run times become similar if the flag torch.backends.cudnn.benchmark is activated, with global kernel sizes being sometimes faster. This is because this flag tells PyTorch to optimize the convolution algorithms used under the hood, and some of these CUDA algorithms seem to be faster than our masking strategy on Python.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">LIMITATIONS</head><p>Dynamic kernel sizes: computation and memory cost of convolutions with large kernels. Performing convolutions with large convolutional kernels is a compute-intensive operation. FlexConvs are initialized with small kernel sizes and their inference cost is relatively small at the start of training. However, despite the cropping operations used to improve computational efficiency <ref type="figure" target="#fig_0">(Figs. 1, 3, Tab.</ref> 3), the inference time may increase to up to double as the learned masks increase in size. At the cost of more memory, convolutions can be sped up by performing them in the frequency domain. However, we observe that this does not bring gains for the image data considered because FFT convolutions are faster only for very large convolutional kernels (in the order of hundreds of pixels).</p><p>Remaining accuracy drop in alias-free FlexNets. Some drop in accuracy is still observed when using alias-free FlexNets at a higher test resolutions (Tab. 4). Although more evidence is needed, this may be caused by aliasing effects introduced by ReLU <ref type="bibr" target="#b62">(Vasconcelos et al., 2021)</ref>, or changes in the activation statistics of the feature maps passed to global average pooling <ref type="bibr" target="#b60">(Touvron et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We propose FlexConv, a convolutional operation able to learn high bandwidth convolutional kernels of varying size during training at a fixed parameter cost. We demonstrate that FlexConvs are able to model long-term dependencies without the need of pooling, and shallow pooling-free FlexNets achieve state-of-the-art performance on several sequential datasets, match performance of recent works with learned kernel sizes with less compute, and are competitive with much deeper ResNets on image benchmark datasets. In addition, we show that our alias-free convolutional kernels allow FlexNets to be deployed at higher resolutions than seen during training with minimal precision loss.</p><p>Future work. MAGNets give control over the bandwidth of the kernel. We anticipate that this control has more uses, such as fighting sub-sampling aliasing <ref type="bibr" target="#b67">(Zhang, 2019;</ref><ref type="bibr" target="#b24">Kayhan &amp; Gemert, 2020;</ref><ref type="bibr" target="#b23">Karras et al., 2021)</ref>. With the ability to upscale FlexNets to different input image sizes comes the possibility of transfer learning representations between previously incompatible datasets, such as CIFAR-10 and Imagenet. In a similar vein, the automatic adaptation of FlexConv to the kernel sizes required for the task at hand may make it possible to generalize the FlexNet architecture across different tasks and datasets. Neural architecture search <ref type="bibr" target="#b68">(Zoph &amp; Le, 2016</ref>) could see benefits from narrowing the search space to exclude kernel size and pooling layers. In addition, we envisage additional improvements from structural developments of FlexConvs such as attentive FlexNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REPRODUCIBILITY STATEMENT</head><p>We hope to inspire others to use and reproduce our work. We publish the source code of this work, for which the link is provided in Sec. 4.2. Sec. 4 and Appx. D.1 detail FlexNet, its hyperparameters and optimization procedure. The full derivation of the aliasing regularization objective is included in Appx. A.1. We report means over multiple runs for many experiments, to ensure the reported results are fair and reproducible, and do not rely on tuning of the random seed. All datasets used in our experiments are publicly available. If any questions remain, we welcome one and all to contact the corresponding author. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ALIAS-FREE FLEXCONV REGULARIZATION</head><p>In this section we provide the complete derivation and analysis for our FlexConv regularization against aliasing. First, we derive the analytic maximum frequency component of a FlexConv kernel. Next, we compute the Nyquist frequency of a FlexConv kernel, and subsequently show how to combine the previous results into a regularization term to train alias-free FlexConvs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 ANALYZING THE FREQUENCY SPECTRUM OF FLEXCONV</head><p>In order to make FlexConv alias-free (Sec. 3.3), we need to compute the maximum frequency component of the kernels generated by a MAGNet, so that we can regularize it during training. In this section we analytically derive this maximum frequency component from the parameters of the MAGNet.</p><p>Recall that MAGNets generate a kernel ?(x, y) through of a succession of anisotropic Gabor filters and linear layers (Sec. 3.2, Eqs. 2-7):</p><formula xml:id="formula_13">h (1) = g [x, y]; ? (1) g ? R 2 ? R N hid h (l) = W (l) h (l?1) + b (l) ? g [x, y]; ? (l) W (l) ? R N hid ?N hid , b (l) ? R N hid ?(x, y) = W (L) h (L?1) + b (L) W (L) ? R (Nout?N in )?N hid , b (L) ? R (Nout?N in ) g [x, y]; ? (l) = exp ? 1 2 ? (l) X x ? ? (l) X 2 + ? (l) Y y ? ? (l) Y 2 Sin W (l) g [x, y] + b (l) g ? (l) = ? (l) X ? R N hid , ? (l) Y ? R N hid , ? (l) X ? R N hid , ? (l) Y ? R N hid , W (l) g ? R N hid ?2 , b (l) g ? R N hid</formula><p>To analyse the maximum frequency component f + MAGNet , we analyse the frequency components of the Gabor filters used in MAGNet, and retain their maximum. We then plug the found frequency component into the analysis of <ref type="bibr" target="#b13">Fathony et al. (2021)</ref> to show how the frequency responses of Gabor filters and linear layers interact in MFNs. Finally, we add the effect of the FlexConv Gaussian mask to our analysis to obtain the maximum frequency component ot the final FlexConv kernel f + FlexConv . Sine term in Gabor filters. In a Gabor filter, the sine term is multiplied with a Gaussian envelope. The frequency (in radians) of a sine function of the form Sin(w T [x, y] + b) is given by w. We divide by 2? to convert the frequency units to Hertz, for compatibility with the rest of the analysis. For 2D inputs, the maximum frequency component of the sine function correspond to the largest frequency in the two input dimensions:</p><formula xml:id="formula_14">f + Sin = max j w j 2? .<label>(10)</label></formula><p>The sine terms in MAGNets have multiple output channels:</p><formula xml:id="formula_15">Sin W g ? [x, y] + b (l) g</formula><p>. Effectively, we compute the sine term independently for each channel:  MAGNet , the maximum frequency is slightly higher than the Nyquist frequency, as the FlexConv mask is not included in the frequency term derivation. This is reflected in the slightly worse resolution generalization results reported in Sec. 4.3. Furthermore, some aliasing effects are still apparent for the aliasing regularized models, as discussed in Sec. 6.</p><formula xml:id="formula_16">f + Sin,i = max j W g,i,j 2? .<label>(11)</label></formula><p>Gaussian term in Gabor filters. In a Gabor filter, a Gaussian envelope modulates a sine term. Let us assume for now that the Gaussian envelope is isotropic, rather than anisotropic as in MAGNets, and has single-channel output. By applying the convolution theorem, the sine term is equivalently convolved with the Fourier transform of the Gaussian envelope in the frequency domain. Since the Fourier transform of a Gaussian envelope is another Gaussian envelope, the application of a Gaussian envelope amounts to blurring with a Gaussian kernel in the frequency domain. The size of the envelope in the Fourier domain ? F can be derived from the standard deviation of the Guassian envelope in the spatial domain ? T as follows:</p><formula xml:id="formula_17">? T ? F = 1 2? ? ? F = 1 2?? T .<label>(12)</label></formula><p>Gaussian blurs induce impulse signals to have a long tail. Consequently, we must define a cutoff point for this tail in terms of standard deviations to derive the maximum added frequency induced by the blur. We describe the cutoff point as ? cut ? N. Typical choices for ? cut are known as the empirical, or the "68-95-99.7" rule <ref type="bibr" target="#b17">(Hald, 2007)</ref>. We choose a standard of two standard deviations, i.e., ? cut =2, which covers 95% of the mass of the Gaussian envelope.</p><p>For an isotropic Gabor filter with ?=? ?1 T , the maximum frequency of its Gaussian envelope f + env is:</p><formula xml:id="formula_18">f + env = ? cut 2?(? T ) ?1 = ? cut ? 2? .<label>(13)</label></formula><p>Anisotropic envelopes. Our analysis so far assumes an isotropic Gaussian envelope in the Gabor filter. However, we need to account for the anisotropic Gaussian envelopes in MAGNets. Anisotropic filters have not one but two ? parameters: {? X , ? Y }. The smallest of these will contribute most to f + env , as it will blur the most, so it is sufficient to compute f + env only using the smallest of the two ? terms:</p><formula xml:id="formula_19">f + env (? X , ? Y ) = f + env (min{? X , ? Y }).<label>(14)</label></formula><p>The other assumption we made before was to work with single-channel outputs. MAGNets however use multi-channel outputs with independent Gaussian terms. The maximum frequency of multichannel Gaussian envelopes is given by:</p><formula xml:id="formula_20">f + env,i (? X , ? Y ) = f + env (min{? X,i , ? Y,i }) = ? cut min{? X,i , ? Y,i } 2? ,<label>(15)</label></formula><p>where the subscript i indexes the channels of the multi-channel Gaussian envelopes.</p><p>Maximum frequency component of anisotropic Gabor filters. Finally, the maximum frequency component of the i-th channel of an anisotropic Gabor filter g is given by: <ref type="figure">Figure 9</ref> illustrates the frequency spectrum of an example Gabor filter. </p><formula xml:id="formula_21">f + Gabor,i = f + Sin,i (W g ) + f + env,i (? X , ? Y ) = max j W g,i,j 2? + ? cut min{? X,i , ? Y,i } 2? .<label>(16)</label></formula><formula xml:id="formula_22">? = s L ? (L) i L + s L?1 ? (L?1) i L?1 + ? + s l ? (2) i2 + ? (1) i1 .<label>(17)</label></formula><p>Here, the indexes i 1 , i 2 , ?, i L?1 range over all possible indices of each hidden unit of each layer of an MFN, and s 2 , ?, s L ? {?1, +1} range over all 2 L?1 possible binary signs. In other words, <ref type="bibr" target="#b13">Fathony et al. (2021)</ref> demonstrate that the representation of an MFN at a particular layer contains an exponential combination of all possible positive and negative combinations of the frequencies of the sine terms in each hidden unit at each layer in the MFN up to the current layer.</p><p>The original analysis uses these terms to argue that MFNs model exponentially many terms through a linear amount of layers. For our purpose of computing the frequency response of the MAGNet generated kernel, we can plug our derivation of the frequencies of the Gabor filter f Gabor into ? to compute the frequency spectrum of the generated kernel:</p><formula xml:id="formula_23">f + MAGNet = s L f (L) Gabor,i L + s L?1 f (L?1) Gabor,i L?1 + ? + s 2 f<label>(2)</label></formula><p>Gabor,i2 + f</p><formula xml:id="formula_24">(1) Gabor,i1<label>(18)</label></formula><p>f (x) = N (x|0.0, 0.1) h(x) = sin(50.3x) g(x) = N (x|0.0, 0.1) sin(50.3x)</p><formula xml:id="formula_25">F (f (x)) F (h(x)) F (g(x))</formula><p>Figure 9: Decomposition of a Gabor filter and its frequency spectrum. Top row: a decomposition of a Gabor filter (right) into its Gaussian term (left) and its sine term (center). Bottom row: frequency responses for each respective filter. The Fourier transform of a Gaussian envelope is a Gaussian envelope (blue circles show ? F for h = {1, 2}). The Fourier transform of a sine pattern is a collection of symmetrical impulse signals (red box shows the Nyquist frequency). The Gaussian envelope blurs the frequency response of the sine term (purple boxes show the frequency response for h = {1, 2, 3}).</p><p>As stated before, we are only interested in the maximum frequency in the frequency spectrum. We can therefore simplify Eq. 18 in two ways. First, we simplify over MAGNet layers by taking the maximum value of the spectrum, which is the sum over all layers using only the positive binary signs in s L (Eq. 19). Next, we simplify over channel indices by retaining only the channel index that results in the highest frequency (Eq. 20). The maximum frequency of a MAGNet is shown in Eq. 21:</p><formula xml:id="formula_26">f + MAGNet = (+1)f + (L) Gabor,i L + (+1)f + (L?1) Gabor,i L?1 + ? + (+1)f + (2)</formula><p>Gabor,i2 + f</p><formula xml:id="formula_27">+ (1) Gabor,i1<label>(19)</label></formula><p>= f + (L)</p><formula xml:id="formula_28">Gabor,i L + f + (L?1) Gabor,i L?1 + ? + f + (2)</formula><p>Gabor,i2 + f</p><formula xml:id="formula_29">+ (1) Gabor,i1 f + MAGNet = max i L f + (L) Gabor,i L + max i L?1 f + (L?1) Gabor,i L?1 ? + max i2 f + (2) Gabor,i2 + max i1 f + (1) Gabor,i1<label>(20)</label></formula><formula xml:id="formula_30">= L l=1 max i l f + (l) Gabor,i l = L l=1 max i l ? ? ? ? max j W (l) g,i l ,j 2? ? ? + ? cut min{? (l) X,i l , ? (l) Y,i l } 2? ? ? .<label>(21)</label></formula><p>Effect of the Gaussian mask in the frequency components of a FlexConv. FlexConvs attenuate the MAGNet output with a Gaussian mask. The Gaussian mask (Eq. 1) works analogously to the Gaussian envelope term in the Gabor filter: it blurs the frequency components of the generated kernel with standard deviation ? F . Therefore, we can reuse our derivation for the Gaussian envelope of the Gabor filter (Eq. 15). The maximum frequency component of a FlexConv kernel is given by:</p><formula xml:id="formula_31">f + FlexConv = f + MAGNet + f + env = f + MAGNet + ? cut min{? ?1 X , ? ?1 Y } 2? = f + MAGNet + ? cut max{? X , ? Y }2? = L l=1 max i l ? ? ? ? max j W (l) g,i l ,j 2? ? ? + ? cut min{? (l) X,i l , ? (l) Y,i l } 2? ? ? + ? cut max{? X , ? Y }2? .<label>(22)</label></formula><p>Visualization of regularized kernels. <ref type="figure" target="#fig_4">Fig. 8</ref> shows example kernels from FlexNets trained with aliasing regularization. The frequency domain plots confirm the accuracy of our frequency component regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 REGULARIZING THE FREQUENCY RESPONSE OF FLEXCONV</head><p>Nyquist frequency of a FlexConv kernel. Given the sampling rate f s of the kernel, we can compute its Nyquist frequency f Nyq as:</p><formula xml:id="formula_32">f Nyq = 1 2 f s<label>(23)</label></formula><p>To compute the sampling rate, we note that the kernel coordinates input to our MAGNet stretch over a [?1, 1] D domain. For a kernel of length k, we therefore sample one point in every f s = k?1 2 units. Knowing the sampling rate in terms of the kernel size allows us to express the Nyquist frequency in terms of the (pre-masked) kernel size:</p><formula xml:id="formula_33">f Nyq (k) = 1 2 k ? 1 2 = k ? 1 4 .<label>(24)</label></formula><p>Note that the kernel size in a FlexConv is initialized to be equal to the resolution of the data, if it is odd. For even resolutions, it corresponds to the resolution of the data plus one.</p><p>Constructing the regularization term. We train FlexConv with a regularization term on the frequency response of the generated kernel to ensure that aliasing effects do not distort the performance of the model when it is inferred at a higher resolution. This section details the implementation of the regularization function.</p><p>From the parameters of each FlexConv module, we compute f + FlexConv according to Eq. 22. For the amount of standard deviations to use in determining f + env (Eq. 15) we use h = 2. From the kernel size k of the FlexConv module we compute f Nyq (k) according to Eq. 24. We then apply an L2 regularizer over the amount that f + FlexConv exceeds f Nyq (k):</p><formula xml:id="formula_34">L HF = max{f + FlexConv , f Nyq (k)} ? f Nyq (k) 2 .<label>(25)</label></formula><p>We weight L HF by ? = 0.1 when adding it to our loss function.</p><p>Improved implementation. Eq. 25 contains a sum over the L layers of the MAGNet. In practice, we prefer to regularize each layer l ? L separately, so that the gradients of the regularization of different layers are not dependent on each other. We therefore implement the anti-aliasing regularization by regularizing each MAGNet layer independently, and spreading the f + env term from the gaussian mask uniformly over all MAGNet layers:</p><formula xml:id="formula_35">L HF,l = max f + MAGNet,l + f + env L , f Nyq (k) L ? f Nyq (k) L 2 (26) = max max i l f + (l) Gabor,i l + f + env L , f Nyq (k) L ? f Nyq (k) L 2 .<label>(27)</label></formula><p>In the code, we refer to this method as the together method, versus the summed method of Eq. 25.</p><p>In preliminary experiments, we observed improved performance of anti-aliasing training when using the together method. All of our experiments anti-aliasing experiments therefore use the together setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATASET DESCRIPTION B.1 IMAGE FITTING DATASETS</head><p>Kodak dataset. The Kodak dataset <ref type="bibr" target="#b27">(Kodak, 1991)</ref> consists of 24 natural images of size 768 ? 512. This dataset is a popular benchmark used for compression and image fitting methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 SEQUENTIAL DATASETS</head><p>Sequential and Permuted MNIST. The sequential MNIST dataset (sMNIST) <ref type="bibr" target="#b30">(Le et al., 2015)</ref>takes the 28?28 images from the original MNIST dataset <ref type="bibr" target="#b32">(LeCun et al., 1998)</ref>, and presents them as a sequence of 784 pixels. The goal of this task is to perform digit classification given the representation of the last sequence element of a sequential model. Consequently, good predictions require the model to preserve long-term dependencies up to 784 steps in the past.</p><p>The permuted MNIST dataset (pMNIST) additionally changes the order of all the sMNIST sequences by a random permutation. Consequently, models can no longer rely on local features to construct good feature representations. As a result, the classification problem becomes more difficult, and the importance of long-term dependencies more pronounced.</p><p>Sequential and Noise-Padded CIFAR10. The sequential CIFAR10 dataset (sCIFAR10) <ref type="bibr" target="#b5">(Chang et al., 2017)</ref> takes the 32?32 images from the original CIFAR10 dataset <ref type="bibr" target="#b28">(Krizhevsky et al., 2009</ref>) and presents them as a sequence of 1,024 pixels. The goal of this task is to perform image classification given the representation of the last sequence element of a sequential model. This task is more difficult than sMNIST, as a larger memory horizon is required to solve the task and more complex structures and intra-class variations are present in the data <ref type="bibr" target="#b2">(Bai et al., 2018b)</ref>.</p><p>The noise-padded CIFAR10 dataset (npCIFAR10) <ref type="bibr" target="#b4">(Chang et al., 2019)</ref> flattens the images from the original CIFAR10 dataset <ref type="bibr" target="#b28">(Krizhevsky et al., 2009</ref>) along their rows to create a sequence of length 32, and 96 channels (32 rows ? 3 channels). Next, these sequences are concatenated with 968 entries of noise to form the final sequences of length 1000. As for sCIFAR10, the goal of the task is to perform image classification given the representation of the last sequence element of a sequential model.</p><p>CharacterTrajectories. The CharacterTrajectories dataset is part of the UEA time series classification archive <ref type="bibr" target="#b0">(Bagnall et al., 2018)</ref>. It consists of 2858 time series of length 182 and 3 channels representing the x, y positions, and the tip force of a pen while writing Latin alphabet characters in a single stroke. The goal is to classify out of 20 classes the written character using the time series data.</p><p>Speech Commands. The Speech Commands dataset <ref type="bibr" target="#b64">(Warden, 2018)</ref> consists of 105,809 onesecond audio recordings of 35 spoken words sampled at 16kHz. Following <ref type="bibr" target="#b25">Kidger et al. (2020)</ref>, we extract 34975 recordings from ten spoken words to construct a balanced classification problem. We refer to this dataset as SpeechCommands_raw, or SC_raw for short. Furhtermore, we utilize the preprocessing steps of <ref type="bibr" target="#b25">Kidger et al. (2020)</ref> and extract mel-frequency cepstrum coefficients from the raw data. The resulting dataset, abreviated SC, consists of time series of length 101, and 20 channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 IMAGE BENCHMARK DATASETS</head><p>MNIST. The MNIST hadwritten digits datset (LeCun &amp; Cortes, 2010) consists of 70,000 grayscale handwritten digits of size 28?28, divided into a training and test sets of 60,000 and 10,000 images, respectively. The goal of the task is to classify these digits as one of the ten possible digits <ref type="figure" target="#fig_0">(0, 1, ..8, 9)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>The CIFAR-10 dataset <ref type="bibr" target="#b28">(Krizhevsky et al., 2009</ref>) consists of 60,000 natural images from 10 classes of size 32?32, divided into training and test sets of 50,000 and 10,000 images, respectively.</p><p>STL-10. The STL-10 dataset <ref type="bibr" target="#b8">(Coates et al., 2011)</ref> is a subset of the ImageNet dataset <ref type="bibr" target="#b29">(Krizhevsky et al., 2012)</ref> consisting of 5,000 natural images from 10 classes of size 96?96, divided into trainint and test sets of 4,500 and 500 images, respectively.</p><p>ImageNet-k. The Imagenet-k <ref type="bibr" target="#b7">(Chrabaszcz et al., 2017)</ref> dataset is derived from the ImageNet dataset <ref type="bibr" target="#b47">Russakovsky et al. (2015)</ref> by downsampling all samples to a resolution k ? <ref type="bibr">[64,</ref><ref type="bibr">32,</ref><ref type="bibr">16,</ref><ref type="bibr">8]</ref>. The dataset contains 1000 classes with 1,281,167 training samples and 50,000 validation samples.    CIFARResNet-32 0.52M 16.1 ? 0.0 -11.6 ? 0.4</p><p>FlexNet-5 w/ N-Jets 0.46M 15.7 ? 0.1 -1.9 ? 0.4</p><p>FlexNet-5 0.44M 14.9 ? 0.1 -1.9 ? 1.7  <ref type="bibr" target="#b34">(Lin et al., 2013)</ref> N/A 99.6* ? VGG-5 (results from Kabir et al. <ref type="formula" target="#formula_23">(2020)</ref> reported by <ref type="bibr" target="#b7">Chrabaszcz et al. (2017)</ref> for Wide ResNets <ref type="bibr" target="#b66">(Zagoruyko &amp; Komodakis, 2016)</ref> outperform FlexNets by a significant margin.</p><p>Alias-free ImageNet-32. We report results for alias-free FlexNets on ImageNet-k <ref type="bibr" target="#b7">(Chrabaszcz et al., 2017)</ref> in <ref type="table" target="#tab_9">Table 8</ref>, to verify the results of alias-free training at a larger scale. We find that FlexConv and N-Jet both mostly retain classification accuracy between source and target resolution, while CIFARResNet-32 degrades drastically.</p><p>MNIST and STL-10. We additionally report results on MNIST (Tab. 9) and STL-10 (Tab. 10. We choose these dataset for the difference in image sizes of the training data. On MNIST, though performance on MNIST is quite saturated, we are competitive with state of the art methods. On STL-10 we are significantly worse than the baseline CIFARResNet from <ref type="bibr" target="#b38">(Luo et al., 2020)</ref>, though with significantly less parameters. We were not able to prepare a more relevant baseline for this experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D EXPERIMENTAL DETAILS D.1 FLEXNET</head><p>We propose an image classification architecture named FlexNet <ref type="figure" target="#fig_0">(Fig. 10)</ref>, consisting of a stack of FlexConv blocks followed by a global average pooling layer and a linear layer. FlexNets are named "FlexNet-L" where L indicates the amount of layers in the architecture.</p><p>FlexBlock. Each FlexBlock consists of two FlexConvs with BatchNorm <ref type="bibr" target="#b20">(Ioffe &amp; Szegedy, 2015)</ref> and dropout <ref type="bibr" target="#b54">(Srivastava et al., 2014</ref>) (d = 0.2) as well as a residual connection. The width of a block i is determined by scaling a base amount c by progressively increasing factors: c i = [c, c ? 1.5, c ? 1.5, c ? 2.0, c ? 2.0](i). The default configuration of FlexNet uses c = 22. In FlexNet-N-Jet models, we scale c to match the amount of parameters of the FlexNet in the comparison. <ref type="table" target="#tab_0">Table 10</ref>: Results on STL-10. We train each model with three different seeds and report mean and standard deviation. *Results are taken from <ref type="bibr" target="#b38">Luo et al. (2020)</ref>. ?Results are from single run.</p><p>MODEL SIZE STL-10 ACC.</p><p>CIFARResNet-18 11.2M 81.0* ? FlexNet-16 0.67M 68.6 ? 0.7 <ref type="figure" target="#fig_0">Figure 10</ref>: FlexNet architecture. FlexNet-L consists of L FlexBlocks, where each FlexBlock is a residual block of FlexConvs.</p><p>FlexConv initialization. We initialize the FlexConv mask variances small, at ? 2 X , ? 2 Y = 0.125. For initializing MAGNet, we initialize the Gaussian envelopes as discussed in Sec. 3.2. We initialize the linear layer weights by the same Gamma distribution as used for the enveloped, modulated by a scaling factor of 25.6. We found that this value of the scaling factor, rather than a higher one, helped in reducing the performance of alias-free models. We initialize the bias of the linear layers by U(??, ?). CIFAR-10. In FlexNet-16 models for CIFAR-10 we use c = 24 to approximate the parameter count of CIFARResNets in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 OPTIMIZATION</head><p>We use Adam (Kingma &amp; Ba, 2014) to optimize FlexNet. Unless otherwise specified, we use a learning rate of 0.01 with a cosine annealing scheme <ref type="bibr" target="#b37">(Loshchilov &amp; Hutter, 2016)</ref> with five warmup epochs. We use a different learning rate of 0.1? the regular learning rate for the FlexConv Gaussian mask parameters. We do not use weight decay, unless otherwise specified.</p><p>Kodak. We overfit on each image of the dataset for 20,000 iterations. To this end, we use a learning rate of 0.01 without any learning rate scheme. We observe that SIRENs diverge with this learning rate and thus, reduce the learning rate to 0.001 for these models.</p><p>CIFAR-10. We train for 350 epochs with a batch size of 64. We use the data augmentation from <ref type="bibr" target="#b19">He et al. (2016)</ref> when training CIFAR-10: a four pixel padding, followed by a random 32 pixel crop and a random horizontal flip.</p><p>ImageNet-32. We train for 350 epochs with a batch size of 2048. We use the same data augmentation as used for CIFAR-10. We do use a weight decay of 1e?5 for ImageNet-32 training.</p><p>Sequential and Permuted MNIST. We train for 200 epochs with a batch size of 64 and a learning rate of 0.01. We use a weight decay of 1e?5.</p><p>Sequential and Noise-Padded CIFAR-10. For sequential CIFAR-10, we train for 200 epochs with a batch size of 64, a learning rate of 0.001 and a weight decay of 1e?5. For noise-padded CIFAR-10, we train for 300 epochs with a batch size of 32, a learning rate of 0.01 and no weight decay.</p><p>Speech Commands and CharTrajectories. We train for 300 epochs with a batch size of 32 and a learning rate of 0.001. For CharTrajectories, we use a weight decay of 1e?5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 ROTATED GAUSSIAN MASKS</head><p>MAGNets use anisotropic Gaussian terms in the Gabor filters, which yields improvements in descriptive power and convergence speed (Sec. 3.2). For the same reason, we explore making the anisotropic FlexConv Gaussian mask steerable, by including an additional vector of learnable angle parameters ? (l) ? R N hid that rotates the Gaussian masks. Although preliminary experiments show rotated masks lead to slight additional improvements, the computational overhead required to rotate the masks is large. Consequently, we do not consider rotated Gaussian masks in our final experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The Flexible Size Continuous Kernel Convolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Left: Final MSE after fitting each model to Gabor filters of different frequencies. N-Jets cannot fit high frequencies. Right: Kernels learned by each model. SIREN and MAGNet can fit all targets. MAGNet-S: a small MAGNet of size akin to N-Jets, still does well on the Gabor and AlexNet targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>(a) MLP ? output (b) ? g [x, y]; ? (l) (c) ?(x, y) = (a) ? (b) (d)After cropping Example kernels, generated step by step. FlexConv samples a kernel from MLP ? (a), which is attenuated by an anistropic Gaussian envelope with learned parameters ? (l) (b), creating (c) which is cropped to contain only values of &gt; 0.1 (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Example kernels from FlexNet-16 models trained (i) without regularization, (ii) with aliasing regularization of f + MAGNet , (iii) with aliasing regularization of f + FlexConv . In the columns, from left to right: (i) original kernel at 33 ? 33, (ii) FFT of the original kernel, (iii) kernel inferred at 65 ? 65, to find aliasing effects, (iiii) FFT of the 65 ? 65 kernel, with the solid line showing the Nyquist frequency of the 33 ? 33 kernel, and the red dotted line showing the maximum frequency component as computed by our analysis. For f + FlexConv the maximum frequency matches almost exactly with the Nyquist frequency, showing that our aliasing regularization works. For f +</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Maximum frequency component of a MAGNet. Fathony et al. (2021) characterize the expansion of each term of the isotropic Gabor layers in MFNs in the final MFN output. In Eq. 25, Fathony et al. (2021) demonstrate that the MFN representation contains a set of sine frequencies ? given by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy and ablation studies on sMNIST, pMNIST, sCIFAR10 and npCIFAR10.</figDesc><table><row><cell>MODEL</cell><cell cols="5">SIZE SMNIST PMNIST SCIFAR10 NPCIFAR10</cell></row><row><cell>DilRNN (Chang et al., 2017)</cell><cell>44K</cell><cell>98.0</cell><cell>96.1</cell><cell>-</cell><cell>-</cell></row><row><cell>IndRNN (Li et al., 2018)</cell><cell>83K</cell><cell>99.0</cell><cell>96.0</cell><cell>-</cell><cell>-</cell></row><row><cell>TCN (Bai et al., 2018a)</cell><cell>70K</cell><cell>99.0</cell><cell>97.2</cell><cell>-</cell><cell>-</cell></row><row><cell>r-LSTM (Trinh et al., 2018)</cell><cell>0.5M</cell><cell>98.4</cell><cell>95.2</cell><cell>72.2</cell><cell>-</cell></row><row><cell>Self-Att. (Trinh et al., 2018)</cell><cell>0.5M</cell><cell>98.9</cell><cell>97.9</cell><cell>62.2</cell><cell>-</cell></row><row><cell>TrellisNet (Bai et al., 2018b)</cell><cell>8M</cell><cell>99.20</cell><cell>98.13</cell><cell>73.42</cell><cell>-</cell></row><row><cell>URLSTM (Gu et al., 2020b)</cell><cell>-</cell><cell>99.28</cell><cell>96.96</cell><cell>71.00</cell><cell>-</cell></row><row><cell>URGRU + Zoneout (Gu et al., 2020b)</cell><cell>-</cell><cell>99.27</cell><cell>96.51</cell><cell>74.40</cell><cell>-</cell></row><row><cell>HiPPO (Gu et al., 2020a)</cell><cell>0.5M</cell><cell>-</cell><cell>98.30</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Lipschitz RNN (Erichson et al., 2020) 158K</cell><cell>99.4</cell><cell>97.3</cell><cell>64.2</cell><cell>59.0</cell></row><row><cell>coRNN (Rusch &amp; Mishra, 2020)</cell><cell>134K</cell><cell>99.4</cell><cell>97.3</cell><cell>-</cell><cell>59.0</cell></row><row><cell cols="2">UnICORNN (Rusch &amp; Mishra, 2021) 135K</cell><cell>-</cell><cell>98.4</cell><cell>-</cell><cell>62.4</cell></row><row><cell>pLMU (Chilkuri &amp; Eliasmith, 2021)</cell><cell>165K</cell><cell>-</cell><cell>98.49</cell><cell>-</cell><cell>-</cell></row><row><cell>CKCNN-2</cell><cell>98K</cell><cell>99.31</cell><cell>98.00</cell><cell>62.25</cell><cell>60.5</cell></row><row><cell>CKCNN-2-Big</cell><cell>1M</cell><cell>99.32</cell><cell>98.54</cell><cell>63.74</cell><cell>62.2</cell></row><row><cell>CKTCNFOURIER-2</cell><cell>105K</cell><cell>99.44</cell><cell>98.40</cell><cell>68.28</cell><cell>66.26</cell></row><row><cell>CKTCNGABOR-2</cell><cell>106K</cell><cell>99.52</cell><cell>98.38</cell><cell>69.26</cell><cell>67.37</cell></row><row><cell>CKTCNMAGNET-2</cell><cell>105K</cell><cell>99.55</cell><cell>98.57</cell><cell>74.58</cell><cell>67.52</cell></row><row><cell>FlexTCN-2</cell><cell>108K</cell><cell>99.60</cell><cell>98.61</cell><cell>78.99</cell><cell>67.11</cell></row><row><cell>FlexTCN-4</cell><cell>241K</cell><cell>99.60</cell><cell>98.72</cell><cell>80.26</cell><cell>67.42</cell></row><row><cell>FlexTCN-6</cell><cell>375K</cell><cell>99.62</cell><cell>98.63</cell><cell>80.82</cell><cell>69.87</cell></row><row><cell>FlexTCNSIREN-6</cell><cell>343K</cell><cell>99.03</cell><cell>95.36</cell><cell>69.24</cell><cell>57.27</cell></row><row><cell>FlexTCNFourier-6</cell><cell>370K</cell><cell>99.49</cell><cell>97.97</cell><cell>74.79</cell><cell>67.35</cell></row><row><cell>FlexTCNGabor-6</cell><cell>373K</cell><cell>99.50</cell><cell>98.37</cell><cell>78.36</cell><cell>67.56</cell></row><row><cell>FlexTCNMAGNet-6</cell><cell>375K</cell><cell>99.62</cell><cell>98.63</cell><cell>80.82</cell><cell>69.87</cell></row></table><note>time and parameter count. MAGNets, on the other hand, accurately model large high frequency signals. This allows FlexNets to learn large kernels with high frequency components.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy on CT, SC and SC_raw</figDesc><table><row><cell>MODEL</cell><cell>SIZE</cell><cell>CT</cell><cell>SC</cell><cell>SC_RAW</cell></row><row><cell>GRU-ODE GRU-?t GRU-D ODE-RNN NCDE</cell><cell>89K 89K 89K 89K 89K</cell><cell>96.2 97.8 95.9 97.1 98.8</cell><cell>44.8 20.0 23.9 93.2 88.5</cell><cell>?10.0 ?10.0 ?10.0 ?10.0 ?10.0</cell></row><row><cell>CKCNN</cell><cell>100K</cell><cell cols="2">99.53 95.27</cell><cell>71.66</cell></row><row><cell>CKTCNFourier</cell><cell></cell><cell>-</cell><cell>95.65</cell><cell>74.90</cell></row><row><cell>CKTCNGabor</cell><cell></cell><cell>-</cell><cell>96.66</cell><cell>78.10</cell></row><row><cell>CKTCNMAGNet</cell><cell>105K</cell><cell cols="2">99.53 97.01</cell><cell>80.69</cell></row><row><cell>FlexTCN-2</cell><cell cols="3">105SK 99.53 97.10</cell><cell>88.03</cell></row><row><cell>FlexTCN-4</cell><cell>239K</cell><cell cols="2">99.53 97.73</cell><cell>90.45</cell></row><row><cell>FlexTCN-6</cell><cell>373K</cell><cell cols="2">99.53 97.67</cell><cell>91.73</cell></row><row><cell>FlexTCNSIREN-6</cell><cell>370K</cell><cell>-</cell><cell>95.83</cell><cell>85.73</cell></row><row><cell>FlexTCNFourier-6</cell><cell>342K</cell><cell>-</cell><cell>97.62</cell><cell>91.02</cell></row><row><cell>FlexTCNGabor-6</cell><cell>373K</cell><cell>-</cell><cell>97.35</cell><cell>91.50</cell></row><row><cell>FlexTCNMAGNet-6</cell><cell>373K</cell><cell>-</cell><cell>97.67</cell><cell>91.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on CIFAR-10. Results from *original works and ? single run.</figDesc><table><row><cell>MODEL</cell><cell>SIZE</cell><cell>CIFAR-10 ACC.</cell><cell>TIME (SEC/EPOCH)</cell></row><row><cell>CIFARResNet-44</cell><cell>0.66M</cell><cell>92.9* ?</cell><cell>22</cell></row><row><cell>DCN</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>results are summarized in Tables 1 and 2. FlexTCNs with two residual blocks obtain state-ofthe-art results on all tasks considered. In addition, depth further improves performance. FlexTCN-6 improves the current state-of-the-art on sCIFAR10 and npCIFAR10 by more than 6%. On the difficult SC_raw dataset -with sequences of length 16000-, FlexTCN-6 outperform the previous state-of-the-art by 20.07%: a remarkable improvement. Furthermore, we conduct ablation studies by changing the parameterization of MLP ? , and switching off the learnable kernel size ("CKTCNs") and considering global kernel sizes instead. CKTCNs and FlexTCNs with MAGNet kernels outperform corresponding models with all other kernel parameterizations: SIRENs<ref type="bibr" target="#b53">(Sitzmann et al., 2020)</ref>, MGNs and MFNs<ref type="bibr" target="#b13">(Fathony et al., 2021)</ref>. Moreover, we see a consistent improvement with respect to CKCNNs (Romero et al., 2021) by using learnable kernel sizes. This shows that both MAGNets and learnable kernel sizes contribute to the performance of FlexTCNs. Note that in 1D, MAGNets are equivalent to MGNs. However, MAGNets consistently perform better than MGNs. This improvement in accuracy is a result of our MAGNet initialization.</figDesc><table><row><cell></cell><cell></cell><cell>3</cell><cell>242</cell></row><row><cell>FlexNet-16 w/ N-Jet</cell><cell>0.70M</cell><cell>91.7 ? 0.1</cell><cell>409</cell></row><row><cell>CKCNN-16 CKCNNMAGNet-16 FlexNetSIREN-16 FlexNetGabor-16</cell><cell>0.63M 0.67M 0.63M 0.67M</cell><cell>72.1 ? 0.2 86.8 ? 0.6 89.0 ? 0.3 91.9 ? 0.2</cell><cell>68 102 89 161</cell></row><row><cell cols="2">FlexNetGabor-16 + anis. Gauss. 0.67M FlexNetGabor-16 + Gabor init. 0.67M</cell><cell>92.0 ? 0.1 92.0 ? 0.2</cell><cell>147 150</cell></row><row><cell>FlexNet-16</cell><cell>0.67M</cell><cell>92.2 ? 0.1</cell><cell>127</cell></row><row><cell>Our</cell><cell></cell><cell></cell><cell></cell></row></table><note>discrete data: sequential MNIST, permuted MNIST (Le et al., 2015), sequential CIFAR10 (Chang et al., 2017), noise-padded CIFAR10 (Chang et al., 2019), as well as time-series data: CharacterTrajectories (CT) (Bagnall et al., 2018), SpeechCommands (Warden, 2018) with raw waveform (SC_raw) and MFCC input representations (SC).Image classification. Next, we evaluate FlexNets for image classification on CIFAR-10 (Krizhevsky</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Accuracy change at upsampling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>f + FlexConv</cell><cell>20?20 16?16</cell><cell>?0.4% ?1.7%</cell><cell>?0.5% ?1.7%</cell><cell>?1.4% ?3.4%</cell><cell>?0.4% ?1.9%</cell><cell>?0.4% ?1.9%</cell><cell>+1.2% +3.2%</cell><cell>+1.4% +3.0%</cell><cell>+3.0% +5.9%</cell><cell>+1.7% +4.0%</cell><cell>+1.8% +3.8%</cell></row><row><cell>f + MAGNet</cell><cell>20?20 16?16</cell><cell>?5.1% ?3.0%</cell><cell>?3.8% ?6.2%</cell><cell>?3.9% ?4.9%</cell><cell>?4.8% ?3.9%</cell><cell>?4.5% ?7.4%</cell><cell>+6.3% +8.6%</cell><cell>+5.1% +7.3%</cell><cell>+5.7% +4.7%</cell><cell>+5.9% +5.6%</cell><cell>+5.6% +9.3%</cell></row><row><cell></cell><cell></cell><cell>28?28</cell><cell>30?30</cell><cell>32?32</cell><cell>34?34</cell><cell>36?36</cell><cell>28?28</cell><cell>30?30</cell><cell>32?32</cell><cell>34?34</cell><cell>36?36</cell></row></table><note>shows that FlexNets outperform CKCNNs with corresponding kernel parameterizations. In addition, a clear difference in performance is apparent for MAGNets with respect to other parameterizations. These results corroborate that both MAGNets and FlexConvs contribute to the performance of FlexNets. Moreover, Tab. 3 illustrates the effect of the two contributions of MAGNet over MGN: anisotropic Gabor filters, and our improved initialization. Our results in image data are in unison with our previous results for sequential data (Tabs. 1, 2) and illustrate the value of the proposed improvements in MAGNets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Alias-free FlexNets on CIFAR-10.</figDesc><table><row><cell>MODEL</cell><cell>SIZE</cell><cell cols="2">CIFAR-10 ACC. 16 px ? 16px 32 px</cell></row><row><cell>CIFARResNet-44</cell><cell cols="3">0.66M 85.8 ? 0.2 -31.6 ? 1.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>.6</cell></row><row><cell>FlexNet-16 w/ N-Jets</cell><cell cols="2">0.70M 86.4 ? 0.2</cell><cell>-5.5 ? 1.3</cell></row><row><cell>CKCNN-16 SIREN</cell><cell cols="3">0.63M 45.9 ? 1.0 -15.8 ? 1.2</cell></row><row><cell>FlexNet-16 SIREN</cell><cell cols="3">0.63M 70.4 ? 0.8 -50.0 ? 16.9</cell></row><row><cell>FlexNet-16 w/o reg.</cell><cell cols="3">0.67M 86.4 ? 0.4 -34.4 ? 14.3</cell></row><row><cell>FlexNet-16 w/ reg. f + MAGNet FlexNet-16 w/ reg. f + FlexConv</cell><cell cols="2">0.67M 86.5 ? 0.1 0.67M 85.1 ? 0.3</cell><cell>-3.8 ? 2.0 -3.3 ? 0.3</cell></row></table><note>FlexNet-16 w/ conv. (k = 3) 0.17M 85.3 ? 0.2 -21.2 ? 1.0 FlexNet-16 w/ conv. (k = 33) 20.0M 67.7 ? 0.6 -57.1 ? 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Average PSNR for fitting of images in the Kodak dataset. Both our improved initialization scheme, as well as the inclusion of anisotropic Gabor functions lead to better reconstructions.</figDesc><table><row><cell>MODEL</cell><cell># PARAMS</cell><cell cols="2">IMPROVED PSNR INIT</cell></row><row><cell>SIREN</cell><cell>7.14K</cell><cell>-</cell><cell>25.665</cell></row><row><cell>MFN Fourier</cell><cell>7.40K</cell><cell>-</cell><cell>23.276</cell></row><row><cell>MFN Gabor</cell><cell>7.11K</cell><cell></cell><cell>25.361 25.606</cell></row><row><cell>MAGNet</cell><cell>7.36K</cell><cell></cell><cell>25.791 25.893</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Full results on CIFAR-10. We report results over three runs per setting. CIFARResNet-44 w/ CKConv is a CIFARResNet-44 where all convolutional layers are replaced with CKConvs with k = 3. CIFARResNet-44 w/ FlexConv is a CIFARResNet-44 where all convolutional layers are replaced with FlexConv with learned kernel size, except for the shortcut connections of the strided convolutional layers, which are pointwise convolutions. *Results are taken from the respective original works instead of reproduced. ?Results are from single run. Tab. 6 shows all results for our CIFAR-10 experiments, including more ablations.ImageNet-32. Results for the ImageNet-32 experiment are shown inTable 7. FlexNets are slightly worse than CIFARResNet-32<ref type="bibr" target="#b19">(He et al., 2016)</ref> with slightly less parameters. However, the results</figDesc><table><row><cell>MODEL</cell><cell>SIZE</cell><cell>CIFAR-10 ACC.</cell></row><row><cell>DCN-? ji (Tomen et al., 2021)</cell><cell cols="2">0.47M 89.7 ? 0.3*</cell></row><row><cell>N-Jet-CIFARResNet32 (Pintea et al., 2021)</cell><cell cols="2">0.52M 92.3 ? 0.3*</cell></row><row><cell>N-Jet-ALLCNN (Pintea et al., 2021)</cell><cell cols="2">1.07M 92.5 ? 0.1*</cell></row><row><cell>CIFARResNet-44 (He et al., 2016)</cell><cell>0.66M</cell><cell>92.9* ?</cell></row><row><cell cols="3">CIFARResNet-44 (He et al., 2016) (our reproduction) 0.66M 90.9 ? 0.2</cell></row><row><cell>CIFARResNet-44 w/ CKConv (k = 3)</cell><cell cols="2">2.58M 86.1 ? 0.9</cell></row><row><cell>CIFARResNet-44 w/ FlexConv</cell><cell cols="2">2.58M 81.6 ? 0.8</cell></row><row><cell>FlexNet-7 w/ conv. (k = 3)</cell><cell cols="2">0.17M 89.5 ? 0.3</cell></row><row><cell>FlexNet-7 w/ conv. (k = 33)</cell><cell cols="2">20.0M 78.0 ? 0.3</cell></row><row><cell>FlexNet-7 w/ N-Jet (Pintea et al., 2021)</cell><cell cols="2">0.70M 91.7 ? 0.1</cell></row><row><cell>CKCNN SIREN -3</cell><cell>0.26M</cell><cell>72.4*</cell></row><row><cell>CKCNN Fourier -3</cell><cell>0.27M</cell><cell>83.8*</cell></row><row><cell>CKCNN Gabor -3</cell><cell>0.28M</cell><cell>85.6*</cell></row><row><cell>CKCNN MAGNet -3</cell><cell>0.28M</cell><cell>86.2*</cell></row><row><cell>CKCNN-7</cell><cell>0.63M</cell><cell>71.7*</cell></row><row><cell>CKCNN Fourier -7</cell><cell>0.63M</cell><cell>84.6*</cell></row><row><cell>CKCNN Gabor -7</cell><cell>0.67M</cell><cell>87.7*</cell></row><row><cell>CKCNN MAGNet -7</cell><cell>0.67M</cell><cell>85.9*</cell></row><row><cell>FlexNet SIREN -7</cell><cell>0.63M</cell><cell>88.9*</cell></row><row><cell>FlexNet Fourier -7</cell><cell>0.66M</cell><cell>91.6*</cell></row><row><cell>FlexNet Gabor -7</cell><cell>0.67M</cell><cell>92.0*</cell></row><row><cell>FlexNet-3</cell><cell cols="2">0.27M 90.4 ? 0.2</cell></row><row><cell>FlexNet-5</cell><cell cols="2">0.44M 91.0 ? 0.5</cell></row><row><cell>FlexNet-7</cell><cell cols="2">0.67M 92.2 ? 0.1</cell></row><row><cell>C ADDITIONAL EXPERIMENTS</cell><cell></cell><cell></cell></row><row><cell>C.1 IMAGE CLASSIFICATION</cell><cell></cell><cell></cell></row><row><cell>CIFAR-10.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Results on ImageNet-32. *Results are taken from the respective original works instead of reproduced. ?Results are from a single run.</figDesc><table><row><cell>MODEL</cell><cell>SIZE</cell><cell cols="2">IMAGENET-32 TOP-1 TOP-5</cell></row><row><cell cols="4">CIFARResNet-32 0.53M 26.41 ? 0.13 49.37 ? 0.15</cell></row><row><cell>WRN-28-1</cell><cell>0.44M</cell><cell>32.03* ?</cell><cell>57.51* ?</cell></row><row><cell>FlexNet-5</cell><cell>0.44M</cell><cell>24.9 ? 0.4</cell><cell>47.7 ? 0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Results for alias-free FlexNets on CIFAR-10 and ImageNet-k. ? denotes difference in accuracy.</figDesc><table><row><cell>MODEL</cell><cell>SIZE</cell><cell>IMAGENET-K TOP-1 k = 16 ? k=16 k = 32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Results on MNIST. We train each model with three different seeds and report mean and standard deviation. *Results are taken from the respective original works instead of reproduced. ?Results are from single run.</figDesc><table><row><cell>MODEL</cell><cell>SIZE</cell><cell>MNIST ACC.</cell></row><row><cell cols="2">Efficient-CapsNet (Mazzia et al., 2021) 0.16M</cell><cell>99.8* ?</cell></row><row><cell>Network in Network</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is publicly available at https://github.com/rjbruin/flexconv.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Nergis T?men for her valuable insights regarding signal processing principles for FlexConv, and Silvia-Laura Pintea for explanations and access to code of her work . We thank Yerlan Idelbayev for the use of the CIFARResNet code. This work was partially carried out on the Dutch national infrastructure with the support of SURF Cooperative. We used Weights &amp; Biases <ref type="bibr" target="#b3">(Biewald, 2020)</ref> for experiment tracking and visualizations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The uea multivariate time series classification archive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Southam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keogh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00075</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06682</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Experiment tracking with weights and biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Biewald</surname></persName>
		</author>
		<ptr target="https://www.wandb.com/.Softwareavailablefromwandb.com" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldad</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09689</idno>
		<title level="m">Antisymmetricrnn: A dynamical system view on recurrent neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Parallelizing legendre memory unit training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narsimha</forename><surname>Chilkuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11417</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the CIFAR datasets. CoRR, abs/1707.08819</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1707.08819" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On the relationship between selfattention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Complete discrete 2-d gabor transforms by neural networks for image analysis and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Daugman</surname></persName>
		</author>
		<idno type="DOI">10.1109/29.1644</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1169" to="1179" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>N Benjamin Erichson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Azencot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Queiruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Hodgkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahoney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12070</idno>
		<title level="m">Lipschitz recurrent neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiplicative filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rizal</forename><surname>Fathony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Willmott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Zico</forename><surname>Kolter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OmtmcPkkhT" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12880</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07669</idno>
		<title level="m">Recurrent memory with optimal polynomial projections</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving the gating mechanism of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3800" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">De moivre&apos;s normal approximation to the binomial, 1733, and its generalization. A History of Parametric Statistical Inference from Bernoulli to Fisher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Hald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimizing filter size in convolutional neural networks for facial action unit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibo</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structured receptive fields in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2610" to="2619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moloud</forename><surname>Hm Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed Mohammad Jafar</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03347</idno>
		<title level="m">Spinalnet: Deep neural network with gradual input</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12423</idno>
		<title level="m">Alias-free generative adversarial networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On translation invariance in cnns: Convolutional layers can exploit absolute spatial location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Osman Semih Kayhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kodak</surname></persName>
		</author>
		<ptr target="http://r0k.us/graphics/kodak/" />
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
	<note type="report_type">Kodak dataset</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scale-covariant and scale-invariant gaussian derivative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
		</author>
		<idno>978-3-030-75548-5. doi: 10.1007/ 978-3-030-75549-2_1</idno>
		<ptr target="https://arxiv.org/abs/2011.14759.NotduplicatewithDiVA" />
	</analytic>
	<monogr>
		<title level="m">Scale Space and Variational Methods in Computer Vision</title>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">12679</biblScope>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Supervised scale-regularized linear convolutionary filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Lauze</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.31.162</idno>
		<ptr target="https://dx.doi.org/10.5244/C.31.162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<editor>Gabriel Brostow Tae-Kyun Kim, Stefanos Zafeiriou and Krystian Mikolajczyk</editor>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Extended batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanling</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05569</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Efficient-capsnet: Capsule network with self-attention routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Mazzia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Salvetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Chiaberge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.12491</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Robust deep learning for computer vision to counteract data scarcity and label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duc</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">01</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nergis</forename><surname>Silvia L Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Goes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03412</idno>
		<title level="m">Resolution learning in deep convolutional networks using scale-space theory</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Ckconv: Continuous kernel convolution for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>David W Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoogendoorn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02611</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Coupled oscillatory recurrent neural network (cornn): An accurate and (gradient) stable architecture for learning long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00951</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unicornn: A recurrent model for learning very long time dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Mishra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05487</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristof</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huziel Enoc Sauceda</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Blurring the line between structure and learning to optimize and adapt receptive fields. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v15/srivastava14a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spatially-adaptive filter units for compact and efficient deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domen</forename><surname>Tabernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01282-1</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep continuous networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nergis</forename><surname>Tomen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia-Laura</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Gemert</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v139/tomen21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<editor>Marina Meila and Tong Zhang</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hugo Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00144</idno>
		<title level="m">Learning longer-term dependencies in rnns with auxiliary losses</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Romijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03489</idno>
		<title level="m">Impact of aliasing on generalization in deep convolutional networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2589" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<title level="m">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Variational context-deformable convnets for indoor scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitong</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianhui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7324" to="7334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
