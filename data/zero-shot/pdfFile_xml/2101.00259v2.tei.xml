<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Code Generation from Natural Language with Less Prior and More Monolingual Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajad</forename><surname>Norouzi</surname></persName>
							<email>sajadn@cs.toronto.edu</email>
							<affiliation key="aff0">
								<address>
									<settlement>Borealis</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyi</forename><surname>Tang</surname></persName>
							<email>keyi.tang@borealisai.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Borealis</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
							<email>yanshuai.cao@borealisai.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Borealis</settlement>
									<region>AI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Code Generation from Natural Language with Less Prior and More Monolingual Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training datasets for semantic parsing are typically small due to the higher expertise required for annotation than most other NLP tasks. As a result, models for this application usually need additional prior knowledge to be built into the architecture or algorithm. The increased dependency on human experts hinders automation and raises the development and maintenance costs in practice. This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance with minimal code-generation-specific inductive bias design. By exploiting a relatively sizeable monolingual corpus of the target programming language, which is cheap to mine from the web, we achieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa. Both are SOTA to the best of our knowledge. This positive evidence highlights a potentially easier path toward building accurate semantic parsers in practice. ?</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For a machine to act upon users' natural language inputs, a model needs to convert the natural language utterances to machine-understandable meaning representation, i.e. semantic parsing (SP). The output meaning representation is beyond shallow identification of topic, intention, entity or relation, but complex structured objects expressed as logical forms, query language or general-purpose programs. Therefore, annotating parallel corpus for semantic parsing requires more costly expertise.</p><p>SP shares some resemblance with machine translation (MT). However, SP datasets are typically smaller, with only a few thousand to at most tens of thousands of examples, even smaller than most low resource MT problems. Simultaneously, because * Work done during internship at BorealisAI ? Code at https://github.com/BorealisAI/code-gen-TAE <ref type="figure" target="#fig_3">Figure 1</ref>: TAE: the monolingual corpus is used both as source and target. The encoder is frozen in the computation branch on the monolingual data.</p><p>the predicted outputs generally need to be exactly correct to execute and produce the right answer, the accuracy requirement is generally higher than MT. As a result, inductive bias design in architecture and algorithm has been prevalent in the SP literature <ref type="bibr" target="#b6">(Dong and Lapata, 2016;</ref><ref type="bibr">Neubig, 2017, 2018;</ref><ref type="bibr" target="#b7">Dong and Lapata, 2018;</ref><ref type="bibr" target="#b12">Guo et al., 2019;</ref><ref type="bibr" target="#b27">Wang et al., 2019;</ref><ref type="bibr" target="#b34">Yin and Neubig, 2019)</ref>. While their progress is remarkable, excessive task-specific expert design makes the models complicated, hard to transfer to new domains, and challenging to deploy in real-world applications. In this work, we look at the opposite end of the spectrum and try to answer the following question: with little inductive bias in the model, and no additional labelled data, is it still possible to achieve competitive performance? This is an important question, as the answer could point to a much shorter road to practical SP without breaking the bank.</p><p>This paper shows that the answer is encouragingly affirmative. By exploiting a relatively large monolingual corpus of the programming language, a transformer-based Seq2Seq model <ref type="bibr" target="#b26">(Vaswani et al., 2017)</ref> with little SP specific prior could potentially attain results superior to or competitive with the state-of-the-art models specially designed for semantic parsing. Our contributions are three-fold:</p><p>? We provide evidence that transformer-based seq2seq models can reach a competitive or superior performance with models specifically designed for semantic parsing. This suggests an alternative route for future progress other than inductive bias design;</p><p>? We do empirical analysis over previously proposed approaches for incorporating monolingual data and show the effectiveness of our modified technique on a range of datasets;</p><p>? We set the new state-of-the-art on Django <ref type="bibr" target="#b19">(Oda et al., 2015)</ref> reaching 81.03% exact match accuracy and on CoNaLa  with a BLEU score of 32.57.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work on Semantic Parsing</head><p>Different sources of prior knowledge about the SP problem structure could be exploited. Input structure: <ref type="bibr" target="#b27">Wang et al. (2019)</ref> adapts the transformer relative position encoding <ref type="bibr" target="#b23">(Shaw et al., 2018)</ref> to express relations among the database schema elements as well as with the input text spans. <ref type="bibr" target="#b13">Herzig and Berant (2020)</ref> proposed a spanbased neural parser with compositional inductive bias built-in. <ref type="bibr" target="#b13">Herzig and Berant (2020)</ref> also leverages a CKY-style <ref type="bibr" target="#b3">(Cocke, 1969;</ref><ref type="bibr" target="#b15">Kasami, 1966;</ref><ref type="bibr" target="#b35">Younger, 1967)</ref> inference to link input features to output codes. Output structure: The implicit tree or graph-like structures in the programs can also be exploited. <ref type="bibr" target="#b6">Dong and Lapata (2016)</ref> proposed parent-feeding LSTM following the tree structure. <ref type="bibr" target="#b7">Dong and Lapata (2018)</ref> proposed a coarse-to-fine decoding approach. <ref type="bibr" target="#b12">Guo et al. (2019)</ref> crafted an intermediate meaning representation to bridge the large gap between input utterance and the output SQL queries. <ref type="bibr">Neubig (2017, 2018)</ref> proposed TranX, a more general-purpose transition-based system, to ensure grammaticality of predictions. Using TranX, the neural model predicts the linear sequence of AST-tree constructing actions instead of the program tokens. However, a human expert needs to craft the grammar, and the design quality impacts the learning and generalization for the neural nets. Sequential models with less SP specific priors have been investigated <ref type="bibr" target="#b6">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b18">Ling et al., 2016b;</ref><ref type="bibr" target="#b36">Zeng et al., 2020)</ref>, However, they generally fell short in accuracy comparing to the best of structure-exploiting ones listed above.</p><p>The most closely related to ours is the work by <ref type="bibr" target="#b29">Xu et al. (2020)</ref> for incorporating external knowledge from extra datasets, which used a noisy parallel dataset from Stackoverflow to pre-train the SP and fine-tuned it on the primary dataset. Their approach's main limitation is still the need for (noisy) parallel data, albeit cheaper than the primary labelled set. Nonetheless, as we shall see in the experiment section later, our approach achieves better results when using the same amount of data mined from the same source despite ignoring the source sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background and Methodology</head><p>BERT <ref type="bibr" target="#b5">(Devlin et al., 2018)</ref> class of pre-trained models can make up for the lack of inductive bias on the input side to some degree. On the output side, we hope to learn the necessary prior knowledge about the target meaning representation from unlabelled monolingual data.</p><p>Using monolingual data to improve seq2seq models is not new and has been extensively studied in MT before. Notable methods include fusion <ref type="bibr" target="#b11">(Gulcehre et al., 2015;</ref><ref type="bibr" target="#b21">Ramachandran et al., 2016;</ref><ref type="bibr" target="#b24">Sriram et al., 2018;</ref><ref type="bibr" target="#b25">Stahlberg et al., 2018)</ref>, backtranslation (BT) <ref type="bibr" target="#b22">(Sennrich et al., 2015;</ref><ref type="bibr" target="#b8">Edunov et al., 2018;</ref><ref type="bibr" target="#b14">Hoang et al., 2018)</ref>, <ref type="bibr" target="#b4">(Currey et al., 2017;</ref><ref type="bibr">Yvon, 2018, 2019)</ref>, and BT with copied monolingual data <ref type="bibr" target="#b4">(Currey et al., 2017;</ref><ref type="bibr" target="#b1">Burlot and Yvon, 2019)</ref>. However, due to more structured outputs, less training data, and different evaluation metrics of exact match correctness instead of BLEU, it is unclear if these lessons transfer from MT to SP. So SP-specific investigation is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Target Autoencoding with Frozen Encoder</head><p>We assume a parallel corpus of natural language utterances and their corresponding programs, B = {x x x i , y y y i }. The goal is to train a translator model (TM) to maximize the conditional log probability of y y y i given x x x i , T ? ? ? (y y y i |x x x i ), over the training set:</p><formula xml:id="formula_0">L sup = B T ? ? ? (y y y i | x x x i )</formula><p>where ? ? ? is the vector of TM model parameters. Let M = {y y y i } denote the monolingual dataset in the target language. <ref type="bibr" target="#b4">Currey et al. (2017)</ref>; Burlot and Yvon (2019) demonstrated that in low resource MT, autoencoding the monolingual data besides the main supervised training is helpful. Following the same path, we add an auto-encoding objective term on monolingual data: L full = L sup + M T ? ? ? (y y y i | y y y i ). The target y y y i 's are reconstructed using the shared encoder-decoder model.</p><p>We conjecture that monolingual data autoencoding mainly helps the decoder, so we propose to freeze the encoder parameters for monolingual data. Writing the encoder and decoder parameters separately with ? ? ? = [? ? ? e , ? ? ? d ], then ? ? ? e is updated using the gradient of the supervised objective L sup , whereas the decoder gradient comes from L full . We verify this hypothesis in section 4.1.</p><p>In terms of model architecture, our TM is a standard transformer-based seq2seq model with copy attention <ref type="bibr" target="#b10">(Gu et al., 2016)</ref> (illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> of C). We fine-tune BERT as the encoder and use a 4-layer transformer decoder. There is little SPspecific inductive bias in the architecture. The only special structure is the copy attention, which is not a strong inductive bias designed for SP as copy attention is widely used in other tasks as well.</p><p>We refer to the method of using copied monolingual data and freezing the encoder over them as target autoencoding (TAE). Unless otherwise specified in the ablation studies, the encoder is always frozen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>For our primary experiments we considered two python datasets namely Django and CoNaLa. The former is based on Django web framework and the latter is annotated code snippets from stackoverflow answers. Additionally, we experiment on the SQL version of GeoQuery and ATIS from Finegan-Dollak et al. (2018) (with query split), WikiSQL <ref type="bibr" target="#b37">(Zhong et al., 2017)</ref>, and Magic (Java) <ref type="bibr" target="#b18">(Ling et al., 2016b)</ref>.</p><p>Python Monolingual Corpora: CoNaLa comes with 600K mined questions from Stackoverflow. We ignored the noisy source intents/sentences and just use the python snippets. To be comparable with Xu et al. <ref type="formula">(2020)</ref>, we also select a corresponding 100K subset version for comparison. See Appendix A for details on the SQL and Java monolingual corpora.</p><p>Experimental Setup: In all experiments, we use label smoothing with a parameter of 0.1 and Polyak averaging (Polyak and Juditsky, 1992) of parameters with a momentum of 0.999 except for GeoQuery which we use 0.995. We use Adam (Kingma and Ba, 2014) and early stopping based on the dataset specific evaluation metric on dev set. The learning rate for the encoder is 1 ? 10 ?5 over all datasets. We used the learning rate of 7.5 ? 10 ?5 on all datasets except GeoQuery and ATIS which we use 1 ? 10 ? 4. The architecture overview is shows in <ref type="figure" target="#fig_0">Fig. 2</ref>. At the inference time we use beam search with beam size of 10 and a length normalization based on <ref type="bibr" target="#b28">(Wu et al., 2016)</ref>. We run each experiment with 5 different random seeds and report the average and standard deviation. WordPiece tokenization is used for both natural language utterances and programming code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Empirical Analysis</head><p>First, we considered a scenario where the monolingual corpus comes from the same distribution as the bitext. We simulate this setup by using 10% of Django training data as labeled data while using all the python examples from Django as the mono-   lingual dataset of 10 times bigger. Results with "Authentic Dataset" in <ref type="figure" target="#fig_1">Fig. 3</ref> shows the effectiveness of TAE vs other approaches.</p><p>Next, we used the monolingual dataset prepared for python (StackOverflow Corpus) which is from a different distribution. <ref type="figure" target="#fig_1">Fig. 3</ref> shows even more considerable improvement, thanks to the larger monolingual set. We considered noisy intents provided in CoNaLa monolingual corpus and dummy source sentences where each monolingual sample is paired along with a random length array containing zeros. We also compared against other well-known approaches like fusion and back-translation, see experiments details in Appendix D. TAE outperforms all those approaches by a large margin. Now one important question is, what part of the model benefits from monolingual data most? In Sec. 3.1, we conjectured that auto-encoding of monolingual data should mostly help the decoder, not the encoder. To verify this, we perform an ablation by comparing freezing encoder parameters versus not freezing over the monolingual set. <ref type="figure" target="#fig_1">Fig. 3</ref> shows that without freezing the encoder, performance drops slightly for TAE on authentic Django while dropping significantly when copying on Stackoverflow data. This confirms that the performance gain is due to its effect on the decoder, while the copied monolingual data might even hurts the encoder.  <ref type="formula">(2020)</ref> that uses the additional python API bitext data. Finally, note that part of our superior performance is due to using BERT as an encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results on Full Data</head><p>Finally, TAE also yields improvements on other programming languages, as shown for GeoQuery (SQL), ATIS (SQL) and Magic (Java) in <ref type="table" target="#tab_7">Table 4</ref>. We observe no improvement on WikiSQL. But it is not surprising given its large dataset size and the simplicity of its targets. As observed by previous works <ref type="bibr" target="#b9">(Finegan-Dollak et al., 2018)</ref>, more than half of queries follow simple pattern of "SELECT col FROM table WHERE col = value".</p><p>The main results in terms of improvement over previous best methods are statistically significant in <ref type="table" target="#tab_2">Table 2</ref>-3. On Django, our result is better than Reranker (Yin and Neubig, 2019) (best previous method in <ref type="table" target="#tab_2">Table 2</ref>) with a P-value &lt; 0.05, under one-tailed two-sample t-test for mean equality. Since the previous state of the art on CoNaLa (EK + 100k + API in <ref type="table" target="#tab_6">Table 3</ref>) did not provide the standard deviation, we cannot conduct a two-sample t-test against it. Instead, we performed a one-tailed two-sample t-test against the TranX+BERT baseline and observed that our improvement is statistically significant with P-value &lt; 0.05. In <ref type="table" target="#tab_7">Table 4</ref>, Model Django YN17 <ref type="bibr" target="#b32">(Yin and Neubig, 2017)</ref> 71.6 TRANX  73.7 Coarse2Fine <ref type="bibr" target="#b7">(Dong and Lapata, 2018)</ref> 74.1 TRANX2 <ref type="bibr" target="#b34">(Yin and Neubig, 2019)</ref> 77.3 ? 0.4 TRANX2 + BERT 79.7 ? 0.42 Reranker <ref type="bibr" target="#b34">(Yin and Neubig, 2019</ref>   <ref type="bibr" target="#b29">(Xu et al., 2020)</ref> 27.20 EK + 100k <ref type="bibr" target="#b29">(Xu et al., 2020)</ref> 28.14 EK + 100k + API <ref type="bibr" target="#b29">(Xu et al., 2020)</ref>    improvements on GeoQuery and ATIS are statistically significant with P-value &lt; 0.05, while it is not the case for Magic and WikiSQL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Thus far, we have verified that the decoder benefits from TAE and the encoder does not. For a better understanding of what TAE improves in the decoder, we propose two metrics namely copy accuracy and generation accuracy. Copy accuracy only considers tokens appearing in the source sentence. If the model produces all of the tokens that need to be copied from the source sentence, and in the right order, then the score is one otherwise zero for the example. Generation-accuracy ignores tokens appearing in the source intent and computes the exact match accuracy of the prediction. We show how to compute these metrics for the following example: Question: define the function timesince with d, now defaulting to none, reversed defaulting to false as arguments.</p><p>Ground Truth: "def timesince(d, now=none, re-versed=false): pass" We iterate over the ground truth script tokens one by one and remove those that can be copied from the source, leading to this code: Generation Ground Truth: "def (=none=):pass", and the removed tokens will be considered for copy ground truth. Copy Ground Truth: "timesince d , now , reversed false".</p><p>We would then use the copy and generation ground truth strings to compute each metric. Note that the order of tokens are still important and exact equality is required.</p><p>As shown in <ref type="table" target="#tab_9">Table 5</ref> both metrics are improved. <ref type="table" target="#tab_1">Table 1</ref> illustrates one example from each type and with more samples in the Appendix E. Copy accuracy is important for producing the right variable names mentioned, and it is improved as expected. It is also encouraging to see quantitatively and qualitatively that grammar mistakes are reduced, meaning that the lack of prior knowledge of target language structure is compensated by learning from monolingual data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work has shown the possibility to achieve a competitive or even SOTA performance on semantic parsing with little or no inductive bias design.</p><p>Besides the usual large-scale pre-trained encoders, the key is to exploit relatively large monolingual corpora of the meaning representation. The modified copied monolingual data approach from machine translation literature works well in this extremely low-resource setting. Our results point to a promising alternative direction for future progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>We used 6 datasets in total. Django includes programs from Django web framework and CoNaLa contains diverse set of intents annotated on python snippets gathered from Stackoverflow. WikiSQL, GeoQuery, and ATIS include natural language questions and their corresponding SQL queries <ref type="table">. WikiSQL includes  single table queries while GeogQuery and ATIS requires queries on more than one table.</ref> Finally, Magic has Java class implementation of game cards with different methods used during the game. <ref type="table">Table 6</ref> summarises all the parallel datasets. For GoeQuery we used query split provided by <ref type="bibr" target="#b9">(Finegan-Dollak et al., 2018)</ref>.</p><p>Monolingual Corpus: CoNaLa comes with 600K mined questions from Stackoverflow. We ignored the noisy source intents/sentences and just use the python snippets. To be comparable with <ref type="bibr" target="#b29">Xu et al. (2020)</ref>, we also select a corresponding 100K subset version for comparison. For SQL, <ref type="bibr" target="#b30">Yao et al. (2018)</ref> automatically parsed StackOverflow questions related to SQL and provided a set containing 120K SQL examples. We automatically parsed the SQL codes and removed samples with grammatical mistakes. We also filtered samples not starting with SELECT special token. <ref type="bibr" target="#b0">Allamanis and Sutton (2013)</ref> downloaded full repositories of individual projects that were forked at least once; duplicate projects were removed. We randomly sampled 100K Java examples from more than 14K projects and use that as monolingual set. <ref type="table" target="#tab_11">Table 7</ref> summarises all the monolingual datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parallel Corpus</head><p>Language Train Dev Test Django <ref type="bibr">(Oda et al., 2015) (link)</ref> Python 16000 1000 1805 CoNaLa <ref type="bibr">(Yin et al., 2018) (link)</ref> Python 2, 179 200 500 WikiSQL <ref type="bibr" target="#b37">(Zhong et al., 2017)</ref>   <ref type="bibr">(Ling et al., 2016a) (link)</ref> Java 8, 457 446 483 <ref type="table">Table 6</ref>: Parallel dataset sizes. We filtered out Magic data with java code longer than 350 tokens in order to fit in GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Corpus</head><p>Source Size Python <ref type="bibr">(Yin et al., 2018) (link)</ref> Stackoverflow 100K SQL <ref type="bibr">(Yao et al., 2018) (link)</ref> Stackoverflow 52K Java <ref type="bibr">(Allamanis and Sutton, 2013) (link)</ref> Github 100k  <ref type="table">Table 8</ref>: Dev set exact match accuracy on all datasets except CoNaLa which uses BLEU. We followed  implementation of BLEU score which can be found here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Architecture and Experiment Details</head><p>We selected the decoder learning rate based on linear search over [1 ? 10 ?3 ? 2.5 ? 10 ?5 ]. Number of decoder layers has been decided based on search over {2, 3, 4, 5, 6} layers and 4 layer decoder shows superior performance (we used a single run for hyperparameter selection). Each model has 150M parameters optimized using a single GTX 1080 Ti GPU. With batch size of 16 each step takes 1.7s on GeoQuery dataset (other datasets have very similar runtime). On Django and CoNaLa, we followed <ref type="bibr" target="#b29">Xu et al., 2020)</ref> on replacing quoted values with a "str#" where # is a unique id. On Magic dataset, we replaced all newline "\n" tokens with "#"; following <ref type="bibr" target="#b17">(Ling et al., 2016a)</ref>, we splitted Camel-Case words (e.g., class TirionFordring ? class Tirion Fordring) and all punctuation characters. We filtered out Magic data with java code longer than 350 tokens in order to fit in GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Back-Translation and Fusion details</head><p>For fusion we follow equation 1 where TM stands for translation model and LM stands for language model. ? limits the confidence of the language model and ? controls the balance between TM and LM. <ref type="figure">figure 4</ref> shows the performance of a base TM trained on 10% of Django training data with test exact match accuracy of 31.80 over different values of ? and ? . The LM is trained over full Django training set.</p><formula xml:id="formula_1">log p(y t i ) = log p T M (y t i ) + ? log p LM (y t i ) = log p T M (y t i ) + ? log e l t i /? i e l t i /?</formula><p>(1) <ref type="figure">Figure 4</ref>: Test exact match accuracy of TM leverage fusion with different parameters</p><p>For back-translation we first trained the model using the same architecture explained above in the backward direction. We used BLEU score as a evaluation metric and use early stopping based on that. Using greedy search we generate the corresponding source intent for each code snippet. In the end, the synthetic data is merged with the bitext and trained a forward model.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Qualitative Examples</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Architecture and Experiment Details</head><p>We selected the decoder learning rate based on linear search over [1 ? 10 ?3 ? 2.5 ? 10 ?5 ]. Number of decoder layers has been decided based on search over {2, 3, 4, 5, 6} layers and 4 layer decoder shows superior performance (we used a single run for hyperparameter selection). Each model has 150M parameters optimized using a single GTX 1080 Ti GPU. With batch size of 16 each step takes 1.7s on GeoQuery dataset (other datasets have very similar runtime). On Django and CoNaLa, we followed (??) on replacing quoted values with a "str#" where # is a unique id. On Magic dataset, we replaced all newline "\n" tokens with "#"; following (?), we splitted Camel-Case words (e.g., class TirionFordring ? class Tirion Fordring) and all punctuation characters. We filtered out Magic data with java code longer than 350 tokens in order to fit in GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Back-Translation and Fusion details</head><p>For fusion we follow equation 1 where TM stands for translation model and LM stands for language model. ? limits the confidence of the language model and ? controls the balance between TM and LM. <ref type="figure" target="#fig_3">figure 1</ref>  For back-translation we first trained the model using the same architecture explained above in the backward direction. We used BLEU score as a evaluation metric and use early stopping based on that. Using greedy search we generate the corresponding source intent for each code snippet. In the end, the synthetic data is merged with the bitext and trained a forward model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Model overview during training: we use a standard transformer-based encoder-decoder model where the positional and word embeddings are shared between encoder and decoder. The modules related to the encoder are represented in blue and the decoder ones are in yellow. Standard teacher forcing and transformer masking is applied during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Analysis using only 10% Django train bitext.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>shows the performance of a base TM trained on 10% of Django training data with test exact match accuracy of 31.80 over different values of ? and ? . The LM is trained over full Django training set. log p(y t i ) = log p T M (y t i ) + ? log p LM (y t i ) = log p T M (y t i ) + ? log Test exact match accuracy of TM leverage fusion with different parameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Source:call the function lazy with 2 arguments : string concat and six.text type , substitute the result for string concat .</figDesc><table><row><cell></cell><cell></cell><cell>Source:</cell><cell>define the function timesince with d , now defaulting to none, reversed defaulting to false as arguments .</cell></row><row><cell>Gold &amp; TAE:</cell><cell>string_concat = lazy(_string_concat, six. text_type)</cell><cell>Gold &amp; TAE:</cell><cell>def timesince(d, now=none, reversed=false): pass</cell></row><row><cell>Baseline:</cell><cell>string_concat = lazy (_concat_concat , six. text_type )</cell><cell>Baseline:</cell><cell>def timesince (d = none, reversed ( d = false ): pass</cell></row><row><cell>Note:</cell><cell>copy mistake: wrong variable resulting from failed copy</cell><cell>Note:</cell><cell>unbalanced paranthesis and multiple semantic mistakes.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Example mistakes by the baseline that are fixed by TAE. More examples in Appendix E.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 -</head><label>2</label><figDesc>3 showcase our SOTA results on Django and CoNaLa. While our simple base seq2seq model does not outperform previous works, with TAE on the monolingual data, our performance improves and outperforms all the previous works. The most direct comparison is with Xu et al. (2020) that also leverage the same extra data mined from StackOverflow (EK inTable 3). As mentioned in Sec. 2, they used the noisy parallel corpus for pre-training, whereas we only leverage the monolingual set. However, we obtain both larger relative improvements over our baseline (32.29 from 30.98) compared toXu et al. (2020) (28.14 from  27.20), as well as better absolute results in the best case. In fact, with only the 100K StackOverflow monolingual data, our result is on par with the best one from Xu et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Exact match accuracy for Django test set. Yin and Neubig (2019) * trained a separate model on top of SP to rank beam search outputs.</figDesc><table><row><cell>Model Reranker (Yin and Neubig, 2019)  *  TRANX (Yin and Neubig, 2019) + BERT 30.47 ? 0.7 CoNaLa 30.11 EK (baseline)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Dataset GeoQuery 47.69 ? 0.05 Baseline (%) Baseline + TAE (%) 51.87 ? 0.02 ATIS 38.04 ? 0.77 40.56 ? 0.57 Magic 41.61 ? 2.07 42.34 ? 0.52 WikiSQL 85.36 ? 0.06 85.30 ? 0.07</cell></row></table><note>CoNaLa test BLEU. Methods with* trained a separate model on top of SP to rerank beam search out- puts. Xu et al. (2020) ? used an additional bitext corpus mined from python API documentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Additional dataset results: test set exact match accuracy on all dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Copy and generation accuracies on Django test set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Monolingual dataset sizes.</figDesc><table><row><cell>B Dev Set Results</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">Baseline (%) Baseline + TAE (%)</cell></row><row><cell cols="2">CoNaLa ATIS GeoQuery 53.33 ? 1.47 32.43 ? 0.21 5.79 ? 0.29 Django 75.52 ? 0.21 Magic 42.26 ? 1.42 WikiSQL 85.92 ? 0.09</cell><cell>34.81 ? 0.36 7.23 ? 0.45 52.58 ? 0.70 78.56 ? 0.39 44.17 ? 0.99 85.83 ? 0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>of escapebytes , created with an argument , reuslt of the call to the function bytes with an argument s .</head><label></label><figDesc></figDesc><table><row><cell>Source:</cell><cell>call the function lazy with 2 arguments</cell><cell>Source:</cell><cell>define the function timesince with d , now</cell></row><row><cell></cell><cell>: _string_concat and six.text_type [ six</cell><cell></cell><cell>defaulting to none , reversed defaulting to false</cell></row><row><cell></cell><cell>. text_type ] , substitute the result for</cell><cell></cell><cell>as arguments .</cell></row><row><cell cols="2">string_concat . string_concat = lazy(_string_concat, six.text_type ) Baseline: string_concat = lazy (_concat_concat , six. Gold: text_type ) TAE: string_concat = lazy ( _string_concat , six. text_type )</cell><cell cols="2">def timesince(d, now=none, reversed=false): pass Baseline: def timesince ( d = none, reversed ( d = false ) : Gold: pass def timesince ( d, now = none, reversed = false ) TAE: : pass</cell></row><row><cell>Note:</cell><cell>wrong var</cell><cell>Note:</cell><cell>unbalanced paranthesis and multiple semantic mistakes.</cell></row><row><cell>Source:</cell><cell>get translation_function attribute of the object</cell><cell>Source:</cell><cell>define the function exec with 3 arguments :</cell></row><row><cell></cell><cell>t , call the result with an argument eol_message ,</cell><cell></cell><cell>_code_ , _globs_ set to none and _locs_ set to</cell></row><row><cell></cell><cell>substitute the result for result .</cell><cell></cell><cell>none .</cell></row><row><cell>Gold:</cell><cell>result = getattr(t, translation_function)( eol_message)</cell><cell>Gold:</cell><cell>def exec_(_code_, _globs_=none, _locs_=none): pass</cell></row><row><cell cols="2">Baseline: result = getattr ( t , translation_message ) ( eol_message )</cell><cell cols="2">Baseline: def exec ( _code_ , _globs= none , _locs_ set ( ) ) : pass</cell></row><row><cell>TAE:</cell><cell>result = getattr ( t , translation_function ) ( eol_message )</cell><cell>TAE:</cell><cell>def exec ( _code_ , _globs_ = none , _locs_ = none ) :</cell></row><row><cell>Note:</cell><cell>wrong var</cell><cell></cell><cell>pass</cell></row><row><cell>Source:</cell><cell>convert whitespace character to unicode and</cell><cell>Note:</cell><cell>wrong variable name and grammar mistake</cell></row><row><cell cols="2">substitute the result for space . space = unicode(' ') Baseline: space = unicode ( character ) Gold: TAE: space = unicode ( ' ' )</cell><cell cols="2">Source: return an instance Gold: return escapebytes(bytes(s))</cell></row><row><cell>Note:</cell><cell>wrongly copied variable name</cell><cell cols="2">Baseline: return escapebytes ( bytes ( s ) . re ( s )</cell></row><row><cell>Source:</cell><cell>assign integer 2 to parts if third element of</cell><cell>TAE:</cell><cell>return escapebytes ( bytes ( s ) )</cell></row><row><cell></cell><cell>version equals to zero , otherwise assign it</cell><cell>Note:</cell><cell>extra semantically incorrect predictions and unbalanced paratheses</cell></row><row><cell></cell><cell>integer 3 .</cell><cell>Source:</cell><cell>call the function blankout with 2 arguments : p</cell></row><row><cell>Gold:</cell><cell>parts = 2 if version[2] == 0 else 3</cell><cell></cell><cell>and str0 , write the result to out .</cell></row><row><cell cols="2">Baseline: parts [ 2 ] = 2</cell><cell>Gold:</cell><cell>out.write(blankout(p, 'str0'))</cell></row><row><cell>TAE:</cell><cell>parts = 2 if version [ 2 ] == 0 else 3</cell><cell cols="2">Baseline: out .write ( blankout ( p , 'str0' )</cell></row><row><cell>Note:</cell><cell>baseline failed to copy a few source tokens, and instead formed a grammati-</cell><cell>TAE:</cell><cell>out .write ( blankout ( p , 'str0' ) )</cell></row><row><cell></cell><cell>cally correct but semantically incorrect output</cell><cell>Note:</cell><cell>unbalanced paratheses</cell></row><row><cell></cell><cell>Copy mistake examples</cell><cell></cell><cell>Grammar or semantic mistake examples</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Mistake examplesTable 1: Parallel dataset sizes. We filtered out Magic data with java code longer than 350 tokens in order to fit in GPU memory.</figDesc><table><row><cell>Monolingual Corpus</cell><cell>Source</cell><cell>Size</cell></row><row><cell>Python (?) (link)</cell><cell cols="2">Stackoverflow 100K</cell></row><row><cell>SQL (?) (link)</cell><cell cols="2">Stackoverflow 52K</cell></row><row><cell>Java (?) (link)</cell><cell>Github</cell><cell>100k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 2 :</head><label>2</label><figDesc>Monolingual dataset sizes.</figDesc><table><row><cell>B Dev Set Results</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">Baseline (%) Baseline + TAE (%)</cell></row><row><cell cols="2">CoNaLa ATIS GeoQuery 53.33 ? 1.47 32.43 ? 0.21 5.79 ? 0.29 Django 75.52 ? 0.21 Magic 42.26 ? 1.42 WikiSQL 85.92 ? 0.09</cell><cell>34.81 ? 0.36 7.23 ? 0.45 52.58 ? 0.70 78.56 ? 0.39 44.17 ? 0.99 85.83 ? 0.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 3 :</head><label>3</label><figDesc>Dev set exact match accuracy on all datasets except CoNaLa which uses BLEU. We followed (?) implementation of BLEU score which can be found here.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We appreciate the ACL anonymous reviewers and area chair for their valuable inputs. We also would like to thank a number of Borealis AI colleagues for helpful discussions, including Wei (Victor) Yang, Peng Xu, Dhruv Kumar, and Simon J. D. Prince for feedback on the writing.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>We used 6 datasets in total. Django includes programs from Django web framework and CoNaLa contains diverse set of intents annotated on python snippets gathered from Stackoverflow. WikiSQL, GeoQuery, and ATIS include natural language questions and their corresponding SQL queries <ref type="table">. WikiSQL includes  single table queries while GeogQuery and ATIS requires queries on more than one table.</ref> Finally, Magic has Java class implementation of game cards with different methods used during the game. <ref type="table">Table 1</ref> summarises all the parallel datasets. For GoeQuery we used query split provided by (?).</p><p>Monolingual Corpus: CoNaLa comes with 600K mined questions from Stackoverflow. We ignored the noisy source intents/sentences and just use the python snippets. To be comparable with ?, we also select a corresponding 100K subset version for comparison. For SQL, ? automatically parsed StackOverflow questions related to SQL and provided a set containing 120K SQL examples. We automatically parsed the SQL codes and removed samples with grammatical mistakes. We also filtered samples not starting with SELECT special token. ? downloaded full repositories of individual projects that were forked at least once; duplicate projects were removed. We randomly sampled 100K Java examples from more than 14K projects and use that as monolingual set.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining Source Code Repositories at Massive Scale using Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 10th Working Conference on Mining Software Repositories</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Using monolingual data in neural machine translation: a systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Burlot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Yvon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11437</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using monolingual data in neural machine translation: a systematic study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Burlot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Yvon</surname></persName>
		</author>
		<idno>abs/1903.11437</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Programming languages and their compilers: Preliminary notes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Cocke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
		</imprint>
		<respStmt>
			<orgName>New York University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Copied monolingual data improves low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli-Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01280</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Coarse-to-fine decoding for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04793</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Understanding back-translation at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09381</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving text-to-sql evaluation methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Finegan-Dollak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sesh</forename><surname>Sadasivam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09029</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1603.06393</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">On using monolingual corpora in neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards complex text-to-sql in cross-domain database with intermediate representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zecheng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4524" to="4535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Span-based semantic parsing for compositional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06040</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iterative backtranslation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Vu Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
		<meeting>the 2nd Workshop on Neural Machine Translation and Generation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An efficient recognition and syntax-analysis algorithm for context-free languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadao</forename><surname>Kasami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Coordinated Science Laboratory Report</title>
		<imprint>
			<biblScope unit="page">257</biblScope>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="599" to="609" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom??</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06744</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to generate pseudo-code from source code using statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Fudaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), ASE &apos;15</title>
		<meeting>the 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), ASE &apos;15<address><addrLine>Lincoln, Nebraska, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised pretraining for sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02683</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cold fusion: Training seq2seq models together with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuroop</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heewoo Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coates</surname></persName>
		</author>
		<idno>abs/1708.06426</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simple fusion: Return of the language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1809.00125</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith ; G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ArXiv</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Incorporating external knowledge through pre-training for natural language to code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09015</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Staqc: A systematically mined questioncode dataset from stack overflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Peng</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1693" to="1703" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to mine aligned code and natural language pairs from stack overflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Mining Software Repositories, MSR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="476" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A syntactic neural model for general-purpose code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01696</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tranx: A transition-based neural abstract syntax parser for semantic parsing and code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02720</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reranking for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4553" to="4559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognition and parsing of context-free languages in time n3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Younger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Photon: A robust cross-domain text-to-sql system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<idno>abs/2007.15280</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00103</idno>
		<title level="m">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
