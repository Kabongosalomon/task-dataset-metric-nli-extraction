<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Warp-Refine Propagation: Semi-Supervised Auto-labeling via Cycle-consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ganeshan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Vallet</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Networks, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasunori</forename><surname>Kudo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Networks, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Networks, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Kerola</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Networks, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rares</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ambrus</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Toyota Research Institute (TRI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Park</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Toyota Research Institute (TRI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Toyota Research Institute (TRI)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Warp-Refine Propagation: Semi-Supervised Auto-labeling via Cycle-consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning models for semantic segmentation rely on expensive, large-scale, manually annotated datasets. Labelling is a tedious process that can take hours per image. Automatically annotating video sequences by propagating sparsely labeled frames through time is a more scalable alternative. In this work, we propose a novel label propagation method, termed Warp-Refine Propagation, that combines semantic cues with geometric cues to efficiently auto-label videos. Our method learns to refine geometrically-warped labels and infuse them with learned semantic priors in a semisupervised setting by leveraging cycle-consistency across time. We quantitatively show that our method improves label-propagation by a noteworthy margin of 13.1 mIoU on the ApolloScape dataset. Furthermore, by training with the auto-labelled frames, we achieve competitive results on three semantic-segmentation benchmarks, improving the state-of-the-art by a large margin of 1.8 and 3.61 mIoU on NYU-V2 and KITTI, while matching the current best results on Cityscapes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation, i.e. assigning a semantic class to each pixel in an input image, is an integral task in understanding shapes, geometry, and interaction of components from images. The field has enjoyed revolutionary improvements thanks to deep learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b34">35]</ref>. However, obtaining a large-scale dataset with pixel-level annotations is particularly expensive: for example, labeling takes 1.5 hours on average per image in the Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref>. Despite the recent introduction of datasets that are significantly larger than their predecessors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>, scarcity of labeled data remains a bottleneck when compared to other recognition tasks in computer vision <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>In the common scenario where data is provided as videos with labels for sparsely subsampled frames, a prominent way to tackle data scarcity is label propagation (LP), which automatically annotates additional video frames by propagating labels through time <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b2">3]</ref>. This intuitive idea to leverage * Work done while A. Ganeshan was at Preferred Networks, Inc. <ref type="figure" target="#fig_5">Figure 1</ref>: Quantitative comparison of different auto-labelling methods for multiple propagation lengths using ApolloScape <ref type="bibr" target="#b42">[43]</ref> framewise ground-truth. We compare our propagation methods, warprefine and warp-inpaint, to prior-arts semantic-only and motiononly propagation, and show that warp-refine is vastly superior to other auto-labeling approaches, especially for large time-steps. motion-cues via temporal consistency in videos has been widely explored, using estimated motion <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>, patch matching <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, or predicting video frames <ref type="bibr" target="#b58">[59]</ref>. However, as discussed in Zhu et al. <ref type="bibr" target="#b58">[59]</ref>, estimating dense motion fields across long periods of time remains notoriously difficult. Further, these methods are often sensitive to hyperparameters (e.g. patch size), cannot handle de-occlusion, or require highly accurate optical flow, thus limiting their applicability.</p><p>Another promising approach for obtaining large-scale annotation in semi-supervised settings is self-training (ST), in which a teacher model, trained to capture semantic cues, is used to generate additional annotations on unlabeled images <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b59">60]</ref>. While there have been significant improvements in ST, various challenges still remain in controlling the noise in pseudo-labels, such as heuristic decisions on confidence thresholds <ref type="bibr" target="#b36">[37]</ref>, class imbalance in pseudo-labels <ref type="bibr" target="#b13">[14]</ref>, inaccurate predictions for small segments, and misalignment of category definition between source and target domain.</p><p>To mitigate the drawbacks of LP and ST, we propose i arXiv:2109.13432v1 [cs.CV] 28 Sep 2021 The motion-only model (i) fails to correctly classify the new regions introduced in the target frame , and (ii) often suffers from drifting . In contrast, the semantic-only model (iii) tends to fail for far-away segments, and (iv) cannot handle misaligned class definitions between the teacher and the student model (e.g. ignore label) . Our method effectively combines the strengths of both of these approaches to overcome their respective limitations. For details of motion-only and semantic-only models, see Section 4.4.</p><p>Warp-Refine Propagation (referred to as warp-refine), a novel method to automatically generate dense pixel-level labels for raw video frames. Our method is built on two key insights: (i) by combining motion cues with semantic cues, we can overcome the respective limitations of LP and ST, and (ii) by leveraging cycle-consistency across time, we can learn to combine these two complementary cues in a semisupervised setting without sequentially-annotated videos. Specifically, our method first constructs an initial estimate by directly combining labels generated via motion cues and semantic cues. This initial estimate, containing erroneous conflict resolution and faulty merges, is then rectified by a separate refinement network. The refinement network is trained in a semi-supervised setting via a novel cycleconsistency loss. This loss compares the ground-truth labels with their cyclically propagated version created by propagating the labels forward-and-backward through time in a cyclic loop (t ? t + k ? t). Our loss is built on the observation that as our auto-labeling method is bi-directional, it can be used to generate different versions of each annotated frame. Once this network is trained, it is used to correct errors caused by propagation of variable length. In <ref type="figure" target="#fig_0">Fig. 2</ref> we show a qualitative comparison of our method against prior-arts, demonstrating drastic improvements in label quality.</p><p>With quantitative analysis on a large scale autonomous driving datasets (ApolloScape <ref type="bibr" target="#b42">[43]</ref>), we concretely establish the superior accuracy of our method against previous state-of-the-art auto-labeling methods. Such an analysis of different methods has been starkly missing from prior works <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27]</ref>. As shown in <ref type="figure" target="#fig_5">Fig. 1</ref>, we observe that warp-refine accurately propagates labels for significantly longer time intervals, with a notable average improvement of 13.1 mIoU over the previous best method on ApolloScape. Further, it accurately labels rare classes such as 'Bicycle' and thin structures such as 'Poles' (cf. Section 4.3). As a result, by training single-frame semantic segmentation models with the additional data labeled by our method, we achieve stateof-the-art performance on KITTI <ref type="bibr" target="#b0">[1]</ref>, NYU-V2 <ref type="bibr" target="#b27">[28]</ref> and Cityscapes <ref type="bibr" target="#b11">[12]</ref> benchmarks (cf. Section 4.5).</p><p>In summary, our main contributions are: 1) A novel algorithm, termed Warp-Refine Propagation, that produces significantly more accurate pseudo-labels, especially for frames distant in time; 2) A novel loss function, based on the cycleconsistency of learned transformations, to train our method in a semi-supervised setting; and 3) A quantitative analysis on the quality and utility of different auto-labeling methods on multiple diverse datasets. To the best of our knowledge, our work is the first to utilize both semantic and geometric understanding for the task of video auto-labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-training (ST). The approach of applying a mature teacher networks to unlabeled images and using the predicted <ref type="figure">Figure 3</ref>: Our proposed warp-refine propagation consists of three steps: (i) warp uses dense optical flow estimation to remap ground-truth labels onto target images; (ii) inpaint blends the results of (i) with predictions of a strong semantic segmentation model; (iii) refine applies the learned refinement network on the results of (ii). labels to supervise student networks has received increasing attention. Xie et al. <ref type="bibr" target="#b46">[47]</ref> introduces a framework for ST for controlling noise in pseudo-labels to exceed the teacher network. Chen et al. <ref type="bibr" target="#b8">[9]</ref> extend it for semantic segmentation. Recently, ST has proved to be effective on unsupervised domain adaptation (UDA) <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b5">6]</ref>, where the goal is to learn a domain invariant representation to address the sim-to-real gap. The advances in ST are mainly driven by improvements in model architectures and loss definitions used in training the teacher and student networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b34">35]</ref>, feature alignment between source and target domain <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b18">19]</ref>, and decision process of which predictions are used as pseudolabels <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b59">60]</ref>. Our approach can be seen as further improving ST by 1) additionally using motion cues to propagate ground-truth labels across long intervals in time and 2) correcting the combined errors from the two sources using learnable components. Label propagation (LP). The goal of LP is to transfer ground-truth pixel labels from adjacent frames using dense pixel-level matching <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b58">59]</ref>. Given the advance in CNN-based optical flow algorithms, various methods using strong geometric cues to propagate labels have been proposed, e.g. using video prediction <ref type="bibr" target="#b58">[59]</ref> and flow-based voting <ref type="bibr" target="#b23">[24]</ref>. The key criteria for success in LP is the distance in time through which one can accurately propagate ground-truth labels. It is a vital aspect, as the pseudo-labels need to contain novel learning signals with respect to the annotated source frames. A common failure of flow-based methods is error propagation; mistakes made on earlier steps persist and get amplified in later steps (i.e. drifting). Here, our method can be seen as improving LP by using a highcapacity semantic segmentation model to re-initialize the labels to prevent error accumulation. Semantic and geometric cues. Methods combining semantic and geometric cues have been proposed in the past for other tasks such as future-frame prediction <ref type="bibr" target="#b21">[22]</ref> and videosegmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b30">31]</ref>. Our chief distinction is the propagation of ground-truth labels, not feature representation or predicted labels, for usage in a self-training framework. In Sec. 4.4, we provide quantitative analysis and report that our method is successful in propagating ground-truth labels accurately across long intervals of time.</p><p>Cycle-consistency. The concept of cycle-consistency has been previously utilized for learning object embeddings <ref type="bibr" target="#b45">[46]</ref>, one-shot semantic segmentation <ref type="bibr" target="#b43">[44]</ref> and video interpolation <ref type="bibr" target="#b32">[33]</ref>. Our work is inspired by work using cycleconsistency for learning a robust tracker <ref type="bibr" target="#b45">[46]</ref>. However, we differ in that we address the noisy nature of our tracking/geometric modelling method itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Warp-Refine Propagation</head><p>We first present the notation used throughout our paper. This is followed by the description of two recursive algorithms for propagating dense pixel labels, followed by the proposed method to train the denoising models by leveraging cyclic consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation</head><p>Given a labeled video frame (I t , L t ) and its adjacent raw frame I t+k , we aim to create approximated labelsL t+k for 1 ? k ? K. To this end, we introduce two greedy propagation algorithms. They are greedy in that the optimal solution forL t+k , namelyL * t+k , is obtained by applying a recursive propagation step to the (approximately optimal) solution for the previous frame,L * t+k ?1 : 1</p><formula xml:id="formula_0">L * k = ?(L * k?1 , I k?1 ) , t + 1 ? k ? t + K , (1) L * t = L t .<label>(2)</label></formula><p>We introduce two algorithms, warp-inpaint and warprefine, that grow in complexity of ?. Notably, the approach of Zhu et al. <ref type="bibr" target="#b58">[59]</ref> can be included in this framework, when only video-prediction algorithm <ref type="bibr" target="#b31">[32]</ref> based motion vectors are used for defining ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Warp-Inpaint</head><p>As commonly observed in online visual tracking <ref type="bibr" target="#b15">[16]</ref>, when ? relies purely on motion cues, the propagated labels are susceptible to propagation error (i.e. drifting) <ref type="bibr" target="#b58">[59]</ref>. In addition, the pixels of new scene elements cannot be explained by labels in previous frames (e.g. cars entering the field of view). One way to address this is to re-initialize the semantic labels using a strong semantic segmentation model.</p><p>We therefore allow each pixel inL * k to be derived either from motion cues encoded in the (I k?1 , I k ) pair or from semantic cues computed solely from I k . Formally, we compute a version ofL k , namelyL m k , by remappingL * k?1 using a transformation ? k?1,k learned to warp I k?1 onto I k .</p><p>We then blendL m k with another version ofL k , namelyL s k , which are semantic labels obtained by applying a pretrained semantic segmentation model g ? on I k :</p><formula xml:id="formula_1">L m k = ? k?1,k (L * k?1 ) ,<label>(3)</label></formula><formula xml:id="formula_2">L s k = g ? (I k ) ,<label>(4)</label></formula><formula xml:id="formula_3">L * k = M L m k + (1 ? M ) L s k ,<label>(5)</label></formula><p>where denotes pixel-wise multiplication. The (x, y) value of the binary mixing coefficients M represents whether we trust the the estimated motion vector at the position (x, y), compared with the semantic label computed by g ? . We determine M by measuring the Euclidean distance of pixel values between I k and ? k?1,k (I k?1 ):</p><formula xml:id="formula_4">M (x, y) = I( I k (x, y) ? ? k?1,k (I t )(x, y) 2 &lt; ? ) ,<label>(6)</label></formula><p>where I(?) is the indicator function. The motion vectors are obtained by applying a pretrained motion estimation model, f ? , to neighboring image pairs: ? k?1,k = f ? (I k?1 , I k ). We let ? W denote the entire propagation process (3) -(5), and L W k denote the resulting pseudo-labels at the k-th frame:</p><formula xml:id="formula_5">L W k = ? W (L W k?1 , I k?1 ) .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Warp-Refine</head><p>WithL W k , we obtain an initial fusion of geometric and semantic cues. This estimate is however still subject to artifacts from imperfect motion estimation and semantic segmentation models (f ? and g ? ). We therefore extend the recursive step to refineL W k by applying a de-noising network ? ? that aims to remove these artifacts:</p><formula xml:id="formula_6">L R k = ? ? (L W k ) .<label>(8)</label></formula><p>Note that the goal of ? ? is to mitigate particular types of errors caused by the propagation steps of ? W , specifically by f ? , g ? , and the choice of ? , and in effect, properly merge the semantic and geometric cues. The extended propagation process and generated pseudo-labels are denoted by ? R and L R k , respectively:</p><formula xml:id="formula_7">L R k = ? R (L R k?1 , I k?1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4:</head><p>For training the refinement network, we generate cyclically propagated labels by applying forward-backward propagation on annotated frame using warp-inpaint transformation, and compare them with the ground-truth labels. <ref type="figure">Fig 3 summarizes</ref> the three steps for our proposed propagation method. In Sec. 4, we quantitatively and qualitatively show thatL W k as well asL R k are more accurate with respect to their ground truth labels for various k, and further that the generated labels are useful for improving single-frame semantic segmentation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning</head><p>There are three sets of learnable parameters in warprefine; in the motion estimation model f ? , in the semantic segmentation model g ? , and in the de-noising model ? ? . We use a constant pretrained motion estimation model <ref type="bibr" target="#b31">[32]</ref> and semantic segmentation model <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b50">51]</ref> for f ? and g ? , respectively, and use a fixed value for ? . Here we describe how to train ? ? .</p><p>Cycle-consistency. Training a de-noising model in a fully-supervised setting typically requires a large dataset of noisy-clean pairs <ref type="bibr" target="#b16">[17]</ref>, which in our case is (L R k , L k ). To address the lack of L k in our semi-supervised setting, we leverage the cycle-consistency inherent in our propagation mechanism. The cyclic propagation consists of two stages:</p><formula xml:id="formula_8">1. forward: we execute propagation steps of ? W (i.e.</formula><p>Eq. 3 -Eq. 5) l times to obtainL W l . 2. backward: we execute l steps of inverted propagation of ? W to obtain a cyclically propagated variant of L t , namelyL ? t . The inverted propagation step is similar to ? W , but executed in a reverse order:</p><formula xml:id="formula_9">L m k?1 = ? k,k?1 (L * k ) ,<label>(9)</label></formula><formula xml:id="formula_10">L s k?1 = g ? (I k?1 ) ,<label>(10)</label></formula><formula xml:id="formula_11">L * k?1 = M L m k?1 + (1 ? M ) L s k?1 ,<label>(11)</label></formula><p>whereL * k is set to the result of the forward stage,L W l , and motion vectors are computed in backward:</p><formula xml:id="formula_12">? k,k?1 = f ? (I k , I k?1 ) .<label>(12)</label></formula><p>The cycle-consistency loss is computed by comparing the annotated labels L t and its cyclically warped counterpartL ? t with de-noising applied:</p><formula xml:id="formula_13">L(L t , ? ? (L ? t ))</formula><p>. This is optimized via standard gradient-based methods during training.</p><p>Notably, the backward steps, and therefore the entire forward-backward process, chains the same set of transformations used in ? W (i.e. f ? , g ? and the blending strategy). Therefore, the de-noising network trained with this cycleconsistency loss is expected to correct the errors in pseudolabels generated by variable length of the ? W , which is the goal of ? ? . In practice, we use varying l in training to maximize this generalizability. <ref type="figure">Fig. 4</ref> presents a visual summary of our approach for learning label refinement via cycle-consistency of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We present quantitative and qualitative experiments on four semantic segmentation benchmarks: NYU. The NYU-Depth V2 dataset <ref type="bibr" target="#b27">[28]</ref> consists of 1449 densely labeled images split into 795 training images and 654 testing images, which are randomly sampled from video sequences. Due to the high fps (20-30 fps) and slow camera movement, we sub-sample the video at 2fps and create sequences of variable lengths of up to 21 frames around the labeled frame, yielding 9786 frames for label propagation. KITTI. The KITTI Vision Benchmark Suite <ref type="bibr" target="#b0">[1]</ref> consists of 200 training and 200 test images with pixel-level annotation. For each of the training images, we use sequential frames (?10) from the scene-flow subset for label propagation. Cityscapes. The Cityscapes dataset <ref type="bibr" target="#b11">[12]</ref> is split into a train, validation, and test set with 2975, 500, and 1525 images, respectively. For each training image, we use the ?10 unlabeled neighboring frames provided as part of the dataset. ApolloScape. The ApolloScape dataset <ref type="bibr" target="#b42">[43]</ref> contains pixellevel annotations for sequentially recorded images, divided as 40960 training, and 8327 validation images, allowing evaluation of the accuracy of the propagated labels. We create continuous partitions of 21 frames, and use the central frame as a training data-point, and the adjacent frames (?10) for label propagation. This yields a train-subset of size containing 2005 and a label-propagation subset of 38095 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Our approach consists of three components: a motionestimation network f ? , the semantic segmentation model g ? , and a de-noising network ? ? . Following Zhu et al. <ref type="bibr" target="#b58">[59]</ref>, for the motion-estimation network f ? , we use video-prediction model based on SDC-net <ref type="bibr" target="#b31">[32]</ref>. For our task, video prediction performed better than warping with optical-flow <ref type="bibr" target="#b37">[38]</ref> (cf. supplementary). For segmentation model g ? , we adopt the architecture (MSA-HrNet-OCR) and training protocols outlined in Tao et al. <ref type="bibr" target="#b36">[37]</ref>. Finally for ? ? , we use a pix2pixstyle network <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b29">30]</ref>. First, an encoder takes the warpinpainted labels as the input. The encoding formed is then concatenated with OCR features <ref type="bibr" target="#b50">[51]</ref> from g ? , and passed through a decoder. For robust refinement, we perform cycleconsistency training with the range of propagation sampled till ?6. Note that the labels (L t /L W t /L ? t ) are utilized as one-hot vectors. Single-frame semantic segmentation. For training single frame semantic segmentation networks with the auto-labeled data, we use the same architecture and training protocol as followed for g ? , with one important modification: independent of the amount of data generated by auto-labeling, we use a fixed epoch size (thrice the dataset size). This ensures that we do not under-train the baseline model (cf. supplementary). When training with the propagated data, we sample 70% of the epoch from the propagated data, and 30% from the manually annotated data. Unless specified otherwise, we sample additional data at time-frames {t?n|n ? <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>} (similar to <ref type="bibr" target="#b26">[27]</ref>). We refer to the model trained with no additional data as the baseline model. Auto-labeling baselines. We compare our methods warpinpaint (? W , cf. section 3.2) and warp-refine (? R , cf. section 3.3) against existing auto-labeling techniques. We use the method proposed by Zhu et al. <ref type="bibr" target="#b58">[59]</ref>, which uses only f ? to generate the labels, and refer to this method as motion-only labeling. When using these labels, we use joint image-label propagation as recommended by <ref type="bibr" target="#b58">[59]</ref>. Similarly, we also use the method proposed by Tao et al. <ref type="bibr" target="#b36">[37]</ref>, which generates labels using only g ? , and refer to this method as semanticonly labeling. For semantic-only labeling, we use the best performing architecture (MSA-HRNet-OCR, trained on the manually annotated images), and use only the pixels with &gt; 0.9 confidence, as recommended in <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quality of Propagated Labels</head><p>We first provide extensive analysis of the our autolabeling methods: warp-inpaint and warp-refine, and existing techniques motion-only and semantic-only. We evaluate the labels generated by these methods against the groundtruth labels provided in the ApolloScape dataset <ref type="bibr" target="#b42">[43]</ref>. We focus on two crucial aspects: (1) long-range propagation, and (2) labeling of hard classes.</p><p>First, we compare the different auto-labeling methods across various propagation lengths. <ref type="figure" target="#fig_5">Fig. 1</ref> reports the mean Intersection over Union (mIoU) between propagated labels and ground-truth labels, given various propagation methods and propagation lengths. We note that the motion-only and the semantic-only models show a clear trade-off with respect ApolloScape <ref type="bibr" target="#b42">[43]</ref> averaged across all time-steps. Our method warprefine (? R ) is able to perform well for classes with thin-structures such as 'Pole'. Further, our method leverages motion-cues when semantic-cues fail, as seen for 'Ignore' and 'Building' class.</p><p>to the propagation length: motion-only model produces more accurate labels over shorter ranges, while semantic-only model over longer ranges. Further, when propagating without refinement (i.e. warp-inpaint), the accuracy degrades for longer propagation lengths, even dropping below the semantic-only labeling. Finally, propagating with refinement (i.e. warp-refine) produces significantly cleaner labels than the others. Due to the refinement module, our method retains its accuracy even at large time-steps, attaining a large margin of 11.35 mIoU over the closest competing method at t ? 10.</p><p>Next, we quantify the IoU of difficult classes and the overall mIoU across all propagation lengths in <ref type="figure" target="#fig_1">Fig. 5</ref>. Notably, both prior methods for auto-labeling fail on thin structures such as 'Poles' -motion-only causes over-labeling due to drifting, while semantic-only causes under-labeling due to low-confidence predictions (such regions are frequently masked out). Further, we note that semantic-only labeling severely fails at estimating the 'Ignore' class, in contrast to motion-only labeling which yields accurate estimates. As the 'Ignore' class consists of widely varying objects, it is difficult to model it as a semantic class (typically 'Ignore' class includes regions labeled as 'Others' and 'Unlabeled'; i.e. labels which do not have any semantic definition). Therefore, following the recommended protocol <ref type="bibr" target="#b36">[37]</ref>, for semantic-only labeling we estimate 'Ignore' regions via probability thresholding <ref type="bibr" target="#b36">[37]</ref> (cf. Sec. 4.2). Despite the careful selection of this threshold parameter, semantic-only labeling fails to accurately label the 'Ignore' class. Our methods (warpinpaint, and warp-refine) effectively combine motion-cues with semantic-cues, thus overcoming this drawback, and properly estimate the 'Ignore' class. Overall, our method again yields an impressive margin of 13.12 mIoU over the closest prior-art (averaged across all propagation lengths).</p><p>Finally, we also present qualitative results in <ref type="figure" target="#fig_2">Fig. 6</ref>, noting that our propagated labels do not suffer from error propagation compared to motion-only, while achieving higher accuracy on the rare classes (e.g. bikes) compared to semantic- refine against other auto-labeling methods on ApolloScape <ref type="bibr" target="#b42">[43]</ref>. As our method combines semantic and motion cues, mistakes in one (e.g., caused by rare class, or new scene elements) are corrected by the other, leading to superior results.</p><p>only. <ref type="figure" target="#fig_3">Fig. 7</ref> also presents a failure case for our method, caused due to the concurrent failure of both semantic and motion cues for the 'Rider'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Utility of Propagated Labels</head><p>We now demonstrate that the significant improvements in auto-labeling directly translate to superior performance for single-frame semantic segmentation models trained with our generated data. We perform our experiments on NYU-V2, ApolloScape and Cityscapes. Following Zhu et al. <ref type="bibr" target="#b58">[59]</ref>, for each experiment, we perform three runs and report the mean (?(mIoU)) and sample standard deviation (?(mIoU)). Our analysis is summarised in <ref type="table" target="#tab_5">Table 1</ref>. NYU-V2 &amp; ApolloScape. Training with warp-refine labels consistently yields better results. On NYU-V2, our labels yield an average improvement of 1.54 mIoU, compared to only 0.35 mIoU for the semantic-only labeling method. Similarly, on ApolloScape, warp-refine labels increase performance by 1.11 mIoU, whereas the closest auto-labeling baseline (semantic-only) yields a benefit of only 0.47. Finally, we note that training with motion-only labels consistently leads to a drop in performance.</p><p>Since ApolloScape contains the ground-truth annotation of the all the provided images, we also provide evaluation using the ground-truth labels instead of the labels generated via auto-labeling. This acts as an oracle propagation model, and we treat it as an empirical upper-bound on the benefits from label propagation. Using the ground-truth instead of propagated labels (at (t ? {2, 4, 6, 8})) yeilds a benefit of <ref type="table" target="#tab_5">Table 1</ref>: We analyse the performance of semantic-segmentation models trained with auto-labelled data. Across the three datasets, we observe that warp-refine labels consistently induce larger improvements than labels from other methods. The results for semantic-only, warp-inpaint and warp-refine are computed by sampling auto-labelled frames at time steps (t ? {2, 4, 6, 8}). Following Zhu et al. <ref type="bibr" target="#b58">[59]</ref>, for motion-only, we only sample frames at time steps (t ? 2). We report the average performance (?(mIoU)), sample standard deviation (?(mIoU)) and average improvement over baseline (?(mIoU)) by performing three runs of each experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>NYU-V2 <ref type="bibr" target="#b27">[28]</ref> ApolloScape <ref type="bibr" target="#b42">[43]</ref> Cityscapes val-split <ref type="bibr" target="#b11">[12]</ref>   method warp-refine on Cityscapes <ref type="bibr" target="#b11">[12]</ref>. Our method can fail when both semantic and motion cues fail (e.g. 'Rider' is mislabeled).</p><p>2.73 mIoU. Notably, training with warp-refine labels attains about 40% of this empirical upper-bound. Cityscapes-val. While warp-refine labels yields performance superior to prior methods, we notice that the gap between semantic-only and warp-refine is smaller on Cityscapes (while still being statistically significant). It is likely due to performance saturation: across the three datasets, we observe that as the performance of the baseline model increase, (1) the utility of semantic-only labels increases (as they are more accurate), and (2) the utility of warp-refine labels decreases (as labels in adjacent images become less useful). Despite this drawback, warp-refine labels are still significantly more effective than the prior arts. Labeling additional data via a teacher model has been recently used in many self-training approaches <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b8">9]</ref>. Our work shows that using geometric cues can further improve the labels, leading to increased performance. Propagating over longer time-horizons. The efficacy of the propagated labels depends on the how far the groundtruth labels can be accurately propagated. This is critical as This ablation is performed on the NYU-V2 dataset <ref type="bibr" target="#b27">[28]</ref>.</p><p>there is little information gain in the immediate neighbors of an annotated frame. In <ref type="figure" target="#fig_4">Figure 8</ref>, we observe that training semantic segmentation models with warp-refine labels sampled from long time ranges yields a large benefit. This demonstrates that the ability of warp-refine to accurately propagate labels onto remote frames is critical for effectively improving semantic-segmentation models. Note that training with motion-only labels only degrade the performance further as the propagation length increases. In contrast, our method shows no degradation, even while sampling labels from [?10, 10].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Semantic Segmentation Benchmarks</head><p>Finally, we tabulate the state-of-the-art performance achieved by our method on three semantic segmentation benchmarks: NYU-V2, KITTI, and Cityscapes-test. As the evaluation rules for KITTI and Cityscapes explicitly state that the test-split should not be used for ablative study, we only evaluate our final model, i.e. MSA-HrNet-OCR <ref type="bibr" target="#b36">[37]</ref> trained with warp-refine labels.  <ref type="bibr" target="#b28">[29]</ref> and using coarse-labels). Our method is comparable to the current state-of-the-art models on Cityscapes, attaining 0.1 mIoU more than the previous state-of-the-art method (Chen et al. <ref type="bibr" target="#b8">[9]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYU-V2</head><p>. We compare our model with the best reported scores on the dataset in <ref type="table" target="#tab_3">Table 3</ref>. Our method yields an improvement of 1.8 mIoU over the prior state-of-the-art, while also attaining favorable statistics for the other metrics, notably, an increase of 3.5% in class-wise mean pixel-accuracy (mean-acc). Due to the semantic complexity of NYU-V2, additional data is decisively beneficial. Specifically, long-tail classes such as 'Bag', 'White-board' and 'Shower-curtain' yield an average benefit of 7.29 IoU over the baseline. KITTI. We report our performance on the KITTI dataset in <ref type="table" target="#tab_4">Table 4</ref>. We show a significant increase over the previous state-of-the-art. Specifically, we improve by a large margin of 3.61 mIoU. To evaluate the benefits from warp-refine labels, we trained our model without warp-refine labels, and qualitatively compare the predictions on test-set images (provided in the supplementary). Training with the proposed warp-refine labels improves performance on classes such as 'Truck', which have only a few labeled examples in the 200 training images. We attribute the superior performance of our method on this dataset to the fact that adding labeled data is vastly beneficial due to the small size of this dataset. Note that as in Zhu et al. <ref type="bibr" target="#b58">[59]</ref>, we use a pretrained Cityscapes model for initialization, and estimate hyperparameters using 4-fold cross-validation on the train set.</p><p>Cityscapes-test. Finally, we explore the benefit of our method on the Cityscapes dataset. As reported in <ref type="table" target="#tab_1">Table 2</ref>, our method yields an mIoU of 85.3, improving upon the state-of-the-art by a small margin of 0.1 mIoU. We attribute the small increase to the highly saturated performance of the baseline model. Specifically, as label-propagation only la- bels the frames neighboring the annotated images, the utility of these pseudo-labels diminishes as the performance of the baseline model gets saturated. We present additional experiments and results with (i) different backbone networks; and (ii) different training regimes in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose a novel approach for video autolabeling: Warp-Refine Propagation, which combines geometric and semantic cues for label propagation. By leveraging the concept of cycle-consistency across time, our method learns to refine propagated labels in a semi-supervised setting. With warp-refine, we can accurately propagate labels for long time horizons (i.e. ?10 frames). Via a detailed ablative analysis we show that warp-refine surpasses previous auto-labeling methods by a notable margin of 13.1 mIoU on ApolloScape. Further, labels generated from warp-refine are shown to be useful for improving single-frame semanticsegmentation models. By training semantic segmentation models with warp-refine labels, we achieve state-of-the-art performance on NYU-V2 (+1.8 mIoU), KITTI (+3.6 mIoU) and Cityscapes <ref type="figure" target="#fig_5">(+0.1 mIoU)</ref>. The optimal way to combine the propagated labels with manual annotations and weaker sources of supervision (i.e. coarse labels) remains an unsolved problem, which we aim to address in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary material</head><p>In this appendix, we provide additional details and experimental results to help further understand and reproduce our proposed method, i.e. Warp-Refine Propagation. The supplementary material is divided into the following sections:</p><p>? Section B -Additional experimental studies: We explore different aspects of training with propagated labels, such as training with different architectures, and training in data scarce setting.</p><p>? Section C -Additional details: We provide details for our training setup, as well as details of ApolloScape dataset <ref type="bibr" target="#b42">[43]</ref> usage.</p><p>? Section D -Qualitative results: We provide qualitative examples visualizing the results of different aspects of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional experimental studies B.1. Propagation for different architectures</head><p>We evaluate the benefit of the propagated labels on different semantic segmentation models. This result is summarized in <ref type="table" target="#tab_5">Table A</ref>. We see that the propagated labels are significantly beneficial for smaller architectures, which have lower performance. However, in the case of motion-only propagated labels, we see that the performance is unaffected or sometimes deteriorated. Note that these results do not use the 20000 additional coarse labels, nor pretraining on Mapillary Vistas <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Motion estimation model ablation</head><p>A simple way to use geometric cues is to simply warp the labels between consecutive frames based on the Optical flow. We tried different optical flow methods including RAFT <ref type="bibr" target="#b37">[38]</ref> for warping, but found them to be unsuitable. Apart from drifting errors, directly warping with optical flow also causes content duplication on de-occluded regions <ref type="bibr" target="#b55">[56]</ref>. Therefore, for warping labels between consecutive frames, we found video prediction to work the best for us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Ablative analysis with motion-only labels</head><p>As indicated in the main paper, we do not train using the Relaxed Label Loss (RLL) proposed by Zhu et al. <ref type="bibr" target="#b58">[59]</ref>, and also use a fixed epoch-size. In this section, we provide additional ablative experiments, validating our choices. Our results are summarized in <ref type="table" target="#tab_5">Table B</ref>, along with the numbers reported by Zhu et al. <ref type="bibr" target="#b58">[59]</ref> under similar training conditions. Note that we add the motion-only propagated labels at time step t?3 as represented by D m 3 . We perform the experiments under different training settings, namely considering the usage of coarse-labels and Mapillary Vistas pre-training. We report the mean and the standard deviation by conducting three runs with different random number generator seeds for each result. We note a significant improvement between our baseline when training with the Cityscapes coarse-labels and with Mapillary Vistas pre-training (80.94 mIoU) and the baseline reported in <ref type="bibr" target="#b58">[59]</ref> (79.46 mIoU) which we attribute to a longer training schedule of our baseline and a modified learning rate schedule: . We therefore modify the training such that B is trained for the same number of iterations as B + D m 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Higher learning rate for the baseline: We increase the learning rate by a factor of 8 for the baseline B (we observed that the scale of cross-entropy loss is much smaller than the scale of Relaxed Label Loss).</p><p>With the updated baseline, we find RLL as well as training with motion-only labels to be ineffective. Further, to avoid the pitfall of under-training the baseline, we fix the epochsize for all the models we compare. This ensure that the improvement by using additional labels is not conflated with improvement by longer training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional details C.1. Training details</head><p>We use an SGD optimizer and employ a polynomial learning rate policy, where the initial learning rate is multiplied by</p><formula xml:id="formula_14">(1? epoch max epoch ) power .</formula><p>The learning rate is varied for different datasets: for KITTI <ref type="bibr" target="#b0">[1]</ref> we utilize a learning rate of 0.0005, for Cityscapes we utilize 0.01 and for NYU-V2 <ref type="bibr" target="#b27">[28]</ref> we utilize 0.001. Momentum and weight decay are set to 0.9 and 0.0001 respectively. We use synchronized batch normalization (batch statistics synchronized across all GPUs) with the batch distributed over 8 V100 GPUs. For data augmentation, we randomly scale the input images (from 0.5 to 2.0), and apply horizontal flipping, Gaussian blur and color jittering during training. Further, we utilize uniform sampling <ref type="bibr" target="#b58">[59]</ref> across semantic classes with 50% of each epoch.</p><p>We introduce two changes from the training configuration outlined by Zhu et al. <ref type="bibr" target="#b58">[59]</ref>:</p><p>? As our approach generates additional training data, the epoch size varies greatly depending on training settings. This can lead to a situation where the observed improvement in performance can be due to longer training rather than generated data (As shown in Section B.3).  <ref type="bibr" target="#b11">[12]</ref> val-split: We evaluate the benefit from warp-refine propagation across different segmentation models. Due to the lack of semantic complexity in the dataset (only 19 classes), and the high performance (mIoU = 83. <ref type="bibr" target="#b34">35</ref>) of the semantic labelling network, we find the semantic-only labels to give significant benefits as well. (Note that for motion-only we utilize only time-frames ? <ref type="bibr" target="#b1">[2]</ref> as recommended by the authors Zhu et al. <ref type="bibr" target="#b58">[59]</ref>). We report the average of three independent runs with different random seeds. Note that we do not use any additional data (coarse labels and Mapillary Vistas <ref type="bibr" target="#b28">[29]</ref> pretraining) for this ablative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Backbone</head><p>Baseline motion-only semantic-only warp-refine 3 ) and relaxed label loss (RLL), on the Cityscapes validation split. These experiments are conducted under different training settings (as shwon by the top two rows) . We also compare the mean IoU to those reported in previous work <ref type="bibr" target="#b58">[59]</ref>. We conduct three runs with different random seeds. To avoid such mis-attribution of the reason for improvement, we ensure that the training regime for all compared experiments is equivalent. To achieve that, we define an epoch to have a fixed size (roughly 3? the size of the normal dataset). With this definition, we train for 175 epochs.</p><p>? We adjust our data sampling such that in each epoch, 30% samples are drawn from the manually annotated dataset, and 70% data is drawn from the generated dataset (through label propagation). Hence, the number of pseudo-labels considered per epoch remains consistent independent of the amount of generated labels (In the presence of Coarse labelled data, we reduce sampling from the generated dataset to 30%).</p><p>For models evaluated on the test set, we use the same training validation split used by Zhu et al. <ref type="bibr" target="#b58">[59]</ref> (cv2 split). The cities M?nchengladbach, Strasbourg and Stuttgart are used as validation set while all the other cities are used as training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. ApolloScape partitioning</head><p>The ApolloScape dataset <ref type="bibr" target="#b42">[43]</ref> contains pixel-level annotations for sequentially recorded images, divided as 40960 training and 8327 validation images. These images are further broken into the subsets based on the road on which they were recorded, and the Record-ID. Each Record-ID consists of variable length sequentially annotated frames. We break these sequentially annotated frames into partitions each consisting of 21 consecutive frame. The images which are not a part of any such 21-frame partition (for example when a Record-ID contains less than 21 frames) are discarded. Now, from each partition, we utilize the central frame as a training data point (i.e. with manual annotation) and all the other frames are treated as frames where labels have to be generated via propagation. This allows us to create a dataset with ground-truth labels containing 2005 frames, and additional 40100 sequential images (we only use the provided ground truth for these images for evaluation purposes).</p><p>Note that to ensure that training and validation data do not have any overlap (which could happen if any partition of 21 frames contains validation samples), we combine the training and validation subset, and re-divide it at a Record-ID level (randomly). This ensures that none of our trainsequences have any overlap with the validation data. Due to this our training and validation split are different from the one provided with the dataset. To encourage and facilitate comparisons with our work, we will release our training and validation splits to the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Denoising module</head><p>Our denoising module ? ? is inspired from semantic-toreal models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b29">30]</ref>. We show our architecture in <ref type="figure">Figure B</ref>. Our network takes the warp-inpainted labels L w t , along with <ref type="figure">Figure A:</ref> We show examples of cyclic warped labels, generated to train the denoising network ? ? . The network is trained to map the samples (t ? t + p ? t) to the ground truth label (t). Using longer cycle of propagation (higher p) allows us to expose the network ? ? to larger amount of warping noise. <ref type="figure">Figure B</ref>: Architecture of the denoiser: The encoder and decoder are based on pix2pix <ref type="bibr" target="#b44">[45]</ref>. g ? is the baseline model trained only with manually annotated labels. The input to the encoder are concatenated along the channels dimension. Similarly, the input of the decoder is the concatenated output of the encoder, and OCR-features <ref type="bibr" target="#b50">[51]</ref> from g ? .</p><p>auxiliary inputs: the warped image I m t , and the image at time t + 1 I t+1 to generate refined labels L R t+1 :</p><formula xml:id="formula_15">L R t+1 = ? ? (I t+1 , I w t , L w t ) (A)</formula><p>The warped labels l w t are used as one-hot vectors per pixel. All the inputs are concatenated along the "channel" dimension and provided to the encoder network N encoder . The generated encoding is then concatenated with OCRfeatures <ref type="bibr" target="#b50">[51]</ref> of the image I t+1 (extracted using the baseline model g ? trained with only manually annotated images). This is done to provide rich semantic cues for regions with new objects. Finally the concatenated encoding is passed through the decoder network N decoder to generate the refined labels L R t+1 . The complete pipeline is visualized in <ref type="figure">Figure B</ref>. Our network is trained with the same optimization setting as detailed in Section C.1. The RMI loss <ref type="bibr" target="#b56">[57]</ref> is used to compute the cycle-consistency loss L(L t , L R t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative results</head><p>In <ref type="figure">Figure A,</ref> we show examples of cyclic warped labels l ? (cf. Section 3.2 in the main paper) for different cycle lengths. As shown, by using different cycle lengths we are able to expose the denoising module ? ? to a larger variety of label noise created due to warp-inpaint propagation. <ref type="figure">Figure C</ref> compares the output of model trained with and without warprefine labels on KITTI <ref type="bibr" target="#b0">[1]</ref> test-split (and nearby images using scene-flow test-split). We observe that on training with warp-refine labels, improves the networks performance on confusing classes such as (i) bus-truck, (ii) truck-car, (iii) rider-pedestrian, and (iv) fence-wall. Finally, <ref type="figure">Figure D</ref> shows additional qualitative comparisons between our propagation method and established baselines: i) motion-only labels <ref type="bibr" target="#b58">[59]</ref>, and ii) semantic-only labels <ref type="bibr" target="#b36">[37]</ref>. (a)-(d) show cases where our approach surpasses the other methods significantly. We also highlight the errors we observe in our method: 1) Our labels are weak for fine edges, 2) Our labels still appear to show some warping noise (as shown in example (f)) and 3) Our labels can sometimes mislabel some classes (as shown in example (e)). Note that examples in <ref type="figure">Figure D</ref> are generated with DeepLabv3 (ResNeXt-50) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b47">48]</ref> architecture for g ? . <ref type="figure">Figure C</ref>: Qualitative comparison of model trained with and without warp-refine labels. We see that training with warp-refine labels increase performance for confusing classes: Baseline model mis-predicts (i) 'bus' as 'truck', (ii) 'truck' as 'car', (iii) 'pedestrian' as 'rider', and (iv) 'fence' as 'wall' and 'sidewalk'. al. <ref type="bibr" target="#b58">[59]</ref> cause drifting near the pedestrian pixels. (c) g ? mislabels thin objects like poles (left side) (d) g ? mislabels part of the building. (e) We note that when both semantic and motion cues fail, our method fails as well. (f) Our method outputs slightly warped labels when the consecutive frames do not contain any ego-motion (note the warping of the pole in the center).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Accuracy of propagated labels. We visually compare the proposed warp-refine propagation (top-center) with the motion-only model (bottom-left), the semantic-only model (bottom-right), and the ground-truth annotation (bottom-center).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of propagated labels for difficult classes in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative comparison: We compare our method warp-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative comparison: We show a failure case for our</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>We evaluate the performance of semantic-segmentation model trained with auto-labeled frames sampled in different timeintervals. Due to the accumulation of errors in motion-only labels, performance drops as we sample frames further away. In contrast, warp-refine labels are especially useful at larger time-steps (?10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 .</head><label>1</label><figDesc>Longer training of the baseline: When training with propagated labels, our dataset size for B + D m 3 is increased. This leads to more training iterations for B + D m 3 with respect to the baseline model (B is trained for only one-third the iterations of B + D m 3 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure</head><label></label><figDesc>D: (a) g ? mislabels the Rider's legs, and motion only (Zhu et al. [59]) shows heavy drifting. (b) g ? mislabels the truck, and Zhu et</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison against state-of-the-art methods on the Cityscapes [12] test-split. All the methods reported here are trained with extra data (i.e. pretraining on Mapillary dataset</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>).Yuan  et al. [51] 98.9 88.3 94.4 68.0 67.8 73.6 80.6 83.9 94.4 74.4 96.0 89.2 75.8 96.8 83.6 94.2 91.3 74.0 80.1 84.5</figDesc><table><row><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>T. light</cell><cell>T. sign</cell><cell>vege.</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>M. bike</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell>Zhu et al. [59]</cell><cell cols="20">98.8 87.8 94.2 64.1 65.0 72.4 79.0 82.8 94.2 74.0 96.1 88.2 75.4 96.5 78.8 94.0 91.6 73.8 79.0 83.5</cell></row><row><cell>Tao et al. [37]</cell><cell cols="20">99.0 89.2 94.9 71.6 69.1 75.8 82.0 85.2 94.5 75.0 96.3 90.0 79.4 96.9 79.8 94.0 85.8 77.4 81.4 85.1</cell></row><row><cell>Chen et al. [9]</cell><cell cols="20">98.8 88.3 94.6 65.3 69.6 75.2 80.9 84.4 94.3 74.4 96.2 90.0 79.7 96.7 83.0 95.6 93.4 78.4 79.6 85.2</cell></row><row><cell>Ours</cell><cell cols="20">98.9 88.6 94.9 71.5 68.7 75.5 82.0 85.2 94.5 74.1 96.3 89.9 79.4 96.9 80.2 94.4 92.5 75.7 81.2 85.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison against state-of-the-art methods on NYU-V2<ref type="bibr" target="#b27">[28]</ref> test-split: Our model trained with warp-refine labels attains a large jump of 1.8 in terms of mIoU.</figDesc><table><row><cell>Method</cell><cell cols="3">pixel-acc mean-acc mIoU</cell></row><row><cell>TRL-ResNet50 [52]</cell><cell>76.2</cell><cell>56.3</cell><cell>46.4</cell></row><row><cell>MTI Net [39]</cell><cell>75.3</cell><cell>62.9</cell><cell>49.0</cell></row><row><cell>PAD-Net [49]</cell><cell>75.2</cell><cell>62.3</cell><cell>50.2</cell></row><row><cell>PAP-Net [53]</cell><cell>76.2</cell><cell>62.5</cell><cell>50.4</cell></row><row><cell>Ours</cell><cell>77.6</cell><cell>66.0</cell><cell>52.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison against state-of-the-art methods on the KITTI [1] test-split: Our model trained with warp-refine labels improves over the prior work by a notable margin of 3.61 mIoU.</figDesc><table><row><cell></cell><cell>Class</cell><cell>Category</cell></row><row><cell>Method</cell><cell cols="2">mIoU iIoU mIoU iIoU</cell></row><row><cell>SegStereo [50]</cell><cell cols="2">59.10 28.00 81.31 60.26</cell></row><row><cell>AHiSS [26]</cell><cell cols="2">61.24 26.94 81.54 53.42</cell></row><row><cell>LDN2 [18]</cell><cell cols="2">63.51 28.31 85.34 59.07</cell></row><row><cell cols="3">MapillaryAI [5] 69.56 43.17 86.52 68.89</cell></row><row><cell>Zhu et al. [59]</cell><cell cols="2">72.83 48.68 88.99 75.26</cell></row><row><cell>Ours</cell><cell cols="2">76.44 50.92 89.63 73.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A :</head><label>A</label><figDesc>Training with different labelling policies on Cityscapes</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To avoid clutter in subscripts, we define k := t + k .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported by Woven Core, Inc.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Augmented reality meets computer vision: Efficient data generation for urban driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised video segmentation using tree structured graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2751" to="2764" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Label propagation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3265" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale labelled video data augmentation for semantic segmentation in driving scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Breen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving in IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>i, iii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Progressive feature alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Naive-student: Leveraging semisupervised learning in video sequences for urban scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><forename type="middle">Gontijo</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>iii, vii, viii</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<ptr target="Septem-ber2018.xiii,xiv" />
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi Lin Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>i, ii, v, vii, viii, xiii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic video cnns through representation warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>i, iii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Simple copy-paste is a strong data augmentation method for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2462" to="2470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ladder-style densenets for semantic segmentation of large natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krapac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K S</forename><surname>?egvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (IC-CVW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bidirectional learning for domain adaptation of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>i, iii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Constructing self-motivated pyramid curriculums for crossdomain semantic segmentation: A non-adversarial approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengmao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>i, iii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting future instance segmentation by forecasting convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ROI-10D: monocular lifting of 2d detection to 6d pose and metric shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2069" to="2078" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantics through time: Semi-supervised segmentation of aerial videos with iterative label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Licaret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Costea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Instance adaptive self-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training of convolutional networks on multiple heterogeneous datasets for street scene semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meletis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1045" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can ground truth label propagation from video help semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Siva Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<meeting><address><addrLine>i, ii, iii, v</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="804" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet Kohli Nathan</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>ii, v, vii, viii, xii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<meeting><address><addrLine>i, viii, xii, xiii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>v, xiii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient video semantic segmentation with labels propagation and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sdcnet: Video prediction using spatially-displaced convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>iii, iv, v</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="718" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised video interpolation using cycle consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<meeting><address><addrLine>i, iii</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">High-resolution representations for labeling pixels and regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>i, ii, iv, v, vi, vii, viii, xiii, xiv</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<meeting><address><addrLine>v, xii</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Mti-net: Multi-scale task interaction networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The apolloscape open dataset for autonomous driving and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qichuan</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<meeting><address><addrLine>i, ii, v, vi, vii, xii, xiii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lt-net: Label transfer by learning reversible voxel-wise correspondence for one-shot medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04252</idno>
		<title level="m">Self-training with noisy student improves imagenet classification</title>
		<meeting><address><addrLine>iii, vii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>xiii, xiv</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Padnet: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Segstereo: Exploiting semantic information for disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhidong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm</editor>
		<meeting><address><addrLine>Cham; iv, v, viii, xiii, xiv</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="173" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint task-recursive learning for semantic segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pattern-affinitive propagation across depth, surface normal and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>i, iii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7285" to="7298" />
		</imprint>
	</monogr>
	<note>iii</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Maskflownet: Asymmetric feature matching with learnable occlusion mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>xii</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Region mutual information loss for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11117" to="11127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CMU CALD tech report</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitsum</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>i, ii, iii, iv, v, vi, vii, viii, xii, xiii, xiv, xvi</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting><address><addrLine>i, iii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>i, iii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
