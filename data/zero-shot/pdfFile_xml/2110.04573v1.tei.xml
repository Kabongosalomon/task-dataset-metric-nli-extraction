<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Space-Time-Separable Graph Convolutional Network for Pose Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Sofianos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessio</forename><surname>Sampieri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Franco</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Galasso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Space-Time-Separable Graph Convolutional Network for Pose Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human pose forecasting is a complex structured-data sequence-modelling task, which has received increasing attention, also due to numerous potential applications. Research has mainly addressed the temporal dimension as time series and the interaction of human body joints with a kinematic tree or by a graph. This has decoupled the two aspects and leveraged progress from the relevant fields, but it has also limited the understanding of the complex structural joint spatio-temporal dynamics of the human pose. Here we propose a novel Space-Time-Separable Graph Convolutional Network (STS-GCN) for pose forecasting. For the first time, STS-GCN models the human pose dynamics only with a graph convolutional network (GCN), including the temporal evolution and the spatial joint interaction within a single-graph framework, which allows the cross-talk of motion and spatial correlations. Concurrently, STS-GCN is the first space-time-separable GCN: the space-time graph connectivity is factored into space and time affinity matrices, which bottlenecks the space-time cross-talk, while enabling full joint-joint and time-time correlations. Both affinity matrices are learnt end-to-end, which results in connections substantially deviating from the standard kinematic tree and the linear-time time series. In experimental evaluation on three complex, recent and large-scale benchmarks, Human3.6M <ref type="bibr" target="#b23">[24]</ref>, AMASS [34] and 3DPW [48], STS-GCN outperforms the state-of-the-art, surpassing the current best technique <ref type="bibr" target="#b35">[35]</ref> by over 32% in average at the most difficult long-term predictions, while only requiring 1.7% of its parameters. We explain the results qualitatively and illustrate the graph interactions by the factored joint-joint and timetime learnt graph connections.</p><p>Our source code is available at: https://github.com/FraLuca/STSGCN</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Forecasting future human poses is the task of modelling the complex structured-sequence of joint spatio-temporal ? indicates equal contribution dynamics of the human body. This has received increasing attention due to its manifold applications to autonomous driving <ref type="bibr" target="#b38">[38]</ref>, healthcare <ref type="bibr" target="#b45">[44]</ref>, teleoperations <ref type="bibr" target="#b39">[39]</ref> and collaborative robots <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">45]</ref>, where e.g. anticipating the human motion avoids crashes and helps the robots plan the future.</p><p>Research has so far addressed modelling space and time in separate frameworks. Time has generally been modelled with convolutions in the temporal dimension <ref type="bibr" target="#b9">[10]</ref>, with recurrent neural networks (RNN <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b10">11]</ref>, GRU <ref type="bibr" target="#b54">[53,</ref><ref type="bibr" target="#b0">1]</ref> and LSTM <ref type="bibr" target="#b53">[52]</ref>) or with Transformer Networks <ref type="bibr" target="#b8">[9]</ref>. Space and the interaction of joints has instead been recently modelled by Graph Convolutional Networks (GCN) <ref type="bibr" target="#b35">[35]</ref>, mostly connecting body joints along a kine-matic tree. The separate approach has side-stepped the complexity of a joint model across the spatial and temporal dimensions, which are diverse in nature, and has leveraged progress in the relevant fields. However this has also limited the understanding of the complex human body dynamics.</p><p>Here we propose to forecast human motion with a novel Space-Time-Separable Graph Convolutional Network (STS-GCN). STS-GCN encodes both the spatial joint-joint and the temporal time-time correlations with a joint spatiotemporal GCN <ref type="bibr" target="#b26">[27]</ref>. The single-graph framework favors the cross-talk of the body joint interactions and their temporal motion patterns. Further to better performance, using the GCN-only model results in considerably less parameters.</p><p>To the best of our knowledge, STS-GCN is the first space-time separable GCN. We realize this by factorizing the graph adjacency matrix A st into A s A t . Our intuition is that bottleneck'ing the cross-talk of the spatial joints and the temporal frames helps to improve the interplay of spatial joints and temporal patterns. This differs substantially from recent work <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b4">5]</ref> which separate the graph interactions from the channel convolutions, being therefore depthwise separable. Still both separable designs are advantageous for the reduction of model parameters. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the encoder-decoder design of our model. Following the body motion encoding by the STS-GCN, the future pose coordinates are forecast with few simple convolutional layers, generally termed Temporal Convolutional Network (TCN) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b33">33]</ref>, robust and fast to train.</p><p>Note from <ref type="figure" target="#fig_0">Fig. 1</ref> that the factored A s A t graph adjacency matrices are learnt. This results in better performance and it allows us to interpret the joint-joint and the time-time interactions, as we further illustrate in <ref type="figure" target="#fig_2">Fig. 3</ref> and in Sec. <ref type="bibr">4.</ref> In extensive experiments over the modern, challenging and large-scale datasets of Human3.6M <ref type="bibr" target="#b23">[24]</ref>, AMASS <ref type="bibr" target="#b34">[34]</ref> and 3DPW <ref type="bibr" target="#b49">[48]</ref>, we demonstrate that STS-GCN improves over the state-of-the-art. Notably, STS-GCN outperforms the current best technique <ref type="bibr" target="#b35">[35]</ref> by over 32% on all three datasets, in average at the most difficult long-term predictions, while only adopting 1.7% of its parameters.</p><p>We summarize our main contributions as follows:</p><p>? We propose the first space-time separable graph convolutional network, which is first to factorize the graph adjacency matrix, rather than depthwise <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b4">5]</ref>;</p><p>? Our space-time human body representation is the first to exclusively use a GCN and it adopts only 1.7% parameters of the current best competing technique <ref type="bibr" target="#b35">[35]</ref>;</p><p>? We improve on the state-of-the-art by over 32% on Hu-man3.6M <ref type="bibr" target="#b23">[24]</ref>, AMASS <ref type="bibr" target="#b34">[34]</ref> and 3DPW <ref type="bibr" target="#b49">[48]</ref>, in average at the most challenging long-term predictions;</p><p>? The joint-joint and time-time graph edge weights are learnt, which allows to explain their interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human pose forecasting is a long-standing problem <ref type="bibr" target="#b9">[10]</ref>. We discuss related work by distinguishing the temporal aspects of sequence modelling and the spatial representations. Finally we relate to separable convolutional networks. Temporal modelling Most recent work in human pose forecasting has leveraged Recurrent Neural Networks (RNN) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b50">49]</ref>, as well as recurrent variants such as Gated Recurrent Units (GRU) <ref type="bibr" target="#b54">[53,</ref><ref type="bibr" target="#b0">1]</ref> and Long Short-Term Memory Networks <ref type="bibr" target="#b53">[52]</ref>. These techniques are flexible, but they have issues with long-term predictions such as inefficient training and poor long-term memory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b35">35]</ref>. Research has attempted to tackle this, e.g. by training with generative adversarial networks <ref type="bibr" target="#b17">[18]</ref> and by imitation learning <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b50">49]</ref>. Emerging trends have adopted (self-)attention to model time <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b8">9]</ref>, which also applies to model spatial relations <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b8">9]</ref>. State-of-the-art performance is also attained with convolutional layers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10]</ref> in the temporal dimension, which is known as Temporal Convolutional Networks (TCN) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b33">33]</ref>. Here we adopt TCN for future frame prediction due to their performance and robustness, but we encode the space-time body dynamics only with GCN. Representation of body joints Nearly all literature adopts 3D coordinates or angles. <ref type="bibr" target="#b37">[37]</ref> has noted that encoding residuals of coordinates, thus velocity, may be beneficial. <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b35">35]</ref> has adopted Discrete cosine transform (DCT), thus frequency, which greatly supports for periodic motion. Here we experiment with 3D coordinates and angles, but those representations are compatible with our model. Representation of human pose Graphs are a natural choice to represent the body. These have mostly been handdesigned, mainly leveraging the natural structure of the kinematic tree <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b52">51]</ref>, and encoded via Graph Convolutional Networks (GCN) <ref type="bibr" target="#b26">[27]</ref>. <ref type="bibr" target="#b52">[51]</ref> learns the adjacency matrix of the graph, still limiting the connectivity to the kinematic tree. Most recently, research has explored all joints linked together and learnt graph edges <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b35">35]</ref>. Ours also let the training learn a data-driven graph connectivity and edge weights (see <ref type="figure" target="#fig_2">Fig. 3</ref> and Sec. 4 for an illustration). Separable Convolutions Separable convolutions <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22</ref>] decouple processing the cross-channel correlations via 1x1 convolutional filters and the spatial correlations via channel-wise spatial convolutions. These are depthwiseseparable convolutions, based on the hypothesis that the cross-channel and spatial correlations are sufficiently decoupled, so it is preferable not to map them jointly <ref type="bibr" target="#b12">[13]</ref>. To the best of our knowledge, only <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b4">[5]</ref> apply this concept to GCNs, but they design different graph edge weights for different channels, in the spatial <ref type="bibr" target="#b28">[29]</ref> or spectral domain <ref type="bibr" target="#b4">[5]</ref>. By contrast, our STS-GCN is the first GCN design which separates the graph connectivity itself, by factoring the space-time adjacency matrix. In the spirit of <ref type="bibr" target="#b12">[13]</ref>, our hypothesis is that the space-time cross-talk is limited and that decoupling them is more effective and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">STS-GCN</head><p>The proposed model proceeds by encoding the coordinates of the body joints which are observed in the given input frames and then it leverages the space-time representation to forecast the future joint coordinates. Encoding is modelled by the proposed STS-GCN graph, which considers the interaction of body joints over time, bottleneck'ing the space-time interplay. Decoding future coordinates is modelled with a TCN. In this section, we further provide insights into the STS-GCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formalization</head><p>We observe the body pose of a person, given by the 3D coordinates or angles of its V joints, for T frames. Then we predict the V body joints for the next K future frames.</p><p>We denote the joints by 3D vectors x v,k representing joint v at time k. The motion history of human poses is denoted by the tensor X in = [X 1 , X 2 ..., X T ] which we construct out of matrices of 3D coordinates or angles of joints</p><formula xml:id="formula_0">X i ? R 3?V for frames i = 1 . . . T . The goal is to predict the future K poses X out = [X T +1 , X T +2 ..., X T +K ].</formula><p>The motion history tensor is encoded into a graph which models the interaction of all body joints across all observed frames. We define the encoding graph G = (V, E), with T V nodes i ? V, which are all body joints across all observed time frames. Edges (i, j) ? E are represented by a spatiotemporal adjacency matrix A st ? R V T ?V T , relating the interactions of all joints at all times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Background on GCN</head><p>The spatio-temporal dependencies of joints across times may be conveniently encoded by a GCN, a graph-based neural network model f (X in ; A, W ). The input to a graph convolutional layer l is the tensor H (l) ? R C (l) ?V ?T , which encodes the observed V joints in the T frames. C (l) is the input dimensionality of the hidden representation H (l) . For the first layer, it is H (1) = X in and C (1) = 3.</p><p>A graph convolutional layer l outputs the H (l+1) ? R C (l+1) ?V ?T , given by the following</p><formula xml:id="formula_1">H (l+1) = ?(A st?(l) H (l) W (l) ) (1) where A st?(l) ? R V T ?V T is the spatio-temporal adjacency matrix of layer l, W (l) ? R C (l) ?C (l+1)</formula><p>are the trainable graph convolutional weights of layer l projecting each graph node from C (l) to C (l+1) dimensions, and ? is an activation function such as ReLU, PReLU or tanh.</p><p>Two notable graph representations are worth mentioning for their robustness and performance. <ref type="bibr" target="#b52">[51]</ref> constrains the graph encoding to the joint-joint relations, thus to a spatial-only A s , only along the kinematic tree, and addresses the time-time relations by a convolutional layer of kernel T ?T ?1?1, mapping T frames to T channels. <ref type="bibr" target="#b35">[35]</ref>, the current state-of-the-art in human pose forecasting, also adopts a spatial-only adjacency matrix A s , but fully connected. In both cases, the adjacency matrices are trainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Space-Time Separable GCN</head><p>The proposed STS-GCN takes motivation from the interaction of the temporal evolution and the spatial joints, as well as from the belief that the interplay of joint-joint and time-time are privileged. Human pose dynamics depend on 3 types of interactions: i. joint-joint; ii. time-time; and iii. joint-time. STS-GCN allows for all 3 types of interactions, but it bottlenecks the joint-time cross-talk.</p><p>The interplay of joints over time is modelled by relating the 3 types of relations within a single spatio-temporal encoding GCN. Bottleneck'ing the space-time cross-talk is realized by factoring the space-time adjacency matrix into the product of separate spatial and temporal adjacency matrices A st = A s A t . A separable space-time graph convolutional layer l is therefore written as follows</p><formula xml:id="formula_2">H (l+1) = ?(A s?(l) A t?(l) H (l) W (l) )<label>(2)</label></formula><p>where the same notation as in Eq. (1) applies, apart from the factored A s?(l) A t?(l) of layer l which we explain next. The adjacency matrix A s is responsible for the joint-joint interplay. It has dimensionality A s ? R V ?V , and it models the full joint-joint relations by trainable V ? V matrices for each instant in time (there are T such matrices). Similarly A t is responsible for the time-time relations. It has dimensionality A t ? R T ?T and it defines a full and trainable time-time T ? T relation matrix for each of the V joints.</p><p>Note that Eq. 2 represents a single GCN layer, encoding the spatio-temporal interplay of the the body dynamics. The factored space-time matrix bottlenecks the space-time cross-talk, it reduces the model parameters and it yields a considerable increase in the forecasting performance, as we illustrate in Sec. 4. Overall, the graph encoding employs four such GCN layers with residual connections PReLU activation functions, cf. 4 for the implementation details.</p><p>Also note that STS-GCN is the sole human pose forecasting graph encoding which exclusively uses GCNs. This contrasts other competing techniques, mostly encoding time with recurrent neural networks <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b54">53,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b53">52]</ref>, or by the use of convolutional layers with kernels across the temporal dimension <ref type="bibr" target="#b52">[51,</ref><ref type="bibr" target="#b9">10]</ref>. This is also a key element to parameter efficiency (see Sec. 4.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion on the STS-GCN</head><p>Here we first relate STS-GCN to self-attention mechanisms, then we comment on STS-GCN in relation to most recent work on signed and directed GCNs.  Separable graph convolutions and self-attention Most recent pose forecasting work has leveraged self-attention to encode the relation of frames <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b8">9]</ref> and/or the relation of joints <ref type="bibr" target="#b8">[9]</ref>. Here we relate the proposed STS-GCN to the self-attention mechanisms of <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b8">9]</ref>. Finally we relate these to Graph Attention Networks (GAT) <ref type="bibr" target="#b48">[47]</ref>. Let us first re-write part of the GCN layer of Eq. 1 with the Einstein summation, omitting the indication of layer l, the projection matrix W and the non-linearity ? for better clarity of notation:</p><formula xml:id="formula_3">A st H = vm A st wkvm H vmc<label>(3)</label></formula><p>having explicitly indicated with indexes the dimensions of A st ? R V T ?V T and H ? R C?V ?T , i.e. indexing spatial joints as v, w = 1, ..., V and times with m, k = 1, ..., T .</p><p>Let us now re-write the corresponding part of the STS-GCN layer of Eq. 2 with the Einstein summation, again omitting the projection matrix W and the non-linearity ? for clarity of notation:</p><formula xml:id="formula_4">A s (A t H) = v A s wkv ( m A t kvm H vmc ) = v A s wkv H t kvc</formula><p>(4) where, as above, we have indicated indexes for A s ? R V ?V (for each of the T times) and A t ? R T ?T (for each of the V joints) as v, w = 1, ..., V for the spatial joints and as m, k = 1, ..., T for the times.</p><p>Let us now turn to the current best technique for pose forecasting <ref type="bibr" target="#b35">[35]</ref>. They adopt a GCN for modelling the spatial interaction of joints at the same time, which coincides with the rightmost term in Eq. 4.</p><p>Their temporal modelling is however different from ours, as they adopt an attention formulation ?(QK)V . Writing it with the Einstein summation yields:</p><formula xml:id="formula_5">m c Q t kvc K t vcm V vmc = m A QK?t kvm V vmc (5)</formula><p>Comparing the right term of Eq. 5 with the separable temporal GCN (the term within parentheses in Eq. 4), we note that the approach of <ref type="bibr" target="#b35">[35]</ref>, modelling space and time with the different mechanisms of GCN and attention, may also be explained as a separable space-time GCN. The main difference is that A QK?t is a function of the product of inner representation vectors, both stemming from H. By contrast our temporal adjacency matrix A t learns the specific pair-wise interaction of relative time shifts. Similar arguments apply when comparing the proposed STS-GCN with the recent GAT <ref type="bibr" target="#b48">[47]</ref>. We evaluate the difference wrt <ref type="bibr" target="#b35">[35]</ref> quantitatively and conduct ablation studies on the adjacency matrices in Sec. 4. Signed and Directed GCNs Let us now consider that the adjacency matrix A st and its factored terms, A s and A t , are trainable parameters. Adjacency matrices were similarly trained by <ref type="bibr" target="#b35">[35]</ref>, which considers a fully connected matrix, and by <ref type="bibr" target="#b52">[51]</ref>, which defines specific learnable parameters (denoted M in <ref type="bibr" target="#b52">[51]</ref>) to multiply the manually-constructed graph (based on the kinematic tree and the sequential time connections). When encoding the spatio-temporal body dynamics, trainable parameters yield better performance and match the intuition, i.e. they learn the interaction between specific joints and at certain relative temporal offsets.</p><p>Trainable parameters result in signed and directed GCNs (see <ref type="figure" target="#fig_2">Figs. 3</ref> for an illustration). Both aspects have been surveyed recently <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b51">50]</ref>. In particular, recent work from <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b2">3]</ref> maintain that directed graphs encode richer information from their neighborhood, instead of being limited to distance ranges. Similarly, recent work from <ref type="bibr" target="#b13">[14]</ref> demonstrate the superior performance of signed GCNs.</p><p>Following the classification of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b51">50]</ref>, the proposed STS-GCN and the GCNs of <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b52">51]</ref> are spatial GCNs. This follows from their non-symmetric and possibly ill-posed signed Laplacian matrices, which do not have orthogonal eigendecompositions and are not easily interpretable by spectral-domain constructions <ref type="bibr" target="#b6">[7]</ref>. We maintain this makes an interesting direction for future investigation, only partly addressed by very recent work <ref type="bibr" target="#b47">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Decoding future coordinates</head><p>Given the encoded observed body dynamics, the estimation of the 3D coordinates or angles of the body joints in the future is delegated to convolutional layers applying to the temporal dimension. These map the observed frames into the future horizon and refine the estimates via a multilayered architecture.</p><p>Altogether, these layers make a decoder which is generally dubbed Temporal Convolutional Networks (TCN) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b33">33]</ref>. While several other sequence modelling options are available, including LSTM <ref type="bibr" target="#b19">[20]</ref>, GRU <ref type="bibr" target="#b11">[12]</ref> and Transformer Networks <ref type="bibr" target="#b16">[17]</ref>, here we adopt TCNs for their simplicity and robustness, further to satisfactory performance <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training</head><p>The proposed architecture is trained end-to-end supervisedly. Supervision is provided by either of the losses that measure error wrt ground truth in terms of Mean Per Joint Position Error (MPJPE) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">36]</ref> and Mean Angle Error (MAE) <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b50">49,</ref><ref type="bibr" target="#b35">35]</ref>. The loss based on MPJPE is:</p><formula xml:id="formula_6">L M P JP E = 1 V (T + K) T +K k=1 V v=1 ||x vk ? x vk || 2 (6)</formula><p>wherex vk ? R 3 denotes the predicted coordinates of the joint v in the frame k and x vk ? R 3 is the corresponding ground truth. The loss based on MAE is given by:</p><formula xml:id="formula_7">L M AE = 1 V (T + K) T +K k=1 V v=1 |x vk ? x vk |<label>(7)</label></formula><p>wherex vk ? R 3 denotes the predicted joint angles in exponential map representation of the joint v in the frame k and x vk ? R 3 is its ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental evaluation</head><p>We experimentally evaluate the proposed model against the state-of-the-art on three recent, large-scale and challenging benchmarks, Human3.6M <ref type="bibr" target="#b23">[24]</ref>, AMASS <ref type="bibr" target="#b34">[34]</ref> and 3DPW <ref type="bibr" target="#b49">[48]</ref>. Additionally we conduct ablation studies, evaluate the model qualitatively and illustrate what spatiotemporal graph G is trained from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and metrics</head><p>Human3.6M <ref type="bibr" target="#b23">[24]</ref> The dataset is wide-spread for human pose forecasting and large, consisting of 3.6 million 3D human poses and the corresponding images. It consists of 7 actors performing 15 different actions (e.g. Walking, Eating, Phoning). The actors are represented as skeletons of 32 joints. The orientation of joints are represented as exponential maps, from which the 3D coordinates may be computed <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b14">15]</ref>. For each pose, we consider 22 joints out of the provided 32 for estimating MPJPE and 16 for the MAE. Following the current literature <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37]</ref>, we use the subject 11 (S11) for validation, the subject 5 (S5) for testing, and all the rest of the subjects for training. AMASS <ref type="bibr" target="#b34">[34]</ref> The Archive of Motion Capture as Surface Shapes (AMASS) dataset has been recently proposed, to gather 18 existing mocap datasets. Following <ref type="bibr" target="#b35">[35]</ref>, we select 13 from those and take 8 for training, 4 for validation and 1 (BMLrub) as the test set. Then we use the SMPL <ref type="bibr" target="#b32">[32]</ref> parameterization to derive a representation of human pose based on a shape vector, which defines the human skeleton, and its joints rotation angles. We obtain human poses in 3D by applying forward kinematics. Overall, AMASS consists of 40 human-subjects that perform the action of walking. Each human pose is represented by 52 joints, including 22 body joints and 30 hand joints. Here we consider for forecasting the body joints only and discard from those 4 static ones, leading to an 18-joint human pose. As for <ref type="bibr" target="#b23">[24]</ref>, also these sequences are downsampled to 25 fps. 3DPW <ref type="bibr" target="#b49">[48]</ref> The 3D Pose in the Wild dataset <ref type="bibr" target="#b49">[48]</ref> consists of video sequences acquired by a moving phone camera. 3DPW includes indoor and outdoor actions. Overall, it contains 51,000 frames captured at 30Hz, divided into 60 video sequences. We use this dataset to test generalization of the models which we train AMASS. Metrics Following the benchmark protocols, we adopt the MPJPE and MAE error metrics (see Sec. 3.6). The first quantifies the error of the 3D coordinate predictions in mm. The second measures the angle error in degrees. We follow the protocol of <ref type="bibr" target="#b36">[36]</ref> and compute MAE with Euler angles. Due to this representation, MAE suffers from an inherent ambiguity, and MPJPE is more effective <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2]</ref>, so mostly adopted here. Implementation details The graph encoding is given by 4 layers of STS-GCN, which only differ in the number of channels C (l) : from 3 (the input 3D coordinates x,y,z or angles), to 64, then 32, 64 and finally 3 (cf. Sec. 3.3), by means of the projection matrices W (l) . At each layer we adopt batch normalization <ref type="bibr" target="#b22">[23]</ref> and residual connections. Our code is in Pytorch and uses ADAM <ref type="bibr" target="#b25">[26]</ref> as optimizer. The learning rate is set to 0.01 and decayed by a factor of 0.1 every 5 epochs after the 20-th. The batch size is 256. On Human3.6M, training for 30 epochs on an NVIDIA RTX 2060 GPU takes 20 minutes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to the state-of-the-art</head><p>We quantitatively evaluate our proposed model against the state-of-the-art both for short-term (&lt;500 msec) and long-term (&gt;500 msec) predictions.</p><p>We include into the comparison: ConvSeq2Seq <ref type="bibr" target="#b30">[30]</ref>, which adopts convolutional layers, separately encoding long-and short-term history; LTD-X-Y <ref type="bibr" target="#b36">[36]</ref>, which encodes the sequence frequency with a DCT, prior to a GCN (X and Y stand for the number of observed and predicted frames); BC-WGAIL-div <ref type="bibr" target="#b50">[49]</ref>, adopting reinforcement learning; and finally DCT-RNN-GCN <ref type="bibr" target="#b35">[35]</ref>, the current best performer, which extends LTD-X-Y with an RNN and motion-attention.</p><p>All algorithms take as input 10 frames (400 msec), with the exception of LTD, for which we also report the case of larger number of input frames. Then algorithms predict future poses for the next 2 to 10 frames (80-400 msec) in the case of short-term, and for 14-25 frames (560-1000 msec) in the long-term case. Human3.6M: 3D Joint Positions Let us consider <ref type="table" target="#tab_1">Tables  1 and 2</ref> for the tests on short-and long-term prediction respectively. Across all time horizons in both tables, our model outperforms all competing techniques, with the only exception of 3 experiments out of 120 (2-frame predictions for Walking, Eating and Directions), where it is within a marginal error.</p><p>Considering the average errors in <ref type="table" target="#tab_1">Table 1</ref>, the improvement of our model over the current best <ref type="bibr" target="#b35">[35]</ref> ranges from 3% in the case of 2 time frames, up to 34% improvement for the more challenging case of 10 frames. Note that, at the 10-frame horizon, improvements are less in the case of periodic actions such as Walking (17%) but larger for aperiodic actions such as Posing (40%). We believe this is because of the DCT encoding of <ref type="bibr" target="#b35">[35]</ref>.</p><p>We illustrate in <ref type="table" target="#tab_3">Table 2</ref> the more ardous long-term prediction horizons. Our predictions at 560 msec (14 frames) are more accurate than those of <ref type="bibr" target="#b35">[35]</ref> by 27 mm, while at 1 sec (25 frames) our model reaches an improvement of 37 mm. In average across predictions over 14-25 frames, our model outperforms the current best <ref type="bibr" target="#b35">[35]</ref>   Human3.6M: Joint Angles Average angle errors are reported in <ref type="table" target="#tab_5">Table 3</ref>. Our model outperforms the current best <ref type="bibr" target="#b35">[35]</ref> with larger improvements on the long-term horizon. The performance increase is 23% for 2 frames and it is 34% for 25 future frames. AMASS Also in the case of AMASS, in <ref type="table" target="#tab_6">Table 4</ref>, for short and long-term predictions of 3D coordinates, our model outperforms the state-of-the-art by 32% on the longest time     We also report here the number of parameters of all techniques, as well as of the current best algorithm <ref type="bibr" target="#b35">[35]</ref>. Our proposed Separable G s?t has only 1.7% of the parameters of <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>importance of spatio-temporal interaction within a single graph G st . Interestingly the errors are much larger for the short-range (nearly 3x larger) than for the long-range (+6% errors). We believe longer-term correlations may aid the variant.</p><p>Full (non-separable) graph G st The variant adopts a full space-time adjacency matrix A st . We observe a similar trend as for distinct graphs, i.e. worse performance with larger error increase (+18%) for short-term predictions but better for long-term ones (+9%). Notably the full graph model requires nearly 4x more parameters than our proposed one, cf. rightmost column in <ref type="table" target="#tab_8">Table 6</ref>. Separable graph G s?t shared across layers This only differs as it learns shared adjacency matrices across all layers, rather than layer-specific ones. Errors are comparable in the long-term (+2%) but larger in the short-term (+12%), against saving 37% of the parameters.</p><p>Learnt separable space and time adjacency matrices In <ref type="figure" target="#fig_2">Fig. 3</ref>, we illustrate two learnt adjacency matrices, upon training on Human3.6M. On the left, we represent a spatial adjacency matrix A s , i.e. imagine the red dots positioned on the 22 keypoints of a frontally posing Vitruvian man. Learnt parameters are directed (as the learnt matrix A s is not symmetric) and signed edges (cf. Sec. 3.3), color-coded weights as in the legend. For clarity of illustration, we represent the two strongest connections for each keypoint. Note how most learnt connections follow the kinematic tree, which confirms the importance of the physical linkage. However additional strong connections also emerge, which bridge distant but motion-related joints, such as the two feet, the feet to the head, and the shoulders to the opposite hips, which intuitively interact for future pose prediction. In <ref type="figure" target="#fig_2">Fig. 3 (right)</ref>, we represent a temporal adjacency matrix A t , also asymmetric and signed. It is noticeable the information flow from the earlier to the later observed frames. So the bottom-left side of the matrix shows larger absolute values. In particular most information is drawn to the last two frames (bottom two rows), corresponding to the 9th and 10th observed frames. Note also that the range of tempo-  ral relation coefficients [?2, 2] is smaller than the spatial [?8, 8], which privileges spatial information above the temporal when forecasting future poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have proposed a novel Space-Time-Separable Graph Convolutional Network (STS-GCN) for pose forecasting. The single-graph framework favors the cross-talk of space and time, while bottleneck'ing the space-time interaction allows to better learn the fully-trainable joint-joint and timetime interactions. The model improves considerably on the state-of-the-art performance and but only requires a fractions of the parameters. These results further support the adoption of GCN and future research on it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the proposed pipeline. Given a sequence of observed 3D poses, the novel STS-GCN encodes the spatio-temporal body dynamics. The encoded representation serves to predict future poses by means of a Temporal Convolutional Network (TCN). STS-GCN allows the spatial and temporal interaction of joints, cf. green-orange linkage on the Vitruvian man and dashed blue lines connecting joints in time, which are both learnt. But it bottlenecks their cross-talk by a new GCN design with factored spacetime adjacency matrices. (Vector image: please zoom in.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Sample long-term predictions (25 frames, 1 sec) for the actions of Walking, Discussion and Posing. (One every three frames are shown for the second two.) Purple/green limbs are the left/right sides of the body. Gray/black pictorials indicate the observed ground-truth (GT) skeletons. Predictions accurately match the GT. Mistakes may be observed on the left hand of the person in Discussion and in the aperiodic motion of Posing, an action performed in different ways in the training dataset. Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Sample spatial A s (left) and temporal A t (right) adjacency matrices, learnt on Human3.6M. (left) Red dots represent the 22 human joints; learnt joint-joint relations mainly follow the kinematic tree, but additionally bring up long-term connections (e.g. foot-foot, head-foot) which support forecasting. (right) The learnt A t shows information flow from the earlier to the later observed frames, i.e. larger absolute values in the bottom-left part of the matrix. (Zoom in for details.) See Sec. 4.3 for a discussion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ConvSeq2Seq [30] 17.7 33.5 56.3 63.6 11.0 22.4 40.7 48.4 11.6 22.8 41.3 48.9 17.1 34.5 64.8 77.6 LTD-10-10 [36] 11.1 21.4 37.3 42.9 7.0 14.8 29.8 37.3 7.5 15.5 30.7 37.5 10.8 24.0 52.7 65.8 DCT-RNN-GCN [35] 10.0 19.5 34.2 39.8 6.4 14.0 28.7 36.2 7.0 14.9 29.9 36.4 10.2 23.4 52.1 65.4 Ours 10.7 16.9 29.1 32.9 6.8 11.3 22.6 25.4 7.2 11.6 22.3 25.8 9.8 16.8 33.4 40.2 29.0 57.6 69.7 22.0 45.0 82.0 96.0 13.5 26.6 49.9 59.9 16.9 36.7 75.7 92.9 20.3 41.8 76.5 89.9 13.5 27.0 52.0 63.1 LTD-10-10 [36] 8.0 18.8 43.7 54.9 14.8 31.4 65.3 79.7 9.3 19.1 39.8 49.7 10.9 25.1 59.1 75.9 13.9 30.3 62.2 75.9 9.8 20.5 44.2 55.9 DCT-RNN-GCN [35] 7.4 18.5 44.5 56.5 13.7 30.1 63.8 78.1 8.6 18.3 39.0 49.2 10.2 24.2 58.5 75.8 13.0 29.2 60.4 73.9 9.3 20.1 44.3 56.0 Ours 7.4 13.5 29.2 34.7 12.4 21.8 42.1 49.2 8.2 13.7 26.9 30.9 9.9 18.0 38.2 45.6 11.9 21.3 42.0 48.7 9.1 15.1 29.9 35.0 GCN [35] 14.9 30.7 59.1 72.0 8.3 18.4 40.7 51.5 8.7 19.2 43.4 54.9 20.1 40.3 73.3 86.3 8.9 18.4 35.1 41.9 10.4 22.6 47.1 58.3 Ours 14.4 23.7 41.9 47.9 8.2 14.2 29.7 33.6 8.6 14.7 29.6 35.2 17.6 29.4 52.6 59.6 8.6 14.3 26.5 30.5 10.1 17.1 33.1 38.3</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Walking</cell><cell></cell><cell>Eating</cell><cell></cell><cell>Smoking</cell><cell></cell><cell></cell><cell>Discussion</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>msec</cell><cell>80</cell><cell>160 320 400</cell><cell>80</cell><cell>160 320 400</cell><cell>80</cell><cell cols="2">160 320 400</cell><cell>80</cell><cell>160 320 400</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Directions</cell><cell></cell><cell>Greeting</cell><cell></cell><cell>Phoning</cell><cell></cell><cell>Posing</cell><cell></cell><cell></cell><cell>Purchases</cell><cell></cell><cell>Sitting</cell></row><row><cell>msec</cell><cell>80</cell><cell>160 320 400</cell><cell>80</cell><cell>160 320 400</cell><cell>80</cell><cell>160 320 400</cell><cell>80</cell><cell cols="2">160 320 400</cell><cell>80</cell><cell>160 320 400</cell><cell>80</cell><cell>160 320 400</cell></row><row><cell>ConvSeq2Seq [30]</cell><cell cols="2">13.5 Sitting Down</cell><cell></cell><cell>Taking Photo</cell><cell></cell><cell>Waiting</cell><cell></cell><cell>Walking Dog</cell><cell></cell><cell></cell><cell>Walking Together</cell><cell></cell><cell>Average</cell></row><row><cell>msec</cell><cell>80</cell><cell>160 320 400</cell><cell>80</cell><cell>160 320 400</cell><cell>80</cell><cell>160 320 400</cell><cell>80</cell><cell>160 320</cell><cell>400</cell><cell>80</cell><cell>160 320 400</cell><cell>80</cell><cell>160 320 400</cell></row><row><cell>ConvSeq2Seq [30]</cell><cell cols="13">20.7 40.6 70.4 82.7 12.7 26.0 52.1 63.6 14.6 29.7 58.1 69.7 27.7 53.6 90.7 103.3 15.3 30.4 53.1 61.2 16.6 33.3 61.4 72.7</cell></row><row><cell>LTD-10-10 [36]</cell><cell cols="9">15.6 31.4 59.1 71.7 8.9 18.9 41.0 51.7 9.2 19.5 43.3 54.4 20.9 40.7 73.6 86.6</cell><cell cols="4">9.6 19.4 36.5 44.0 11.2 23.4 47.9 58.9</cell></row><row><cell>DCT-RNN-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>MPJPE error in mm for short-term prediction of 3D joint positions on Human3.6M. Our model outperforms the state-of-the-art by a large margin. The margin is smaller for very-short-term predictions on periodic actions, e.g. 2-4 frame (80-160 msec) for Walking and Eating. The margin is larger for the more challenging case of longer-term and aperiodic actions, e.g. up to 40% for Posing at 10-frame (400 msec). See Sec. 4.2 for the discussion.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>77.2 80.9 82.3 61.3 72.8 81.8 87.1 60.0 69.4 77.2 81.7 98.1 112.9 123.0 129.3 LTD-50-25 [36] 50.7 54.4 57.4 60.3 51.5 62.6 71.3 75.8 50.5 59.3 67.1 72.1 88.9 103.9 113.6 118.5 LTD-10-25 [36] 51.8 56.2 58.9 60.9 50.0 61.1 69.6 74.1 51.3 60.8 68.7 73.6 87.6 103.2 113.1 118.6 LTD-10-10 [36] 53.1 59.9 66.2 70.7 51.1 62.5 72.9 78.6 49.4 59.2 66.9 71.8 88.1 104.4 115.5 121.6 DCT-RNN-GCN [35] 47.4 52.1 55.5 58.1 50.0 61.4 70.6 75.5 47.5 56.6 64.4 69.5 86.6 102.2 113.2 119.8 Ours 40.6 45.0 48.0 51.8 33.9 40.2 46.2 52.4 33.6 39.6 45.4 50.0 53.4 63.6</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Walking</cell><cell></cell><cell cols="2">Eating</cell><cell></cell><cell></cell><cell cols="2">Smoking</cell><cell></cell><cell></cell><cell cols="2">Discussion</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>msec</cell><cell></cell><cell cols="12">560 720 880 1000 560 720 880 1000 560 720 880 1000 560</cell><cell>720</cell><cell>880</cell><cell>1000</cell></row><row><cell></cell><cell></cell><cell cols="3">ConvSeq2Seq [30]</cell><cell cols="14">72.2 72.3</cell><cell>78.8</cell></row><row><cell></cell><cell></cell><cell cols="2">Directions</cell><cell></cell><cell></cell><cell cols="2">Greeting</cell><cell></cell><cell cols="2">Phoning</cell><cell></cell><cell></cell><cell cols="2">Posing</cell><cell></cell><cell></cell><cell cols="2">Purchases</cell><cell></cell><cell>Sitting</cell></row><row><cell>msec</cell><cell cols="2">560 720</cell><cell>880</cell><cell>1000</cell><cell>560</cell><cell>720</cell><cell>880</cell><cell cols="2">1000 560 720</cell><cell>880</cell><cell>1000</cell><cell>560</cell><cell>720</cell><cell>880</cell><cell>1000</cell><cell>560</cell><cell>720</cell><cell>880</cell><cell cols="2">1000 560 720</cell><cell>880</cell><cell>1000</cell></row><row><cell>ConvSeq2Seq [30]</cell><cell cols="20">86.6 99.8 109.9 115.8 116.9 130.7 142.7 147.3 77.1 92.1 105.5 114.0 122.5 148.8 171.8 187.4 111.3 129.1 143.1 151.5 82.4 98.8 112.4 120.7</cell></row><row><cell>LTD-50-25 [36]</cell><cell cols="20">74.2 88.1 99.4 105.5 104.8 119.7 132.1 136.8 68.8 83.6 96.8 105.1 110.2 137.8 160.8 174.8 99.2 114.9 127.1 134.9 79.2 96.2 110.3 118.7</cell></row><row><cell>LTD-10-25 [36]</cell><cell cols="20">76.1 91.0 102.8 108.8 104.3 120.9 134.6 140.2 68.7 84.0 97.2 105.1 109.9 136.8 158.3 171.7 99.4 114.9 127.9 135.9 78.5 95.7 110.0 118.8</cell></row><row><cell>LTD-10-10 [36]</cell><cell cols="20">72.2 86.7 98.5 105.8 103.7 120.6 134.7 140.9 67.8 83.0 96.4 105.1 107.6 136.1 159.5 175.0 98.3 115.1 130.1 139.3 76.4 93.1 106.9 115.7</cell></row><row><cell cols="21">DCT-RNN-GCN [35] 73.9 88.2 100.1 106.5 101.9 118.4 132.7 138.8 67.4 82.9 96.5 105.0 107.6 136.8 161.4 178.2 95.6 110.9 125.0 134.2 76.4 93.1 107.0 115.9</cell></row><row><cell>Ours</cell><cell cols="3">47.6 56.5 64.5</cell><cell>71.0</cell><cell>64.8</cell><cell>76.3</cell><cell>85.5</cell><cell cols="3">91.6 41.8 51.1 59.3</cell><cell>66.1</cell><cell>64.3</cell><cell>79.3</cell><cell cols="3">94.5 106.4 63.7</cell><cell>74.9</cell><cell>86.2</cell><cell cols="2">93.5 47.7 57.0 67.4</cell><cell>75.2</cell></row><row><cell></cell><cell></cell><cell cols="2">Sitting Down</cell><cell></cell><cell></cell><cell cols="2">Taking Photo</cell><cell></cell><cell cols="2">Waiting</cell><cell></cell><cell></cell><cell cols="2">Walking Dog</cell><cell></cell><cell></cell><cell cols="3">Walking Together</cell><cell>Average</cell></row><row><cell>msec</cell><cell>560</cell><cell>720</cell><cell>880</cell><cell cols="2">1000 560</cell><cell>720</cell><cell>880</cell><cell>1000 560</cell><cell>720</cell><cell>880</cell><cell>1000</cell><cell>560</cell><cell>720</cell><cell>880</cell><cell cols="5">1000 560 720 880 1000 560</cell><cell>720</cell><cell>880</cell><cell>1000</cell></row><row><cell>ConvSeq2Seq [30]</cell><cell cols="20">106.5 125.1 139.8 150.3 84.4 102.4 117.7 128.1 87.3 100.3 110.7 117.7 122.4 133.8 151.1 162.4 72.0 77.7 82.9 87.4 90.7 104.7 116.7 124.2</cell></row><row><cell>LTD-50-25 [36]</cell><cell cols="20">100.2 118.2 133.1 143.8 75.3 93.5 108.4 118.8 77.2 90.6 101.1 108.3 107.8 120.3 136.3 146.4 56.0 60.3 63.1 65.7 79.6 93.6 105.2 112.4</cell></row><row><cell>LTD-10-25 [36]</cell><cell cols="9">99.5 118.5 133.6 144.1 76.8 95.3 110.3 120.2 75.1 88.7</cell><cell cols="11">99.5 106.9 105.8 118.7 132.8 142.2 58.0 63.6 67.0 69.6 79.5 94.0 105.6 112.7</cell></row><row><cell>LTD-10-10 [36]</cell><cell cols="9">96.2 115.2 130.8 142.2 72.5 90.9 105.9 116.3 73.4 88.2</cell><cell cols="11">99.8 107.5 109.7 122.8 139.0 150.1 55.7 61.3 66.4 69.8 78.3 93.3 106.0 114.0</cell></row><row><cell cols="21">DCT-RNN-GCN [35] 97.0 116.1 132.1 143.6 72.1 90.1 105.5 115.9 74.5 89.0 100.3 108.2 108.2 120.6 135.9 146.9 52.7 57.8 62.0 64.9 77.3 91.8 104.1 112.1</cell></row><row><cell>Ours</cell><cell>63.3</cell><cell>73.9</cell><cell>86.2</cell><cell cols="3">94.3 47.0 57.4</cell><cell>67.2</cell><cell cols="2">76.9 47.3 56.8</cell><cell>66.1</cell><cell>72.0</cell><cell>74.7</cell><cell>85.7</cell><cell cols="7">96.2 102.6 38.9 44.0 48.2 51.1 50.8 60.1</cell><cell>68.9</cell><cell>75.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>MPJPE error in mm for long-term prediction of 3D joint positions on Human3.6M. Our model outperforms the state-of-the-art by a large margin for each time prediction horizon and each action. Largest improvements wrt the current best [35] are obtained for the most challenging cases of longer-term predictions (22-25 frame, 880-1000 msec) of aperiodic actions such as Sitting (36%), Phoning (43%) and Posing (40%). The average improvement over the 14-25 frame (560-1000 msec) predictions is 34%. See Sec. 4.2 for the discussion.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>by 34%.</figDesc><table><row><cell></cell><cell></cell><cell>Average</cell><cell></cell></row><row><cell>msec</cell><cell>80</cell><cell cols="3">160 320 400 560 720 880 1000</cell></row><row><cell>LTD-10-25 [36]</cell><cell cols="4">0.34 0.57 0.93 1.06 1.27 1.44 1.57 1.66</cell></row><row><cell>LTD-10-10 [36]</cell><cell cols="4">0.32 0.55 0.91 1.04 1.26 1.44 1.59 1.68</cell></row><row><cell cols="3">BC-WGAIL-div [49] 0.31 0.57 0.90 1.02 1.23</cell><cell>-</cell><cell>-</cell><cell>1.65</cell></row><row><cell cols="5">DCT-RNN-GCN [35] 0.31 0.55 0.90 1.04 1.25 1.42 1.56 1.65</cell></row><row><cell>Ours</cell><cell cols="4">0.24 0.39 0.59 0.66 0.79 0.92 1.00 1.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Average MAE prediction errors over all actions of Human3.6M. Our model improves consistently over the state-of-the-art by a large margin.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Average MPJPE in mm over the BMLrub test se-Number of parametersTable 6(rightmost column) compares the number of parameters of our model Vs.<ref type="bibr" target="#b35">[35]</ref>. Ours uses a fraction of parameters, 57.5k Vs 3.4M , only 1.7%. Qualitative evaluation We provide sample predictions (purple/green) inFig. 2on Human3.6M against ground truth sequences (gray/black). All predictions are long-term (25 frames) but we only display one every three frames for Discussion and Posing, to fit the illustrations into a row. Results are in line with the long-term error statistics ofTable 2. The forecast Walking is accurate, within 5.2 cm-accuracy in average at 25 frames (1 sec) and pictorially matching the</figDesc><table><row><cell></cell><cell></cell><cell>3DPW</cell></row><row><cell>msec</cell><cell>80</cell><cell>160 240 400 560 720 880 1000</cell></row><row><cell>convSeq2Seq [30]</cell><cell cols="2">18.8 32.9 52.0 58.8 69.4 77.0 83.6 87.8</cell></row><row><cell>LTD-10-10 [36]</cell><cell cols="2">12.0 22.0 38.9 46.2 59.1 69.1 76.5 81.1</cell></row><row><cell>LTD-10-25 [36]</cell><cell cols="2">12.6 23.2 39.7 46.6 57.9 65.8 71.5 75.5</cell></row><row><cell cols="3">DCT-RNN-GCN [35] 12.6 23.1 39.0 45.4 56.0 63.6 69.7 73.7</cell></row><row><cell>Ours</cell><cell cols="2">8.6 12.8 21.0 24.5 30.4 35.7 39.6 42.3</cell></row><row><cell>quences of AMASS. Our model outperforms the current</cell><cell></cell><cell></cell></row><row><cell>best [35] by 32% for 25-frame (1000 msec) predictions.</cell><cell></cell><cell></cell></row><row><cell>horizon (25-frame, 1000 msec).</cell><cell></cell><cell></cell></row><row><cell>3DPW In Table 5, we test the generalizability of our model</cell><cell></cell><cell></cell></row><row><cell>by training on AMASS and testing on 3DPW. Results are</cell><cell></cell><cell></cell></row><row><cell>significantly beyond the state-of-the-art. For 2-frame pre-</cell><cell></cell><cell></cell></row><row><cell>dictions we reduce the error by 32%, compared to the sec-</cell><cell></cell><cell></cell></row><row><cell>ond best. For any other time horizon above 4 frames, we</cell><cell></cell><cell></cell></row><row><cell>reduce the error by at least 43%.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Average MPJPE in mm, testing the generalizability on 3DPW of models trained on AMASS. Our model scores significantly beyond the state-of-the-art, i.e. it outperforms<ref type="bibr" target="#b35">[35]</ref>, on 4-25 frames (160-1000 msec) by at least 43%. ground truth. This shows how our model learns periodic motion well. Predicted future poses are also relatively accurate for Discussion, where the average error is 7.9 cm (cf. competing algorithms are nearly 12 cm). In this case, our model predicts well the mostly static pose of the discussing person, but the error is larger on the waving left hand. Finally our model is producing larger errors on Posing (10.6 cm in average), as it is a more challenging aperiodic action, which different people perform in different ways.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>illustrates the following ablative variants of our proposed STS-GCN encoding technique: Distinct graphs G s and G t This stands for separate GCNs for space and time, with separate adjacency and projection matrices, intertwined by an activation function. The variant underperforms our proposed model, which confirms the RNN-GCN [35] 10.4 22.6 47.1 58.3 77.3 91.8 104.1 112.1 3.4M Distinct G s , G t 28.9 26.4 40.2 48.7 58.7 66.9 75.2 79.9 59.8k Full G st 11.9 19.4 34.1 40.8 53.1 65.6 75.1 82.5 222.9k Separable G s?t shared 11.3 19.4 34.7 40.5 52.5 62.1 69.2 76.9 36.4k Separable G s?t (proposed) 10.1 17.1 33.1 38.3 50.8 60.1 68.9 75.6 57.5k</figDesc><table><row><cell></cell><cell></cell><cell>Average</cell><cell></cell><cell></cell></row><row><cell>msec</cell><cell>80</cell><cell>160 240 400 560 720</cell><cell>880</cell><cell>1000 Parameters</cell></row><row><cell>DCT-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Average MPJPE error in mm on Human3.6M, comparing ablating variants of our model. See 4.3 for the detailed discussion.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors wish to acknowledge Panasonic for partially supporting this work and the project of the Italian Ministry of Education, Universities and Research (MIUR) "Dipartimenti di Eccellenza 2018-2022".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Socially and contextually aware human motion and pose forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vida</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RA-L)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ijaz</forename><surname>Akhter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno>abs/1803.01271</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral-designed depthwise separable graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML) -Workshop on Graph Representation Learning and Beyond (GRL+ 2020)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">HP-GAN: probabilistic 3d human motion prediction via GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emad</forename><surname>Barsoum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>B?tepage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kjellstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1591" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning progressive joint propagation for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term human motion prediction with scene context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV), 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action-agnostic human pose forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Signed graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1808.06354</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent network models for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformer networks for trajectory forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Giuliari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irtiza</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Galasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial geometry-aware human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Liang-Yan Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">M F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human motion prediction via spatio-temporal inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning motion manifolds with convolutional autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Joyce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structuralrnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Anticipating human activities for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2071" to="2071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning depthwise separable graph convolution from data manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">The International Conference on Learning Representations (ICLR) rej</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence model for human dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">History repeats itself: Human motion prediction via motion attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning trajectory dependencies for human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey of motion planning and control techniques for selfdriving urban vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>??p</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yershov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Intelligent Vehicles (T-IV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-autonomous robot teleoperation with obstacle avoidance via model predictive control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Rubagotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasbolat</forename><surname>Taunyazov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bukeikhan</forename><surname>Omarali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Almas</forename><surname>Shintemirov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RA-L)</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Longterm human motion prediction by modeling motion context and enhancing motion dynamic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Directed graph convolutional network. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Decomposing biological motion: A framework for analysis and synthesis of human gait patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Troje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="371" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human-aware robotic assistant for collaborative assembly: Integrating human motion prediction with planning in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Unhelkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Lasota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tyroller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Buhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marceau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters (RA-L)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2394" to="2401" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A primer on laplacian dynamics in directed graphs. arXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J P</forename><surname>Veerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Lyons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Imitation learning for human pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ego-pose estimation and forecasting as real-time pd control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dlow: Diversifying latent flows for diverse human motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV), 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
